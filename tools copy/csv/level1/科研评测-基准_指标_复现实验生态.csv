id,name,one_line_profile,detailed_description,domains,subtask_category,application_level,primary_language,repo_url,help_website,license,tags
1,SLM4Mol,Quantitative benchmark and analysis suite for molecular large language models,"A comprehensive benchmark and analysis framework designed for Molecular Large Language Models (MolLLMs). It provides quantitative evaluation metrics and datasets to assess the performance of LLMs on molecular tasks, facilitating the development of AI for molecular science.",AI4;AI4-01,model_evaluation;molecular_modeling,solver,Jupyter Notebook,https://github.com/AI-HPC-Research-Team/SLM4Mol,,MIT,molecular-llm;benchmark;chemistry;ai4science
2,AISBench Benchmark,Model evaluation tool extending OpenCompass for service-based models,"An AI model evaluation tool built upon the OpenCompass framework. It is compatible with OpenCompass's configuration system and dataset structure but extends capabilities to support service-based models, facilitating comprehensive benchmarking of AI systems.",AI4;AI4-01,model_evaluation;benchmarking,platform,Python,https://github.com/AISBench/benchmark,,Apache-2.0,benchmarking;model-evaluation;opencompass;ai-infrastructure
3,ThinkingAgent,Evaluation framework for rating overthinking behavior in LLMs,A systematic evaluation framework designed to automatically rate and analyze 'overthinking' behavior in Large Language Models. It serves as a tool for behavioral analysis and interpretability of LLM reasoning processes.,AI4;AI4-01,model_evaluation;interpretability,solver,Shell,https://github.com/AlexCuadron/ThinkingAgent,,MIT,llm-evaluation;reasoning;behavioral-analysis
4,DocGenome,Large-scale scientific document benchmark for multi-modal large models,"An open, large-scale benchmark specifically designed for training and testing Multi-modal Large Models (MLLMs) on scientific documents. It facilitates the evaluation of AI models' capabilities in understanding and processing complex scientific literature.",AI4;AI4-01,benchmarking;scientific_literature_mining,dataset,Jupyter Notebook,https://github.com/Alpha-Innovator/DocGenome,,CC-BY-4.0,scientific-documents;multimodal-llm;benchmark;dataset
5,FedHeteroBench,Benchmark framework for data heterogeneity in federated learning,A repository of frameworks and unified implementations for handling data heterogeneity in federated learning (Non-IID settings). It provides reproducible experiments and benchmarks to evaluate FL algorithms under heterogeneous data conditions.,AI4;AI4-01,benchmarking;federated_learning,library,Python,https://github.com/AntonioZC666/FedHeteroBench,,MIT,federated-learning;benchmark;non-iid;data-heterogeneity
6,EmotionCircuits-LLM,Framework for discovering and controlling emotion circuits in LLMs,"A complete and reproducible framework for analyzing, discovering, and controlling 'emotion circuits' within Large Language Models. It provides tools for interpretability and mechanistic analysis of LLM internal states.",AI4;AI4-01,interpretability;model_analysis,library,Python,https://github.com/Aurora-cx/EmotionCircuits-LLM,,GPL-3.0,llm-interpretability;mechanistic-interpretability;emotion-analysis
7,ai4sci-2021-denovo-benchmarks,Benchmarks for de novo molecular design from NeurIPS AI4Sci workshop,Code and benchmark definitions for the 'A Fresh Look at De Novo Molecular Design Benchmarks' paper presented at the NeurIPS 2021 AI for Science Workshop. It provides a standardized environment for evaluating molecular generation models.,AI4;AI4-01,benchmarking;molecular_design,solver,Python,https://github.com/AustinT/ai4sci-2021-denovo-benchmarks,,MIT,molecular-design;benchmark;neurips-workshop;drug-discovery
8,BigVectorBench,Benchmark suite for vector database embedding performance,A benchmark suite designed to evaluate the embedding performance of vector databases. It supports heterogeneous data and abstracts compound queries (multimodal or single-modal) to assess vector search capabilities critical for AI applications.,AI4;AI4-01,benchmarking;vector_search,solver,Python,https://github.com/BenchCouncil/BigVectorBench,,MIT,vector-database;benchmark;embedding;ai-infrastructure
9,ko-lm-evaluation-harness,Evaluation harness for Korean language models,A specialized fork of the lm-evaluation-harness adapted for Korean Language Models. It provides a framework for few-shot evaluation of autoregressive language models on Korean datasets.,AI4;AI4-01,model_evaluation;nlp_benchmarking,library,Python,https://github.com/Beomi/ko-lm-evaluation-harness,,MIT,llm-evaluation;korean-nlp;benchmark
10,MolGenBench,Benchmark for molecular generative models from de novo design to lead optimization,"A codebase and benchmark suite for evaluating the real-world applicability of molecular generative models. It covers tasks ranging from de novo design to lead optimization, providing metrics for chemical validity and optimization success.",AI4;AI4-01,benchmarking;molecular_generation,solver,Python,https://github.com/CAODH/MolGenBench,,MIT,molecular-generation;drug-design;benchmark;lead-optimization
11,ctf4science,Benchmarking framework for modeling dynamic systems (ODEs/PDEs),A modular and extensible framework for benchmarking modeling methods on dynamic systems. It supports the evaluation of models for ordinary differential equations (ODEs) and partial differential equations (PDEs) using standardized datasets and metrics.,AI4;AI4-01,benchmarking;dynamic_systems_modeling,library,Jupyter Notebook,https://github.com/CTF-for-Science/ctf4science,,MIT,pde-solving;ode-solving;benchmark;scientific-modeling
12,Matcha (variationanalysis),Framework for training and evaluating deep learning models for genomic variation calling,"The Matcha framework, part of the variationanalysis repository, is designed to help train and evaluate deep learning models for calling genomic variations. It supports semi-simulation workflows for generating training data and assessing model performance.",AI4;AI4-01,model_evaluation;variant_calling,library,Java,https://github.com/CampagneLaboratory/variationanalysis,,NOASSERTION,genomics;variant-calling;deep-learning;bioinformatics
13,PoseX,Benchmark for molecular docking algorithms,A benchmark suite specifically designed for evaluating molecular docking methods. It provides datasets and metrics to assess the accuracy of pose prediction and binding affinity estimation in drug discovery contexts.,AI4;AI4-01,benchmarking;molecular_docking,dataset,Python,https://github.com/CataAI/PoseX,,MIT,molecular-docking;benchmark;drug-discovery;pose-prediction
14,jetson-orin-matmul-analysis,Scientific CUDA benchmarking framework for matrix multiplication on Jetson Orin,"A scientific benchmarking framework for evaluating CUDA matrix multiplication performance on Jetson Orin edge devices. It includes implementations for different power modes and matrix sizes, providing performance (GFLOPS) and power efficiency analysis.",AI4;AI4-01,benchmarking;hpc_performance,solver,Python,https://github.com/Cre4T3Tiv3/jetson-orin-matmul-analysis,,MIT,cuda;benchmarking;edge-ai;matrix-multiplication
15,Benchmark-Gemma-Models,Customizable Python suite for evaluating Gemma and LLaMA-based LLMs,"A lightweight and customizable framework designed for benchmarking Large Language Models (LLMs) like Gemma and LLaMA. It supports custom scripts for models, datasets, tasks, and metrics, facilitating reproducible evaluation experiments.",AI4;AI4-01,model_evaluation;benchmarking,library,Python,https://github.com/D0men1c0/Benchmark-Gemma-Models,,Apache-2.0,llm;evaluation;gemma;llama
16,ABC (Annotated Beethoven Corpus),Dataset of harmonic analyses for Beethoven's string quartets,The Annotated Beethoven Corpus (ABC) provides expert harmonic analyses of Beethoven's string quartets. It serves as a benchmark dataset for symbolic music analysis and computational musicology tasks.,AI4;AI4-01,music_analysis;harmonic_analysis,dataset,None,https://github.com/DCMLab/ABC,,NOASSERTION,musicology;dataset;beethoven;harmonics
17,DeepDIVA,Framework for reproducible deep learning experiments in document image analysis,"A Python framework designed to enable quick and reproducible experiments in Deep Learning, specifically for Document Image Analysis (DIA). Although deprecated, it represents a tool for managing experimental workflows and hyperparameters.",AI4;AI4-01,experiment_reproduction;document_analysis,workflow,Python,https://github.com/DIVA-DIA/DeepDIVA,https://diva-dia.github.io/DeepDIVA/,LGPL-3.0,reproducibility;deep-learning;document-analysis
18,DMind-Benchmark,Evaluation framework for LLMs on blockchain and Web3 knowledge,"A comprehensive benchmark framework designed to evaluate Large Language Models (LLMs) on domain-specific knowledge related to blockchain, cryptocurrency, and Web3 technologies.",AI4;AI4-01,model_evaluation;domain_specific_benchmarking,library,Python,https://github.com/DMindAI/DMind-Benchmark,,None,llm;blockchain;benchmark;web3
19,DDRL4NAV,Reproducible reinforcement learning training framework based on OpenAI Five,"A lightweight reinforcement learning training framework that reproduces the architecture used in OpenAI Five. It separates Forward, Backward, and Env modules to facilitate RL research and reproduction of navigation tasks.",AI4;AI4-01,reinforcement_learning;reproduction,library,Python,https://github.com/DRL-Navigation/DDRL4NAV,,AGPL-3.0,reinforcement-learning;openai-five;navigation
20,DeepSpark,Evaluation platform for industrial AI algorithms and models,An open platform that provides a multi-dimensional evaluation system for open-source application algorithms and models. It focuses on industrial applications and supports mainstream frameworks.,AI4;AI4-01,model_evaluation;industrial_ai,platform,None,https://github.com/Deep-Spark/DeepSpark,,Apache-2.0,evaluation;industrial-ai;benchmark
21,SPDEBench,Benchmark for learning regular and singular stochastic PDEs,"SPDEBench is an extensive benchmark suite designed for evaluating machine learning models on regular and singular Stochastic Partial Differential Equations (SPDEs), facilitating research in scientific machine learning (SciML).",AI4;AI4-01,pde_solving;sciml_benchmarking,library,Python,https://github.com/DeepIntoStreams/SPDE_hackathon,,Apache-2.0,spde;pde;benchmark;sciml
22,Khmer OCR Benchmark Dataset,Standardized benchmark dataset for Khmer Optical Character Recognition,"A standardized dataset designed to benchmark Optical Character Recognition (OCR) engines specifically for the Khmer language, facilitating evaluation and comparison of OCR models.",AI4;AI4-01,ocr_evaluation;dataset,dataset,Python,https://github.com/EKYCSolutions/khmer-ocr-benchmark-dataset,,MIT,ocr;khmer;benchmark;dataset
23,EvoxBench,Benchmark suite for evolutionary neural architecture search,"A benchmark suite that transforms Neural Architecture Search (NAS) into multi-objective optimization problems, designed for testing evolutionary algorithms in deep learning contexts.",AI4;AI4-01,neural_architecture_search;evolutionary_algorithms,library,Python,https://github.com/EMI-Group/evoxbench,https://evoxbench.readthedocs.io/,GPL-3.0,nas;evolutionary-computation;benchmark
24,LM Evaluation Harness,Framework for few-shot evaluation of language models,A widely used framework for evaluating autoregressive language models (LLMs) on a large number of tasks. It provides a unified interface for few-shot evaluation and benchmarking.,AI4;AI4-01,model_evaluation;nlp_benchmarking,library,Python,https://github.com/EleutherAI/lm-evaluation-harness,,MIT,llm;evaluation;few-shot;benchmark
25,JamAIBase,Collaborative spreadsheet platform for LLM experimentation and evaluation,"A platform that combines a spreadsheet interface with LLM capabilities, allowing users to chain cells into pipelines, experiment with prompts, and evaluate LLM responses in real-time.",AI4;AI4-01,prompt_engineering;model_evaluation,platform,Python,https://github.com/EmbeddedLLM/JamAIBase,https://jamaibase.com/,Apache-2.0,llm;spreadsheet;no-code;evaluation
26,MoSQITo,Modular framework for sound quality metrics and psychoacoustics,A unified and modular development framework for calculating key sound quality metrics. It supports reproducible science in acoustics and psychoacoustics by providing standardized implementations of metrics.,AI4;AI4-01,signal_processing;psychoacoustics,library,Python,https://github.com/Eomys/MoSQITo,https://github.com/Eomys/MoSQITo,Apache-2.0,acoustics;sound-quality;metrics
27,WEHUB (Water Environmental Hub),Cloud-based platform for accessing and sharing environmental data,"An open-source web platform designed to facilitate the search, access, and sharing of water and environmental data. It supports international data standards and provides tools for data translation and cataloging.",AI4;AI4-01,data_management;environmental_science,platform,Ruby,https://github.com/ExplorusDataSolutions/WaterEnvironmentalHub-WEHUB,,None,environmental-data;water;data-sharing
28,openFDA,API and data access service for FDA public datasets,"A project providing open APIs, raw data downloads, and documentation for a large collection of FDA public datasets, enabling researchers and developers to access health and drug-related data programmatically.",AI4;AI4-01,data_access;public_health,service,Python,https://github.com/FDA/openfda,https://open.fda.gov/,CC0-1.0,fda;health-data;api
29,FluxBench.jl,Benchmark suite for the FluxML ecosystem and scientific machine learning,"A benchmarking tool for the FluxML ecosystem, covering deep learning, scientific machine learning (SciML), differentiable programming, and CUDA-accelerated workloads in Julia.",AI4;AI4-01,benchmarking;sciml,library,Julia,https://github.com/FluxML/FluxBench.jl,,None,julia;fluxml;benchmark;sciml
30,LLM Zoo,"Repository of data, models, and benchmarks for LLMs","A project providing a collection of instruction-tuning data, models, and evaluation benchmarks for Large Language Models, facilitating the development and assessment of LLMs.",AI4;AI4-01,model_evaluation;dataset,dataset,Python,https://github.com/FreedomIntelligence/LLMZoo,,Apache-2.0,llm;benchmark;instruction-tuning
31,LAB-Bench,Benchmark dataset for AI capabilities in biological research,"An evaluation dataset designed to benchmark AI systems on capabilities foundational to scientific research in biology, such as literature retrieval, protocol planning, and data interpretation.",AI4;AI4-01,biology_evaluation;scientific_reasoning,dataset,Python,https://github.com/Future-House/LAB-Bench,,CC-BY-SA-4.0,biology;benchmark;ai4science
32,MAYE,Framework for evaluating RL scaling in Vision Language Models,"A framework and comprehensive evaluation scheme for Reinforcement Learning (RL) scaling in Vision Language Models (VLMs), providing tools for transparent and from-scratch training and assessment.",AI4;AI4-01,rl_evaluation;vlm_benchmarking,library,Python,https://github.com/GAIR-NLP/MAYE,,None,vlm;reinforcement-learning;evaluation
33,Molecules Dataset Collection,Collection of molecular datasets for property inference validation,"A curated collection of datasets containing molecular structures and properties, intended for the validation and benchmarking of molecular property inference models.",AI4;AI4-01,molecular_property_prediction;dataset,dataset,None,https://github.com/GLambard/Molecules_Dataset_Collection,,MIT,molecules;dataset;chemistry
34,GPT-Fathom,Reproducible evaluation suite for LLMs on curated benchmarks,An open-source LLM evaluation suite that benchmarks leading open-source and closed-source models on over 20 curated benchmarks under aligned settings to ensure reproducibility.,AI4;AI4-01,model_evaluation;benchmarking,library,Python,https://github.com/GPT-Fathom/GPT-Fathom,,MIT,llm;evaluation;reproducibility
35,MolCap-Arena,Benchmark for language-enhanced molecular property prediction,"A comprehensive captioning benchmark designed for evaluating language-enhanced molecular property prediction models, facilitating research at the intersection of chemistry and NLP.",AI4;AI4-01,molecular_captioning;property_prediction,dataset,Python,https://github.com/Genentech/molcap-arena,,NOASSERTION,molecular-captioning;benchmark;chemistry
36,MultimodalHugs,Framework for training and evaluating multimodal AI models,"An extension of Hugging Face Transformers that provides a generalized framework for training, evaluating, and using multimodal AI models with unified code interfaces.",AI4;AI4-01,multimodal_training;evaluation,library,Python,https://github.com/GerrySant/multimodalhugs,,MIT,multimodal;huggingface;framework
37,Giskard,Evaluation and testing library for LLM agents and AI models,"An open-source library for testing and evaluating AI models and LLM agents. It helps detect vulnerabilities, hallucinations, and performance issues through automated tests.",AI4;AI4-01,model_testing;quality_assurance,library,Python,https://github.com/Giskard-AI/giskard-oss,https://docs.giskard.ai/,Apache-2.0,testing;llm;evaluation;qa
38,GeSS,Benchmark for Geometric Deep Learning under distribution shifts,A benchmarking suite for Geometric Deep Learning (GDL) focusing on scientific applications and robustness against distribution shifts in geometric data.,AI4;AI4-01,geometric_deep_learning;benchmarking,library,Python,https://github.com/Graph-COM/GESS,,MIT,gdl;benchmark;scientific-applications
39,SciKnowEval,Benchmark for evaluating scientific knowledge of LLMs,"A benchmark designed to evaluate the multi-level scientific knowledge capabilities of Large Language Models, assessing their proficiency in various scientific domains.",AI4;AI4-01,scientific_knowledge_evaluation;benchmarking,library,Python,https://github.com/HICAI-ZJU/SciKnowEval,,None,llm;science;evaluation
40,NewtonBench,Benchmark for scientific law discovery in LLM agents,"A benchmark suite for evaluating the ability of LLM-based agents to discover generalizable scientific laws, testing their reasoning and discovery capabilities.",AI4;AI4-01,scientific_discovery;benchmarking,library,Python,https://github.com/HKUST-KnowComp/NewtonBench,,MIT,scientific-discovery;llm-agent;benchmark
41,MD-Bench,Prototyping harness for Molecular Dynamics algorithms,"A performance-oriented benchmarking harness for developing and testing state-of-the-art Molecular Dynamics (MD) algorithms, focusing on computational efficiency.",AI4;AI4-01,molecular_dynamics;benchmarking,library,C,https://github.com/HPC-Dwarfs/MD-Bench,,LGPL-3.0,molecular-dynamics;hpc;benchmark
42,BubbleML,Dataset and benchmarks for multiphase multiphysics SciML,"A dataset and benchmark suite for scientific machine learning (SciML) focused on multiphase multiphysics simulations, providing data for training and evaluating physics-informed models.",AI4;AI4-01,multiphysics_simulation;sciml_benchmarking,dataset,Python,https://github.com/HPCForge/BubbleML,,None,sciml;multiphysics;dataset
43,GMNS Plus Dataset,Standardized transportation network dataset collection,"A standardized collection of transportation network datasets based on the General Modeling Network Specification (GMNS), supporting reproducible research in transportation modeling and planning.",AI4;AI4-01,transportation_modeling;dataset,dataset,Python,https://github.com/HanZhengIntelliTransport/GMNS_Plus_Dataset,,Apache-2.0,transportation;gmns;dataset
44,Helicone,Observability and evaluation platform for LLM applications,"An open-source platform for monitoring, evaluating, and experimenting with Large Language Models (LLMs). It provides tools for logging, caching, and analyzing model performance.",AI4;AI4-01,llm_observability;model_evaluation,platform,TypeScript,https://github.com/Helicone/helicone,https://docs.helicone.ai/,Apache-2.0,observability;llm;evaluation
45,DL-based-MI-EEG-models,Deep learning models and leaderboard for Motor Imagery EEG analysis,A repository collecting source code of representative deep learning-based Motor Imagery EEG (MI-EEG) models and running a leaderboard for fair comparison.,AI4;AI4-01,eeg_analysis;model_benchmarking,library,Python,https://github.com/Henrywang621/DL-based-MI-EEG-models,,MIT,eeg;brain-computer-interface;deep-learning
46,SDE-Harness,Framework for evaluating scientific discovery capabilities of agents,SDE-Harness (Scientific Discovery Evaluation Framework) is designed to evaluate AI agents on scientific discovery tasks.,AI4;AI4-01,scientific_discovery_evaluation;agent_benchmarking,workflow,Python,https://github.com/HowieHwong/sde-harness,,MIT,scientific-discovery;evaluation-framework;agents
47,Multimodal 3D Image Segmentation,Frameworks for multimodal 3D medical image segmentation,"Source code for multimodal image segmentation frameworks, providing network architectures and training procedures for 3D medical imaging analysis.",AI4;AI4-01,medical_image_segmentation;3d_imaging,library,Python,https://github.com/IBM/multimodal-3d-image-segmentation,,Apache-2.0,medical-imaging;segmentation;multimodal
48,VLM4Bio,Benchmark dataset of scientific QA pairs for biological images,A benchmark dataset of scientific question-answer pairs used to evaluate pretrained Vision-Language Models (VLMs) for trait discovery from biological images.,AI4;AI4-01,biological_image_analysis;visual_question_answering,dataset,Python,https://github.com/Imageomics/VLM4Bio,,Apache-2.0,biology;vlm;benchmark
49,InternManip,Robot manipulation learning suite for policy training and evaluation,"An all-in-one robot manipulation learning suite for policy models training and evaluation on various datasets and benchmarks, relevant to embodied AI in science.",AI4;AI4-01,robotic_manipulation;policy_learning,library,Python,https://github.com/InternRobotics/InternManip,,MIT,robotics;manipulation;embodied-ai
50,SGI-Bench,Benchmark defining and evaluating Scientific General Intelligence,A benchmark suite designed to define and evaluate Scientific General Intelligence capabilities in AI models.,AI4;AI4-01,scientific_reasoning_evaluation;general_intelligence,dataset,Python,https://github.com/InternScience/SGI-Bench,,None,scientific-intelligence;benchmark;reasoning
51,Computer-Generated-Hologram,Simulation framework for digital holography and computer-generated holograms,"A computational framework for simulating the production process of computer holography, recording, and reproducing holograms using MATLAB and Python.",AI4;AI4-01,holography_simulation;optical_computing,library,Python,https://github.com/JackHCC/Computer-Generated-Hologram,,None,holography;optics;simulation
52,Science-T2I,Benchmark for addressing scientific illusions in text-to-image synthesis,"A benchmark designed to evaluate and address scientific illusions in image synthesis models, ensuring scientific accuracy in generated imagery.",AI4;AI4-01,scientific_image_synthesis;hallucination_evaluation,dataset,Python,https://github.com/Jialuo-Li/Science-T2I,,Apache-2.0,image-synthesis;scientific-accuracy;benchmark
53,FAERS Data Toolkit,Tools for processing FDA Adverse Event Reporting System datasets,"Script tools for downloading, data preprocessing, data merging, and standardizing the FDA Adverse Event Reporting System (FAERS) dataset for pharmacovigilance research.",AI4;AI4-01,pharmacovigilance;data_preprocessing,workflow,Python,https://github.com/Judenpech/FAERS-data-toolkit,,MIT,faers;pharmacovigilance;medical-data
54,LLM-MAP,Bimanual robot task planning using Large Language Models,"Implementation of LLM+MAP for bimanual robot task planning using LLMs and PDDL, relevant to lab automation and embodied AI.",AI4;AI4-01,robot_task_planning;embodied_ai,library,,https://github.com/Kchu/LLM-MAP,,MIT,robotics;task-planning;llm
55,MolPuzzle,Multimodal benchmark for molecular structure elucidation using LLMs,A multimodal benchmark designed to evaluate the capability of Large Language Models in solving molecule puzzles and elucidating molecular structures.,AI4;AI4-01,molecular_structure_prediction;chemistry_reasoning,dataset,Python,https://github.com/KehanGuo2/MolPuzzle,,None,chemistry;molecular-structure;benchmark
56,Guaranteed-Non-Local-Molecular-Dataset,Dataset for benchmarking non-local capabilities of geometric ML models,A dataset specifically created to benchmark the non-local capabilities of geometric machine learning models in molecular contexts.,AI4;AI4-01,molecular_modeling;geometric_deep_learning,dataset,,https://github.com/LarsSchaaf/Guaranteed-Non-Local-Molecular-Dataset,,None,molecular-dataset;geometric-ml;benchmark
57,Spyglass,Neuroscience data analysis framework for reproducible research,"A neuroscience data analysis framework built for reproducible research, facilitating the management and analysis of complex neurophysiological data.",AI4;AI4-01,neuroscience_data_analysis;reproducible_research,workflow,Jupyter Notebook,https://github.com/LorenFrankLab/spyglass,,MIT,neuroscience;data-analysis;framework
58,MolData,Molecular benchmark for disease and target-based machine learning,A benchmark suite for evaluating machine learning models on molecular datasets focused on diseases and targets.,AI4;AI4-01,drug_discovery;molecular_property_prediction,dataset,Python,https://github.com/LumosBio/MolData,,NOASSERTION,molecular-benchmark;drug-discovery;machine-learning
59,Proteomics Lab Agent,Multimodal agentic AI framework for automating proteomics laboratory work,"A multimodal, agentic AI framework that links written instructions to real-world laboratory work in proteomics, using video analysis for documentation and guidance.",AI4;AI4-01,lab_automation;proteomics,workflow,Python,https://github.com/MannLabs/proteomics_lab_agent,,Apache-2.0,proteomics;lab-automation;agentic-ai
60,MedMNIST,Collection of standardized datasets for biomedical image classification,"A large-scale MNIST-like collection of standardized biomedical images, including 2D and 3D datasets, designed for lightweight benchmarking of biomedical image analysis models.",AI4;AI4-01,biomedical_image_classification;benchmark_suite,dataset,Python,https://github.com/MedMNIST/MedMNIST,https://medmnist.com/,Apache-2.0,biomedical-imaging;classification;dataset
61,PaperArena,Benchmark for tool-augmented agentic reasoning on scientific literature,An evaluation benchmark designed to assess the performance of tool-augmented agents in reasoning over scientific literature.,AI4;AI4-01,scientific_literature_mining;agent_evaluation,dataset,Python,https://github.com/Melmaphother/PaperArena,,None,scientific-literature;agents;benchmark
62,MobiSurvStd,Standardization tool for French mobility survey datasets,"A Python tool designed to standardize various French mobility survey datasets (such as EMC², EGT, EMP) into a unified format to facilitate transportation research and social science analysis.",AI4;AI4-01,data_standardization;data_processing,library,Python,https://github.com/MobiSurvStd/MobiSurvStd,,MIT,mobility-surveys;data-standardization;transportation-science
63,Modalities,Distributed and reproducible foundation model training framework,"A PyTorch-native framework designed for the distributed and reproducible training of foundation models, emphasizing scientific rigor in model development experiments.",AI4;AI4-01,model_training;reproducibility,platform,Python,https://github.com/Modalities/modalities,,MIT,foundation-models;distributed-training;reproducibility
64,smiles-featurizers,Molecular SMILES embedding extractor using language models,"A tool to extract molecular embeddings from SMILES strings using various pre-trained language model architectures, facilitating downstream cheminformatics and drug discovery tasks.",AI4;AI4-01,feature_extraction;molecular_representation,library,Python,https://github.com/MoleculeTransformers/smiles-featurizers,,Apache-2.0,smiles;molecular-embeddings;cheminformatics
65,MolScore,Automated scoring function for de novo molecular design,"An automated scoring framework to facilitate and standardize the evaluation of goal-directed generative models for de novo molecular design, supporting various objective functions.",AI4;AI4-01,model_evaluation;molecular_design,library,Python,https://github.com/MorganCThomas/MolScore,,MIT,molecular-design;generative-models;scoring-function
66,BiomedSQL,Text-to-SQL benchmark for biomedical scientific reasoning,A benchmark dataset and task definition for evaluating Text-to-SQL capabilities specifically within the domain of biomedical scientific reasoning.,AI4;AI4-01,benchmarking;scientific_reasoning,dataset,Python,https://github.com/NIH-CARD/biomedsql,,NOASSERTION,text-to-sql;biomedical;benchmark
67,ComputeEval,Evaluation framework for CUDA code generation models,"A framework designed to generate and evaluate CUDA code from Large Language Models, targeting high-performance computing and scientific code generation tasks.",AI4;AI4-01,code_generation;model_evaluation,library,Python,https://github.com/NVIDIA/compute-eval,,NOASSERTION,cuda;llm-evaluation;hpc
68,Framework Reproducibility,Tools for ensuring reproducibility in deep learning frameworks,"A collection of tools and methodologies to ensure deterministic and reproducible results across deep learning frameworks, essential for scientific validity of AI experiments.",AI4;AI4-01,reproducibility;experiment_control,library,Python,https://github.com/NVIDIA/framework-reproducibility,,Apache-2.0,reproducibility;deep-learning;determinism
69,Rapidae,Framework for developing and comparing autoencoder models,"A back-end agnostic framework to explore, compare, and develop autoencoder models, facilitating research into representation learning and generative modeling.",AI4;AI4-01,model_development;autoencoders,library,Python,https://github.com/NahuelCostaCortez/rapidae,,Apache-2.0,autoencoder;model-comparison;deep-learning
70,CanarySEFI,Robustness evaluation framework for image recognition models,"A comprehensive framework for evaluating the robustness of deep learning-based image recognition models against adversarial attacks, including metrics for attack and defense effectiveness.",AI4;AI4-01,robustness_evaluation;adversarial_testing,library,Python,https://github.com/NeoSunJZ/Canary_Master,,Apache-2.0,robustness;adversarial-attacks;model-evaluation
71,Atropos,LLM reinforcement learning environment and evaluation framework,"A framework for collecting and evaluating Large Language Model trajectories through diverse environments, supporting reinforcement learning research and agent evaluation.",AI4;AI4-01,agent_evaluation;reinforcement_learning,library,Python,https://github.com/NousResearch/atropos,,MIT,llm;reinforcement-learning;evaluation
72,ScienceBoard,Benchmark for multimodal autonomous agents in scientific workflows,A benchmark suite and environment designed to evaluate the performance of multimodal autonomous agents in realistic scientific workflows and tasks.,AI4;AI4-01,agent_benchmarking;scientific_workflow,dataset,Python,https://github.com/OS-Copilot/ScienceBoard,,MIT,autonomous-agents;scientific-benchmark;multimodal
73,OneIG-Bench,Fine-grained evaluation benchmark for Text-to-Image models,"A comprehensive benchmark framework for evaluating Text-to-Image models across dimensions like subject-element alignment, reasoning, and scientific/technical accuracy in generation.",AI4;AI4-01,model_evaluation;text-to-image,library,Python,https://github.com/OneIG-Bench/OneIG-Benchmark,,None,text-to-image;benchmark;generative-ai
74,OlympiadBench,Benchmark for Olympiad-level scientific problems,A challenging benchmark dataset and evaluation framework featuring Olympiad-level bilingual multimodal scientific problems to promote AGI research in scientific reasoning.,AI4;AI4-01,scientific_reasoning;benchmarking,dataset,Python,https://github.com/OpenBMB/OlympiadBench,,MIT,scientific-reasoning;olympiad;multimodal
75,UltraEval,Open source framework for evaluating foundation models,"A comprehensive framework for evaluating foundation models across various capabilities, providing a standardized interface for model assessment.",AI4;AI4-01,model_evaluation;benchmarking,library,Python,https://github.com/OpenBMB/UltraEval,,Apache-2.0,foundation-models;evaluation-framework;llm
76,OpenBioLink,Evaluation framework for biomedical link prediction,"A resource and evaluation framework designed for assessing link prediction models on heterogeneous biomedical graph data, facilitating network biology research.",AI4;AI4-01,link_prediction;graph_evaluation,library,Python,https://github.com/OpenBioLink/OpenBioLink,,MIT,biomedical-graph;link-prediction;benchmark
77,BioSimSpace,Interoperable framework for biomolecular simulation,"A Python framework that provides an interoperable interface for various biomolecular simulation tools, facilitating the setup, execution, and analysis of molecular dynamics simulations.",AI4;AI4-01,molecular_simulation;workflow_management,workflow,Python,https://github.com/OpenBioSim/biosimspace,,GPL-3.0,molecular-dynamics;simulation;interoperability
78,MULTI-Benchmark,Multimodal understanding leaderboard with text and images,A benchmark suite for evaluating multimodal AI models on their understanding of complex text and image data.,AI4;AI4-01,multimodal_evaluation;benchmarking,dataset,Python,https://github.com/OpenDFM/MULTI-Benchmark,,MIT,multimodal;benchmark;leaderboard
79,SciEval,Multi-level LLM evaluation benchmark for scientific research,A benchmark designed to evaluate Large Language Models specifically on their capability to perform scientific research tasks across multiple levels of complexity.,AI4;AI4-01,scientific_evaluation;benchmarking,dataset,Python,https://github.com/OpenDFM/SciEval,,None,scientific-research;llm-evaluation;benchmark
80,OpenHands Benchmarks,Evaluation harness for OpenHands agents,"The evaluation harness and benchmark suite for OpenHands, enabling the assessment of autonomous coding and task-solving agents.",AI4;AI4-01,agent_evaluation;benchmarking,library,Python,https://github.com/OpenHands/benchmarks,,MIT,autonomous-agents;evaluation;harness
81,GAOKAO-Bench,LLM evaluation framework using GAOKAO questions,An evaluation framework that utilizes questions from the Chinese National College Entrance Examination (GAOKAO) to assess the knowledge and reasoning capabilities of Large Language Models.,AI4;AI4-01,knowledge_evaluation;benchmarking,library,Python,https://github.com/OpenLMLab/GAOKAO-Bench,,Apache-2.0,gaokao;llm-evaluation;reasoning
82,OG-Core,Overlapping generations model framework for fiscal policy evaluation,"A Python framework for modeling overlapping generations (OG) to evaluate the economic effects of fiscal policies, serving as a tool for economic simulation and analysis.",AI4;AI4-01,economic_modeling;policy_evaluation,library,Python,https://github.com/PSLmodels/OG-Core,,CC0-1.0,economics;simulation;fiscal-policy
83,GBBS,Graph Based Benchmark Suite,"A comprehensive benchmark suite for evaluating the performance of graph algorithms and systems, essential for research in high-performance graph processing.",AI4;AI4-01,graph_benchmarking;algorithm_evaluation,library,C++,https://github.com/ParAlg/gbbs,,MIT,graph-algorithms;benchmark;hpc
84,SkillMetricsToolbox,Matlab toolbox for model skill assessment,A collection of Matlab functions for calculating statistical metrics (like Taylor diagrams) to evaluate the skill of model predictions against observational data.,AI4;AI4-01,model_verification;statistical_analysis,library,MATLAB,https://github.com/PeterRochford/SkillMetricsToolbox,,NOASSERTION,model-evaluation;statistics;matlab
85,The Well,Physics simulation dataset collection and access tools,A large-scale collection of physics simulation datasets accompanied by tools/loaders to facilitate research in physics-informed machine learning and simulation surrogates.,AI4;AI4-01,physics_simulation;dataset_access,dataset,Jupyter Notebook,https://github.com/PolymathicAI/the_well,,BSD-3-Clause,physics;simulation;dataset
86,LIKWID,Performance monitoring and benchmarking suite for HPC,"A tool suite for performance oriented programmers to measure and benchmark hardware performance counters, critical for optimizing scientific computing applications.",AI4;AI4-01,performance_benchmarking;hpc_optimization,solver,C,https://github.com/RRZE-HPC/likwid,,GPL-3.0,hpc;performance-monitoring;benchmarking
87,LLMBox,Unified library for LLM training and evaluation,"A comprehensive library for implementing Large Language Models, featuring a unified pipeline for training and rigorous model evaluation.",AI4;AI4-01,model_training;model_evaluation,library,Python,https://github.com/RUCAIBox/LLMBox,,MIT,llm;training-pipeline;evaluation
88,STAVER,Algorithm for variation reduction in DIA MS data,A standardized dataset-based algorithm designed to efficiently reduce variation in large-scale Data-Independent Acquisition (DIA) Mass Spectrometry data.,AI4;AI4-01,data_processing;mass_spectrometry,solver,Jupyter Notebook,https://github.com/Ran485/STAVER,,MIT,proteomics;mass-spectrometry;data-normalization
89,RobustBench,Standardized adversarial robustness benchmark,"A standardized benchmark and library for evaluating the adversarial robustness of image classification models, providing a leaderboard and model zoo.",AI4;AI4-01,robustness_evaluation;benchmarking,library,Python,https://github.com/RobustBench/robustbench,,NOASSERTION,adversarial-robustness;benchmark;computer-vision
90,RouterArena,Evaluation framework for LLM routers,"An open framework for evaluating LLM routing strategies with standardized datasets and metrics, facilitating the optimization of compound AI systems.",AI4;AI4-01,model_routing;system_evaluation,library,Python,https://github.com/RouteWorks/RouterArena,,Apache-2.0,llm-routing;evaluation;benchmark
91,RAG Evaluation Harnesses,Evaluation suite for Retrieval-Augmented Generation,A comprehensive evaluation suite designed to assess the performance of Retrieval-Augmented Generation (RAG) systems across various metrics.,AI4;AI4-01,rag_evaluation;benchmarking,library,Python,https://github.com/RulinShao/RAG-evaluation-harnesses,,MIT,rag;evaluation;retrieval
92,ServerlessBench,Benchmark suite for serverless computing,"A benchmark suite designed to evaluate the performance and characteristics of serverless computing platforms, aiding systems research.",AI4;AI4-01,system_benchmarking;serverless,library,C++,https://github.com/SJTU-IPADS/ServerlessBench,,NOASSERTION,serverless;benchmark;systems-research
93,Guardians MT Eval,Machine Translation meta-evaluation metrics,"Implementation of metrics for the meta-evaluation of Machine Translation systems, as proposed in the ACL 2024 paper 'Guardians of the Machine Translation Meta-Evaluation'.",AI4;AI4-01,mt_evaluation;meta_evaluation,library,Python,https://github.com/SapienzaNLP/guardians-mt-eval,,NOASSERTION,machine-translation;evaluation-metrics;nlp
94,SceMQA,Benchmark for scientific multimodal question answering,A scientific college entrance level multimodal question answering benchmark designed to evaluate the reasoning capabilities of multimodal models in scientific domains.,AI4;AI4-01,question_answering;scientific_reasoning,dataset,Python,https://github.com/SceMQA/SceMQA,,Apache-2.0,multimodal;qa;scientific-benchmark
95,DiffEqDevTools.jl,Benchmarking and testing tools for differential equations,"A Julia library providing tools for benchmarking, testing, and developing solvers for differential equations and scientific machine learning (SciML).",AI4;AI4-01,benchmarking;differential_equations,library,Julia,https://github.com/SciML/DiffEqDevTools.jl,https://docs.sciml.ai/DiffEqDevTools/stable/,NOASSERTION,sciml;differential-equations;benchmarking
96,SciMLBenchmarks.jl,Scientific machine learning and differential equation solver benchmarks,"A comprehensive benchmark suite for scientific machine learning (SciML) and differential equation solvers, covering Julia, Python, MATLAB, and R implementations.",AI4;AI4-01,benchmarking;solver_evaluation,dataset,MATLAB,https://github.com/SciML/SciMLBenchmarks.jl,https://benchmarks.sciml.ai/,MIT,sciml;benchmarks;ode-solvers
97,AstroVisBench,Benchmark for astronomy scientific computing and visualization,The first benchmark designed to evaluate both scientific computing and visualization capabilities specifically within the astronomy domain.,AI4;AI4-01,visualization;scientific_computing,dataset,Python,https://github.com/SebaJoe/AstroVisBench,,None,astronomy;visualization;benchmark
98,nonbonded,Framework for optimizing and benchmarking molecular force fields,"A management system designed for optimizing and benchmarking molecular force fields against physical property data, facilitating computational chemistry research.",AI4;AI4-01,force_field_optimization;molecular_modeling,workflow,Python,https://github.com/SimonBoothroyd/nonbonded,,MIT,molecular-dynamics;force-fields;chemistry
99,CSVQA,Benchmark for scientific reasoning in VLMs,A multimodal benchmark specifically designed to evaluate the scientific reasoning capabilities of Vision-Language Models (VLMs).,AI4;AI4-01,scientific_reasoning;multimodal_evaluation,dataset,Python,https://github.com/SkyworkAI/CSVQA,,Apache-2.0,vlm;scientific-reasoning;benchmark
100,EmoLLM,LLM framework for mental health evaluation and therapy,"A comprehensive framework including datasets, evaluation, and deployment tools for Large Language Models focused on mental health and psychology applications.",AI4;AI4-01,mental_health_analysis;psychological_evaluation,platform,Python,https://github.com/SmartFlowAI/EmoLLM,,MIT,mental-health;llm;psychology
101,OmixBench,Evaluation framework for LLMs in multi-omics analysis,A systematic evaluation framework designed to assess the performance of Large Language Models in the context of multi-omics data analysis.,AI4;AI4-01,multi_omics;bioinformatics_evaluation,solver,R,https://github.com/SolvingLab/OmixBench,,GPL-3.0,multi-omics;llm-evaluation;bioinformatics
102,StreetView-NatureVisibility,Framework for greenness visibility modelling using street view data,"A scalable and reproducible framework for utilising Mapillary street view data to model nature visibility, supporting environmental and urban science research.",AI4;AI4-01,environmental_modelling;urban_science,workflow,Jupyter Notebook,https://github.com/Spatial-Data-Science-and-GEO-AI-Lab/StreetView-NatureVisibility,,None,geo-ai;environmental-science;street-view
103,SMDG-19,Standardized multi-channel dataset for glaucoma,"A collection and standardization of 19 public full-fundus glaucoma images and associated metadata, serving as a benchmark for medical imaging analysis.",AI4;AI4-01,medical_imaging;disease_diagnosis,dataset,,https://github.com/TheBeastCoding/standardized-multichannel-dataset-glaucoma,,None,medical-imaging;glaucoma;dataset
104,TSB-UAD,Benchmark suite for univariate time-series anomaly detection,"An end-to-end benchmark suite for univariate time-series anomaly detection, including datasets from scientific and engineering domains (e.g., NASA spacecraft, water treatment).",AI4;AI4-01,anomaly_detection;time_series_analysis,dataset,Jupyter Notebook,https://github.com/TheDatumOrg/TSB-UAD,,Apache-2.0,time-series;anomaly-detection;scientific-data
105,AgML-CY-Bench,Crop yield forecasting benchmark dataset,"A comprehensive dataset and benchmark for forecasting crop yields at the subnational level, harmonizing public yield statistics with relevant predictors for agricultural research.",AI4;AI4-01,crop_yield_forecasting;agricultural_modeling,dataset,Jupyter Notebook,https://github.com/WUR-AI/AgML-CY-Bench,,NOASSERTION,agriculture;crop-yield;benchmark
106,SciTab,Benchmark for reasoning on scientific tables,A challenging benchmark designed for compositional reasoning and claim verification specifically on scientific tables.,AI4;AI4-01,scientific_reasoning;table_verification,dataset,,https://github.com/XinyuanLu00/SciTab,,MIT,scientific-tables;reasoning;benchmark
107,EHRStruct,Benchmark for LLMs on structured electronic health records,A comprehensive benchmark framework for evaluating Large Language Models on tasks involving structured Electronic Health Records (EHR).,AI4;AI4-01,ehr_analysis;medical_informatics,solver,Python,https://github.com/YXNTU/EHRStruct,,NOASSERTION,ehr;medical-benchmark;llm
108,mimic3-benchmarks,Benchmark datasets from MIMIC-III clinical database,A Python suite to construct benchmark machine learning datasets from the MIMIC-III clinical database for healthcare research.,AI4;AI4-01,clinical_prediction;medical_dataset_construction,dataset,Python,https://github.com/YerevaNN/mimic3-benchmarks,,MIT,mimic-iii;clinical-data;benchmark
109,MUBen,Benchmark for uncertainty of molecular representation models,A benchmark suite for evaluating the uncertainty estimation capabilities of molecular representation models in cheminformatics.,AI4;AI4-01,molecular_representation;uncertainty_estimation,solver,Python,https://github.com/Yinghao-Li/MUBen,,Apache-2.0,molecular-modeling;uncertainty;benchmark
110,math-evaluation-harness,Toolkit for benchmarking LLMs on mathematical reasoning tasks,"A specialized evaluation framework designed to assess the performance of Large Language Models (LLMs) on mathematical reasoning problems, providing a standardized harness for metrics calculation and result comparison.",AI4;AI4-01,math_reasoning;model_evaluation,solver,Python,https://github.com/ZubinGou/math-evaluation-harness,,MIT,llm;benchmark;mathematics;reasoning
111,AidanBench,Benchmark suite for measuring specific behavioral metrics in LLMs,"A benchmarking tool aimed at measuring 'big_model_smell' and other behavioral characteristics in Large Language Models, contributing to the evaluation ecology of AI models.",AI4;AI4-01,model_evaluation;behavioral_analysis,solver,Python,https://github.com/aidanmclaughlin/AidanBench,,None,llm;benchmark;evaluation
112,PeerRead,Dataset and code for analyzing scientific peer reviews,"A dataset and accompanying code for the analysis of scientific peer reviews, enabling NLP research into scientific literature assessment and review processes.",AI4;AI4-01,scientific_literature_mining;nlp_dataset,dataset,Python,https://github.com/allenai/PeerRead,,None,nlp;peer-review;scientific-literature
113,tau2-bench-verified,Verified benchmark for evaluating AI agents on database tasks,"A corrected and verified version of the τ²-bench benchmark, providing reliable task definitions and evaluation criteria for assessing the performance of AI agents in database interaction scenarios.",AI4;AI4-01,agent_evaluation;database_interaction,solver,Python,https://github.com/amazon-agi/tau2-bench-verified,,MIT,benchmark;agents;database
114,SARosPerceptionKitti,ROS package for KITTI vision benchmark perception tasks,"A ROS-based framework for executing and evaluating perception tasks (detection, tracking) using the KITTI Vision Benchmark Suite, facilitating robotics and computer vision research.",AI4;AI4-01,object_detection;tracking;benchmark_implementation,solver,Python,https://github.com/appinho/SARosPerceptionKitti,,MIT,ros;kitti;computer-vision;robotics
115,AD-ML,Framework for reproducible classification of Alzheimer's disease,"A machine learning framework designed for the reproducible classification of Alzheimer's disease using neuroimaging data, serving as a benchmark for medical AI methods.",AI4;AI4-01,disease_classification;medical_imaging,workflow,Python,https://github.com/aramis-lab/AD-ML,,MIT,alzheimers;machine-learning;medical-imaging
116,clinicadl,Deep learning framework for neuroimaging data processing,"A framework for reproducible processing and analysis of neuroimaging data using deep learning, supporting tasks like classification, reconstruction, and segmentation in medical research.",AI4;AI4-01,medical_imaging;deep_learning_framework,library,Python,https://github.com/aramis-lab/clinicadl,https://clinicadl.readthedocs.io,MIT,neuroimaging;deep-learning;medical-research
117,BigCodeBench-X,Multilingual programming task benchmark for LLMs,"A benchmark suite for evaluating Large Language Models on programming tasks across multiple programming languages, assessing code generation and reasoning capabilities.",AI4;AI4-01,code_generation;model_evaluation,solver,Python,https://github.com/arjunguha/BigCodeBench-X,,Apache-2.0,llm;benchmark;code-generation
118,Tartarus,Benchmark platform for inverse molecular design,"A benchmarking platform designed for realistic and practical inverse molecular design, evaluating generative models in chemistry and materials science contexts.",AI4;AI4-01,molecular_design;generative_chemistry,platform,Python,https://github.com/aspuru-guzik-group/Tartarus,,None,molecular-design;chemistry;benchmark
119,File-Format-Testing,"Benchmark for scientific file formats (HDF5, netCDF4, Zarr)","A configurable benchmarking tool for comparing the performance of common scientific file formats (HDF5, netCDF4, Zarr) across various I/O operations.",AI4;AI4-01,io_benchmarking;scientific_data_management,solver,Python,https://github.com/asriniket/File-Format-Testing,,None,hdf5;netcdf;zarr;benchmark
120,adaptrapezoid_benchmark,Benchmark for adaptive trapezoid numeric integration algorithms,"A benchmarking tool to evaluate the performance of different programming languages for scientific computing tasks, specifically using the adaptive trapezoid numeric integration algorithm.",AI4;AI4-01,numeric_integration;performance_benchmarking,solver,Scala,https://github.com/astrojhgu/adaptrapezoid_benchmark,,None,numeric-integration;scientific-computing;benchmark
121,TerjamaBench,Evaluation code for the TerjamaBench dataset,"Code repository for running evaluations on the TerjamaBench dataset, facilitating benchmarking of NLP models.",AI4;AI4-01,model_evaluation;nlp_benchmark,solver,Python,https://github.com/atlasia-ma/TerjamaBench,,None,nlp;benchmark;evaluation
122,fm-leaderboarder,Tool for creating custom LLM evaluation leaderboards,"A utility to create custom leaderboards for evaluating Large Language Models (LLMs) based on specific data, tasks, and prompts, enabling tailored model selection for business or research use cases.",AI4;AI4-01,model_evaluation;leaderboard_generation,platform,Python,https://github.com/aws-samples/fm-leaderboarder,,Apache-2.0,llm;leaderboard;evaluation
123,RT-CUDA,Optimizing compiler for CUDA-based scientific computing,"A restructuring compiler and optimization tool designed to bridge high-level languages and CUDA for scientific applications, supporting linear algebra solvers and simulations (e.g., reservoir, molecular dynamics).",AI4;AI4-01,scientific_computing;hpc_optimization;linear_algebra,solver,C,https://github.com/ayazhassan/RT-CUDA-GUI-Development,,MIT,cuda;hpc;compiler;scientific-simulation
124,MolNet Geometric Lightning,Benchmarking framework for molecular property prediction using PyTorch Lightning and Torch Geometric,"A repository implementing MoleculeNet benchmarks specifically adapted for Graph Neural Networks using PyTorch Lightning and Torch Geometric, facilitating reproducible evaluation of molecular machine learning models.",AI4;AI4-01,molecular_property_prediction;graph_representation_learning,library,Jupyter Notebook,https://github.com/bayer-science-for-a-better-life/molnet-geometric-lightning,,MIT,molecular-graphs;drug-discovery;benchmark
125,BEIR,Heterogeneous benchmark for zero-shot information retrieval including scientific corpora,"A heterogeneous benchmark for information retrieval that includes diverse datasets, significantly covering scientific domains (BioASQ, TREC-COVID, NFCorpus, SciFact), enabling the evaluation of retrieval models for scientific literature mining.",AI4;AI4-01,information_retrieval;scientific_literature_mining,library,Python,https://github.com/beir-cellar/beir,https://github.com/beir-cellar/beir/wiki,Apache-2.0,information-retrieval;biomedical-ir;benchmark
126,CiteME,Benchmark for evaluating citation recommendation in scientific texts,"A benchmark designed to test the abilities of language models in identifying and retrieving papers that are cited in scientific texts, supporting the development of AI assistants for scientific writing.",AI4;AI4-01,citation_recommendation;scientific_literature_mining,dataset,Python,https://github.com/bethgelab/CiteME,,NOASSERTION,nlp-for-science;citation-analysis;benchmark
127,MDBenchmark,Tool for generating and analyzing molecular dynamics simulation benchmarks,"A command-line tool and Python library to quickly generate, start, and analyze benchmarks for molecular dynamics simulations (e.g., GROMACS, NAMD), helping researchers optimize simulation performance on HPC systems.",AI4;AI4-01,molecular_dynamics_simulation;performance_benchmarking,workflow,Python,https://github.com/bio-phys/MDBenchmark,https://mdbenchmark.readthedocs.io/,NOASSERTION,molecular-dynamics;hpc;benchmarking
128,Brain-Score Vision,Framework for evaluating vision models against brain and behavioral data,"A framework that evaluates artificial neural networks on their alignment with biological brain measurements (neural recordings) and behavioral data, serving as a key benchmark in computational neuroscience and vision science.",AI4;AI4-01,neural_alignment_evaluation;computational_neuroscience,library,Python,https://github.com/brain-score/vision,http://www.brain-score.org/,MIT,neuroscience;computer-vision;brain-alignment
129,ConvolutionalNeuralOperator,Implementation of Convolutional Neural Operators for PDE solving,"Official implementation of Convolutional Neural Operators (CNO), a deep learning framework for robust and accurate learning of Partial Differential Equations (PDEs), used in physics simulations and fluid dynamics.",AI4;AI4-01,pde_solving;fluid_dynamics_simulation,solver,Python,https://github.com/camlab-ethz/ConvolutionalNeuralOperator,,MIT,pde;neural-operators;scientific-machine-learning
130,gee_brazil_sv,Benchmark maps and code for secondary forest age in Brazil,"Code repository and dataset for generating benchmark maps of secondary forest age in Brazil using Google Earth Engine, supporting ecological research and carbon stock estimation.",AI4;AI4-01,forest_age_mapping;remote_sensing,dataset,JavaScript,https://github.com/celsohlsj/gee_brazil_sv,,GPL-3.0,ecology;remote-sensing;google-earth-engine
131,PDF Text Extraction Benchmark,Benchmark for evaluating PDF text extraction tools on scientific articles,"A benchmarking suite designed to evaluate the semantic capabilities of PDF extraction tools specifically on scientific articles, addressing the challenge of parsing complex scientific document layouts.",AI4;AI4-01,document_parsing;scientific_literature_mining,library,TeX,https://github.com/ckorzen/pdf-text-extraction-benchmark,,MIT,pdf-extraction;scientific-documents;benchmark
132,Quantum-PDE-Benchmark,Benchmark for near-term quantum algorithms solving PDEs,"A repository for benchmarking near-term quantum algorithms designed to solve Partial Differential Equations (PDEs), facilitating the evaluation of quantum computing applications in computational physics.",AI4;AI4-01,quantum_algorithm_benchmarking;pde_solving,library,Jupyter Notebook,https://github.com/comp-physics/Quantum-PDE-Benchmark,,Apache-2.0,quantum-computing;pde;computational-physics
133,DIEF_BTS,Building TimeSeries dataset with Brick schema standardization,"A dataset containing time-series data from buildings, standardized using the Brick schema, designed for benchmarking algorithms in building energy modeling and smart building applications.",AI4;AI4-01,building_energy_modeling;time_series_forecasting,dataset,Jupyter Notebook,https://github.com/cruiseresearchgroup/DIEF_BTS,,MIT,smart-buildings;energy-data;brick-schema
134,GBM_Benchmarking,Benchmarking Gradient Boosting for molecular property prediction,"Scripts and guidelines for reproducing benchmarks of Gradient Boosting models on molecular property prediction tasks, providing a baseline for chemoinformatics research.",AI4;AI4-01,molecular_property_prediction;model_benchmarking,workflow,Python,https://github.com/dahvida/GBM_Benchmarking,,MIT,chemoinformatics;gradient-boosting;drug-discovery
135,FixaTons,Datasets and metrics for evaluating human fixation scanpath similarity,"A collection of datasets and metrics specifically designed to calculate and evaluate the similarity of human eye-fixation scanpaths, supporting research in visual attention and saliency modeling.",AI4;AI4-01,metric_calculation;evaluation,library,Python,https://github.com/dariozanca/FixaTons,,None,scanpath;eye-tracking;metrics;saliency
136,zk-benchmark-r1cs,Tools for benchmarking Zero-Knowledge provers using R1CS,"A set of tools to serialize R1CS (Rank-1 Constraint Systems) and witness representations into standardized datasets, enabling the benchmarking of different Zero-Knowledge provers.",AI4;AI4-01,benchmarking;data_generation,tool,Rust,https://github.com/dcbuild3r/zk-benchmark-r1cs,,MIT,zero-knowledge;cryptography;benchmarking;r1cs
137,WEFE,Word Embeddings Fairness Evaluation Framework,"An open source framework for measuring and mitigating bias in word embedding models, providing standardized metrics for fairness evaluation in NLP.",AI4;AI4-01,fairness_evaluation;bias_measurement,framework,Python,https://github.com/dccuchile/wefe,https://wefe.readthedocs.io/,MIT,nlp;fairness;bias;word-embeddings
138,FunctionBench,Workload suite for benchmarking serverless cloud functions,"A suite of workloads designed to benchmark serverless cloud function services, evaluating performance across different providers and configurations for systems research.",AI4;AI4-01,benchmarking;performance_evaluation,benchmark_suite,Python,https://github.com/ddps-lab/serverless-faas-workbench,,Apache-2.0,serverless;cloud-computing;benchmark;faas
139,LLM-SRBench,Benchmark for Scientific Equation Discovery with LLMs,"A benchmark designed to evaluate Large Language Models on the task of scientific equation discovery, assessing their ability to recover symbolic relationships from data.",AI4;AI4-01,benchmarking;equation_discovery,benchmark_suite,Python,https://github.com/deep-symbolic-mathematics/llm-srbench,,None,symbolic-regression;llm;scientific-discovery;benchmark
140,SciAssess,Benchmark for LLMs in scientific literature analysis,"A comprehensive benchmark for evaluating Large Language Models' proficiency in scientific literature analysis, covering memorization, comprehension, and analysis across various scientific fields.",AI4;AI4-01,benchmarking;literature_analysis,benchmark_suite,Python,https://github.com/deepmodeling/SciAssess,,LGPL-3.0,llm;scientific-literature;benchmark;evaluation
141,DeathStarBench,Benchmark suite for cloud microservices,"An open-source benchmark suite for cloud microservices, designed to evaluate the performance and implications of microservices architectures in systems research.",AI4;AI4-01,benchmarking;systems_evaluation,benchmark_suite,Lua,https://github.com/delimitrou/DeathStarBench,http://microservices.csl.cornell.edu/,Apache-2.0,microservices;cloud;benchmark;systems
142,pyhpc-benchmarks,Benchmarks for Python high-performance computing libraries,"A suite of benchmarks for evaluating the CPU and GPU performance of popular high-performance computing libraries in the Python ecosystem, aiding in library selection for scientific computing.",AI4;AI4-01,benchmarking;performance_profiling,benchmark_suite,Python,https://github.com/dionhaefner/pyhpc-benchmarks,,Unlicense,hpc;python;benchmark;gpu;cpu
143,AL4PDE,Benchmark for Active Learning in Neural PDE Solvers,A benchmark suite specifically designed for evaluating active learning strategies applied to neural partial differential equation (PDE) solvers.,AI4;AI4-01,benchmarking;active_learning;pde_solving,benchmark_suite,Python,https://github.com/dmusekamp/al4pde,,NOASSERTION,pde;active-learning;neural-solver;benchmark
144,dockstring,Dataset and benchmark tasks for molecular docking,"A package providing a curated dataset and realistic benchmark tasks for molecular docking, facilitating the evaluation of machine learning models in drug discovery.",AI4;AI4-01,benchmarking;molecular_docking,library,Python,https://github.com/dockstring/dockstring,,Apache-2.0,molecular-docking;drug-discovery;benchmark;dataset-loader
145,docling-eval,Evaluation framework for document processing models,"A framework for evaluating document processing models and services, providing metrics and tools to assess performance on document understanding tasks.",AI4;AI4-01,evaluation;document_processing,framework,Python,https://github.com/docling-project/docling-eval,,MIT,document-processing;evaluation;metrics
146,ADaM Standard Code,Standardized code for creating ADaM clinical datasets,"A repository of standardized SAS and R code designed to create ADaM (Analysis Data Model) datasets from SDTM data, supporting clinical trial data analysis and submission.",AI4;AI4-01,data_processing;standardization,library,SAS,https://github.com/dominodatalab/ADaM_Standard_Code,,None,clinical-trials;adam;sdtm;sas;r
147,dpbench,Dataplane benchmarking suite,"A benchmarking suite for evaluating dataplane performance, useful for systems research and network optimization.",AI4;AI4-01,benchmarking;performance_evaluation,benchmark_suite,Shell,https://github.com/dpbench/dpbench,,MIT,dataplane;networking;benchmark;systems
148,Blind Image Quality Toolbox,Collection of blind image quality metrics,"A MATLAB toolbox containing various blind (no-reference) image quality metrics, used for evaluating image processing algorithms and quality assessment.",AI4;AI4-01,metric_calculation;image_quality_assessment,library,MATLAB,https://github.com/dsoellinger/blind_image_quality_toolbox,,None,image-quality;matlab;metrics;blind-assessment
149,SAR-ShipDet-Dataset-Processor,Processing tool for SAR ship detection datasets,"A unified tool for processing various Synthetic Aperture Radar (SAR) ship detection datasets (HRSID, SSDD, etc.) into standardized formats for research and model training.",AI4;AI4-01,data_processing;normalization,tool,Python,https://github.com/egshkim/SAR-ShipDet-Dataset-Processor,,MIT,sar;remote-sensing;dataset-processing;ship-detection
150,AdaTime,Benchmarking suite for domain adaptation on time series,"A benchmarking suite designed to evaluate domain adaptation algorithms specifically on time series data, facilitating reproducible research in temporal distribution shifts.",AI4;AI4-01,benchmarking;domain_adaptation,benchmark_suite,Python,https://github.com/emadeldeen24/AdaTime,,MIT,time-series;domain-adaptation;benchmark;machine-learning
151,repurpose,Framework for drug repurposing classifiers,"A Python-based framework for building and evaluating drug-disease association classifiers, supporting reproducible research in drug repurposing.",AI4;AI4-01,modeling;drug_repurposing,framework,Python,https://github.com/emreg00/repurpose,,None,drug-repurposing;bioinformatics;machine-learning;drug-discovery
152,text-to-image-eval,Evaluation metrics for text-to-image models,"A tool to evaluate custom and HuggingFace text-to-image and zero-shot image classification models using metrics like Zero-shot accuracy, Linear Probe, and Image retrieval.",AI4;AI4-01,evaluation;metric_calculation,tool,Jupyter Notebook,https://github.com/encord-team/text-to-image-eval,,Apache-2.0,text-to-image;evaluation;metrics;computer-vision
153,Kurtis,Fine-tuning and evaluation tool for Small Language Models,"A tool designed for fine-tuning, inference, and evaluation of Small Language Models (SLMs), streamlining the development lifecycle for smaller AI models.",AI4;AI4-01,evaluation;fine_tuning,tool,Python,https://github.com/ethicalabs-ai/kurtis,,MIT,slm;fine-tuning;evaluation;inference
154,EvalPlus,Rigorous evaluation framework for LLM-synthesized code,"A framework for rigorously evaluating code synthesized by Large Language Models, providing enhanced test generation and benchmarking capabilities.",AI4;AI4-01,evaluation;code_generation,framework,Python,https://github.com/evalplus/evalplus,https://evalplus.github.io/,Apache-2.0,llm;code-generation;evaluation;benchmark
155,lammps-bulk-benchmark,Benchmark for LAMMPS molecular dynamics simulations,A benchmarking tool for evaluating CPU and GPU performance by running bulk molecular dynamics simulations using LAMMPS.,AI4;AI4-01,benchmarking;performance_evaluation,benchmark_suite,Shell,https://github.com/evenmn/lammps-bulk-benchmark,,None,lammps;molecular-dynamics;benchmark;hpc
156,Evidently,ML and LLM observability and evaluation framework,"An open-source framework to evaluate, test, and monitor ML models and LLMs, providing over 100 metrics for data quality, drift detection, and model performance.",AI4;AI4-01,evaluation;monitoring;quality_control,framework,Jupyter Notebook,https://github.com/evidentlyai/evidently,https://docs.evidentlyai.com/,Apache-2.0,observability;evaluation;mlops;drift-detection
157,HydroNet,Benchmark tasks for molecular data modeling,"A set of benchmark tasks designed to evaluate predictive and generative models for molecular data, focusing on preserving long-range interactions and structural motifs.",AI4;AI4-01,benchmarking;molecular_modeling,benchmark_suite,Jupyter Notebook,https://github.com/exalearn/hydronet,,Apache-2.0,molecular-data;benchmark;generative-models;chemistry
158,datamaestro,Scripts for automated dataset handling and standardization,"A library to automatize and standardize the handling of datasets for experiments, facilitating reproducible data pipelines.",AI4;AI4-01,data_management;workflow_automation,library,Python,https://github.com/experimaestro/datamaestro,,GPL-3.0,dataset-management;reproducibility;workflow
159,BenchMARL,Benchmarking library for Multi-Agent Reinforcement Learning,"A library for benchmarking Multi-Agent Reinforcement Learning (MARL) algorithms, enabling standardized comparison of tasks, models, and algorithms.",AI4;AI4-01,benchmarking;reinforcement_learning,library,Python,https://github.com/facebookresearch/BenchMARL,https://benchmarl.readthedocs.io/,MIT,marl;reinforcement-learning;benchmark;torchrl
160,DCPerf,Benchmark suite for hyperscale cloud applications,"A benchmark suite designed to evaluate the performance of hyperscale cloud applications and datacenter workloads, useful for systems research.",AI4;AI4-01,benchmarking;systems_evaluation,benchmark_suite,Python,https://github.com/facebookresearch/DCPerf,,MIT,cloud;benchmark;datacenter;performance
161,ParlAI,A unified framework for training and evaluating dialogue models,"A python framework for sharing, training and testing dialogue models, from open-domain chitchat to task-oriented dialogue. It provides access to many popular datasets and a wide set of reference models.",AI4;AI4-01,model_evaluation;dialogue_system;dataset_access,framework,Python,https://github.com/facebookresearch/ParlAI,https://parl.ai/,MIT,nlp;dialogue;evaluation-framework;chatbot
162,video-transformers,Streamlined fine-tuning interface for video classification models,"A wrapper tool designed to simplify the fine-tuning process of HuggingFace video classification models, providing an accessible interface for model training and experimentation.",AI4;AI4-01,model_training;fine_tuning;video_classification,solver,Python,https://github.com/fcakyon/video-transformers,,MIT,video-classification;transformers;fine-tuning
163,transfer-nlp,Experiment management library for reproducible NLP research,"A library designed to manage NLP experiments, ensuring reproducibility and easing the process of transfer learning and model configuration.",AI4;AI4-01,experiment_management;reproducibility;nlp,library,Python,https://github.com/feedly/transfer-nlp,,MIT,nlp;experiment-tracking;reproducibility
164,FlagEvalMM,Comprehensive evaluation framework for multimodal models,"A flexible framework designed to evaluate multimodal AI models across various dimensions, supporting the assessment of model capabilities in processing diverse data types.",AI4;AI4-01,model_evaluation;multimodal_learning;benchmarking,framework,Python,https://github.com/flageval-baai/FlagEvalMM,,None,multimodal;evaluation;benchmark
165,FluidML,Lightweight framework for developing machine learning pipelines,"A framework for building ML pipelines that supports caching, parallel execution, and experiment tracking, facilitating efficient model development and scientific experimentation.",AI4;AI4-01,pipeline_orchestration;experiment_management,workflow,Python,https://github.com/fluidml/fluidml,,Apache-2.0,pipeline;machine-learning;experimentation
166,frictionless-js,Library for standardized access to tabular scientific data,"A JavaScript library implementing the Frictionless Data specifications, providing standardized methods to access, validate, and process tabular datasets (CSV, Excel) often used in open science.",AI4;AI4-01,data_access;data_validation;interoperability,library,JavaScript,https://github.com/frictionlessdata/frictionless-js,https://frictionlessdata.io/,None,data-standard;csv;open-science
167,pLitter,Dataset and model for plastic litter detection,"A standardized dataset and pre-trained deep learning model specifically designed for the detection of plastic litter in environmental settings, aiding in environmental science research.",AI4;AI4-01,object_detection;environmental_monitoring;dataset,dataset,Jupyter Notebook,https://github.com/gicait/pLitter,,None,dataset;environmental-science;plastic-detection
168,language-table,Benchmark for open vocabulary visuolinguomotor learning,A suite of datasets and a multi-task continuous control benchmark designed for researching open vocabulary visual-language-motor learning in robotics.,AI4;AI4-01,robotics_benchmark;visuomotor_learning;dataset,dataset,Jupyter Notebook,https://github.com/google-research/language-table,,Apache-2.0,robotics;benchmark;multimodal
169,realworldrl_suite,Benchmark suite for Real-World Reinforcement Learning,"A collection of reinforcement learning tasks designed to capture the challenges of real-world control problems, serving as a benchmark for RL algorithm evaluation.",AI4;AI4-01,reinforcement_learning;benchmarking;control_systems,framework,Python,https://github.com/google-research/realworldrl_suite,,Apache-2.0,reinforcement-learning;benchmark;real-world-rl
170,BIG-bench,Collaborative benchmark for large language models,The Beyond the Imitation Game Benchmark (BIG-bench) is a collaborative initiative to measure and extrapolate the capabilities of large language models across a diverse range of tasks.,AI4;AI4-01,model_evaluation;llm_benchmark;task_definition,framework,Python,https://github.com/google/BIG-bench,,Apache-2.0,llm;benchmark;evaluation
171,Google ADK (Go),Toolkit for building and evaluating AI agents (Go),"An open-source toolkit for building, evaluating, and deploying sophisticated AI agents, providing frameworks for agentic workflows and evaluation.",AI4;AI4-01,agent_evaluation;agent_development,framework,Go,https://github.com/google/adk-go,,Apache-2.0,ai-agents;toolkit;evaluation
172,Google ADK (Java),Toolkit for building and evaluating AI agents (Java),"An open-source toolkit for building, evaluating, and deploying sophisticated AI agents, providing frameworks for agentic workflows and evaluation.",AI4;AI4-01,agent_evaluation;agent_development,framework,Java,https://github.com/google/adk-java,,Apache-2.0,ai-agents;toolkit;evaluation
173,Google ADK (Python),Toolkit for building and evaluating AI agents (Python),"An open-source toolkit for building, evaluating, and deploying sophisticated AI agents, providing frameworks for agentic workflows and evaluation.",AI4;AI4-01,agent_evaluation;agent_development,framework,Python,https://github.com/google/adk-python,,Apache-2.0,ai-agents;toolkit;evaluation
174,h2o-LLM-eval,LLM evaluation framework with Elo leaderboard,"A framework for evaluating Large Language Models, featuring an Elo rating system and A/B testing capabilities to assess model performance.",AI4;AI4-01,model_evaluation;llm_ranking;ab_testing,framework,Jupyter Notebook,https://github.com/h2oai/h2o-LLM-eval,,Apache-2.0,llm;evaluation;leaderboard
175,pytorch-worker,Framework for training and evaluating PyTorch models,"A lightweight framework designed to streamline the training, evaluation, and testing processes for deep learning models built with PyTorch.",AI4;AI4-01,model_training;model_evaluation;workflow_automation,framework,Python,https://github.com/haoxizhong/pytorch-worker,,MIT,pytorch;training-framework;evaluation
176,SeisFlowBench,Benchmark for seismic wave propagation simulations,"A reproducible scientific project and benchmark suite for seismic wave propagation and fluid flow simulations in porous media, built using Julia.",AI4;AI4-01,seismic_simulation;benchmarking;geophysics,solver,Julia,https://github.com/haoyunl2/SeisFlowBench,,MIT,seismology;simulation;julia
177,dlio_benchmark,I/O benchmark for scientific deep learning workloads,A benchmark suite designed to represent and evaluate the I/O patterns and performance of scientific deep learning applications.,AI4;AI4-01,io_benchmarking;scientific_computing;performance_analysis,solver,Python,https://github.com/hariharan-devarajan/dlio_benchmark,,MIT,hpc;deep-learning;io-benchmark
178,big-ann-benchmarks,Benchmark for billion-scale approximate nearest neighbor search,"A framework for evaluating Approximate Nearest Neighbor Search (ANNS) algorithms on billion-scale datasets, critical for vector search in AI applications.",AI4;AI4-01,algorithm_evaluation;vector_search;benchmarking,framework,Jupyter Notebook,https://github.com/harsha-simhadri/big-ann-benchmarks,http://big-ann-benchmarks.com/,MIT,ann;vector-search;benchmark
179,EvalView,Test harness for AI agents evaluation,"A pytest-style test harness for evaluating AI agents, supporting YAML scenarios, tool-call checks, and reporting on cost, latency, and safety.",AI4;AI4-01,agent_evaluation;testing_framework;safety_eval,framework,Python,https://github.com/hidai25/eval-view,,Apache-2.0,ai-agents;evaluation;testing
180,This-is-not-a-Dataset,Dataset for evaluating negation and commonsense in LLMs,"A large semi-automatically generated dataset of descriptive sentences about commonsense knowledge, focusing on negation, used for evaluating Large Language Models.",AI4;AI4-01,model_evaluation;commonsense_reasoning;dataset,dataset,Python,https://github.com/hitz-zentroa/This-is-not-a-Dataset,,Apache-2.0,llm-evaluation;dataset;commonsense
181,Latxa,Language model and evaluation suite for Basque,"An open language model and a comprehensive evaluation suite specifically designed for the Basque language, facilitating NLP research in low-resource languages.",AI4;AI4-01,model_evaluation;nlp_benchmark;low_resource_language,framework,Shell,https://github.com/hitz-zentroa/latxa,,MIT,nlp;basque;evaluation-suite
182,ColorTransferLib,Library for color transfer algorithms and evaluation metrics,"A collection of algorithms for color and style transfer, accompanied by objective evaluation metrics for quantitative assessment of image processing results.",AI4;AI4-01,image_processing;algorithm_evaluation;metrics,library,Python,https://github.com/hpotechius/ColorTransferLib,,MIT,color-transfer;image-processing;metrics
183,hf_benchmarks,Starter kit for evaluating benchmarks on HuggingFace Hub,"A toolkit designed to facilitate the evaluation of benchmarks hosted on the HuggingFace Hub, providing starter code and utilities for model assessment.",AI4;AI4-01,benchmarking;model_evaluation;huggingface_hub,framework,Python,https://github.com/huggingface/hf_benchmarks,,Apache-2.0,benchmark;evaluation;huggingface
184,LightEval,Toolkit for evaluating LLMs across multiple backends,"A comprehensive toolkit for evaluating Large Language Models, supporting multiple backends and providing a unified interface for running various evaluation tasks.",AI4;AI4-01,model_evaluation;llm;inference_backend,framework,Python,https://github.com/huggingface/lighteval,,MIT,llm;evaluation;toolkit
185,ScreenSuite,Benchmarking suite for GUI Agents,"A comprehensive benchmarking suite designed to evaluate the performance of GUI Agents, enabling standardized testing of agent interactions with graphical user interfaces.",AI4;AI4-01,agent_evaluation;gui_automation;benchmark,framework,Python,https://github.com/huggingface/screensuite,,Apache-2.0,gui-agents;benchmark;evaluation
186,PINNacle,Benchmark for Physics-Informed Neural Networks,"A comprehensive benchmark suite for Physics-Informed Neural Networks (PINNs) applied to solving Partial Differential Equations (PDEs), facilitating fair comparison of different PINN methods.",AI4;AI4-01,pde_solving;model_benchmarking;scientific_ml,framework,Python,https://github.com/i207M/PINNacle,,MIT,pinns;pde;benchmark;ai4science
187,transformers-lightning,Integration library for PyTorch Lightning and Transformers,"A collection of utilities, metrics, and modules to seamlessly integrate HuggingFace Transformers with PyTorch Lightning, facilitating efficient model training and evaluation workflows.",AI4;AI4-01,model_training;integration;workflow_optimization,library,Python,https://github.com/iKernels/transformers-lightning,,GPL-2.0,pytorch-lightning;transformers;integration
188,ChainForge,Visual programming environment for LLM prompt evaluation,"An open-source visual programming environment designed for battle-testing and evaluating prompts for Large Language Models, enabling systematic comparison of model outputs.",AI4;AI4-01,prompt_engineering;model_evaluation;visual_programming,platform,TypeScript,https://github.com/ianarawjo/ChainForge,https://chainforge.ai/,MIT,prompt-engineering;llm;evaluation;visualization
189,coref-data,Standardized collection of coreference resolution datasets,A repository containing a collection of coreference datasets formatted in a standardized way to facilitate training and evaluation of coreference resolution models.,AI4;AI4-01,dataset_management;evaluation,dataset,Python,https://github.com/ianporada/coref-data,,Apache-2.0,nlp;coreference-resolution;dataset
190,PPTAgent,Framework for generating and evaluating presentation slides,"A research framework designed to generate and evaluate presentations, moving beyond simple text-to-slides by incorporating multimodal reasoning and evaluation metrics.",AI4;AI4-01,evaluation;generation,workflow,Python,https://github.com/icip-cas/PPTAgent,,MIT,multimodal;evaluation-framework;presentation-generation
191,scaling-resnets,Framework investigating scaling limits of ResNets vs Neural ODEs,"A research framework used to investigate the scaling limits of ResNets and compare them to Neural ODEs, tested on synthetic and standardized datasets for scientific modeling analysis.",AI4;AI4-01,modeling;evaluation,library,Python,https://github.com/instadeepai/scaling-resnets,,MIT,neural-odes;resnet;scaling-laws
192,itu-p1203-open-dataset,Open dataset for ITU-T P.1203 video quality standardization,"An open dataset used for the standardization of ITU-T P.1203, facilitating the evaluation and benchmarking of video quality assessment models.",AI4;AI4-01,evaluation;quality_control,dataset,Python,https://github.com/itu-p1203/open-dataset,,None,video-quality;standardization;dataset
193,mallet,Evaluation harness for VLMs controlling robots,Cloud-based tools and an evaluation harness designed to test and benchmark Vision-Language Models (VLMs) for controlling real-world robots.,AI4;AI4-01,evaluation;robotics,platform,Python,https://github.com/jacobphillips99/mallet,,Apache-2.0,vlm;robotics;evaluation-harness
194,matbench-discovery,Evaluation framework for high-throughput materials discovery,"An evaluation framework for machine learning models that simulates high-throughput materials discovery, providing metrics and benchmarks for materials science.",AI4;AI4-01,evaluation;modeling,library,Python,https://github.com/janosh/matbench-discovery,https://matbench-discovery.materialsproject.org,MIT,materials-science;discovery;benchmark
195,41-llms-evaluated-on-19-benchmarks,Benchmark suite and results for 41 LLMs across 19 tasks,"A project benchmarking 41 open-source large language models across 19 evaluation tasks using the lm-evaluation-harness library, providing reproducible evaluation scripts and data.",AI4;AI4-01,evaluation;benchmarking,workflow,Jupyter Notebook,https://github.com/jayminban/41-llms-evaluated-on-19-benchmarks,,None,llm;benchmark;evaluation
196,text2sql-data,Collection of Text-to-SQL datasets,"A collection of datasets that pair natural language questions with SQL queries, serving as a benchmark resource for semantic parsing and NLP tasks.",AI4;AI4-01,dataset_management;evaluation,dataset,Python,https://github.com/jkkummerfeld/text2sql-data,https://jkk.name/text2sql-data/,NOASSERTION,nlp;text-to-sql;dataset
197,MicroVQA,Multimodal reasoning benchmark for microscopy,"Evaluation code and benchmark for MicroVQA, a multimodal reasoning task focused on microscopy-based scientific research, including the RefineBot method.",AI4;AI4-01,evaluation;image_analysis,library,Python,https://github.com/jmhb0/microvqa,,None,microscopy;vqa;benchmark
198,julia-pde-benchmark,Benchmark for PDE integration algorithms in Julia,A benchmarking tool for evaluating the performance of simple Partial Differential Equation (PDE) integration algorithms in Julia compared to other languages.,AI4;AI4-01,benchmarking;modeling,workflow,Jupyter Notebook,https://github.com/johnfgibson/julia-pde-benchmark,,MIT,pde;julia;benchmark
199,SciFIBench,Benchmark for scientific figure interpretation,A benchmark suite for evaluating Large Multimodal Models (LMMs) on their ability to interpret and reason about scientific figures.,AI4;AI4-01,evaluation;image_analysis,dataset,,https://github.com/jonathan-roberts1/SciFIBench,,MIT,scientific-figures;multimodal;benchmark
200,ScientificComputingBenchmarks.jl,Benchmarks for scientific computing in Julia,A collection of benchmarks designed to evaluate the performance of various scientific computing tasks and algorithms in the Julia programming language.,AI4;AI4-01,benchmarking;performance_analysis,library,Julia,https://github.com/jonathanBieler/ScientificComputingBenchmarks.jl,,None,scientific-computing;julia;benchmark
201,entity-recognition-datasets,Collection of Named Entity Recognition (NER) corpora,"A comprehensive collection of annotated datasets for named entity recognition tasks across various languages and domains, formatted for research use.",AI4;AI4-01,dataset_management;evaluation,dataset,Python,https://github.com/juand-r/entity-recognition-datasets,,MIT,ner;nlp;dataset
202,eva,Evaluation framework for oncology foundation models,"A framework designed for the evaluation of oncology foundation models, providing metrics and workflows for assessing model performance in cancer research contexts.",AI4;AI4-01,evaluation;medical_analysis,library,Python,https://github.com/kaiko-ai/eva,,Apache-2.0,oncology;foundation-models;evaluation
203,MalDataGen,Synthetic tabular dataset generation and evaluation framework,"An advanced framework for generating and evaluating synthetic tabular datasets using generative models like diffusion and adversarial architectures, applicable to scientific data augmentation.",AI4;AI4-01,data_generation;evaluation,library,Python,https://github.com/kayua/MalDataGen,,MIT,synthetic-data;generative-models;tabular-data
204,Mars-Bench,Benchmark for vision models on Martian imagery,"A standardized benchmark for evaluating computer vision models on Martian surface and orbital imagery, covering classification, segmentation, and detection tasks.",AI4;AI4-01,evaluation;image_analysis,dataset,Python,https://github.com/kerner-lab/Mars-Bench,,None,planetary-science;mars;computer-vision
205,DROP-Fixed-Income,Java libraries for fixed income analytics and curve construction,"A collection of libraries for quantitative finance, including multi-curve construction, valuation, and stochastic evolution, serving as a scientific modeling tool for financial mathematics.",AI4;AI4-01,modeling;analysis,library,HTML,https://github.com/lakshmiDRIP/DROP-Fixed-Income,,Apache-2.0,quantitative-finance;numerical-analysis;modeling
206,deeplearning-benchmark,Benchmark suite for deep learning infrastructure,A suite of scripts and tools for benchmarking the performance of deep learning models on various hardware configurations.,AI4;AI4-01,benchmarking;performance_analysis,workflow,Shell,https://github.com/lambdal/deeplearning-benchmark,,None,deep-learning;benchmark;gpu
207,lamindb,Data framework for biology and scientific data management,"A data framework specifically designed for biology, enabling queryable, traceable, and reproducible data management (FAIR principles) integrating lakehouse and lineage tracking.",AI4;AI4-01,data_management;reproducibility,platform,Python,https://github.com/laminlabs/lamindb,https://lamin.ai,Apache-2.0,biology;data-management;fair-data
208,langfuse,LLM engineering platform for observability and evaluation,"An open-source platform for LLM engineering that includes tools for observability, metrics collection, dataset management, and model evaluation.",AI4;AI4-01,evaluation;monitoring,platform,TypeScript,https://github.com/langfuse/langfuse,https://langfuse.com,NOASSERTION,llm-ops;evaluation;observability
209,langwatch,"LLM Ops platform for traces, analytics, and evaluations","A platform for LLM operations focusing on traces, analytics, and evaluations, allowing for the optimization of prompts and assessment of model performance.",AI4;AI4-01,evaluation;analysis,platform,TypeScript,https://github.com/langwatch/langwatch,https://langwatch.ai,NOASSERTION,llm-ops;evaluation;analytics
210,latitude-llm,Prompt engineering and evaluation platform,"An open-source platform designed to build, evaluate, and refine prompts for Large Language Models, facilitating systematic prompt engineering and testing.",AI4;AI4-01,evaluation;prompt_engineering,platform,TypeScript,https://github.com/latitude-dev/latitude-llm,https://latitude.so,LGPL-3.0,prompt-engineering;evaluation;llm
211,BenchmarkDatasetCreator,Pipeline for creating standardized bioacoustic datasets,"A standardized pipeline for creating, storing, sharing, and using bioacoustic datasets, designed to facilitate the training and testing of AI models in bioacoustics.",AI4;AI4-01,data_generation;dataset_management,workflow,Python,https://github.com/leabouffaut/BenchmarkDatasetCreator,,NOASSERTION,bioacoustics;dataset-creation;standardization
212,les-audits-affaires-eval-harness,Evaluation harness for French business law LLMs,"A lightweight CLI tool for benchmarking French Large Language Models specifically in the domain of business law, evaluating aspects like action, delay, and risk assessment.",AI4;AI4-01,evaluation;benchmarking,solver,Python,https://github.com/legml-ai/les-audits-affaires-eval-harness,,MIT,legal-tech;llm-evaluation;french-law
213,llm_benchmarks,Collection of benchmarks and datasets for evaluating Large Language Models,"A comprehensive collection of benchmarks and datasets designed for the evaluation of Large Language Models (LLMs), facilitating comparative analysis and performance assessment.",AI4;AI4-01,model_evaluation;benchmarking,dataset,Python,https://github.com/leobeeson/llm_benchmarks,,None,llm;benchmark;dataset;evaluation
214,Blades,Unified benchmark suite for attacks and defenses in Federated Learning,"A unified benchmark suite designed to evaluate attacks and defenses in Federated Learning systems, providing a standard environment for security and robustness research.",AI4;AI4-01,federated_learning;security_evaluation;benchmarking,solver,Python,https://github.com/lishenghui/blades,,Apache-2.0,federated-learning;security;benchmark;attacks;defenses
215,llm-jp-eval-mm,Lightweight framework for evaluating visual-language models,"A lightweight evaluation framework specifically designed for visual-language models, supporting various metrics and datasets for multimodal performance assessment.",AI4;AI4-01,multimodal_evaluation;vlm_benchmarking,solver,Python,https://github.com/llm-jp/llm-jp-eval-mm,,Apache-2.0,vlm;evaluation;multimodal;benchmark
216,RouteLLM,Framework for serving and evaluating LLM routers,"A framework designed to serve and evaluate Large Language Model (LLM) routers, enabling researchers to optimize the trade-off between cost and response quality in LLM deployments.",AI4;AI4-01,model_routing;cost_optimization;evaluation,platform,Python,https://github.com/lm-sys/RouteLLM,,Apache-2.0,llm-routing;evaluation;inference-optimization
217,SWT-Bench,Evaluation harness for benchmarking LLM repository-level test generation,"An evaluation harness for SWT-Bench, a benchmark designed to assess the capabilities of Large Language Models in generating repository-level tests, specifically for software testing tasks.",AI4;AI4-01,code_generation;test_generation;benchmarking,solver,Python,https://github.com/logic-star-ai/swt-bench,,MIT,llm;software-testing;benchmark;code-generation
218,Loghub,Large collection of system log datasets for AI-driven log analytics,"A comprehensive collection of system log datasets designed to support research in AI-driven log analytics, including tasks such as anomaly detection and failure prediction.",AI4;AI4-01,log_analysis;anomaly_detection;dataset,dataset,Python,https://github.com/logpai/loghub,,NOASSERTION,log-analysis;aiops;dataset;anomaly-detection
219,Matbench,Benchmarks for materials science property prediction,"A benchmark suite for evaluating machine learning models on materials science property prediction tasks, providing a standardized set of datasets and evaluation metrics.",AI4;AI4-01,materials_science;property_prediction;benchmarking,solver,Python,https://github.com/materialsproject/matbench,https://matbench.materialsproject.org/,MIT,materials-science;benchmark;property-prediction
220,pysaliency,Python Framework for Saliency Modeling and Evaluation,"A Python framework designed for the modeling and evaluation of visual saliency, providing tools to handle datasets, models, and standard metrics in saliency research.",AI4;AI4-01,saliency_prediction;model_evaluation;computer_vision,library,Jupyter Notebook,https://github.com/matthias-k/pysaliency,,MIT,saliency;evaluation;computer-vision;neuroscience
221,Video-ChatGPT,Video conversation model with quantitative evaluation benchmarking,"A video conversation model capable of generating meaningful conversations about videos, accompanied by a rigorous quantitative evaluation benchmark for assessing video-based conversational models.",AI4;AI4-01,video_understanding;multimodal_conversation;benchmarking,solver,Python,https://github.com/mbzuai-oryx/Video-ChatGPT,,CC-BY-4.0,video-llm;benchmark;multimodal
222,Torcheval,Library for performant PyTorch model metrics and evaluation,A library providing a rich collection of performant PyTorch model metrics and tools to facilitate metric computation in distributed training and model evaluation.,AI4;AI4-01,model_evaluation;metrics_computation,library,Python,https://github.com/meta-pytorch/torcheval,,NOASSERTION,pytorch;metrics;evaluation
223,micarraylib,Software for aggregation and processing of microphone array datasets,"A software tool for the reproducible aggregation, standardization, and signal processing of microphone array datasets, facilitating research in acoustics and audio processing.",AI4;AI4-01,signal_processing;dataset_standardization;acoustics,library,Python,https://github.com/micarraylib/micarraylib,,CC-BY-4.0,audio-processing;microphone-array;dataset-tools
224,OpenTTDLab,Framework for running reproducible experiments using OpenTTD,"A Python framework designed to run reproducible experiments using the OpenTTD game engine, suitable for research in reinforcement learning and simulation.",AI4;AI4-01,simulation;reinforcement_learning;experiment_management,platform,Python,https://github.com/michalc/OpenTTDLab,,GPL-2.0,simulation;reinforcement-learning;openttd
225,FS-Mol,Few-Shot Learning Dataset and Benchmark for Molecules,"A dataset and evaluation benchmark for few-shot learning in molecular property prediction, containing molecular compounds with activity measurements against various protein targets.",AI4;AI4-01,molecular_property_prediction;few_shot_learning;benchmarking,dataset,Python,https://github.com/microsoft/FS-Mol,,NOASSERTION,drug-discovery;molecules;few-shot-learning;benchmark
226,Eureka ML Insights,Framework for standardizing evaluations of large foundation models,"A framework designed to standardize the evaluation of large foundation models, moving beyond single-score reporting to provide deeper insights into model capabilities.",AI4;AI4-01,model_evaluation;foundation_models,library,Python,https://github.com/microsoft/eureka-ml-insights,,Apache-2.0,evaluation;foundation-models;metrics
227,PromptBench,Unified evaluation framework for large language models,"A unified evaluation framework for Large Language Models (LLMs) that supports various tasks, datasets, and metrics to assess model performance and robustness.",AI4;AI4-01,model_evaluation;prompt_engineering;benchmarking,solver,Python,https://github.com/microsoft/promptbench,,MIT,llm;evaluation;benchmark;prompting
228,Clustering-Datasets,Collection of datasets for clustering algorithm benchmarking,"A collection of UCI real-life and synthetic datasets, formatted and ready for use in benchmarking and evaluating clustering algorithms.",AI4;AI4-01,clustering;benchmarking;dataset,dataset,,https://github.com/milaan9/Clustering-Datasets,,MIT,clustering;dataset;machine-learning
229,SPECTRA,Spectral framework for evaluation of biomedical AI models,"A framework utilizing spectral analysis methods for the evaluation of biomedical AI models, providing specialized metrics for this domain.",AI4;AI4-01,biomedical_ai;model_evaluation;spectral_analysis,solver,Jupyter Notebook,https://github.com/mims-harvard/SPECTRA,,MIT,biomedical;evaluation;spectral-analysis
230,GenAI Energy Leaderboard,Benchmark and measurements for Generative AI energy consumption,"A canonical source and benchmark suite for measuring and evaluating the energy consumption of Generative AI models, providing standardized metrics for efficiency analysis.",AI4;AI4-01,benchmarking;energy_efficiency_analysis,dataset,Python,https://github.com/ml-energy/leaderboard-v2,,Apache-2.0,genai;energy-efficiency;benchmark
231,CK-MLOps,Portable workflows and automation recipes for MLOps,"A collection of portable workflows, automation recipes, and components for MLOps in the Unified Collective Knowledge (CK) format, facilitating reproducible ML experiments and benchmarking.",AI4;AI4-01,workflow_automation;reproducibility,workflow,Python,https://github.com/mlcommons/ck-mlops,,Apache-2.0,mlops;reproducibility;workflow
232,MLPerf Storage,Benchmark suite for ML storage performance,"The MLPerf Storage Benchmark Suite measures the performance of storage systems in the context of machine learning workloads, providing standardized metrics for system evaluation.",AI4;AI4-01,benchmarking;system_evaluation,solver,Python,https://github.com/mlcommons/storage,https://mlcommons.org,Apache-2.0,mlperf;storage;benchmark
233,MLPerf Tiny,ML benchmark suite for low-power embedded systems,"MLPerf Tiny is a machine learning benchmark suite specifically designed for extremely low-power systems such as microcontrollers, evaluating inference performance on constrained hardware.",AI4;AI4-01,benchmarking;inference_evaluation,solver,C,https://github.com/mlcommons/tiny,https://mlcommons.org,Apache-2.0,tinyml;embedded-systems;benchmark
234,EvalScope,Framework for large model evaluation and benchmarking,"A streamlined and customizable framework for efficient evaluation and performance benchmarking of large models (LLM, VLM, AIGC), supporting various datasets and metrics.",AI4;AI4-01,model_evaluation;benchmarking,platform,Python,https://github.com/modelscope/evalscope,,Apache-2.0,llm-evaluation;benchmark;modelscope
235,MOSES,Benchmarking platform for molecular generation models,"Molecular Sets (MOSES) is a benchmarking platform for molecular generation models, providing standard datasets and metrics to evaluate the quality and diversity of generated chemical structures.",AI4;AI4-01,molecular_generation;benchmarking,platform,Python,https://github.com/molecularsets/moses,,MIT,drug-discovery;molecular-generation;benchmark
236,eICU Benchmark Updated,Benchmark for clinical tasks on eICU dataset,An updated version of the eICU Benchmark providing problem definitions and evaluation frameworks for clinical tasks such as Length of Stay (LoS) prediction and Decompensation detection.,AI4;AI4-01,clinical_prediction;benchmarking,dataset,Python,https://github.com/mostafaalishahi/eICU_Benchmark_updated,,None,healthcare;clinical-benchmark;eicu
237,SciREX,Benchmark for scientific information extraction,"A benchmark dataset and evaluation framework for document-level information extraction from scientific articles, focusing on identifying entities and relationships in scientific text.",AI4;AI4-01,information_extraction;benchmarking,dataset,Python,https://github.com/n0w0f/scirex,,MIT,nlp;scientific-extraction;benchmark
238,NASA Prognostic Algorithms,Framework for model-based prognostics of engineering systems,"A Python framework for model-based prognostics, providing algorithms for state estimation, uncertainty propagation, and remaining useful life (RUL) computation for engineering systems.",AI4;AI4-01,prognostics;state_estimation,library,Python,https://github.com/nasa/prog_algs,,None,prognostics;nasa;engineering-systems
239,latrend,R package for clustering longitudinal datasets,"An R package designed for clustering longitudinal datasets in a standardized way, providing interfaces to various clustering methods and facilitating method evaluation.",AI4;AI4-01,clustering;longitudinal_analysis,library,R,https://github.com/niekdt/latrend,,GPL-2.0,r-package;clustering;longitudinal-data
240,ProkEvo,Framework for bacterial population genomics analyses,"An automated, reproducible, and scalable framework for high-throughput bacterial population genomics analyses, facilitating large-scale bioinformatics workflows.",AI4;AI4-01,genomics;bioinformatics_workflow,workflow,Jupyter Notebook,https://github.com/npavlovikj/ProkEvo,,GPL-3.0,genomics;bioinformatics;pipeline
241,pcapML,Tool for standardizing network traffic analysis datasets,"pcapML standardizes network traffic analysis datasets by directly encoding metadata information into raw traffic captures, facilitating machine learning on network data.",AI4;AI4-01,data_standardization;network_analysis,solver,C++,https://github.com/nprint/pcapml,,Apache-2.0,network-traffic;dataset-creation;pcap
242,Jury,Comprehensive NLP evaluation system,"A comprehensive evaluation system for Natural Language Processing (NLP) tasks, providing a unified interface for various metrics to assess model performance.",AI4;AI4-01,nlp_evaluation;metrics,library,Python,https://github.com/obss/jury,,MIT,nlp;evaluation;metrics
243,mteval,Evaluation metrics for machine translation,"A collection of evaluation metrics and algorithms specifically designed for Machine Translation tasks, implemented in C++ for efficiency.",AI4;AI4-01,machine_translation_evaluation;metrics,library,C++,https://github.com/odashi/mteval,,LGPL-3.0,machine-translation;evaluation;cpp
244,blurr,Integration library for Transformers and fastai,"A library that integrates Hugging Face Transformers with the fastai framework, enabling efficient training, evaluation, and deployment of transformer-based models for scientific and general NLP tasks.",AI4;AI4-01,model_training;integration,library,Jupyter Notebook,https://github.com/ohmeow/blurr,,Apache-2.0,fastai;transformers;deep-learning
245,SRSD Benchmark,Symbolic Regression datasets and benchmarks,"A benchmark suite rethinking Symbolic Regression datasets for scientific discovery, providing standardized tasks to evaluate algorithms that discover mathematical expressions from data.",AI4;AI4-01,symbolic_regression;scientific_discovery,dataset,Python,https://github.com/omron-sinicx/srsd-benchmark,,MIT,symbolic-regression;scientific-discovery;benchmark
246,GenAIEval,Evaluation and benchmark for Generative AI,"A comprehensive evaluation framework, benchmark, and scorecard for Generative AI, targeting performance (throughput/latency), accuracy, safety, and hallucination metrics.",AI4;AI4-01,model_evaluation;benchmarking,platform,Jupyter Notebook,https://github.com/opea-project/GenAIEval,,Apache-2.0,genai;evaluation;safety
247,ATLAS,Benchmark for frontier scientific reasoning,"ATLAS is a high-difficulty, multidisciplinary benchmark designed to evaluate the scientific reasoning capabilities of AI models across various scientific domains.",AI4;AI4-01,scientific_reasoning;benchmarking,dataset,,https://github.com/open-compass/ATLAS,,None,scientific-reasoning;benchmark;llm
248,CompassJudger,Judge models for automated evaluation,"A collection of 'Judge Models' introduced by OpenCompass, designed to act as automated evaluators for assessing the performance of other large language models on various benchmarks.",AI4;AI4-01,automated_evaluation;model_judging,solver,,https://github.com/open-compass/CompassJudger,,Apache-2.0,llm-judge;evaluation;opencompass
249,VLMEvalKit,Evaluation toolkit for large multi-modality models,"An open-source evaluation toolkit for Large Multi-modality Models (LMMs), supporting over 220 models and 80 benchmarks, facilitating comprehensive performance assessment.",AI4;AI4-01,multimodal_evaluation;benchmarking,platform,Python,https://github.com/open-compass/VLMEvalKit,,Apache-2.0,vlm;multimodal;evaluation
250,OpenCompass,Comprehensive LLM evaluation platform,"OpenCompass is a comprehensive platform for evaluating Large Language Models (LLMs), supporting a wide range of models and over 100 datasets to assess capabilities across various domains.",AI4;AI4-01,model_evaluation;benchmarking,platform,Python,https://github.com/open-compass/opencompass,https://opencompass.org.cn/,Apache-2.0,llm-evaluation;benchmark;platform
251,OpenAI Evals,Framework for evaluating LLMs and registry of benchmarks,"Evals is a framework for evaluating Large Language Models (LLMs) and LLM systems, providing an open-source registry of benchmarks to test model performance on diverse tasks.",AI4;AI4-01,model_evaluation;benchmarking,platform,Python,https://github.com/openai/evals,,NOASSERTION,llm;evaluation;benchmark
252,Oumi,"Open-source framework for fine-tuning, evaluating, and deploying LLMs/VLMs","Oumi is a comprehensive platform that simplifies the lifecycle of large language models (LLMs) and vision-language models (VLMs), providing tools for fine-tuning, evaluation, and deployment, facilitating AI research and application development.",AI4;AI4-01,model_training;model_evaluation;inference,framework,Python,https://github.com/oumi-ai/oumi,,Apache-2.0,llm;fine-tuning;evaluation;vlm
253,ToR[e]cSys,PyTorch framework for recommendation system algorithms,"ToR[e]cSys is a PyTorch-based framework designed to implement, experiment with, and reproduce recommendation system algorithms, including CTR prediction and learning-to-rank models.",AI4;AI4-01,modeling;recommendation_system,framework,Python,https://github.com/p768lwy3/torecsys,,MIT,recsys;pytorch;ctr-prediction
254,CloudSuite,Benchmark suite for cloud services,"CloudSuite is a benchmark suite for cloud services, covering a wide range of applications such as data analytics, web serving, and media streaming, used for evaluating system performance in cloud environments.",AI4;AI4-01,benchmarking;performance_evaluation,solver,C,https://github.com/parsa-epfl/cloudsuite,http://cloudsuite.ch,NOASSERTION,cloud-computing;benchmark;systems-research
255,PDEBench,Extensive benchmark for Scientific Machine Learning (SciML),"PDEBench is a comprehensive benchmark suite for Scientific Machine Learning, specifically focused on solving Partial Differential Equations (PDEs). It provides diverse datasets and tasks to evaluate ML models in scientific contexts.",AI4;AI4-01,benchmarking;sciml;pde_solving,framework,Python,https://github.com/pdebench/PDEBench,https://arxiv.org/abs/2210.07182,NOASSERTION,sciml;pde;benchmark;physics
256,Japanese LM Financial Harness,Evaluation harness for Japanese financial language models,"A specialized evaluation framework for assessing the performance of Japanese Language Models on financial domain tasks, facilitating domain-specific AI research.",AI4;AI4-01,evaluation;benchmarking;nlp,framework,Shell,https://github.com/pfnet-research/japanese-lm-fin-harness,,MIT,llm;finance;japanese;evaluation
257,Phoronix Test Suite,Automated testing and benchmarking software,"The Phoronix Test Suite is a comprehensive, open-source testing and benchmarking platform for Linux, Windows, and macOS, widely used in computer science research for system performance evaluation.",AI4;AI4-01,benchmarking;performance_testing,solver,PHP,https://github.com/phoronix-test-suite/phoronix-test-suite,https://www.phoronix-test-suite.com/,GPL-3.0,benchmark;hardware;performance
258,OpenCGRA,Framework for modeling and evaluating CGRAs,"OpenCGRA is an open-source framework designed for modeling, testing, and evaluating Coarse-Grained Reconfigurable Architectures (CGRAs), supporting computer architecture research.",AI4;AI4-01,modeling;architecture_evaluation,framework,Verilog,https://github.com/pnnl/OpenCGRA,,NOASSERTION,cgra;computer-architecture;modeling
259,QASMBench,Low-level OpenQASM benchmark suite for NISQ evaluation,"QASMBench is a benchmark suite for evaluating Noisy Intermediate-Scale Quantum (NISQ) devices and simulators using OpenQASM, facilitating quantum computing research.",AI4;AI4-01,benchmarking;quantum_computing,dataset,OpenQASM,https://github.com/pnnl/QASMBench,,NOASSERTION,quantum;benchmark;openqasm
260,LitSearch,Retrieval benchmark for scientific literature search,"LitSearch is a benchmark designed to evaluate retrieval systems on the task of scientific literature search, supporting research in NLP and meta-science.",AI4;AI4-01,benchmarking;information_retrieval,dataset,Python,https://github.com/princeton-nlp/LitSearch,,MIT,nlp;retrieval;scientific-literature
261,PhysGym,Benchmark suite for LLM-based interactive scientific reasoning,"PhysGym is a benchmark suite for evaluating the capabilities of Large Language Models in interactive scientific reasoning tasks, specifically in physics.",AI4;AI4-01,benchmarking;scientific_reasoning,framework,Python,https://github.com/principia-ai/PhysGym,,MIT,llm;physics;reasoning;benchmark
262,Etalon,LLM serving performance evaluation harness,"Etalon is a framework for evaluating the performance of LLM serving systems, providing metrics for throughput, latency, and cost, essential for systems research in AI.",AI4;AI4-01,benchmarking;performance_evaluation,framework,Python,https://github.com/project-etalon/etalon,,Apache-2.0,llm-serving;performance;benchmark
263,Prometheus-Eval,Library for evaluating LLM responses using Prometheus and GPT-4,"Prometheus-Eval is a Python library that facilitates the evaluation of Large Language Model outputs by leveraging the Prometheus model and GPT-4 as judges, supporting research in automated evaluation metrics.",AI4;AI4-01,evaluation;metrics,library,Python,https://github.com/prometheus-eval/prometheus-eval,,Apache-2.0,llm-evaluation;metrics;nlp
264,pyperformance,Python performance benchmark suite,"pyperformance is the standard benchmark suite for the Python programming language, used to evaluate and track the performance of Python implementations, supporting computer science and programming language research.",AI4;AI4-01,benchmarking;performance_evaluation,solver,Python,https://github.com/python/pyperformance,https://pyperformance.readthedocs.io/,MIT,python;benchmark;performance
265,psychopy_ext,Framework for behavioral neuroscience and psychology experiments,"psychopy_ext is a framework built on top of PsychoPy to facilitate rapid, reproducible design, analysis, and plotting of experiments in behavioral neuroscience and psychology.",AI4;AI4-01,experiment_control;data_analysis,framework,Python,https://github.com/qbilius/psychopy_ext,,NOASSERTION,neuroscience;psychology;experiment-design
266,Quaterion Models,Building blocks for fine-tunable metric learning models,"Quaterion Models provides a collection of pre-trained models and building blocks for metric learning, enabling researchers to build and fine-tune models for similarity search and related tasks.",AI4;AI4-01,modeling;metric_learning,library,Python,https://github.com/qdrant/quaterion-models,,Apache-2.0,metric-learning;similarity-search;embeddings
267,Renaissance,Benchmark suite for the JVM,"Renaissance is a modern benchmark suite for the Java Virtual Machine (JVM), focusing on parallelism and concurrency, used in systems research to evaluate JVM performance.",AI4;AI4-01,benchmarking;performance_evaluation,solver,SMT,https://github.com/renaissance-benchmarks/renaissance,http://renaissance.dev,GPL-3.0,jvm;benchmark;concurrency
268,Phylociraptor,Rapid phylogenomic tree calculator,"Phylociraptor is a highly customizable and reproducible framework for phylogenomic inference, automating the calculation of phylogenetic trees from genomic data.",AI4;AI4-01,inference;phylogenetics,framework,Python,https://github.com/reslp/phylociraptor,https://phylociraptor.readthedocs.io/,MIT,phylogenomics;bioinformatics;tree-inference
269,Auto-Evaluator,Evaluation tool for LLM QA chains,"Auto-Evaluator is a tool designed to evaluate the quality of Question Answering (QA) chains powered by LLMs, providing automated metrics for assessing model performance.",AI4;AI4-01,evaluation;qa_evaluation,solver,Python,https://github.com/rlancemartin/auto-evaluator,,None,llm;evaluation;qa
270,RobotPerf Benchmarks,Benchmarking suite for robotics computing performance,"RobotPerf is a vendor-neutral benchmarking suite designed to evaluate robotics computing performance, covering both grey-box and black-box approaches for robotics systems.",AI4;AI4-01,benchmarking;robotics,framework,Python,https://github.com/robotperf/benchmarks,https://robotperf.org,Apache-2.0,robotics;benchmark;performance
271,YAIB,Yet Another ICU Benchmark for clinical prediction models,"YAIB is a holistic framework for standardizing clinical prediction model experiments, providing datasets, cohorts, tasks, and preprocessing pipelines for ICU data.",AI4;AI4-01,benchmarking;clinical_prediction,framework,Python,https://github.com/rvandewater/YAIB,https://arxiv.org/abs/2306.05109,MIT,clinical-ml;benchmark;icu
272,RXN Reaction Preprocessing,Preprocessing library for chemical reaction datasets,"A Python library for the standardization, filtering, augmentation, and tokenization of chemical reaction datasets, developed by IBM Research for chemical AI tasks.",AI4;AI4-01,data_processing;chemistry,library,Python,https://github.com/rxn4chemistry/rxn-reaction-preprocessing,,MIT,chemistry;preprocessing;reaction-prediction
273,DialogStudio,Unified dataset collection and instruction-aware models for conversational AI,"A large-scale collection of unified datasets for conversational AI, designed to facilitate research in dialogue systems and instruction tuning.",AI4;AI4-01,dataset_collection;conversational_ai,dataset,Python,https://github.com/salesforce/DialogStudio,,Apache-2.0,nlp;dialogue-systems;dataset
274,RePlay,Framework for building and evaluating recommendation systems,"A comprehensive framework for building, training, and evaluating recommendation systems, supporting various state-of-the-art models and data preprocessing pipelines.",AI4;AI4-01,recommendation_system;model_evaluation,framework,Python,https://github.com/sb-ai-lab/RePlay,https://replay.readthedocs.io/en/latest/,Apache-2.0,recsys;pytorch;evaluation
275,GAP Benchmark Suite,Benchmark suite for graph processing algorithms,"A benchmark suite designed to standardize the evaluation of graph processing algorithms and systems, providing reference implementations and datasets.",AI4;AI4-01,graph_processing;performance_benchmarking,benchmark,C++,https://github.com/sbeamer/gapbs,http://gap.cs.berkeley.edu/benchmark.html,BSD-3-Clause,graph-algorithms;hpc;benchmark
276,SciCode,Benchmark for code generation in scientific domains,A benchmark designed to evaluate the capability of language models to generate code for solving scientific problems across various disciplines.,AI4;AI4-01,code_generation;model_evaluation,benchmark,Python,https://github.com/scicode-bench/SciCode,,Apache-2.0,llm;scientific-computing;benchmark
277,TorchSpatial,Framework and benchmark for Spatial Representation Learning,"A comprehensive framework and benchmark suite designed to advance Spatial Representation Learning (SRL), providing tools for modeling and evaluating spatial data.",AI4;AI4-01,spatial_representation_learning;model_evaluation,framework,Python,https://github.com/seai-lab/TorchSpatial,,MIT,spatial-data;representation-learning;benchmark
278,selva86/datasets,Collection of standard datasets for machine learning,"A repository hosting a variety of standard datasets commonly used for machine learning problems, facilitating easy access for experimentation and benchmarking.",AI4;AI4-01,dataset_collection,dataset,R,https://github.com/selva86/datasets,,MIT,machine-learning;datasets;statistics
279,numpy-benchmarks,Collection of scientific kernels for NumPy benchmarking,"A set of scientific computing kernels implemented using NumPy, designed for benchmarking and performance analysis of numerical operations.",AI4;AI4-01,performance_benchmarking;numerical_computing,benchmark,Python,https://github.com/serge-sans-paille/numpy-benchmarks,,BSD-3-Clause,numpy;hpc;benchmark
280,gnn-benchmark,Benchmarking framework for Graph Neural Networks,"A framework for evaluating Graph Neural Network (GNN) models on semi-supervised node classification tasks, ensuring fair and reproducible comparisons.",AI4;AI4-01,graph_neural_networks;model_evaluation,benchmark,Python,https://github.com/shchur/gnn-benchmark,,MIT,gnn;graph-learning;benchmark
281,ChemBench,Benchmark datasets for molecular machine learning,"A collection of benchmark datasets including MoleculeNet and MolMapNet, designed for evaluating machine learning models in chemistry and drug discovery.",AI4;AI4-01,molecular_modeling;dataset_collection,dataset,HTML,https://github.com/shenwanxiang/ChemBench,,MIT,cheminformatics;moleculenet;benchmark
282,DataRec,Library for reproducible data management in recommender systems,A Python library designed to standardize and ensure reproducibility in data management and preprocessing for recommender system research.,AI4;AI4-01,data_management;reproducibility,library,Python,https://github.com/sisinflab/DataRec,,MIT,recsys;data-processing;reproducibility
283,Elliot,Framework for reproducible recommender systems evaluation,"A comprehensive and rigorous framework for the evaluation of recommender systems, focusing on reproducibility and standardized metrics.",AI4;AI4-01,model_evaluation;recommendation_system,framework,Python,https://github.com/sisinflab/elliot,https://elliot.readthedocs.io/en/latest/,Apache-2.0,recsys;evaluation;benchmark
284,Seamless,Framework for reproducible and interactive computations,"A framework to set up reproducible computations and visualizations that respond to changes in data or code, suitable for scientific workflows.",AI4;AI4-01,workflow_management;reproducibility,workflow,Python,https://github.com/sjdv1982/seamless,http://sjdv1982.github.io/seamless/,MIT,reproducibility;workflow;interactive-computing
285,ZSC-Eval,Evaluation toolkit for multi-agent zero-shot coordination,An evaluation toolkit and benchmark designed for assessing multi-agent reinforcement learning agents in zero-shot coordination scenarios.,AI4;AI4-01,multi_agent_rl;model_evaluation,benchmark,JavaScript,https://github.com/sjtu-marl/ZSC-Eval,,MIT,marl;reinforcement-learning;benchmark
286,SpeechLLM,Training and evaluation framework for SpeechLLM models,"A repository containing code for training, inference, and evaluation of SpeechLLM models, facilitating research in speech-language modeling.",AI4;AI4-01,speech_processing;model_training;evaluation,library,Python,https://github.com/skit-ai/SpeechLLM,,Apache-2.0,speech-recognition;llm;audio-processing
287,matbench-genmetrics,Metrics for generative materials benchmarking,"A Python library providing benchmarking metrics for generative models in materials science, inspired by Guacamol and CDVAE.",AI4;AI4-01,materials_science;generative_models;metrics,library,Jupyter Notebook,https://github.com/sparks-baird/matbench-genmetrics,,MIT,materials-informatics;generative-ai;benchmark
288,SeBS,Serverless benchmarking suite for performance analysis,"A benchmarking suite for automatic performance analysis of FaaS platforms, useful for systems research and infrastructure evaluation.",AI4;AI4-01,performance_benchmarking;systems_research,benchmark,Python,https://github.com/spcl/serverless-benchmarks,,BSD-3-Clause,serverless;benchmark;faas
289,HELM,Holistic Evaluation of Language Models framework,"A framework for holistic, reproducible, and transparent evaluation of foundation models, including LLMs, covering a wide range of metrics and scenarios.",AI4;AI4-01,model_evaluation;llm_benchmarking,framework,Python,https://github.com/stanford-crfm/helm,https://crfm.stanford.edu/helm/latest/,Apache-2.0,llm;evaluation;benchmark
290,SciML Bench,Benchmarking suite for AI for Science,A benchmarking suite specifically designed for evaluating AI models and methods in scientific domains (AI for Science).,AI4;AI4-01,ai4science;model_evaluation,benchmark,Python,https://github.com/stfc-sciml/sciml-bench,,MIT,sciml;benchmark;scientific-computing
291,BIG-Bench-Hard,Challenging subset of BIG-Bench tasks for LLMs,"A collection of challenging tasks from the BIG-Bench suite, designed to evaluate the reasoning capabilities of large language models.",AI4;AI4-01,model_evaluation;reasoning,benchmark,Python,https://github.com/suzgunmirac/BIG-Bench-Hard,,MIT,llm;benchmark;reasoning
292,tdoku,Fast Sudoku solver and benchmark suite,"A highly optimized Sudoku solver and generator, including a benchmark suite for comparing solver performance, relevant for constraint satisfaction research.",AI4;AI4-01,constraint_satisfaction;solver_benchmarking,solver,C++,https://github.com/t-dillon/tdoku,,BSD-2-Clause,sudoku;optimization;benchmark
293,NPF,Network Performance Framework for automated experiments,"An experiment manager for network performance testing, providing automated execution, result collection, and graphing for reproducible research.",AI4;AI4-01,network_experiments;performance_analysis,workflow,Python,https://github.com/tbarbette/npf,https://npf.readthedocs.io/,GPL-3.0,networking;reproducibility;benchmark
294,bigint-benchmark-rs,Benchmarks for Rust big integer implementations,"A benchmarking suite for comparing the performance of various big integer arithmetic implementations in Rust, useful for cryptography and number theory research.",AI4;AI4-01,numerical_computing;performance_benchmarking,benchmark,Rust,https://github.com/tczajka/bigint-benchmark-rs,,MIT,rust;bigint;benchmark
295,GPTeacher,Modular datasets generated by GPT-4 for instruction tuning,"A collection of datasets generated by GPT-4, including General-Instruct, Roleplay-Instruct, Code-Instruct, and Toolformer, for training and evaluating models.",AI4;AI4-01,dataset_collection;instruction_tuning,dataset,Python,https://github.com/teknium1/GPTeacher,,MIT,llm;dataset;synthetic-data
296,TensorFlow Datasets,Collection of ready-to-use datasets for TensorFlow,"A library of datasets ready to use with TensorFlow, JAX, and other machine learning frameworks, handling downloading and preparing data.",AI4;AI4-01,dataset_access;data_loading,library,Python,https://github.com/tensorflow/datasets,https://www.tensorflow.org/datasets,Apache-2.0,tensorflow;datasets;machine-learning
297,Safety-Prompts,Chinese safety prompts dataset for evaluating LLM safety,"A collection of Chinese safety prompts designed to evaluate and improve the safety of Large Language Models (LLMs), covering various safety categories.",AI4;AI4-01,safety_evaluation;benchmark_dataset,dataset,No Code,https://github.com/thu-coai/Safety-Prompts,,Apache-2.0,llm-safety;benchmark;prompts
298,TransformerLab,Desktop application for LLM engineering and evaluation,"An open-source application designed for advanced Large Language Model (LLM) engineering, enabling users to interact with, train, fine-tune, and evaluate models locally.",AI4;AI4-01,model_evaluation;fine_tuning;training_platform,platform,Python,https://github.com/transformerlab/transformerlab-app,https://transformerlab.ai,AGPL-3.0,llm;evaluation;fine-tuning;desktop-app
299,uci_datasets,Python library for loading standardized UCI regression datasets,A Python package that provides easy access to regression datasets from the UCI Machine Learning Repository with standardized train-test splits for benchmarking.,AI4;AI4-01,dataset_loading;benchmark_data,dataset,Python,https://github.com/treforevans/uci_datasets,,MIT,uci;datasets;regression;benchmark
300,TruLens,Evaluation and tracking library for LLM experiments,"A library for evaluating and tracking Large Language Model (LLM) experiments and AI agents, providing feedback functions to measure performance and quality.",AI4;AI4-01,model_evaluation;experiment_tracking;metrics,library,Python,https://github.com/truera/trulens,https://www.trulens.org,MIT,llm-evaluation;observability;metrics
301,ApeBench,Benchmark suite for autoregressive neural emulation of PDEs,"A comprehensive benchmark suite for evaluating autoregressive neural emulators for Partial Differential Equations (PDEs), covering 1D, 2D, and 3D problems with differentiable physics metrics.",AI4;AI4-01,pde_solving;neural_operator_benchmark;scientific_machine_learning,solver,Python,https://github.com/tum-pbs/apebench,,MIT,pde;benchmark;neural-operators;physics-ml
302,Petastorm,Library for deep learning data access from Apache Parquet,"A library enabling single-machine or distributed training and evaluation of deep learning models directly from datasets in Apache Parquet format, supporting TensorFlow and PyTorch.",AI4;AI4-01,data_loading;training_infrastructure,library,Python,https://github.com/uber/petastorm,,Apache-2.0,parquet;data-loading;deep-learning
303,RADIal,High-definition radar dataset for multi-task learning,"A raw high-definition radar dataset designed for multi-task learning in autonomous driving, including object detection and segmentation tasks.",AI4;AI4-01,autonomous_driving_benchmark;radar_perception,dataset,Jupyter Notebook,https://github.com/valeoai/RADIal,,No Assertion,radar;dataset;autonomous-driving
304,Whisper Finetune,Tools for fine-tuning and evaluating Whisper ASR models,A collection of scripts and tools to fine-tune and evaluate OpenAI's Whisper models for Automatic Speech Recognition (ASR) on custom or Hugging Face datasets.,AI4;AI4-01,model_finetuning;asr_evaluation,workflow,Python,https://github.com/vasistalodagala/whisper-finetune,,MIT,whisper;asr;fine-tuning
305,SHOC,Scalable Heterogeneous Computing Benchmark Suite,"A benchmark suite designed to test the performance and stability of scalable heterogeneous computing systems, including GPUs and multi-core processors, for scientific workloads.",AI4;AI4-01,hpc_benchmark;system_evaluation,solver,Makefile,https://github.com/vetter/shoc,,No Assertion,hpc;benchmark;gpu;opencl;cuda
306,Ragas,Evaluation framework for RAG pipelines,"A framework for evaluating Retrieval Augmented Generation (RAG) pipelines, providing metrics to assess the faithfulness, answer relevance, and context retrieval quality of LLM applications.",AI4;AI4-01,rag_evaluation;llm_metrics,library,Python,https://github.com/vibrantlabsai/ragas,https://docs.ragas.io,Apache-2.0,rag;evaluation;llm;metrics
307,Rdatasets,Collection of standard datasets from R packages,"A comprehensive collection of datasets originally distributed in R packages, made available as CSV files for use in other data analysis environments and benchmarks.",AI4;AI4-01,dataset_collection;benchmark_data,dataset,HTML,https://github.com/vincentarelbundock/Rdatasets,https://vincentarelbundock.github.io/Rdatasets,No Assertion,datasets;r;csv;benchmark
308,WfCommons,Framework for scientific workflow research and trace generation,"A framework for enabling research and development in scientific workflows, providing tools to generate synthetic workflow traces and analyze workflow execution logs.",AI4;AI4-01,workflow_research;trace_generation;simulation,library,Python,https://github.com/wfcommons/WfCommons,https://wfcommons.org,LGPL-3.0,scientific-workflows;simulation;benchmark
309,PhaseLLM,Framework for LLM evaluation and workflow management,"A framework designed to streamline the evaluation and workflow management of Large Language Models (LLMs), helping developers build more robust AI applications.",AI4;AI4-01,model_evaluation;workflow_management,library,Python,https://github.com/wgryc/phasellm,,MIT,llm;evaluation;workflow
310,whylogs,Data logging and quality monitoring library for ML,"An open-source library for logging data profiles, enabling data quality monitoring, model performance tracking, and drift detection in machine learning pipelines.",AI4;AI4-01,data_quality;model_monitoring;logging,library,Jupyter Notebook,https://github.com/whylabs/whylogs,https://whylabs.ai/whylogs,Apache-2.0,data-quality;mlops;monitoring
311,ControlGym,Benchmark environments for reinforcement learning in control,"A large-scale benchmark suite of control environments for evaluating Reinforcement Learning algorithms, focusing on control theory and dynamic systems.",AI4;AI4-01,rl_benchmark;control_systems,library,Python,https://github.com/xiangyuan-zhang/controlgym,,MIT,reinforcement-learning;control-theory;benchmark
312,Multiphysics-Bench,Benchmark for scientific machine learning on multiphysics PDEs,A benchmark suite for investigating and evaluating Scientific Machine Learning (SciML) methods for solving multiphysics Partial Differential Equations (PDEs).,AI4;AI4-01,pde_benchmark;sciml;multiphysics,solver,Python,https://github.com/xie-lab-ml/multiphysics-bench,,MIT,sciml;pde;benchmark;multiphysics
313,GAN-Metrics,Collection of evaluation metrics for GAN models,A Python library implementing various metrics for evaluating the performance and quality of Generative Adversarial Networks (GANs).,AI4;AI4-01,gan_evaluation;metrics,library,Python,https://github.com/yhlleo/GAN-Metrics,,No Assertion,gan;metrics;evaluation
314,RaLLe,Framework for developing and evaluating Retrieval-Augmented Large Language Models (RAG),"RaLLe is a framework designed to facilitate the development and evaluation of Retrieval-Augmented Generation (RAG) systems for Large Language Models. It provides tools for assessing the performance of RAG pipelines, ensuring the reliability and accuracy of retrieved information in AI-driven workflows.",AI4;AI4-01,model_evaluation;rag_framework,library,Python,https://github.com/yhoshi3/RaLLe,,MIT,rag;llm;evaluation;benchmark
315,MGBench,Benchmark suite for molecular glue ternary structure prediction methods,MGBench is a benchmarking tool specifically designed for evaluating cofolding methods used in molecular glue ternary structure prediction. It provides a standardized environment and datasets to assess the accuracy and performance of computational models in drug discovery and structural biology contexts.,AI4;AI4-01,structure_prediction;molecular_modeling,library,Jupyter Notebook,https://github.com/yiyanliao/MGBench,,MIT,molecular-glue;protein-folding;drug-discovery;benchmark
316,torchdistill,Reproducible deep learning framework for knowledge distillation,"torchdistill is a coding-free framework built on PyTorch designed to facilitate reproducible deep learning studies, specifically focusing on knowledge distillation. It offers implementations of numerous state-of-the-art distillation methods and provides configurations to ensure consistent benchmarking and experimental reproduction.",AI4;AI4-01,model_optimization;reproducibility,library,Python,https://github.com/yoshitomo-matsubara/torchdistill,https://torchdistill.readthedocs.io/,MIT,knowledge-distillation;pytorch;reproducibility;deep-learning
317,EasyLM,"One-stop solution for pre-training, finetuning, and serving LLMs using JAX/Flax","EasyLM is a comprehensive library for Large Language Models (LLMs) built on JAX and Flax. It simplifies the process of pre-training, fine-tuning, evaluating, and serving LLMs, providing a scalable and efficient toolchain for AI model development.",AI4;AI4-01,model_training;inference,library,Python,https://github.com/young-geng/EasyLM,,Apache-2.0,llm;jax;flax;model-serving
318,MT-Consistency,Benchmark for evaluating acquiescence bias and consistency in LLMs,MT-Consistency is a research framework and benchmark designed to investigate and mitigate acquiescence bias in Large Language Models during sequential QA interactions. It includes datasets and evaluation scripts to assess the robustness and conversational consistency of AI models.,AI4;AI4-01,model_evaluation;bias_analysis,library,Python,https://github.com/yubol-bobo/MT-Consistency,,MIT,llm;bias;consistency;benchmark
319,fret,Framework for Reproducible ExperimenTs in scientific computing,"fret is a lightweight framework designed to simplify the management and reproduction of scientific experiments. It provides tools to configure, run, and track experiments, ensuring that research results are reproducible and manageable.",AI4;AI4-01,experiment_management;reproducibility,workflow,Python,https://github.com/yxonic/fret,https://fret.readthedocs.io/,MIT,reproducibility;experiment-tracking;workflow
320,MedSegBench,Standardized benchmark collection for medical image segmentation,MedSegBench is a comprehensive benchmarking library that provides access to standardized medical segmentation datasets across various modalities. It facilitates the evaluation and comparison of medical image segmentation models by offering a unified interface for data loading and processing.,AI4;AI4-01,image_segmentation;benchmarking,library,Python,https://github.com/zekikus/MedSegBench,,Apache-2.0,medical-imaging;segmentation;benchmark;datasets
321,dabestr,Data analysis tool using bootstrap estimation for robust statistical inference,"A package for Data Analysis with Bootstrap-coupled ESTimation (DABEST). It provides a user-friendly interface to calculate and visualize effect sizes and their confidence intervals using bootstrap methods, moving away from traditional significance testing.",AI4;AI4-02,statistical_analysis;estimation;visualization,library,R,https://github.com/ACCLAB/dabestr,,Apache-2.0,statistics;bootstrap;estimation-plots;r-package
322,continual_rl,Baselines and metrics library for continual reinforcement learning,"A repository containing baselines, experiment specifications, and common metrics for continual reinforcement learning. It is designed to be easily extensible for new methods and facilitates reproducible research in continual RL.",AI4;AI4-02,reinforcement_learning;benchmarking;metrics,library,Python,https://github.com/AGI-Labs/continual_rl,,MIT,reinforcement-learning;continual-learning;benchmarks;metrics
323,MTT,Bayesian multi-target tracking algorithms and evaluation metrics,"Implementation of several Bayesian multi-target tracking algorithms, including Poisson multi-Bernoulli mixture filters. It specifically includes implementations of GOSPA and T-GOSPA metrics for evaluating tracking performance.",AI4;AI4-02,object_tracking;evaluation_metrics;bayesian_filtering,library,MATLAB,https://github.com/Agarciafernandez/MTT,,BSD-2-Clause,multi-target-tracking;bayesian-methods;gospa;metrics
324,eval4imagecaption,Evaluation metrics toolkit for image captioning tasks,"A collection of evaluation tools for image captioning, implementing standard metrics including BLEU, ROUGE-L, CIDEr, METEOR, and SPICE scores to assess the quality of generated captions.",AI4;AI4-02,image_captioning;evaluation_metrics;nlp_metrics,library,Python,https://github.com/Aldenhovel/bleu-rouge-meteor-cider-spice-eval4imagecaption,,MIT,image-captioning;bleu;rouge;cider;meteor
325,GuardBench,Evaluation library for guardrail models in AI safety,"A Python library designed for the evaluation of guardrail models, facilitating the assessment of safety and compliance mechanisms in AI systems.",AI4;AI4-02,model_evaluation;ai_safety;benchmarking,library,Python,https://github.com/AmenRa/GuardBench,,EUPL-1.2,guardrails;evaluation;ai-safety;benchmarking
326,ttest,JavaScript library for performing Student's t hypothesis tests,"A lightweight JavaScript library to perform the Student's t hypothesis test, enabling statistical analysis directly within JavaScript environments.",AI4;AI4-02,statistical_testing;hypothesis_testing,library,JavaScript,https://github.com/AndreasMadsen/ttest,,MIT,statistics;t-test;hypothesis-testing;javascript
327,rexmex,General purpose recommender systems metrics library,"A general-purpose library for evaluating recommender systems, providing a comprehensive set of metrics to ensure fair and standardized evaluation of recommendation algorithms.",AI4;AI4-02,recommender_systems;evaluation_metrics,library,Python,https://github.com/AstraZeneca/rexmex,,None,recommender-systems;metrics;evaluation;data-science
328,SORDI-AI-Evaluation-GUI,GUI tool for evaluating computer vision models on SORDI datasets,"A graphical user interface tool developed by BMW Innovation Lab to evaluate trained computer vision models, specifically designed to work with the SORDI dataset ecosystem, providing general information and evaluation metrics.",AI4;AI4-02,model_evaluation;computer_vision;metrics_visualization,solver,Python,https://github.com/BMW-InnovationLab/SORDI-AI-Evaluation-GUI,,Apache-2.0,computer-vision;evaluation;gui;sordi
329,scikit-llm,Integration of Large Language Models into scikit-learn framework,"A library that seamlessly integrates Large Language Models (LLMs) into the scikit-learn framework, allowing users to utilize LLMs for text analysis tasks (like classification and embedding) using the familiar sklearn API.",AI4,text_analysis;modeling;nlp,library,Python,https://github.com/BeastByteAI/scikit-llm,,MIT,scikit-learn;llm;nlp;text-classification
330,madmom,Python audio and music signal processing library,"A comprehensive Python library for audio and music signal processing. It includes tools for feature extraction, beat tracking, onset detection, and other music information retrieval (MIR) tasks.",AI4,signal_processing;audio_analysis;feature_extraction,library,Python,https://github.com/CPJKU/madmom,,NOASSERTION,audio-processing;music-information-retrieval;signal-processing;mir
331,mAP,Evaluation code for object detection neural networks,A widely used tool to calculate the mean Average Precision (mAP) for object detection models. It provides a standardized way to evaluate the performance of neural networks on detection tasks.,AI4;AI4-02,object_detection;evaluation_metrics;performance_analysis,solver,Python,https://github.com/Cartucho/mAP,,Apache-2.0,computer-vision;object-detection;map;metrics
332,pymar,Markov Switching Models extension for Statsmodels,"A Python library that implements Markov Switching Models, designed to work with and extend the capabilities of the Statsmodels library for time series analysis.",AI4;AI4-02,time_series_analysis;statistical_modeling;markov_models,library,Python,https://github.com/ChadFulton/pymar,,None,statsmodels;time-series;markov-switching;statistics
333,metric-learning-divide-and-conquer,Implementation of Divide and Conquer the Embedding Space for Metric Learning,"A PyTorch implementation of the 'Divide and Conquer the Embedding Space for Metric Learning' (CVPR 2019) approach, providing a solver for deep metric learning tasks by partitioning the embedding space.",AI4;AI4-01,metric_learning;embedding_learning,solver,Python,https://github.com/CompVis/metric-learning-divide-and-conquer,,LGPL-3.0,metric-learning;computer-vision;embedding
334,Deep-Metric-Learning-Baselines,PyTorch implementation for Deep Metric Learning pipelines and baselines,"A comprehensive library providing implementations of various deep metric learning algorithms and baselines in PyTorch, facilitating benchmarking and reproduction of metric learning experiments.",AI4;AI4-01,metric_learning;benchmarking,library,Python,https://github.com/Confusezius/Deep-Metric-Learning-Baselines,,Apache-2.0,deep-metric-learning;pytorch;benchmark
335,Revisiting_Deep_Metric_Learning_PyTorch,Code for benchmarking training strategies in Deep Metric Learning,"The official repository for the ICML 2020 paper 'Revisiting Training Strategies and Generalization Performance in Deep Metric Learning', providing a framework for consistent research and evaluation in deep metric learning.",AI4;AI4-01,metric_learning;evaluation,solver,Python,https://github.com/Confusezius/Revisiting_Deep_Metric_Learning_PyTorch,https://arxiv.org/abs/2002.08473,MIT,metric-learning;generalization;icml-2020
336,Avalanche,End-to-End Library for Continual Learning based on PyTorch,"A comprehensive library for Continual Learning (CL) research, providing benchmarks, metrics, and evaluation protocols to assess the performance of CL algorithms.",AI4;AI4-02,continual_learning;evaluation_metrics,library,Python,https://github.com/ContinualAI/avalanche,https://avalanche.continualai.org,MIT,continual-learning;pytorch;benchmarks
337,Yellowbrick,Visual analysis and diagnostic tools for machine learning model selection,"A suite of visual analysis and diagnostic tools designed to facilitate machine learning model selection, enabling data scientists to steer the model selection process.",AI4;AI4-02,model_visualization;model_selection;diagnostics,library,Python,https://github.com/DistrictDataLabs/yellowbrick,https://www.scikit-yb.org,Apache-2.0,visualization;machine-learning;model-selection
338,ESMValTool,Community diagnostic and performance metrics tool for Earth system models,"A community diagnostic and performance metrics tool for routine evaluation of Earth system models in CMIP, facilitating the analysis of climate data and model performance.",AI4;AI4-02,climate_model_evaluation;diagnostics;earth_system_modeling,platform,NCL,https://github.com/ESMValGroup/ESMValTool,https://esmvaltool.org,Apache-2.0,climate-science;cmip;model-evaluation
339,BlonDe,Automatic Evaluation Metric for Document-level Machine Translation,"Official implementation of BlonDe, an automatic evaluation metric designed for document-level machine translation, focusing on discourse-related phenomena.",AI4;AI4-02,machine_translation_evaluation;nlp_metrics,library,Python,https://github.com/EleanorJiang/BlonDe,,MIT,nlp;metrics;machine-translation
340,TPOT,Automated Machine Learning tool using genetic programming,"A Python Automated Machine Learning (AutoML) tool that optimizes machine learning pipelines using genetic programming to automate the process of feature selection, model selection, and parameter tuning.",AI4;AI4-01,automl;pipeline_optimization,solver,Python,https://github.com/EpistasisLab/tpot,http://epistasislab.github.io/tpot/,LGPL-3.0,automl;genetic-programming;machine-learning
341,LanguageGuidance_for_DML,Integrating Language Guidance into Vision-based Deep Metric Learning,"Implementation of a method for integrating language guidance into vision-based deep metric learning, as presented in CVPR 2022.",AI4;AI4-01,metric_learning;multimodal_learning,solver,Python,https://github.com/ExplainableML/LanguageGuidance_for_DML,,Apache-2.0,deep-metric-learning;vision-language;cvpr-2022
342,Dolmen,"Library to parse, typecheck, and evaluate automated deduction languages","A library and binary tool to parse, typecheck, and evaluate various languages used in automated deduction and formal verification.",AI4;AI4-01,automated_deduction;parsing;formal_methods,library,OCaml,https://github.com/Gbury/dolmen,,BSD-2-Clause,automated-deduction;parsing;logic
343,PixTrack,Object Tracking using NeRF templates and feature-metric alignment,A Computer Vision method for Object Tracking which uses NeRF templates and feature-metric alignment to robustly track the 6DoF pose of a known object.,AI4;AI4-01,object_tracking;nerf;pose_estimation,solver,Jupyter Notebook,https://github.com/GiantAI/pixtrack,,NOASSERTION,computer-vision;nerf;tracking
344,TextDescriptives,Library for calculating a large variety of metrics from text,"A Python library for calculating a wide range of text metrics, including readability, complexity, and other linguistic features, useful for NLP analysis.",AI4;AI4-02,text_metrics;nlp_analysis;readability,library,Python,https://github.com/HLasse/TextDescriptives,,Apache-2.0,nlp;metrics;text-analysis
345,MM-SHAP,Metric for Measuring Multimodal Contributions in Vision and Language Models,"Official implementation of MM-SHAP, a performance-agnostic metric for measuring multimodal contributions in vision and language models and tasks.",AI4;AI4-02,model_interpretability;multimodal_metrics,library,Python,https://github.com/Heidelberg-NLP/MM-SHAP,,MIT,shap;multimodal;interpretability
346,Few-shot-via-ensembling-Transformer-with-Mahalanobis-distance,Few-Shot Bearing Fault Diagnosis via Ensembling Transformer and Mahalanobis Distance,Implementation of a few-shot bearing fault diagnosis method using an ensembling Transformer-based model with Mahalanobis distance metric learning.,AI4;AI4-01,fault_diagnosis;metric_learning;few_shot_learning,solver,Python,https://github.com/HungVu307/Few-shot-via-ensembling-Transformer-with-Mahalanobis-distance,,None,fault-diagnosis;transformer;metric-learning
347,MUTIS,Analysis of correlations of light curves and their statistical significance,A Python package developed by IAA-CSIC for the analysis of correlations of light curves and their statistical significance in astrophysical data.,AI4;AI4-02,time_series_analysis;correlation_analysis;astrophysics,library,Python,https://github.com/IAA-CSIC/MUTIS,,BSD-3-Clause,astronomy;statistics;light-curves
348,transition-amr-parser,AMR parsing with word-node alignments and Smatch metrics,"State-of-the-Art Abstract Meaning Representation (AMR) parsing tool with word-node alignments, including tools for statistical significance testing using Smatch.",AI4;AI4-02,amr_parsing;nlp_evaluation;smatch_metric,solver,Python,https://github.com/IBM/transition-amr-parser,,Apache-2.0,nlp;amr;parsing
349,ImagePy,Image processing framework based on plugins,"An image processing framework based on a plugin architecture (similar to ImageJ), designed to easily integrate with Scipy, Scikit-image, OpenCV, and other Numpy-based libraries for scientific image analysis.",AI4;AI4-01,image_processing;scientific_imaging,platform,Python,https://github.com/Image-Py/imagepy,,BSD-4-Clause,image-processing;microscopy;scientific-imaging
350,TrackEval,Evaluation metrics library for Multi-Object Tracking (MOT),"A Python library providing implementations of various evaluation metrics for Multi-Object Tracking (MOT), including HOTA, CLEAR, and Identity metrics.",AI4;AI4-02,model_evaluation;metrics_calculation,library,Python,https://github.com/JonathonLuiten/TrackEval,,MIT,mot;evaluation-metrics;computer-vision
351,Distances.jl,Julia package for evaluating distances and metrics between vectors,"A Julia package that provides a collection of distance metrics and divergence measures for evaluating the similarity or dissimilarity between vectors, essential for statistical analysis and machine learning.",AI4;AI4-02,metrics_calculation;statistical_analysis,library,Julia,https://github.com/JuliaStats/Distances.jl,,NOASSERTION,julia;distance-metrics;statistics
352,HypothesisTests.jl,Hypothesis testing library for Julia,"A comprehensive Julia package for conducting various statistical hypothesis tests, including t-tests, chi-squared tests, and non-parametric tests.",AI4;AI4-02,statistical_testing;hypothesis_testing,library,Julia,https://github.com/JuliaStats/HypothesisTests.jl,,MIT,julia;statistics;hypothesis-testing
353,StatsModels.jl,Statistical model specification and fitting in Julia,"A Julia package for specifying, fitting, and evaluating statistical models using a formula-based interface similar to R, facilitating statistical analysis and inference.",AI4;AI4-02,statistical_modeling;model_evaluation,library,Julia,https://github.com/JuliaStats/StatsModels.jl,,None,julia;statistical-modeling;data-analysis
354,common_metrics_on_video_quality,Video quality evaluation metrics calculator,"A Python toolkit to calculate common video quality metrics such as FVD, PSNR, SSIM, and LPIPS for evaluating generated or predicted videos.",AI4;AI4-02,quality_control;metrics_calculation,library,Python,https://github.com/JunyaoHu/common_metrics_on_video_quality,,None,video-quality;fvd;psnr;ssim
355,deep-significance,Statistical significance testing for deep neural networks,"A Python tool designed to facilitate statistical significance testing specifically for comparing deep neural network models, ensuring robust performance evaluation.",AI4;AI4-02,statistical_testing;model_evaluation,library,Python,https://github.com/Kaleidophon/deep-significance,,GPL-3.0,deep-learning;significance-testing;statistics
356,DocumentFeatureSelection,Metrics for feature selection from text data,"A Python library providing a set of metrics and methods for selecting features from text data, aiding in NLP model optimization and analysis.",AI4;AI4-02,feature_selection;metrics_calculation,library,Python,https://github.com/Kensuke-Mitsuzawa/DocumentFeatureSelection,,NOASSERTION,nlp;feature-selection;text-mining
357,pytorch-metric-learning,Deep metric learning library for PyTorch,"A comprehensive and modular PyTorch library for deep metric learning, providing loss functions, miners, and trainers to learn distance metrics for various applications.",AI4;AI4-01;AI4-02,metric_learning;model_training,library,Python,https://github.com/KevinMusgrave/pytorch-metric-learning,,MIT,pytorch;metric-learning;deep-learning
358,YACHT,Hypothesis test for organism presence in metagenomes,"A mathematically characterized hypothesis testing tool for determining the presence or absence of organisms in metagenomic samples, enabling scalable and accurate metagenome profiling.",AI4;AI4-02,hypothesis_testing;metagenomics,solver,C++,https://github.com/KoslickiLab/YACHT,,MIT,bioinformatics;metagenomics;hypothesis-testing
359,FPChecker,Floating-point error detection for HPC applications,A dynamic analysis tool developed by LLNL to detect floating-point errors and instabilities in High-Performance Computing (HPC) applications.,AI4;AI4-02,quality_control;numerical_analysis,solver,Python,https://github.com/LLNL/FPChecker,,Apache-2.0,hpc;floating-point;debugging
360,torchmetrics,Machine learning metrics for PyTorch,"A collection of machine learning metrics for distributed, scalable PyTorch applications, offering implementations of common metrics for classification, regression, and more.",AI4;AI4-02,metrics_calculation;model_evaluation,library,Python,https://github.com/Lightning-AI/torchmetrics,,Apache-2.0,pytorch;metrics;machine-learning
361,MAC-VO,Metrics-aware Covariance for Stereo Visual Odometry,A learning-based Stereo Visual Odometry solver that incorporates metrics-aware covariance estimation to improve trajectory accuracy and reliability.,AI4;AI4-01;AI4-02,visual_odometry;state_estimation,solver,Python,https://github.com/MAC-VO/MAC-VO,,MIT,visual-odometry;robotics;covariance-estimation
362,SparseOcc,Sparse 3D Occupancy Prediction and Evaluation,A fully sparse 3D occupancy prediction method that includes the RayIoU evaluation metric for assessing geometric accuracy in 3D vision tasks.,AI4;AI4-01;AI4-02,occupancy_prediction;model_evaluation,solver,Python,https://github.com/MCG-NJU/SparseOcc,,Apache-2.0,3d-vision;occupancy-prediction;evaluation-metric
363,opinionated,Clean stylesheets for scientific plotting,"A Python library providing opinionated, clean stylesheets for matplotlib and seaborn to create publication-quality scientific plots.",AI4;AI4-02,visualization;plotting,library,Jupyter Notebook,https://github.com/MNoichl/opinionated,,MIT,visualization;matplotlib;seaborn
364,dcurves,Decision Curve Analysis for prediction models,"A Python package for performing Decision Curve Analysis (DCA), a method to evaluate prediction models and diagnostic tests by calculating net benefit.",AI4;AI4-02,model_evaluation;decision_analysis,library,Python,https://github.com/MSKCC-Epi-Bio/dcurves,,Apache-2.0,decision-curve-analysis;statistics;medical-statistics
365,pcr,Analysis and statistical testing of qPCR data,"An R package for quality assessment, analysis, and statistical significance testing of real-time quantitative PCR (qPCR) data.",AI4;AI4-02,statistical_analysis;bioinformatics,library,R,https://github.com/MahShaaban/pcr,,GPL-3.0,qpcr;bioinformatics;statistics
366,nlg-eval,Evaluation metrics for Natural Language Generation,A Python library containing code for various unsupervised automated metrics for evaluating Natural Language Generation (NLG) systems.,AI4;AI4-02,model_evaluation;metrics_calculation,library,Python,https://github.com/Maluuba/nlg-eval,,NOASSERTION,nlg;evaluation-metrics;nlp
367,nervaluate,Named Entity Recognition evaluation metrics,"A Python library for full named-entity evaluation metrics based on SemEval-2013 standards, allowing for detailed performance analysis of NER models.",AI4;AI4-02,model_evaluation;metrics_calculation,library,Python,https://github.com/MantisAI/nervaluate,,MIT,ner;evaluation-metrics;nlp
368,localization_evaluation_toolkit,Toolkit for localization evaluation in robotics,"A toolkit for evaluating localization algorithms, supporting CSV and ROS 2 bag formats, and providing error analysis and NDT performance metrics.",AI4;AI4-02,model_evaluation;robotics_localization,library,Python,https://github.com/MapIV/localization_evaluation_toolkit,,None,localization;robotics;evaluation
369,volkscv,Computer vision research toolbox,"A Python toolbox designed to facilitate computer vision research and projects, providing common utilities and implementations.",AI4;AI4-01;AI4-02,image_processing;model_evaluation,library,Python,https://github.com/Media-Smart/volkscv,,Apache-2.0,computer-vision;toolbox;research
370,evo,Evaluation package for odometry and SLAM,"A Python package for the evaluation of odometry and SLAM trajectories, providing tools for trajectory alignment, error calculation, and visualization.",AI4;AI4-02,model_evaluation;slam_evaluation,library,Python,https://github.com/MichaelGrupp/evo,,GPL-3.0,slam;odometry;evaluation
371,PySR,High-performance symbolic regression,"A high-performance symbolic regression library for Python and Julia, used to discover mathematical expressions that best describe a dataset.",AI4;AI4-01,symbolic_regression;scientific_discovery,solver,Python,https://github.com/MilesCranmer/PySR,,Apache-2.0,symbolic-regression;scientific-discovery;machine-learning
372,IOHMM,Input Output Hidden Markov Models,"A Python library for Input Output Hidden Markov Models (IOHMM), enabling statistical modeling of sequential data with inputs.",AI4;AI4-01,statistical_modeling;sequence_analysis,library,Python,https://github.com/Mogeng/IOHMM,,BSD-3-Clause,hmm;statistical-modeling;sequence-analysis
373,Model_Log,Lightweight ML/DL training metrics visualization,"A lightweight Python tool for logging and visualizing machine learning and deep learning model training metrics such as loss, accuracy, precision, and F1 score.",AI4;AI4-02,visualization;experiment_tracking,library,Python,https://github.com/NLP-LOVE/Model_Log,,Apache-2.0,visualization;metrics;experiment-tracking
374,NVIDIA-NeMo/Evaluator,Scalable and reproducible evaluation library for AI models and benchmarks,"A library designed for the evaluation of AI models, particularly in the domains of Large Language Models (LLMs), Automatic Speech Recognition (ASR), and Text-to-Speech (TTS). It provides tools for running benchmarks and computing metrics.",AI4;AI4-02,model_evaluation;benchmarking;metrics_calculation,library,Python,https://github.com/NVIDIA-NeMo/Evaluator,,Apache-2.0,evaluation;benchmarks;llm;asr;tts
375,ggforestplot,R package for creating forest plots with confidence intervals,"An R package designed to visualize measures of effects and their confidence intervals using forest plots, commonly used in medical and statistical research.",AI4;AI4-02,statistical_visualization;confidence_intervals,library,R,https://github.com/NightingaleHealth/ggforestplot,,MIT,visualization;forestplot;confidence-intervals;r-package
376,Open Metric Learning,Library for metric learning pipelines and models,"A library providing pipelines, models, and a model zoo for metric learning and retrieval tasks. It facilitates the training and evaluation of models designed to learn distance metrics.",AI4;AI4-02,metric_learning;image_retrieval;model_training,library,Python,https://github.com/OML-Team/open-metric-learning,https://oml-team.github.io/open-metric-learning/,Apache-2.0,metric-learning;retrieval;pytorch
377,Thermo4PFM,Library to evaluate alloy compositions in Phase-Field models,"A C++ library developed by ORNL for evaluating thermodynamic properties of alloy compositions, specifically designed for use within Phase-Field modeling simulations in materials science.",AI4;AI4-02,materials_modeling;thermodynamic_evaluation;phase_field_simulation,library,C++,https://github.com/ORNL/Thermo4PFM,,BSD-3-Clause,materials-science;thermodynamics;phase-field;calphad
378,PyTorch-NLP,Basic utilities for PyTorch Natural Language Processing,"A library providing basic utilities, datasets, and metrics for Natural Language Processing (NLP) tasks using PyTorch. It simplifies common NLP operations and data handling.",AI4;AI4-02,nlp_utilities;text_processing;metrics,library,Python,https://github.com/PetrochukM/PyTorch-NLP,https://pytorchnlp.readthedocs.io/,BSD-3-Clause,nlp;pytorch;utilities
379,basicTrendline,R package for adding trendlines and confidence intervals,"An R package that facilitates adding trendlines and confidence intervals for basic linear or nonlinear models to plots, including displaying the equation.",AI4;AI4-02,statistical_visualization;trendline_fitting;confidence_intervals,library,R,https://github.com/PhDMeiwp/basicTrendline,,None,r-package;visualization;statistics;trendline
380,ggtrendline,R package for adding trendlines and confidence intervals to ggplot,"An R package designed to add trendlines, confidence intervals, and regression equations to 'ggplot2' visualizations.",AI4;AI4-02,statistical_visualization;trendline_fitting;confidence_intervals,library,R,https://github.com/PhDMeiwp/ggtrendline,,None,r-package;ggplot2;visualization;statistics
381,Causal Canvas,Tool for Causal discovery and Structural Learning,A tool for Causal discovery using Structural Learning and Probabilistic modelling. It includes features for error analysis of the learnt structure and Pearlian do-why inference.,AI4;AI4-02,causal_discovery;structural_learning;probabilistic_modeling,platform,HTML,https://github.com/PlaytikaOSS/causal-canvas,,MIT,causal-inference;structural-learning;probabilistic-modeling
382,Person ReID Benchmark,Systematic evaluation and benchmark for Person Re-Identification,"A comprehensive benchmark and evaluation system for Person Re-Identification (ReID), covering features, metrics, and datasets to standardize performance assessment in the field.",AI4;AI4-02,benchmarking;model_evaluation;person_reid,library,HTML,https://github.com/RSL-NEU/person-reid-benchmark,,BSD-3-Clause,reid;benchmark;evaluation;computer-vision
383,Multi-Camera Object Tracking via Deep Metric Learning,Implementation of multi-camera object tracking using deep metric learning,A solver implementation for multi-camera object tracking that leverages deep metric learning to transfer representations to a top-view perspective.,AI4;AI4-02,object_tracking;metric_learning;computer_vision,solver,Python,https://github.com/Robootx/Multi-Camera-Object-Tracking-via-Transferring-Representation-to-Top-View,,None,tracking;metric-learning;multi-camera
384,wv,R package for standard and robust wavelet variance analysis,"An R package providing tools to perform standard and robust wavelet variance analysis for time series signal processing, including inference tools like confidence intervals.",AI4;AI4-02,time_series_analysis;wavelet_variance;statistical_inference,library,R,https://github.com/SMAC-Group/wv,,None,wavelet-analysis;time-series;statistics;r-package
385,Hypothesis Testing for MT,Statistical hypothesis testing for Machine Translation metrics,A tool to evaluate the statistical significance of BLEU scores by comparing Reference and Target files among different machine translation systems.,AI4;AI4-02,hypothesis_testing;mt_evaluation;statistical_significance,library,C++,https://github.com/STC-MT-i2/Hypothesis-Testing-for-MT,,None,machine-translation;hypothesis-testing;bleu-score;statistics
386,guardians-mt-eval,Metrics for Machine Translation Meta-Evaluation,"Official repository for the ACL 2024 paper 'Guardians of the Machine Translation Meta-Evaluation', providing sentinel metrics for evaluating MT systems.",AI4;AI4-02,mt_evaluation;meta_evaluation;metrics,library,Python,https://github.com/SapienzaNLP/guardians-mt-eval,,NOASSERTION,machine-translation;evaluation;metrics;nlp
387,TADAM,Implementation of Task-Dependent Adaptive Metric for few-shot learning,"Implementation of the TADAM algorithm (Task-Dependent Adaptive Metric) for improved few-shot learning, developed by ServiceNow Research.",AI4;AI4-02,few_shot_learning;metric_learning;image_classification,solver,Jupyter Notebook,https://github.com/ServiceNow/TADAM,,Apache-2.0,few-shot-learning;metric-learning;computer-vision
388,QAConv,Interpretable and Generalizable Person Re-Identification methods,Implementation of QAConv (Query-Adaptive Convolution) and GS (Graph Sampling Based Deep Metric Learning) for Person Re-Identification tasks.,AI4;AI4-02,person_reid;metric_learning;graph_sampling,solver,Python,https://github.com/ShengcaiLiao/QAConv,,MIT,reid;metric-learning;computer-vision
389,onemetric,Unified metrics library for machine learning,"A library designed to provide a unified interface for calculating various machine learning metrics, simplifying the evaluation process.",AI4;AI4-02,metrics_calculation;model_evaluation,library,Python,https://github.com/SkalskiP/onemetric,,BSD-3-Clause,metrics;machine-learning;evaluation
390,Nyoka,Python library for exporting ML models to PMML,"A Python library that facilitates the export of machine learning models into PMML (Predictive Model Markup Language) standard, enabling model interoperability.",AI4;AI4-02,model_export;interoperability;pmml,library,Python,https://github.com/SoftwareAG/nyoka,,Apache-2.0,pmml;model-export;machine-learning
391,stable-audio-metrics,Metrics for evaluating music and audio generative models,"A library providing metrics specifically designed for evaluating long-form, full-band, and stereo generations from music and audio generative models.",AI4;AI4-02,audio_evaluation;generative_model_metrics;music_analysis,library,Python,https://github.com/Stability-AI/stable-audio-metrics,,MIT,audio;metrics;generative-ai;evaluation
392,EasyCNTK,C# wrapper library for CNTK Deep Learning API,"A C# library that wraps the CNTK API to provide an easier interface for Deep Learning and Deep Reinforcement Learning, including implementation of layers, optimizers, and evaluation helpers.",AI4;AI4-02,deep_learning;model_training;evaluation,library,C#,https://github.com/StanislavGrigoriev/EasyCNTK,,NOASSERTION,cntk;deep-learning;c-sharp;wrapper
393,valor,Lightweight library for fast evaluation of machine learning models,"A numpy-based library designed for fast and seamless evaluation of machine learning models, providing tools for computing various performance metrics.",AI4;AI4-02,model_evaluation;metrics_calculation,library,Python,https://github.com/Striveworks/valor,,MIT,evaluation;metrics;machine-learning
394,emh,R Package for testing the Efficient Market Hypothesis,An R package providing statistical tests and tools for evaluating the Efficient Market Hypothesis (EMH) in financial economics.,AI4;AI4-02,hypothesis_testing;financial_analysis;statistical_testing,library,R,https://github.com/StuartGordonReid/emh,,None,economics;finance;hypothesis-testing;r-package
395,ImagenHub,Standardized evaluation and inference library for conditional image generation models,"A comprehensive library designed to standardize the inference and evaluation processes for various conditional image generation models, facilitating fair comparison and benchmarking.",AI4;AI4-02,model_evaluation;benchmarking;image_generation,library,Python,https://github.com/TIGER-AI-Lab/ImagenHub,,MIT,generative-ai;evaluation;benchmarking;image-generation
396,VideoGenHub,Standardized evaluation and inference library for conditional video generation models,"A one-stop library to standardize the inference and evaluation of conditional video generation models, providing metrics and workflows for assessing video quality and consistency.",AI4;AI4-02,model_evaluation;benchmarking;video_generation,library,Python,https://github.com/TIGER-AI-Lab/VideoGenHub,,MIT,video-generation;evaluation;benchmarking
397,DiscreteSpeechMetrics,Reference-aware automatic speech evaluation toolkit,"A toolkit providing reference-aware automatic evaluation metrics for speech generation tasks, focusing on discrete speech representations.",AI4;AI4-02,speech_evaluation;metrics,library,Python,https://github.com/Takaaki-Saeki/DiscreteSpeechMetrics,,MIT,speech-processing;evaluation-metrics;audio
398,eli5,Library for debugging and explaining machine learning classifiers,A Python library which allows to visualize and debug various machine learning models using unified API. It has built-in support for several ML frameworks and provides ways to explain black-box models.,AI4;AI4-02,model_interpretation;debugging;explanation,library,Python,https://github.com/TeamHG-Memex/eli5,https://eli5.readthedocs.io/,MIT,interpretability;machine-learning;debugging
399,tonic_validate,Metrics framework for evaluating RAG application responses,"A library providing metrics to evaluate the quality of responses from Retrieval Augmented Generation (RAG) applications, helping developers assess answer correctness and relevance.",AI4;AI4-02,rag_evaluation;metrics;llm_validation,library,Python,https://github.com/TonicAI/tonic_validate,,MIT,rag;llm;evaluation;metrics
400,AIF360,Comprehensive toolkit for fairness metrics and bias mitigation,The AI Fairness 360 toolkit is an extensible open source library to help detect and mitigate bias in machine learning models throughout the AI application lifecycle.,AI4;AI4-02,fairness_evaluation;bias_mitigation;metrics,library,Python,https://github.com/Trusted-AI/AIF360,https://aif360.mybluemix.net/,Apache-2.0,fairness;bias;machine-learning;ethics
401,HtFLlib,Heterogeneous Federated Learning library,"A library designed to support model heterogeneity in Federated Learning, allowing consistent GPU memory usage for single or multiple clients and simplifying configuration.",AI4,federated_learning;distributed_training,library,Python,https://github.com/TsingZ0/HtFLlib,,Apache-2.0,federated-learning;heterogeneity;distributed-systems
402,MI-optimize,Tool for quantization and evaluation of LLMs,"A versatile tool designed for the quantization and evaluation of large language models (LLMs), integrating various quantization methods and evaluation techniques.",AI4;AI4-02,model_quantization;model_evaluation;llm,library,Python,https://github.com/TsingmaoAI/MI-optimize,,NOASSERTION,quantization;llm;evaluation
403,LLM-judge-reporting,Framework for bias correction and confidence intervals in LLM evaluation,"A plug-in framework that corrects bias and computes confidence intervals in reporting LLM-as-a-judge evaluation, including an adaptive algorithm for sample allocation.",AI4;AI4-02,statistical_analysis;bias_correction;llm_evaluation,library,Jupyter Notebook,https://github.com/UW-Madison-Lee-Lab/LLM-judge-reporting,,None,statistics;llm-as-a-judge;confidence-intervals
404,scope,Package for detecting oscillatory signals in time series,A Python-based package for detecting oscillatory signals in observational or experimental time series using the Empirical Mode Decomposition (EMD) technique and assessing statistical significance.,AI4;Physics,time_series_analysis;signal_detection;statistical_testing,library,Python,https://github.com/Warwick-Solar/scope,,Apache-2.0,time-series;oscillations;emd;physics
405,Adansons Base,Data programming tool for error-analysis and dataset organization,"A tool for error-analysis of training results that organizes metadata of unstructured data, creates datasets, and helps find low-quality data to improve AI performance.",AI4,data_quality_control;error_analysis;dataset_management,library,Jupyter Notebook,https://github.com/adansons/base,,MIT,data-centric-ai;error-analysis;dataset-creation
406,Flower,A Friendly Federated AI Framework,"A unified framework for federated learning, analytics, and evaluation. It allows running federated learning workloads on heterogeneous devices and scales to large numbers of clients.",AI4,federated_learning;distributed_training;privacy_preserving_ml,library,Python,https://github.com/adap/flower,https://flower.ai/,Apache-2.0,federated-learning;distributed-ml;privacy
407,object-centric-library,Library for training and evaluation of object-centric models,"A library dedicated to the training and evaluation of object-centric machine learning models, facilitating research in object-centric representation learning.",AI4;AI4-02,model_training;model_evaluation;object_centric_learning,library,Python,https://github.com/addtt/object-centric-library,,MIT,object-centric;representation-learning;evaluation
408,ahkab,SPICE-like electronic circuit simulator,"A SPICE-like electronic circuit simulator written in Python, capable of performing time-domain analysis, AC analysis, and operating point analysis.",Physics;Electronics,circuit_simulation;scientific_modeling,solver,Python,https://github.com/ahkab/ahkab,https://ahkab.readthedocs.io/,GPL-2.0,circuit-simulation;spice;electronics
409,Featuretools,Automated feature engineering library for transforming temporal and relational datasets,"Featuretools is an open source library for performing automated feature engineering. It excels at transforming temporal and relational datasets into feature matrices for machine learning, using a method known as Deep Feature Synthesis.",AI4;AI4-02,feature_engineering;data_processing,library,Python,https://github.com/alteryx/featuretools,https://featuretools.alteryx.com/,BSD-3-Clause,feature-engineering;automl;data-science
410,BOLD,Dataset and metrics for measuring biases in open-ended language generation,BOLD (Bias in Open-Ended Language Generation Dataset) is a dataset and associated metrics designed to evaluate fairness and bias in open-ended language generation models across multiple domains.,AI4;AI4-02,bias_evaluation;fairness_metrics,dataset,Python,https://github.com/amazon-science/bold,,NOASSERTION,bias;nlp;evaluation-metrics
411,werpy,Fast Python package for calculating Word Error Rate (WER) in speech recognition,"A lightweight and fast Python library specifically designed for calculating and analyzing the Word Error Rate (WER), a common metric for evaluating automatic speech recognition (ASR) systems.",AI4;AI4-02,speech_recognition_evaluation;metric_calculation,library,Python,https://github.com/analyticsinmotion/werpy,https://pypi.org/project/werpy/,BSD-3-Clause,wer;speech-recognition;metrics
412,newsreclib,PyTorch-Lightning library for neural news recommendation models,"A library built on PyTorch-Lightning for developing, training, and evaluating neural news recommendation systems, providing standard metrics and model implementations.",AI4;AI4-02,recommendation_system;model_evaluation,library,Python,https://github.com/andreeaiana/newsreclib,,MIT,recommender-system;pytorch-lightning;news-recommendation
413,ml_uncertainty,Library for prediction intervals and uncertainty estimation in ML models,"A Python library to obtain prediction intervals, confidence intervals, and parameter uncertainties for various machine learning models, aiding in the reliability assessment of predictions.",AI4;AI4-02,uncertainty_estimation;confidence_intervals,library,Python,https://github.com/architdatar/ml_uncertainty,,MIT,uncertainty;confidence-intervals;machine-learning
414,PCAtest,R package for statistical significance testing of PCA,"An R package designed to evaluate the statistical significance of Principal Component Analysis (PCA) results, helping to determine the number of significant axes and variable contributions.",AI4;AI4-02,statistical_testing;pca_analysis,library,R,https://github.com/arleyc/PCAtest,,GPL-3.0,pca;statistics;significance-testing
415,FactScoreLite,Lightweight implementation of FactScore metric for text generation,"A Python package implementing the FactScore metric for assessing the factual accuracy of generated text, serving as a maintained alternative to the original repository.",AI4;AI4-02,text_generation_evaluation;factuality_metric,library,Python,https://github.com/armingh2000/FactScoreLite,,MIT,nlp;metrics;factuality
416,T-NER,Library for Transformer-based Named Entity Recognition evaluation and fine-tuning,"T-NER is a Python library for fine-tuning transformer-based language models on Named Entity Recognition (NER) tasks, featuring cross-domain evaluation capabilities and an easy-to-use interface.",AI4;AI4-02,ner_evaluation;model_finetuning,library,Python,https://github.com/asahi417/tner,https://pypi.org/project/tner/,MIT,ner;transformers;evaluation
417,hypothetical,Hypothesis and statistical testing library for Python,"A Python library dedicated to hypothesis testing and statistical analysis, providing a collection of statistical tests and tools similar to those found in R or other statistical software.",AI4;AI4-02,hypothesis_testing;statistical_analysis,library,Python,https://github.com/aschleg/hypothetical,https://hypothetical.readthedocs.io/,MIT,statistics;hypothesis-testing;python
418,SimSIMD,High-performance SIMD-accelerated similarity metrics and dot products,"SimSIMD provides ultra-fast implementations of distance and similarity metrics (like cosine similarity, dot product) using SIMD instructions (AVX2, AVX-512, NEON, SVE) for multiple languages including Python and C.",AI4;AI4-02,similarity_metric;vector_math,library,C,https://github.com/ashvardanian/SimSIMD,,Apache-2.0,simd;metrics;performance
419,hypors,Hypothesis testing library for Polars dataframes,"A Rust-based library that brings statistical hypothesis testing capabilities directly to Polars dataframes, enabling efficient statistical analysis on large datasets.",AI4;AI4-02,hypothesis_testing;statistical_analysis,library,Rust,https://github.com/astronights/hypors,,MIT,polars;statistics;rust
420,AutoGluon,"Automated machine learning library for tabular, text, and image data","AutoGluon automates machine learning tasks, enabling users to achieve strong predictive performance with minimal code. It handles feature engineering, model selection, and ensembling.",AI4;AI4-02,automl;model_training,library,Python,https://github.com/autogluon/autogluon,https://auto.gluon.ai/,Apache-2.0,automl;machine-learning;deep-learning
421,auto-sklearn,Automated machine learning toolkit based on scikit-learn,"auto-sklearn is an automated machine learning toolkit and a drop-in replacement for a scikit-learn estimator. It leverages Bayesian optimization, meta-learning, and ensemble construction to automate the ML pipeline.",AI4;AI4-02,automl;model_selection,library,Python,https://github.com/automl/auto-sklearn,https://automl.github.io/auto-sklearn/,BSD-3-Clause,automl;scikit-learn;optimization
422,Kolibri,Framework for search system evaluation and batch processing,"Kolibri is a Scala-based framework designed for concurrent multi-node execution of search system evaluations. It provides out-of-the-box functionality for common IR metrics like NDCG, ERR, Precision, and Recall.",AI4;AI4-02,search_evaluation;ranking_metrics,workflow,Scala,https://github.com/awagen/kolibri,,Apache-2.0,evaluation;information-retrieval;metrics
423,ErrorAnalysisToolkit,MATLAB toolkit for image registration and tracking error analysis,"A collection of MATLAB tools designed to examine and quantify errors in image registration and tracking tasks, useful for medical imaging or computer vision research.",AI4;AI4-02,image_registration;error_analysis,library,MATLAB,https://github.com/awiles/ErrorAnalysisToolkit,,None,matlab;image-processing;error-analysis
424,fmeval,Library for evaluating Foundation Models,"A library provided by AWS to evaluate Foundation Models (FMs) across various dimensions such as accuracy, toxicity, bias, and robustness, facilitating responsible AI development.",AI4;AI4-02,llm_evaluation;foundation_model_metrics,library,Python,https://github.com/aws/fmeval,,Apache-2.0,llm;evaluation;responsible-ai
425,ggBrackets,ggplot2 extension for annotating plots with statistical significance brackets,"An R package that adds a layer to ggplot2 for drawing brackets annotated with p-values and significance testing results, facilitating the visualization of statistical comparisons.",AI4;AI4-02,statistical_visualization;significance_annotation,library,R,https://github.com/azzoam/ggBrackets,,GPL-3.0,r;ggplot2;visualization
426,fancylit,Streamlit wrapper for data visualization and modeling tasks,"A Python module that provides pre-packaged Streamlit components to facilitate data exploration, visualization, and running simple modeling tasks, acting as a lightweight data science tool.",AI4;AI4-02,data_visualization;data_exploration,library,Python,https://github.com/banjtheman/fancylit,,Apache-2.0,streamlit;visualization;data-science
427,nlg-metrics,Library for Natural Language Generation evaluation metrics,"A Python library implementing various metrics for evaluating Natural Language Generation (NLG) systems, helping researchers assess the quality of generated text.",AI4;AI4-02,nlg_evaluation;text_metrics,library,Python,https://github.com/baojunshan/nlg-metrics,,MIT,nlg;metrics;nlp
428,linearmodels,Extended linear models for Python including panel data and IV,"A Python library that extends statsmodels with additional linear models, specifically focusing on instrumental variable models, panel data models, and asset pricing tests.",AI4;AI4-02,statistical_modeling;econometrics,library,Python,https://github.com/bashtage/linearmodels,https://bashtage.github.io/linearmodels/,NCSA,econometrics;statistics;panel-data
429,timbre-dissimilarity-metrics,Metrics for evaluating audio timbre dissimilarity,"A collection of metrics implemented using the TorchMetrics API for evaluating timbre dissimilarity in audio signals, useful for music information retrieval and audio synthesis research.",AI4;AI4-02,audio_analysis;timbre_metrics,library,Python,https://github.com/ben-hayes/timbre-dissimilarity-metrics,,None,audio;metrics;timbre
430,Metrics,Collection of machine learning evaluation metrics,"A repository containing implementations of various machine learning evaluation metrics in Python, R, Haskell, and MATLAB, often used in data science competitions.",AI4;AI4-02,metric_calculation;model_evaluation,library,Python,https://github.com/benhamner/Metrics,,NOASSERTION,metrics;machine-learning;evaluation
431,drtmle,Nonparametric estimators of average treatment effect with doubly-robust confidence intervals,"An R package that implements nonparametric estimators of the average treatment effect with doubly-robust confidence intervals and hypothesis tests, suitable for causal inference.",AI4;AI4-02,statistical_inference;hypothesis_testing,library,R,https://github.com/benkeser/drtmle,,NOASSERTION,causal-inference;statistics;confidence-intervals
432,Orange3,Interactive data analysis and visualization workflow tool,"An open-source machine learning and data visualization software. It features a visual programming front-end for explorative data analysis and interactive data visualization, and can also be used as a Python library.",AI4;AI4-02,data_analysis;visualization;modeling,platform,Python,https://github.com/biolab/orange3,https://orangedatamining.com/,NOASSERTION,data-mining;machine-learning;visualization
433,Deep_Metric,Library for Deep Metric Learning algorithms,"A repository providing implementations for various Deep Metric Learning algorithms, facilitating the learning of distance metrics for tasks like image retrieval and clustering.",AI4;AI4-02,metric_learning;representation_learning,library,Python,https://github.com/bnu-wangxun/Deep_Metric,,Apache-2.0,metric-learning;deep-learning;computer-vision
434,freqtables,R package for creating descriptive statistics tables,"A package designed to quickly make tables of descriptive statistics (counts, percentages, confidence intervals) for categorical variables, compatible with tidyverse pipelines.",AI4;AI4-02,descriptive_statistics;reporting,library,R,https://github.com/brad-cannell/freqtables,,NOASSERTION,statistics;r-package;data-analysis
435,poverlap,Significance testing over interval overlaps,"A tool for calculating the significance of overlaps between genomic intervals or other interval data, useful in bioinformatics and statistical analysis of spatial data.",AI4;AI4-02,statistical_testing;genomics,library,Python,https://github.com/brentp/poverlap,,MIT,bioinformatics;statistics;intervals
436,Granger,Frequency-domain Granger causality with significance testing,"Matlab code for performing frequency-domain Granger causality analysis, including significance testing to determine causal relationships in time series data.",AI4;AI4-02,causality_analysis;time_series_analysis,library,MATLAB,https://github.com/brian-lau/Granger,,BSD-3-Clause,granger-causality;matlab;statistics
437,MatlabAUC,AUC calculation with confidence intervals in Matlab,"Matlab code for calculating the area under the receiver operating characteristic curve (AUC) and estimating confidence intervals, essential for evaluating classifier performance.",AI4;AI4-02,performance_evaluation;metrics,library,MATLAB,https://github.com/brian-lau/MatlabAUC,,GPL-3.0,auc;roc;statistics
438,modeltime.resample,Resampling tools for time series forecasting,"An R package providing resampling tools for time series forecasting validation, designed to work with the Modeltime ecosystem.",AI4;AI4-02,time_series_validation;resampling,library,R,https://github.com/business-science/modeltime.resample,,NOASSERTION,time-series;forecasting;validation
439,MediationToolbox,Mediation analysis toolbox for neuroimaging and behavioral data,"A toolbox for single-level and multi-level mediation analyses with bootstrap-based significance testing, featuring neuroimaging-oriented functions for parametric mapping.",AI4;AI4-02,statistical_analysis;neuroimaging,library,HTML,https://github.com/canlab/MediationToolbox,,None,mediation-analysis;neuroscience;statistics
440,scikit-hts,Hierarchical Time Series Forecasting library,"A Python library for hierarchical time series forecasting, providing a familiar scikit-learn like API for modeling and analyzing hierarchical data structures.",AI4;AI4-02,time_series_forecasting;modeling,library,Python,https://github.com/carlomazzaferro/scikit-hts,,MIT,time-series;forecasting;hierarchical
441,TreeMix-Pipeline,Pipeline for TreeMix analysis with bootstrapping and visualization,"Scripts to automate and enhance data analysis using TreeMix, including bootstrapping, migration event estimation, consensus tree creation, and statistical plotting.",AI4;AI4-02,population_genetics;phylogenetics;statistical_analysis,workflow,R,https://github.com/carolindahms/TreeMix,,None,genetics;treemix;visualization
442,BinningAnalysis.jl,Statistical standard error estimation for correlated data,"A Julia package for statistical standard error estimation tools specifically designed for correlated data, often used in Monte Carlo simulations.",AI4;AI4-02,error_estimation;statistical_analysis,library,Julia,https://github.com/carstenbauer/BinningAnalysis.jl,,MIT,statistics;julia;monte-carlo
443,catacomb,ML library for launching UIs and running evaluations,"A machine learning library designed to simplify the process of launching user interfaces, running model evaluations, and comparing performance metrics.",AI4;AI4-02,model_evaluation;experiment_tracking,library,,https://github.com/catacomb-ai/catacomb,,Apache-2.0,machine-learning;evaluation;ui
444,scikits-bootstrap,Bootstrap confidence interval estimation for Python,"A Python library providing functions for bootstrap confidence interval estimation, built on top of NumPy.",AI4;AI4-02,statistical_estimation;bootstrap,library,Python,https://github.com/cgevans/scikits-bootstrap,,BSD-3-Clause,statistics;bootstrap;confidence-intervals
445,CollMetric,Collaborative Metric Learning (CML) implementation,"A TensorFlow implementation of Collaborative Metric Learning (CML), a method for recommendation systems that learns a joint metric space for users and items.",AI4;AI4-02,metric_learning;recommendation,solver,Python,https://github.com/changun/CollMetric,,GPL-3.0,metric-learning;tensorflow;recommender-systems
446,sr-metric,No-Reference Quality Metric for Single-Image Super-Resolution,Implementation of a learning-based no-reference quality metric specifically designed for evaluating single-image super-resolution results.,AI4;AI4-02,image_quality_assessment;super_resolution,solver,MATLAB,https://github.com/chaoma99/sr-metric,,MIT,image-quality;super-resolution;metrics
447,PointNet++,Deep Hierarchical Feature Learning on Point Sets,"Implementation of PointNet++, a deep neural network architecture for processing point sets in a metric space, widely used for 3D data analysis.",AI4;AI4-02,3d_analysis;feature_learning,solver,Python,https://github.com/charlesq34/pointnet2,,NOASSERTION,point-cloud;deep-learning;3d-vision
448,bootstrap,Library for bootstrapping statistics,"A Python library designed to facilitate bootstrapping statistics, allowing for robust statistical inference.",AI4;AI4-02,statistical_inference;bootstrap,library,Python,https://github.com/christopherjenness/bootstrap,,None,statistics;bootstrap;python
449,piecewise_linear_fit_py,Piecewise linear data fitting tool,"A Python tool to fit piecewise linear data for a specified number of line segments, useful for modeling non-linear relationships with linear approximations.",AI4;AI4-02,curve_fitting;data_modeling,library,Python,https://github.com/cjekel/piecewise_linear_fit_py,,MIT,curve-fitting;optimization;modeling
450,Pingouin.jl,Statistical package for Julia,"A reimplementation of the Python Pingouin statistical package in Julia, providing simple yet exhaustive statistical functions.",AI4;AI4-02,statistical_analysis;hypothesis_testing,library,Julia,https://github.com/clementpoiret/Pingouin.jl,,MIT,statistics;julia;pingouin
451,TedEval,Evaluation metric for Scene Text Detectors,"Implementation of TedEval, a fair evaluation metric for scene text detectors that accounts for instance-level matching and coverage.",AI4;AI4-02,evaluation_metric;computer_vision,solver,Python,https://github.com/clovaai/TedEval,,MIT,ocr;evaluation;metrics
452,generative-evaluation-prdc,"Precision, Recall, Density, and Coverage metrics for generative models","Code base for calculating precision, recall, density, and coverage metrics to evaluate the performance and diversity of generative models.",AI4;AI4-02,generative_model_evaluation;metrics,library,Python,https://github.com/clovaai/generative-evaluation-prdc,,MIT,generative-models;evaluation;gan
453,voxceleb_trainer,Metric learning framework for speaker recognition,"A training framework for speaker recognition focusing on metric learning approaches, providing state-of-the-art baselines and loss functions.",AI4;AI4-02,speaker_recognition;metric_learning,solver,Python,https://github.com/clovaai/voxceleb_trainer,,MIT,speaker-recognition;metric-learning;deep-learning
454,gec-ranking,Ground Truth for Grammatical Error Correction Metrics,"Data and code for evaluating Grammatical Error Correction (GEC) metrics, providing a ground truth ranking for metric comparison.",AI4;AI4-02,metric_evaluation;nlp_benchmark,dataset,Python,https://github.com/cnap/gec-ranking,,None,nlp;evaluation;benchmark
455,deepeval,LLM Evaluation Framework,"A comprehensive framework for evaluating Large Language Models (LLMs), providing various metrics and test cases to ensure model quality and safety.",AI4;AI4-02,llm_evaluation;metrics,framework,Python,https://github.com/confident-ai/deepeval,https://docs.confident-ai.com/,Apache-2.0,llm;evaluation;testing
456,SpectralGV,Spectral Geometric Verification for Metric Localization,"Implementation of Spectral Geometric Verification (SGV) for re-ranking point cloud retrieval, enhancing metric localization accuracy.",AI4;AI4-02,localization;geometric_verification,solver,Python,https://github.com/csiro-robotics/SpectralGV,,NOASSERTION,robotics;localization;point-cloud
457,sacrerouge,Library for text generation evaluation metrics,"A library dedicated to the use and development of evaluation metrics for text generation tasks, with a strong emphasis on summarization.",AI4;AI4-02,text_evaluation;metrics,library,Python,https://github.com/danieldeutsch/sacrerouge,,Apache-2.0,nlp;summarization;evaluation
458,Dask,Flexible parallel computing library for analytic computing,"A flexible library for parallel computing in Python that scales NumPy, Pandas, and Scikit-Learn workflows. It provides advanced parallelism for analytics, enabling performance at scale for scientific computing tasks.",AI4,parallel_computing;data_processing,library,Python,https://github.com/dask/dask,https://dask.org/,BSD-3-Clause,parallel-computing;distributed-systems;scaling
459,NER-Evaluation,Evaluation metrics for Named Entity Recognition (NER) systems,An implementation of named-entity evaluation metrics based on SemEval'13 Task 9. It evaluates NER performance by considering full entity spans rather than just tag/token level accuracy.,AI4;AI4-02,model_evaluation;metrics,library,Python,https://github.com/davidsbatista/NER-Evaluation,,MIT,ner;evaluation-metrics;nlp
460,multiway_bootstrap,Implementation of the multiway bootstrap for R,"Provides an implementation of the multiway bootstrap (including Pigeonhole bootstrap and reweighting tensor bootstrap) for bootstrapping data arrays of arbitrary order, useful for statistical inference in complex data structures.",AI4;AI4-02,statistical_inference;bootstrap,library,R,https://github.com/deaneckles/multiway_bootstrap,,None,bootstrap;statistics;inference
461,CK-Caffe,Collective Knowledge workflow for Caffe optimization,"A Collective Knowledge (CK) workflow to automate the installation, evaluation, and optimization of Caffe-based workloads across diverse hardware and software platforms. It facilitates reproducible research and benchmarking.",AI4,benchmarking;reproducibility,workflow,CMake,https://github.com/dividiti/ck-caffe,http://cKnowledge.org/ai,BSD-3-Clause,caffe;benchmarking;optimization
462,AB Test Advanced Toolkit,Advanced toolkit for A/B testing analysis,A suite of tools for A/B testing that includes advanced techniques like CUPED (Controlled-experiment Using Pre-Experiment Data) and Gradient Boosting for variance reduction and faster statistical significance.,AI4;AI4-02,ab_testing;statistical_significance,library,Python,https://github.com/dmitry-brazhenko/ab-test-advanced-toolkit,,MIT,ab-testing;cuped;statistics
463,Delay Discounting Analysis,Hierarchical Bayesian estimation for delay discounting,A MATLAB toolbox for hierarchical Bayesian estimation and hypothesis testing specifically designed for delay discounting tasks. It allows for robust parameter estimation and statistical inference in behavioral economics research.,AI4;AI4-02,bayesian_estimation;hypothesis_testing,library,MATLAB,https://github.com/drbenvincent/delay-discounting-analysis,,MIT,bayesian;matlab;behavioral-science
464,LANLGeoMag,Library for magnetic field models and coordinate transforms,"A C-based library from Los Alamos National Laboratory for computing geophysical quantities, including magnetic field models and high-precision coordinate transformations. Used for geospace research and magnetic field line tracing.",AI4,magnetic_field_modeling;coordinate_conversion,library,C,https://github.com/drsteve/LANLGeoMag,,NOASSERTION,geophysics;magnetic-field;lanl
465,performance,Assessment of regression models performance,"An R package to compute various performance metrics for regression models, including R2, ICC, LOO, AIC, BIC, and Bayes Factor. It provides a unified interface for assessing model quality.",AI4;AI4-02,model_evaluation;performance_metrics,library,R,https://github.com/easystats/performance,https://easystats.github.io/performance/,GPL-3.0,r-package;model-metrics;statistics
466,report,Automated statistical reporting for R objects,"An R package that automatically generates reports from statistical models and data frames. It interprets the results of statistical tests and models, providing text descriptions suitable for scientific publication.",AI4;AI4-02,statistical_reporting;automated_reporting,library,R,https://github.com/easystats/report,https://easystats.github.io/report/,NOASSERTION,r-package;reporting;reproducibility
467,Pybotics,Python toolbox for robotics kinematics and calibration,"A Python toolbox for robot kinematics and calibration. It provides tools for modeling robot geometry, calculating forward/inverse kinematics, and performing calibration tasks, useful for robotics research.",AI4,robotics_kinematics;calibration,library,Python,https://github.com/engnadeau/pybotics,https://pybotics.readthedocs.io/,MIT,robotics;kinematics;calibration
468,Seismometer,AI model evaluation framework for healthcare,"A framework for evaluating AI models with a specific focus on healthcare applications. It provides tools for assessing model performance, fairness, and calibration in clinical settings.",AI4;AI4-02,model_evaluation;healthcare_ai,library,Jupyter Notebook,https://github.com/epic-open-source/seismometer,https://epic-open-source.github.io/seismometer/,BSD-3-Clause,healthcare;evaluation;fairness
469,bootstrapped,Library for generating bootstrapped confidence intervals for A/B testing,A Python library designed to generate bootstrapped confidence intervals for A/B testing purposes. It allows researchers to estimate the sampling distribution of a statistic by resampling with replacement from the original data.,AI4-02,statistical_inference;confidence_intervals;ab_testing,library,Python,https://github.com/facebookarchive/bootstrapped,,NOASSERTION,statistics;bootstrapping;confidence-intervals
470,fronni,Fast ML performance metrics and charts with confidence intervals,A library for calculating machine learning model performance metrics and generating charts with confidence intervals. It is optimized with Numba for high-performance computation.,AI4-02,model_evaluation;performance_metrics;confidence_intervals,library,Python,https://github.com/facebookarchive/fronni,,Apache-2.0,metrics;numba;visualization
471,EvalGIM,Evaluation library for generative image models,"EvalGIM (EvalGym) is a library designed for the automatic evaluation of text-to-image generative models. It supports reproducible evaluations with user-defined metrics, datasets, and visualizations.",AI4;AI4-02,model_evaluation;generative_models;image_generation,library,Python,https://github.com/facebookresearch/EvalGIM,,NOASSERTION,generative-ai;evaluation;text-to-image
472,unibench,Evaluation library for VLM robustness across benchmarks,A Python library specifically designed to evaluate the robustness of Vision-Language Models (VLMs) across a diverse set of benchmarks.,AI4;AI4-02,model_evaluation;robustness_testing;vlm,library,Jupyter Notebook,https://github.com/facebookresearch/unibench,,NOASSERTION,vision-language-models;benchmarking;robustness
473,pooch,Data fetching and management library for scientific datasets,"Pooch manages your Python library's sample data files. It automatically downloads and stores data files, checking their integrity via hash comparisons, which is essential for reproducible scientific workflows.",AI4,data_management;data_retrieval;reproducibility,library,Python,https://github.com/fatiando/pooch,https://www.fatiando.org/pooch,NOASSERTION,data-fetching;reproducibility;scientific-python
474,verde,Spatial data processing and gridding library,"Verde is a Python library for processing and gridding spatial data using a machine-learning style API. It provides tools for interpolation, trend removal, and cross-validation of spatial models.",AI4,spatial_analysis;gridding;interpolation,library,Python,https://github.com/fatiando/verde,https://www.fatiando.org/verde,BSD-3-Clause,geospatial;interpolation;machine-learning
475,prediction-interval-NN,Confidence and prediction intervals for Neural Networks,"A library providing methods to estimate confidence and prediction intervals for feedforward neural networks and RNNs, enabling uncertainty quantification in deep learning models.",AI4-02,uncertainty_quantification;prediction_intervals;neural_networks,library,Python,https://github.com/fishjh2/prediction-interval-NN,,None,uncertainty;neural-networks;intervals
476,medmnistc-api,Robustness evaluation library for medical image analysis,"A Python library developed for evaluating and enhancing the robustness of machine learning models in medical image analysis, associated with the ADSMI@MICCAI2024 workshop.",AI4;AI4-02,model_evaluation;robustness;medical_imaging,library,Jupyter Notebook,https://github.com/francescodisalvo05/medmnistc-api,,Apache-2.0,medical-imaging;robustness;miccai
477,DeepMatch,Deep learning library for metric and embedding learning,"A library for metric and embedding learning using convolutional neural networks, with applications in keypoint matching, stereo matching, and image retrieval.",AI4,metric_learning;image_matching;embedding,library,Python,https://github.com/galad-loth/DeepMatch,,Apache-2.0,deep-learning;computer-vision;matching
478,NLPMetrics,Collection of NLP evaluation metrics,"A Python repository providing implementations for various Natural Language Processing (NLP) metrics, facilitating the evaluation of language models.",AI4-02,model_evaluation;nlp_metrics,library,Jupyter Notebook,https://github.com/gcunhase/NLPMetrics,,MIT,nlp;metrics;evaluation
479,long-form-factuality,Benchmark for long-form factuality in LLMs,A library and benchmark suite from Google DeepMind for evaluating the factuality of long-form text generated by Large Language Models (LLMs).,AI4;AI4-02,model_evaluation;factuality_checking;llm_benchmark,library,Python,https://github.com/google-deepmind/long-form-factuality,,NOASSERTION,llm;factuality;benchmarking
480,surface-distance,Surface distance metrics for segmentation evaluation,"A library to compute surface distance-based performance metrics (e.g., Hausdorff distance) for segmentation tasks, widely used in medical imaging and computer vision.",AI4-02,model_evaluation;segmentation_metrics;surface_distance,library,Python,https://github.com/google-deepmind/surface-distance,,Apache-2.0,segmentation;metrics;medical-imaging
481,bleurt,Learned metric for Natural Language Generation,"BLEURT is a metric for evaluating Natural Language Generation systems based on transfer learning, providing a more human-correlated evaluation than traditional metrics.",AI4-02,model_evaluation;nlg_metrics;learned_metrics,library,Python,https://github.com/google-research/bleurt,,Apache-2.0,nlp;evaluation;transfer-learning
482,rl-reliability-metrics,Metrics for measuring reliability of RL algorithms,A library providing a set of metrics and statistical tools for measuring and comparing the reliability and performance stability of reinforcement learning (RL) algorithms.,AI4-02,model_evaluation;reinforcement_learning;reliability_metrics,library,Python,https://github.com/google-research/rl-reliability-metrics,,Apache-2.0,reinforcement-learning;statistics;reliability
483,best,Bam Error Stats Tool for aligned reads analysis,"Bam Error Stats Tool (best) is a utility for analyzing error types in aligned sequencing reads (BAM files), useful for quality control in bioinformatics workflows.",AI4,bioinformatics_analysis;quality_control;sequencing_error_analysis,solver,Rust,https://github.com/google/best,,MIT,bioinformatics;bam;genomics
484,yggdrasil-decision-forests,Library for decision forest models,"A comprehensive library to train, evaluate, interpret, and productionize decision forest models such as Random Forest and Gradient Boosted Decision Trees.",AI4,scientific_modeling;machine_learning;decision_trees,library,C++,https://github.com/google/yggdrasil-decision-forests,https://ydf.readthedocs.io,Apache-2.0,machine-learning;decision-forests;random-forest
485,nlp-metrics,Implementations of BLEU and ROUGE metrics,"A Python implementation of standard NLP evaluation metrics, specifically BLEU and ROUGE, used for assessing text generation quality.",AI4-02,model_evaluation;nlp_metrics,library,Python,https://github.com/harpribot/nlp-metrics,,None,nlp;bleu;rouge
486,hcrystalball,Unified API for time-series forecasting,"A library that unifies the API for various time-series forecasting libraries and modeling techniques in the Python ecosystem, facilitating scientific data analysis and prediction.",AI4,time_series_forecasting;scientific_modeling,library,Python,https://github.com/heidelbergcement/hcrystalball,,MIT,time-series;forecasting;machine-learning
487,ros-network-analysis,Network analysis tools for ROS,"A ROS package providing tools to analyze wireless network metrics (signal quality, latency, throughput) between ROS nodes, useful for robotics research and engineering.",AI4,network_analysis;robotics_evaluation;performance_metrics,library,Python,https://github.com/herolab-uga/ros-network-analysis,,None,ros;robotics;network-analysis
488,hmmlearn,Hidden Markov Models in Python,"A library for Hidden Markov Models (HMM) in Python, featuring a scikit-learn like API. It is widely used for modeling sequence data in various scientific domains.",AI4,scientific_modeling;sequence_analysis;statistical_inference,library,Python,https://github.com/hmmlearn/hmmlearn,https://hmmlearn.readthedocs.io,BSD-3-Clause,hmm;machine-learning;sequence-modeling
489,huggingface/evaluate,A library for easily evaluating machine learning models and datasets,"A library for easily evaluating machine learning models and datasets. It provides a unified API for a wide range of evaluation metrics, comparison statistics, and measurements, supporting various domains including NLP, Computer Vision, and Audio.",AI4;AI4-02,model_evaluation;metrics,library,Python,https://github.com/huggingface/evaluate,https://huggingface.co/docs/evaluate,Apache-2.0,metrics;evaluation;nlp;machine-learning
490,ing-bank/sparse_dot_topn,Fast sparse matrix multiplication and top-n similarity selection,"A Python package to accelerate sparse matrix multiplication and top-n similarity selection. It is widely used for string matching, entity resolution, and other tasks requiring efficient cosine similarity calculations on large datasets.",AI4;AI4-02,similarity_calculation;metrics,library,C++,https://github.com/ing-bank/sparse_dot_topn,,Apache-2.0,sparse-matrix;similarity;cosine-similarity;optimization
491,insysbio/LikelihoodProfiler.jl,Practical identifiability analysis and confidence intervals estimation in Julia,"LikelihoodProfiler is a Julia package designed for practical identifiability analysis and confidence intervals estimation in dynamic modeling, particularly useful in systems biology and pharmacology.",AI4;AI4-02,statistical_analysis;confidence_intervals;identifiability_analysis,library,Jupyter Notebook,https://github.com/insysbio/LikelihoodProfiler.jl,,MIT,julia;statistics;confidence-intervals;systems-biology
492,interpretml/interpret,Fit interpretable models and explain blackbox machine learning,InterpretML is a library for training interpretable models and explaining blackbox systems. It includes the Explainable Boosting Machine (EBM) and supports various visualization and metric tools for model analysis.,AI4;AI4-02,interpretability;model_analysis,library,C++,https://github.com/interpretml/interpret,https://interpret.ml,MIT,interpretability;xai;machine-learning;visualization
493,jacobgil/confidenceinterval,Library for calculating confidence intervals in Python,"A Python library dedicated to calculating confidence intervals for various statistical distributions and machine learning metrics, filling a gap in standard libraries.",AI4;AI4-02,statistical_analysis;confidence_intervals,library,Python,https://github.com/jacobgil/confidenceinterval,,MIT,statistics;confidence-intervals;python
494,jhclark/multeval,Bootstrap Resampling and Approximate Randomization for MT metrics,"A tool for statistical hypothesis testing in Machine Translation evaluation. It implements bootstrap resampling and approximate randomization to control for optimizer instability and provide reliable significance testing for metrics like BLEU, METEOR, and TER.",AI4;AI4-02,statistical_testing;evaluation;nlp_metrics,solver,Groff,https://github.com/jhclark/multeval,,NOASSERTION,machine-translation;significance-testing;bootstrap;evaluation
495,jiwei0921/Saliency-Evaluation-Toolbox,Evaluation metrics toolbox for salient object detection,"A MATLAB toolbox providing comprehensive evaluation metrics for salient object detection, including E-measure, S-measure, weighted F-measure, MAE, and PR curves.",AI4;AI4-02,evaluation;metrics;computer_vision,library,MATLAB,https://github.com/jiwei0921/Saliency-Evaluation-Toolbox,,None,saliency-detection;evaluation-metrics;matlab
496,jlsuarezdiaz/pyDML,Distance Metric Learning Algorithms for Python,A Python library implementing various Distance Metric Learning (DML) algorithms. It allows learning distance metrics from data to improve the performance of distance-based machine learning algorithms like k-NN.,AI4;AI4-02,metric_learning;distance_metrics,library,Python,https://github.com/jlsuarezdiaz/pyDML,https://pydml.readthedocs.io/,GPL-3.0,metric-learning;machine-learning;distance-metric
497,jm4474/SVARIV,Inference and confidence intervals for Structural Vector Autoregressions,A Matlab suite to construct weak-instrument robust confidence intervals for impulse response coefficients in Structural Vector Autoregressions (SVAR) identified with an external instrument.,AI4;AI4-02,statistical_inference;confidence_intervals;econometrics,library,MATLAB,https://github.com/jm4474/SVARIV,,BSD-3-Clause,econometrics;confidence-intervals;matlab;time-series
498,jongwook/spark-ranking-metrics,Offline Recommender System Evaluation for Spark,"A library for calculating ranking metrics (like MAP, NDCG) for recommender system evaluation on Apache Spark, enabling scalable offline evaluation.",AI4;AI4-02,evaluation;metrics;recommender_systems,library,Scala,https://github.com/jongwook/spark-ranking-metrics,,Unlicense,spark;evaluation;ranking-metrics;recommender-system
499,icp,Python implementation of Invariant Causal Prediction (ICP) algorithm,"A Python implementation of the Invariant Causal Prediction (ICP) algorithm for causal inference, enabling identification of causal relationships and confidence intervals from data.",AI4;AI4-02,causal_inference;statistical_analysis,library,Python,https://github.com/juangamella/icp,,BSD-3-Clause,causal-inference;statistics;python
500,Bootstrap.jl,Statistical bootstrapping library for Julia,"A Julia library for statistical bootstrapping, providing methods to estimate sampling distributions and confidence intervals for various statistics.",AI4;AI4-02,statistical_analysis;confidence_intervals,library,Julia,https://github.com/juliangehring/Bootstrap.jl,,NOASSERTION,julia;statistics;bootstrapping
501,mantel,Python implementation of the Mantel test for correlation between distance matrices,"A Python library implementing the Mantel test, a statistical test used to evaluate the correlation between two distance matrices, commonly used in ecology and genetics.",AI4;AI4-02,statistical_analysis;hypothesis_testing,library,Python,https://github.com/jwcarr/mantel,,MIT,statistics;mantel-test;correlation
502,CodeBLEU,Evaluation metric for code generation tasks,"A Python implementation of CodeBLEU, a metric designed to evaluate the quality of code generated by AI models by considering syntactic and semantic features.",AI4;AI4-02,metrics;model_evaluation,library,Python,https://github.com/k4black/codebleu,,MIT,nlp;metrics;code-generation
503,boundedline-pkg,Matlab tool for plotting lines with error bounds/confidence intervals,"A Matlab package for visualizing data with associated uncertainty, allowing the plotting of lines with shaded error bounds or confidence intervals.",AI4;AI4-02,visualization;statistical_analysis,library,MATLAB,https://github.com/kakearney/boundedline-pkg,,MIT,matlab;visualization;plotting;error-bars
504,matrixTests,R package for high-performance matrix-based hypothesis testing,"An R package designed for efficient computation of multiple hypothesis tests (t-tests, F-tests, etc.) on rows or columns of matrices, useful for high-dimensional biological data.",AI4;AI4-02,statistical_analysis;hypothesis_testing,library,R,https://github.com/karoliskoncevicius/matrixTests,,None,r;statistics;matrix-operations
505,ROUGE-2.0,Toolkit for evaluating automatic summarization using ROUGE metrics,A Java-based toolkit for computing ROUGE metrics (Recall-Oriented Understudy for Gisting Evaluation) to evaluate automatic text summarization systems.,AI4;AI4-02,metrics;model_evaluation,library,Java,https://github.com/kavgan/ROUGE-2.0,,Apache-2.0,nlp;summarization;metrics;rouge
506,rouge,JavaScript implementation of ROUGE evaluation metric,"A JavaScript library implementing the ROUGE metric for evaluating automatic summarization, enabling evaluation in web-based or Node.js environments.",AI4;AI4-02,metrics;model_evaluation,library,JavaScript,https://github.com/kenlimmj/rouge,,MIT,javascript;nlp;metrics
507,hera,Dashboard for tracking and visualizing Keras model training metrics,"A tool to stream and visualize metrics from Keras model training to a browser-based dashboard, facilitating experiment monitoring.",AI4;AI4-02,visualization;experiment_tracking,solver,JavaScript,https://github.com/keplr-io/hera,,MIT,keras;visualization;dashboard;deep-learning
508,abacus,Fast hypothesis testing and experiment design solution,"A Python library for conducting fast hypothesis testing and designing experiments (A/B testing), providing statistical tools for decision making.",AI4;AI4-02,statistical_analysis;hypothesis_testing,library,Python,https://github.com/kolmogorov-lab/abacus,,MIT,statistics;ab-testing;experiment-design
509,globox,Object detection dataset converter and evaluator,"A Python package to read, convert, and evaluate object detection datasets (COCO, YOLO, PascalVOC, etc.) using standard metrics like mAP.",AI4;AI4-02,metrics;data_processing,library,Python,https://github.com/laclouis5/globox,,MIT,object-detection;metrics;dataset-conversion
510,PyIRSTDMetrics,Evaluation metrics for Infrared Small Target Detection,A Python toolbox providing specialized evaluation metrics for the task of Infrared Small Target Detection (IRSTD).,AI4;AI4-02,metrics;model_evaluation,library,Python,https://github.com/lartpang/PyIRSTDMetrics,,MIT,computer-vision;metrics;infrared-detection
511,PySODEvalToolkit,Evaluation toolbox for Salient and Camouflaged Object Detection,A Python-based evaluation toolbox designed for assessing the performance of models in Salient Object Detection (SOD) and Camouflaged Object Detection (COD).,AI4;AI4-02,metrics;model_evaluation,library,Python,https://github.com/lartpang/PySODEvalToolkit,,MIT,computer-vision;saliency-detection;metrics
512,dgm-eval,Evaluation metrics for deep generative models,"A codebase for evaluating deep generative models, addressing flaws in existing metrics and providing fairer assessment for diffusion models.",AI4;AI4-02,metrics;model_evaluation,library,Jupyter Notebook,https://github.com/layer6ai-labs/dgm-eval,,MIT,generative-models;metrics;diffusion-models
513,deception,Benchmark for evaluating LLM disinformation capabilities,"A benchmark suite designed to evaluate Large Language Models on their ability to create and resist disinformation, including standardized metrics.",AI4;AI4-02,metrics;model_evaluation;benchmark,dataset,,https://github.com/lechmazur/deception,,None,llm;benchmark;safety;evaluation
514,cvAUC,Confidence intervals for cross-validated AUC estimates in R,An R package for computationally efficient estimation of confidence intervals for cross-validated Area Under the ROC Curve (AUC) estimates.,AI4;AI4-02,statistical_analysis;metrics,library,R,https://github.com/ledell/cvAUC,,Apache-2.0,r;statistics;auc;confidence-intervals
515,rouge-metric,Python wrapper and re-implementation of ROUGE metrics,A Python package that wraps the official ROUGE script and provides a native re-implementation for evaluating text summarization.,AI4;AI4-02,metrics;model_evaluation,library,Perl,https://github.com/li-plus/rouge-metric,,MIT,nlp;metrics;rouge
516,librosa,Python library for audio and music analysis,"A comprehensive Python library for audio and music analysis, providing tools for feature extraction, signal processing, and visualization.",AI4;AI4-02,signal_processing;feature_extraction,library,Python,https://github.com/librosa/librosa,https://librosa.org,ISC,audio;signal-processing;music-analysis
517,t2v_metrics,Evaluation metrics for text-to-visual models using VQAScore,"A toolkit for evaluating text-to-image, text-to-video, and text-to-3D models using VQAScore, a metric based on Visual Question Answering.",AI4;AI4-02,metrics;model_evaluation,library,Jupyter Notebook,https://github.com/linzhiqiu/t2v_metrics,,Apache-2.0,generative-ai;metrics;multimodal
518,lmfit-py,Non-Linear Least Squares Minimization and Curve Fitting,"A Python library for non-linear least-squares minimization and curve fitting, building on scipy.optimize to provide a high-level interface for modeling data.",AI4;AI4-02,statistical_analysis;curve_fitting,library,Python,https://github.com/lmfit/lmfit-py,https://lmfit.github.io/lmfit-py/,NOASSERTION,curve-fitting;optimization;statistics
519,KoBERTScore,BERTScore implementation for Korean language,A Python implementation of the BERTScore metric specifically adapted for evaluating Korean text generation.,AI4;AI4-02,metrics;model_evaluation,library,Python,https://github.com/lovit/KoBERTScore,,None,nlp;metrics;korean
520,ConfidenceIntervals,Confidence interval computation for ML evaluation,A Python package to calculate confidence intervals for machine learning evaluation metrics using bootstrapping methods.,AI4;AI4-02,statistical_analysis;metrics,library,Jupyter Notebook,https://github.com/luferrer/ConfidenceIntervals,,MIT,statistics;machine-learning;confidence-intervals
521,waipy,Wavelet analysis library for time series with significance testing,"A Python library for Continuous Wavelet Transform (CWT) and Cross Wavelet Analysis (CWA), including significance tests based on Torrence and Compo (1998) for analyzing time series data.",AI4;AI4-02,time_series_analysis;statistical_test;signal_processing,library,Jupyter Notebook,https://github.com/mabelcalim/waipy,,BSD-3-Clause,wavelet-transform;time-series;significance-test
522,Mars,Tensor-based unified framework for large-scale data computation,"A tensor-based framework for large-scale data computation that scales libraries like numpy, pandas, and scikit-learn, enabling distributed scientific computing and data analysis.",AI4,distributed_computing;data_processing;scientific_computing,platform,Python,https://github.com/mars-project/mars,https://mars-project.readthedocs.io/,Apache-2.0,distributed-systems;tensor-computation;numpy-compatible
523,py-img-seg-eval,Evaluation metrics for image segmentation,"A Python library implementing standard evaluation metrics for image segmentation tasks, such as pixel accuracy and Intersection over Union (IoU), inspired by FCN literature.",AI4;AI4-02,model_evaluation;image_segmentation;metrics,library,Python,https://github.com/martinkersner/py-img-seg-eval,,None,computer-vision;segmentation;evaluation-metrics
524,ml-stat-util,Statistical functions for comparing ML models,A collection of statistical functions based on bootstrapping for computing confidence intervals and p-values to compare machine learning models against human readers or other models.,AI4;AI4-02,statistical_test;model_comparison;confidence_interval,library,Jupyter Notebook,https://github.com/mateuszbuda/ml-stat-util,,MIT,bootstrapping;p-value;model-evaluation
525,AMeThyst,Metrics and hypothesis testing tools for artifact analysis,"A set of tools for calculating metrics and performing hypothesis tests, designed for analyzing specific artifacts or datasets (implied 'Art Metrics').",AI4;AI4-02,hypothesis_testing;metrics;statistical_analysis,library,Jupyter Notebook,https://github.com/mattyamonaca/AMeThyst,,Apache-2.0,hypothesis-test;metrics;analysis
526,SAMFailureMetrics,Metrics for assessing segmentation object properties,"A library providing metrics for quantifying tree-likeness and textural contrast of objects, used for analyzing failure modes in segmentation models (e.g., SAM).",AI4;AI4-02,model_evaluation;error_analysis;segmentation_metrics,library,Python,https://github.com/mazurowski-lab/SAMFailureMetrics,,Apache-2.0,segmentation;failure-analysis;metrics
527,torcheval,Performant PyTorch model metrics library,"A library containing a rich collection of performant PyTorch model metrics, tools for creating new metrics, and utilities for distributed training evaluation.",AI4;AI4-02,model_evaluation;metrics;distributed_training,library,Python,https://github.com/meta-pytorch/torcheval,,NOASSERTION,pytorch;metrics;evaluation
528,FLAML,Fast and lightweight AutoML library,"A fast library for Automated Machine Learning (AutoML) and hyperparameter tuning, designed to find accurate models with low computational cost.",AI4,automl;hyperparameter_tuning;model_optimization,solver,Jupyter Notebook,https://github.com/microsoft/FLAML,https://microsoft.github.io/FLAML/,MIT,automl;tuning;optimization
529,FeatureBroker,Feature collection and inference library for ML evaluation,"A library for collecting features and performing inference for machine learning evaluations, facilitating scenarios where feature publishing is decoupled from model consumption.",AI4;AI4-02,feature_extraction;model_inference;evaluation_infrastructure,library,C++,https://github.com/microsoft/FeatureBroker,,MIT,feature-engineering;inference;evaluation
530,LMChallenge,Library and tools for evaluating predictive language models,A library and set of tools designed to evaluate predictive language models against standard benchmarks and challenges.,AI4;AI4-02,model_evaluation;nlp_metrics;benchmarking,library,Python,https://github.com/microsoft/LMChallenge,,NOASSERTION,nlp;language-models;evaluation
531,dstoolkit-e2e-presidio-evaluation,End-to-end evaluation toolkit for PII detection,"A toolkit for assessing PII detection frameworks (specifically Presidio) using Hugging Face transformers and Azure services, providing an end-to-end evaluation pipeline.",AI4;AI4-02,model_evaluation;pii_detection;security_metrics,workflow,Python,https://github.com/microsoft/dstoolkit-e2e-presidio-evaluation,,MIT,pii;evaluation;presidio
532,Hummingbird,Compiler for translating ML models to tensor computations,"A library that compiles trained traditional machine learning models into tensor computations (e.g., PyTorch, TorchScript, ONNX) for faster inference.",AI4,model_compilation;inference_optimization;scientific_computing,solver,Python,https://github.com/microsoft/hummingbird,https://microsoft.github.io/hummingbird/,MIT,inference;compiler;optimization
533,ONNX Runtime,Cross-platform high-performance ML inference accelerator,"A cross-platform, high-performance engine for machine learning inference and training, supporting models from various frameworks via the ONNX format.",AI4,model_inference;model_training;acceleration,solver,C++,https://github.com/microsoft/onnxruntime,https://onnxruntime.ai/,MIT,onnx;inference;acceleration
534,rankerEval,Numpy-based ranking metrics implementation,"A fast, numpy-based implementation of ranking metrics (such as NDCG, ERR) for evaluating information retrieval and recommendation systems.",AI4;AI4-02,ranking_evaluation;metrics;information_retrieval,library,Python,https://github.com/microsoft/rankerEval,,MIT,ranking;metrics;ir
535,Table Transformer,Table extraction model and GriTS evaluation metric,"A deep learning model for extracting tables from unstructured documents, which includes the official implementation of the GriTS evaluation metric for table structure recognition.",AI4;AI4-02,table_extraction;model_evaluation;metrics,solver,Python,https://github.com/microsoft/table-transformer,,MIT,table-extraction;grits-metric;document-analysis
536,sacrebleu,Standardized BLEU score implementation for NLP,A reference implementation of the BLEU metric that automatically downloads test sets and reports version strings to facilitate reproducible cross-lab comparisons in machine translation.,AI4;AI4-02,nlp_evaluation;metrics;translation_quality,library,Python,https://github.com/mjpost/sacrebleu,,Apache-2.0,bleu;nlp;reproducibility
537,ML Workspace,All-in-one web-based IDE for machine learning,"A web-based Integrated Development Environment (IDE) specialized for machine learning and data science, pre-configured with popular libraries and tools.",AI4,development_environment;scientific_workflow;reproducibility,platform,Jupyter Notebook,https://github.com/ml-tooling/ml-workspace,,Apache-2.0,ide;docker;data-science
538,mljar-supervised,AutoML for tabular data with explanation generation,"A Python package for Automated Machine Learning (AutoML) on tabular data, featuring automatic feature engineering, hyperparameter tuning, model explanations, and documentation generation.",AI4,automl;tabular_data;explainable_ai,solver,Python,https://github.com/mljar/mljar-supervised,https://supervised.mljar.com/,MIT,automl;tabular;explainability
539,glm-sklearn,Scikit-learn wrappers for Statsmodels GLM,"A library providing scikit-learn compatible wrappers for Generalized Linear Models (GLM) from the statsmodels library, facilitating their use in scikit-learn pipelines.",AI4;AI4-02,statistical_modeling;regression;glm,library,Python,https://github.com/modusdatascience/glm-sklearn,,BSD-3-Clause,sklearn;statsmodels;glm
540,Pweave,Scientific report generator and literate programming tool for Python,"Pweave is a scientific report generator and a literate programming tool for Python, capable of capturing results and plots from data analysis, similar to R Markdown.",AI4;Scientific Reporting,literate_programming;report_generation,workflow,Python,https://github.com/mpastell/Pweave,http://mpastell.com/pweave,BSD-3-Clause,literate-programming;reporting;reproducible-research
541,MS-Loss,Multi-Similarity Loss implementation for Deep Metric Learning,"Implementation of Multi-Similarity Loss for Deep Metric Learning, designed to improve the training of models requiring metric learning objectives.",AI4;Deep Learning,metric_learning;loss_function,library,Python,https://github.com/msight-tech/research-ms-loss,,NOASSERTION,metric-learning;loss-function;deep-learning
542,Crab,Flexible recommender engine for Python,"Crab is a recommender engine for Python that integrates classic information filtering recommendation algorithms within the scientific Python ecosystem (numpy, scipy).",AI4;Recommender Systems,recommendation;information_filtering,library,Python,https://github.com/muricoca/crab,,BSD-3-Clause,recommender-system;collaborative-filtering;scikit
543,Seaborn,Statistical data visualization library based on matplotlib,Seaborn is a Python data visualization library based on matplotlib that provides a high-level interface for drawing attractive and informative statistical graphics.,AI4;Visualization,data_visualization;statistical_plotting,library,Python,https://github.com/mwaskom/seaborn,https://seaborn.pydata.org,BSD-3-Clause,visualization;statistics;plotting
544,scores,Metrics for verification and evaluation of forecasts and models,"A library containing metrics for the verification, evaluation, and optimisation of forecasts, predictions, or models, particularly in scientific contexts.",AI4;AI4-02,model_evaluation;forecasting_verification,library,Python,https://github.com/nci/scores,https://scores.readthedocs.io,Apache-2.0,metrics;forecasting;evaluation
545,cute_ranking,Python module for calculating ranking metrics,"A lightweight Python module for calculating various information retrieval ranking metrics such as MAP, NDCG, etc.",AI4;AI4-02,ranking_evaluation;metrics,library,Python,https://github.com/ncoop57/cute_ranking,,Apache-2.0,ranking;metrics;information-retrieval
546,image-similarity-measures,Evaluation metrics for image similarity,"Implementation of eight evaluation metrics to assess the similarity between two images, including RMSE, PSNR, SSIM, ISSM, FSIM, SRE, SAM, and UIQ.",AI4;AI4-02;Computer Vision,image_similarity;evaluation_metrics,library,Python,https://github.com/nekhtiari/image-similarity-measures,,MIT,image-processing;metrics;similarity
547,CodeBERTScore,Automatic metric for code generation based on BERTScore,"An automatic evaluation metric for code generation tasks, adapting BERTScore to evaluate the quality of generated code.",AI4;AI4-02;NLP,code_generation_evaluation;metrics,library,Python,https://github.com/neulab/code-bert-score,,MIT,evaluation;code-generation;bertscore
548,ROUGE (Perl),Implementation of ROUGE metrics for summarization,"An implementation of the ROUGE family of metrics, widely used for evaluating automatic summarization and machine translation.",AI4;AI4-02;NLP,summarization_evaluation;metrics,library,Perl,https://github.com/neural-dialogue-metrics/rouge,,MIT,rouge;nlp;evaluation
549,hyppo,Multivariate hypothesis testing library,"A comprehensive Python package for multivariate hypothesis testing, including independence testing and k-sample testing.",AI4;AI4-02;Statistics,hypothesis_testing;statistical_inference,library,Python,https://github.com/neurodata/hyppo,https://hyppo.neurodata.io,MIT,statistics;hypothesis-testing;multivariate
550,fit_neuron,Estimation and evaluation of neural models from recordings,"A neuroscience package for the estimation and evaluation of neural models from patch clamp neural recordings, including spike distance metrics.",AI4;Neuroscience,neural_modeling;parameter_estimation;spike_metrics,solver,Python,https://github.com/nicodjimenez/fit_neuron,,MIT,neuroscience;modeling;spiking-neurons
551,Igel,CLI tool for training and using machine learning models,"A machine learning tool that allows users to train, test, and use models without writing code, using a YAML configuration approach.",AI4;AutoML,model_training;automl,workflow,Python,https://github.com/nidhaloff/igel,https://igel.readthedocs.io,MIT,automl;cli;machine-learning
552,NMSLIB,Non-Metric Space Library for efficient similarity search,An efficient similarity search library and a toolkit for evaluation of k-NN methods for generic non-metric spaces.,AI4;Algorithms,similarity_search;knn;nearest_neighbor,library,C++,https://github.com/nmslib/nmslib,,Apache-2.0,similarity-search;knn;indexing
553,NTUSD-Fin,Financial sentiment dictionary and scoring methods,"A financial sentiment analysis resource providing scoring methods (frequency, CFIDF, etc.) and a dictionary of words, hashtags, and emojis with sentiment scores.",AI4;Finance NLP,sentiment_analysis;lexicon_creation,dataset,Python,https://github.com/ntunlplab/Finance-NTUSD-Fin,,MIT,sentiment-analysis;finance;nlp-dataset
554,nuclia-eval,Library for evaluating RAG pipelines,A library designed for evaluating Retrieval-Augmented Generation (RAG) systems using Nuclia's models and metrics.,AI4;AI4-02;NLP,rag_evaluation;metrics,library,Python,https://github.com/nuclia/nuclia-eval,,MIT,rag;evaluation;llm
555,nullranges,Generation of null hypothesis ranges for genomic analysis,"A modular R package for generating sets of genomic ranges representing the null hypothesis, including bootstrapped ranges and matched control ranges.",AI4;Genomics,statistical_genomics;null_hypothesis_generation,library,R,https://github.com/nullranges/nullranges,https://nullranges.github.io/nullranges/,GPL-3.0,genomics;statistics;bioconductor
556,cuPyNumeric,NumPy and SciPy drop-in replacement for multi-node multi-GPU systems,"A library that enables NumPy and SciPy code to run on multi-node multi-GPU systems with minimal code changes, leveraging the Legate system.",AI4;HPC,numerical_computing;distributed_computing,library,Python,https://github.com/nv-legate/cupynumeric,https://cupynumeric.readthedocs.io,Apache-2.0,hpc;gpu;numpy
557,cosine_metric_learning,Deep Cosine Metric Learning for Person Re-identification,"Implementation of Deep Cosine Metric Learning, a method for training deep networks to learn a metric space suitable for person re-identification tasks.",AI4;Computer Vision,metric_learning;person_reidentification,library,Python,https://github.com/nwojke/cosine_metric_learning,,GPL-3.0,metric-learning;reid;deep-learning
558,ONNX,Open standard format and tools for machine learning model interoperability,"Open Neural Network Exchange (ONNX) is an open standard format for representing machine learning models, enabling interoperability between different frameworks and providing tools for model optimization and validation.",AI4;AI4-02,model_interoperability;model_optimization,library,Python,https://github.com/onnx/onnx,https://onnx.ai/,Apache-2.0,interoperability;deep-learning;standard
559,MMEval,Unified evaluation library for multiple machine learning frameworks,"A unified evaluation library that provides a wide range of metrics for various machine learning tasks, supporting multiple frameworks like PyTorch and TensorFlow, designed to streamline the model evaluation process.",AI4;AI4-02,model_evaluation;metrics_calculation,library,Python,https://github.com/open-mmlab/mmeval,https://mmeval.readthedocs.io/,Apache-2.0,evaluation;metrics;computer-vision
560,OpenLIT,OpenTelemetry-native platform for LLM observability and evaluation,"An open-source platform for AI engineering that provides observability, monitoring, and evaluation capabilities for Large Language Models (LLMs) and GPUs, integrating with OpenTelemetry standards.",AI4;AI4-02,llm_evaluation;observability;monitoring,platform,Python,https://github.com/openlit/openlit,https://docs.openlit.io/,Apache-2.0,llm;observability;opentelemetry;evaluation
561,XProf,Profiling and performance analysis tool for machine learning workloads,"A profiling tool designed to analyze the performance of machine learning models and workloads, helping developers identify bottlenecks and optimize execution on various hardware accelerators.",AI4;AI4-02,performance_profiling;optimization,solver,C++,https://github.com/openxla/xprof,,Apache-2.0,profiling;performance;machine-learning
562,optimagic,Unified interface for numerical optimization and statistical inference,"A Python package for numerical optimization that provides a unified interface to various optimizers (SciPy, NlOpt, etc.) and includes tools for diagnostic analysis and parallel numerical derivatives.",AI4;AI4-02,numerical_optimization;parameter_estimation,library,Python,https://github.com/optimagic-dev/optimagic,https://optimagic.readthedocs.io/,MIT,optimization;statistics;numerical-methods
563,pandas-ml,"Integration library for pandas, scikit-learn, and xgboost","A library that integrates pandas with scikit-learn, xgboost, and seaborn to streamline data analysis, modeling, and visualization workflows in Python.",AI4;AI4-02,data_analysis;machine_learning,library,Python,https://github.com/pandas-ml/pandas-ml,,BSD-3-Clause,pandas;scikit-learn;integration
564,dtreeviz,Visualization and interpretation library for decision trees,"A Python library for visualizing decision trees and interpreting their structure and prediction paths, aiding in model analysis and explainability.",AI4;AI4-02,model_visualization;interpretability,library,Jupyter Notebook,https://github.com/parrt/dtreeviz,,MIT,visualization;decision-trees;explainable-ai
565,sigclust2,Statistical significance testing for clustering results,"An R package for testing the statistical significance of clustering results, helping to determine if identified clusters are genuine or artifacts of random sampling.",AI4;AI4-02,statistical_testing;clustering_analysis,library,R,https://github.com/pkimes/sigclust2,,None,statistics;clustering;significance-test
566,patchworklib,Subplot manager for matplotlib and seaborn layouts,"A library that provides an intuitive interface for creating complex subplot layouts with matplotlib and seaborn, facilitating scientific data visualization.",AI4;AI4-02,scientific_visualization;plotting,library,Jupyter Notebook,https://github.com/ponnhide/patchworklib,,GPL-3.0,matplotlib;visualization;plotting
567,SecML-Torch,Library for robustness evaluation of deep learning models,"A library designed for evaluating the robustness of deep learning models against adversarial attacks, providing tools for security analysis in AI.",AI4;AI4-02,robustness_evaluation;adversarial_attacks,library,Python,https://github.com/pralab/secml-torch,https://secml-torch.readthedocs.io/,MIT,security;robustness;adversarial-ml
568,Skore,Library for ML model evaluation and reporting,"An open-source Python library that accelerates machine learning model development by providing automated evaluation reports, methodological guidance, and cross-validation analysis.",AI4;AI4-02,model_evaluation;reporting,library,Python,https://github.com/probabl-ai/skore,,MIT,evaluation;data-science;reporting
569,Psi4NumPy,Interactive quantum chemistry programming environment using Psi4 and NumPy,"A framework and collection of tutorials that bridges the Psi4 quantum chemistry package with NumPy, allowing for the development and prototyping of new quantum chemical methods and algorithms in Python.",AI4;AI4-02,quantum_chemistry_modeling;electronic_structure_calculation,library,Jupyter Notebook,https://github.com/psi4/psi4numpy,http://psi4numpy.readthedocs.io/,BSD-3-Clause,quantum-chemistry;psi4;numpy
570,MultiPy,Python library for multiple hypothesis testing,"A Python library dedicated to controlling error rates in multiple hypothesis testing, implementing various statistical correction methods like Bonferroni and FDR.",AI4;AI4-02,hypothesis_testing;statistical_analysis,library,Python,https://github.com/puolival/multipy,,BSD-3-Clause,statistics;hypothesis-testing;p-value
571,pyannote-metrics,Evaluation toolkit for speaker diarization systems,"A toolkit for reproducible evaluation, diagnostic, and error analysis of speaker diarization systems, providing standard metrics for audio processing research.",AI4;AI4-02,speaker_diarization_evaluation;error_analysis,library,Python,https://github.com/pyannote/pyannote-metrics,http://pyannote.github.io/pyannote-metrics,MIT,audio;speaker-diarization;metrics
572,PyTorch Ignite,High-level library for training and evaluating neural networks in PyTorch,"A high-level library to help with training and evaluating neural networks in PyTorch flexibly and transparently, providing metrics and event handlers.",AI4;AI4-02,model_training;model_evaluation,library,Python,https://github.com/pytorch/ignite,https://pytorch.org/ignite/,BSD-3-Clause,pytorch;training;evaluation
573,Object-Detection-Metrics,Implementation of common metrics for object detection evaluation,"A repository providing implementations of the most popular metrics used to evaluate object detection algorithms, serving as a standard reference for performance measurement.",AI4;AI4-02,object_detection_evaluation;metrics_calculation,library,Python,https://github.com/rafaelpadilla/Object-Detection-Metrics,,MIT,object-detection;metrics;computer-vision
574,Pingouin,Statistical package in Python based on Pandas,"A statistical package written in Python that provides a wide range of statistical tests and plotting functions, designed to be a simple yet exhaustive alternative to other statistical libraries.",AI4;AI4-02,statistical_analysis;hypothesis_testing,library,Python,https://github.com/raphaelvallat/pingouin,https://pingouin-stats.org/,GPL-3.0,statistics;pandas;hypothesis-testing
575,stable-worldmodel,Library for evaluating and conducting world model research,"A reliable, minimal, and scalable library designed to facilitate research and evaluation of world models in reinforcement learning and AI.",AI4;AI4-02,model_evaluation;reinforcement_learning,library,Python,https://github.com/rbalestr-lab/stable-worldmodel,,MIT,world-models;reinforcement-learning;evaluation
576,esci,Estimation Statistics with Confidence Intervals for R,"An R package for estimation statistics, focusing on effect sizes and confidence intervals to support better statistical practices in scientific research.",AI4;AI4-02,statistical_analysis;estimation_statistics,library,R,https://github.com/rcalinjageman/esci,,None,statistics;confidence-intervals;effect-size;r-package
577,scikit-plot,Visualization library for scikit-learn objects,"An intuitive library to add plotting functionality to scikit-learn objects, facilitating the visualization of machine learning metrics like confusion matrices, ROC curves, and precision-recall curves.",AI4;AI4-02,visualization;model_evaluation,library,Python,https://github.com/reiinakano/scikit-plot,https://scikit-plot.readthedocs.io,MIT,visualization;scikit-learn;machine-learning;plotting
578,supervision,Reusable computer vision tools for processing and visualization,"A comprehensive library for computer vision tasks, providing utilities for filtering, processing, and visualizing detections, segmentations, and other vision data.",AI4;AI4-02,visualization;data_processing;computer_vision,library,Python,https://github.com/roboflow/supervision,https://supervision.roboflow.com,MIT,computer-vision;visualization;object-detection;metrics
579,eulerian-remote-heartrate-detection,Remote heart rate detection via Eulerian video magnification,"A tool that implements Eulerian magnification to detect heart rates remotely from face videos, serving as a solver for physiological signal extraction.",AI4;AI4-02,signal_processing;physiological_measurement,solver,Python,https://github.com/rohintangirala/eulerian-remote-heartrate-detection,,None,computer-vision;signal-processing;heart-rate;eulerian-magnification
580,deep_metric_learning,Deep metric learning methods implemented in Chainer,A library implementing various deep metric learning algorithms and loss functions using the Chainer framework.,AI4;AI4-02,metric_learning;model_training,library,Python,https://github.com/ronekko/deep_metric_learning,,MIT,metric-learning;chainer;deep-learning
581,abcTau,Unbiased estimation of timescales and hypothesis testing,"A Python package for unbiased estimation of timescales from autocorrelation functions and performing hypothesis testing, useful in neuroscience and time series analysis.",AI4;AI4-02,statistical_analysis;time_series_analysis;hypothesis_testing,library,Jupyter Notebook,https://github.com/roxana-zeraati/abcTau,,BSD-3-Clause,neuroscience;timescales;statistics;hypothesis-testing
582,pylift,Uplift modeling and evaluation library,"A library designed for uplift modeling, providing tools for model training and evaluation to measure the incremental impact of treatments.",AI4;AI4-02,causal_inference;model_evaluation;uplift_modeling,library,Python,https://github.com/rsyi/pylift,https://pylift.readthedocs.io,BSD-2-Clause,uplift-modeling;causal-inference;machine-learning
583,paired-perm-test,Exact paired permutation significance test for accuracy,"A Python implementation of the exact paired permutation test for comparing the accuracy of two classifiers, endorsed by Rycolab.",AI4;AI4-02,statistical_testing;model_evaluation,library,Python,https://github.com/rycolab/paired-perm-test,,MIT,statistics;significance-test;permutation-test;nlp
584,nbashots,NBA shot chart visualization tool,"A tool for creating and visualizing NBA shot charts using matplotlib, seaborn, and bokeh, facilitating sports analytics.",AI4;AI4-02,visualization;sports_analytics,library,Jupyter Notebook,https://github.com/savvastj/nbashots,,BSD-3-Clause,visualization;sports-analytics;nba;matplotlib
585,pysepm,Speech enhancement performance metrics implementation,"A Python implementation of standard objective quality metrics for speech enhancement (e.g., PESQ, STOI) based on Loizou's book.",AI4;AI4-02,signal_processing;quality_metrics;speech_processing,library,Python,https://github.com/schmiph2/pysepm,,GPL-3.0,speech-enhancement;metrics;pesq;stoi
586,scikit-fuzzy,Fuzzy logic toolkit for SciPy,"A collection of fuzzy logic algorithms for use in Python, working on top of NumPy and SciPy.",AI4;AI4-02,modeling;fuzzy_logic,library,Python,https://github.com/scikit-fuzzy/scikit-fuzzy,https://pythonhosted.org/scikit-fuzzy/,NOASSERTION,fuzzy-logic;scipy;control-systems
587,pyhf,Pure-Python HistFactory implementation with tensors and autodiff,"A library for statistical modeling in High Energy Physics (HEP), implementing the HistFactory specification with support for automatic differentiation and hardware acceleration.",AI4;AI4-02,statistical_modeling;high_energy_physics,library,Python,https://github.com/scikit-hep/pyhf,https://pyhf.readthedocs.io,Apache-2.0,physics;statistics;histfactory;fitting
588,forest-confidence-interval,Confidence intervals for scikit-learn forest algorithms,A library that adds the capability to calculate confidence intervals for predictions made by scikit-learn's random forest regressors.,AI4;AI4-02,uncertainty_estimation;statistical_analysis,library,HTML,https://github.com/scikit-learn-contrib/forest-confidence-interval,http://scikit-learn-contrib.github.io/forest-confidence-interval/,MIT,random-forest;confidence-intervals;uncertainty;scikit-learn
589,metric-learn,Metric learning algorithms in Python,"A Python module implementing various supervised and weakly supervised metric learning algorithms, compatible with scikit-learn.",AI4;AI4-02,metric_learning;dimensionality_reduction,library,Python,https://github.com/scikit-learn-contrib/metric-learn,http://contrib.scikit-learn.org/metric-learn/,MIT,metric-learning;scikit-learn;machine-learning
590,scikit-learn,Machine learning in Python,"A comprehensive machine learning library for Python, providing simple and efficient tools for data mining, data analysis, and modeling.",AI4;AI4-02,machine_learning;data_analysis;modeling,library,Python,https://github.com/scikit-learn/scikit-learn,https://scikit-learn.org,BSD-3-Clause,machine-learning;data-science;statistics
591,scikit-optimize,Sequential model-based optimization,"A library for sequential model-based optimization, built on top of NumPy, SciPy, and scikit-learn, useful for hyperparameter tuning and black-box optimization.",AI4;AI4-02,optimization;hyperparameter_tuning,library,Python,https://github.com/scikit-optimize/scikit-optimize,https://scikit-optimize.github.io/,BSD-3-Clause,optimization;bayesian-optimization;hyperparameter-tuning
592,scipy,Scientific computing library for Python,"A fundamental library for scientific computing in Python, providing algorithms for optimization, integration, interpolation, eigenvalue problems, algebraic equations, and more.",AI4;AI4-02,scientific_computing;mathematical_modeling,library,Python,https://github.com/scipy/scipy,https://scipy.org,BSD-3-Clause,scientific-computing;mathematics;physics;engineering
593,SDMetrics,Metrics library for evaluating the quality and efficacy of synthetic datasets,A Python library designed to evaluate synthetic data by comparing it with real data using various statistical metrics and machine learning efficacy tests.,AI4;AI4-02,data_evaluation;synthetic_data_metrics,library,Python,https://github.com/sdv-dev/SDMetrics,https://docs.sdv.dev/sdmetrics,MIT,synthetic-data;evaluation-metrics;statistics
594,PermTest,Permutation algorithms for statistical significance testing,"A C++ library implementing efficient permutation-based statistical tests to evaluate the significance of experimental results, particularly useful in information retrieval and machine learning.",AI4;AI4-02,statistical_testing;significance_test,library,C++,https://github.com/searchivarius/PermTest,,None,permutation-test;statistics;significance
595,matchmaker,Library for training and evaluating neural re-ranking and retrieval models,"A PyTorch-based library for the training, evaluation, and analysis of dense retrieval and neural re-ranking models in information retrieval tasks.",AI4;AI4-02,model_evaluation;information_retrieval,library,Python,https://github.com/sebastian-hofstaetter/matchmaker,,Apache-2.0,neural-ir;re-ranking;evaluation
596,mht,Multiple Hypothesis Testing Procedure implementation,"A MATLAB implementation of the Multiple Hypothesis Testing procedures described in List, Shaikh, and Xu (2015) for robust statistical inference.",AI4;AI4-02,statistical_testing;hypothesis_testing,library,MATLAB,https://github.com/seidelj/mht,,None,statistics;multiple-hypothesis;matlab
597,pycomets,Significance testing for supervised learning with multimodal data,"A Python package for performing algorithm-agnostic statistical significance testing on the performance of supervised learning models, specifically designed for multimodal data contexts.",AI4;AI4-02,statistical_testing;model_evaluation,library,Python,https://github.com/shimenghuang/pycomets,,GPL-3.0,significance-testing;multimodal-learning;statistics
598,qstest,Significance test for individual communities in networks,A Python implementation of a generalized significance test for evaluating the quality and statistical significance of individual communities detected in network data.,AI4;AI4-02,statistical_testing;network_analysis,library,Python,https://github.com/skojaku/qstest,,GPL-3.0,network-science;community-detection;significance-test
599,skorch,Scikit-learn compatible neural network library wrapping PyTorch,"A library that wraps PyTorch to provide a scikit-learn compatible interface, facilitating the training, evaluation, and pipeline integration of neural networks.",AI4;AI4-02,model_training;model_evaluation,library,Python,https://github.com/skorch-dev/skorch,https://skorch.readthedocs.io/,BSD-3-Clause,pytorch;scikit-learn;wrapper
600,sktime,Unified framework for machine learning with time series,"A comprehensive library for time series analysis in Python, providing unified interfaces for various learning tasks including forecasting, classification, and regression, along with evaluation tools.",AI4;AI4-02,time_series_analysis;model_evaluation,library,Python,https://github.com/sktime/sktime,https://www.sktime.net/,BSD-3-Clause,time-series;machine-learning;forecasting
601,POPPER,Automated Hypothesis Testing with Agentic Sequential Falsifications,"A framework for automated scientific discovery that uses agentic sequential falsifications to test hypotheses, designed to support the scientific inference process.",AI4;AI4-02,hypothesis_testing;scientific_discovery,solver,Python,https://github.com/snap-stanford/POPPER,,None,hypothesis-testing;automated-discovery;falsification
602,snips-nlu-metrics,Metrics for NLU intent parsing pipelines,"A Python package providing metrics to evaluate the performance of Natural Language Understanding (NLU) intent parsing pipelines, including precision, recall, and f1-scores.",AI4;AI4-02,model_evaluation;nlp_metrics,library,Python,https://github.com/snipsco/snips-nlu-metrics,,Apache-2.0,nlp;metrics;intent-parsing
603,FPTaylor,Rigorous estimation of round-off floating-point errors,"A tool for the formal verification and rigorous estimation of round-off errors in floating-point computations, useful for ensuring numerical stability in scientific computing.",AI4;AI4-02,numerical_analysis;error_estimation,solver,OCaml,https://github.com/soarlab/FPTaylor,,MIT,floating-point;error-analysis;formal-methods
604,TidyDensity,Tidy probability/density tibbles and plots in R,"An R package that provides functions to generate and visualize probability distributions and density estimates in a tidy format, facilitating statistical analysis.",AI4;AI4-02,statistical_analysis;visualization,library,R,https://github.com/spsanderson/TidyDensity,https://www.spsanderson.com/TidyDensity/,NOASSERTION,r-package;statistics;probability-distributions
605,pauc,ROC AUC calculation with confidence intervals,"A Python package to calculate the Area Under the ROC Curve (AUC) along with confidence intervals using DeLong's method, essential for rigorous model evaluation.",AI4;AI4-02,model_evaluation;statistical_metrics,library,Jupyter Notebook,https://github.com/srijitseal/pauc,,None,roc-auc;confidence-intervals;delong-method
606,recmetrics,Metrics library for evaluating recommender systems,A Python library containing a suite of metrics specifically designed for evaluating the performance and quality of recommender systems.,AI4;AI4-02,model_evaluation;recommender_metrics,library,Jupyter Notebook,https://github.com/statisticianinstilettos/recmetrics,,MIT,recommender-systems;evaluation;metrics
607,dnn-inference,Significance tests of feature relevance for black-box learners,"A Python library for conducting statistical significance tests on feature relevance in deep neural networks and other black-box models, aiding in interpretability.",AI4;AI4-02,statistical_testing;feature_importance,library,Python,https://github.com/statmlben/dnn-inference,,MIT,significance-testing;interpretability;deep-learning
608,statsmodels,Statistical modeling and econometrics in Python,"A comprehensive Python module that provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests and statistical data exploration.",AI4;AI4-02,statistical_analysis;statistical_modeling,library,Python,https://github.com/statsmodels/statsmodels,https://www.statsmodels.org/,BSD-3-Clause,statistics;econometrics;data-analysis
609,XCurve,Library for X-Curve metrics optimizations in machine learning,"An end-to-end PyTorch library focused on optimizing and evaluating X-Curve metrics (like AUROC, AUPRC) for machine learning models, particularly in imbalance learning scenarios.",AI4;AI4-02,model_optimization;evaluation_metrics,library,Python,https://github.com/statusrank/XCurve,,None,optimization;metrics;auroc
610,mean-opinion-score,Calculate MOS and confidence intervals for TTS ratings,"A Python library for calculating the Mean Opinion Score (MOS) and its 95% confidence interval from text-to-speech ratings, implementing standard statistical methods.",AI4;AI4-02,statistical_analysis;audio_metrics,library,Python,https://github.com/stefantaubert/mean-opinion-score,,MIT,mos;statistics;tts
611,nestedcv,Nested cross-validation for prediction error confidence intervals,An R package implementing nested cross-validation procedures to provide accurate confidence intervals for prediction errors in statistical learning models.,AI4;AI4-02,statistical_validation;error_estimation,library,R,https://github.com/stephenbates19/nestedcv,,MIT,cross-validation;confidence-intervals;r-package
612,sjstats,Effect size measures and significance tests in R,"An R package providing a collection of convenient functions for common statistical computations, including effect size measures and significance tests.",AI4;AI4-02,statistical_analysis;effect_size,library,R,https://github.com/strengejacke/sjstats,,None,r-package;statistics;significance-tests
613,randomForestCI,Confidence intervals for random forests (Deprecated),An R package for calculating confidence intervals for predictions made by random forests. (Note: Deprecated in favor of 'grf' or 'ranger').,AI4;AI4-02,statistical_analysis;uncertainty_quantification,library,R,https://github.com/swager/randomForestCI,,MIT,random-forest;confidence-intervals;r-package
614,ESSENCE,Statistical significance evaluation for interferometric images,"A Python package for evaluating the statistical significance of image analysis and signal detection under correlated noise, specifically for interferometric data (e.g., ALMA, NOEMA).",AI4;AI4-02,statistical_analysis;image_analysis,library,Jupyter Notebook,https://github.com/takafumi291/ESSENCE,,MIT,astronomy;interferometry;statistics
615,GAN_Metrics-Tensorflow,Tensorflow implementation of GAN evaluation metrics,"A library providing Tensorflow implementations of standard metrics for evaluating Generative Adversarial Networks (GANs), including Inception Score, FID, and KID.",AI4;AI4-02,model_evaluation;generative_models,library,Python,https://github.com/taki0112/GAN_Metrics-Tensorflow,,MIT,gan;metrics;tensorflow
616,wer-sigtest,Statistical significance test for ASR hypotheses,"A script to perform statistical significance testing (e.g., bootstrap tests) between Automatic Speech Recognition (ASR) hypotheses, evaluating Word Error Rate (WER) differences.",AI4;AI4-02,statistical_testing;speech_recognition,solver,Shell,https://github.com/talhanai/wer-sigtest,,None,asr;significance-test;wer
617,Skflow,Simplified interface for TensorFlow mimicking Scikit Learn,"Scikit Flow (skflow) provides a simplified interface for TensorFlow, allowing users to build and train deep learning models using a syntax similar to Scikit Learn. It serves as a high-level wrapper to facilitate rapid prototyping and experimentation in machine learning research.",AI4,model_training;api_wrapper,library,Python,https://github.com/tensorflow/skflow,,Apache-2.0,tensorflow;scikit-learn;deep-learning;wrapper
618,financial-data-science,Library for financial data science workflows and econometrics,A Python library designed to support financial data science workflows. It provides tools for managing large structured and unstructured datasets and applying financial econometrics and machine learning techniques for analysis and modeling.,AI4;FinTech,data_analysis;econometrics,library,Python,https://github.com/terence-lim/financial-data-science,,MIT,finance;econometrics;data-science
619,COTK,Toolkit for fast development and fair evaluation of text generation,Conversational Toolkit (COTK) is an open-source library designed to facilitate the development and fair evaluation of text generation models. It provides standard metrics and benchmark datasets to ensure reproducible research in natural language processing.,AI4;AI4-02,evaluation;metrics;text_generation,library,Python,https://github.com/thu-coai/cotk,,Apache-2.0,nlp;evaluation;benchmark;text-generation
620,edaviz,Library for Exploratory Data Analysis and Visualization in Jupyter,Edaviz is a Python library tailored for Exploratory Data Analysis (EDA) and visualization within Jupyter Notebook or Jupyter Lab environments. It aims to streamline the process of inspecting data distributions and relationships for data science tasks.,AI4,visualization;exploratory_data_analysis,library,Python,https://github.com/tkrabel/edaviz,,NOASSERTION,eda;visualization;jupyter;data-analysis
621,torch-fidelity,High-fidelity performance metrics for generative models in PyTorch,"Torch-fidelity is a library for calculating high-fidelity performance metrics for generative models, such as Inception Score (IS) and Fréchet Inception Distance (FID), within the PyTorch framework. It ensures precise and reproducible evaluation of generative AI models.",AI4;AI4-02,evaluation;metrics;generative_models,library,Python,https://github.com/toshas/torch-fidelity,,NOASSERTION,pytorch;metrics;fid;inception-score;generative-ai
622,statannotations,Statistical significance annotations for seaborn plots,"Statannotations is a Python library that adds statistical significance annotations (such as p-values) to plots generated by seaborn. It automates the process of conducting statistical tests and visualizing the results on boxplots, barplots, and other visualizations.",AI4;AI4-02,visualization;statistical_test,library,Python,https://github.com/trevismd/statannotations,,NOASSERTION,statistics;visualization;seaborn;significance-testing
623,sovereign,Tools for state-dependent empirical analysis and forecasting,"Sovereign is an R package providing tools for state-dependent empirical analysis, including state-dependent forecasts, impulse response functions, historical decomposition, and forecast error variance decomposition. It is designed for econometric and time-series research.",AI4;AI4-02,forecasting;time_series_analysis;econometrics,library,R,https://github.com/tylerJPike/sovereign,,GPL-3.0,econometrics;forecasting;time-series;r-package
624,deltacomp,Analysis of compositional data with confidence intervals,"Deltacomp is an R package containing functions to analyze compositional data. It specifically produces confidence intervals for relative increases and decreases in compositional components, aiding in statistical inference for compositional datasets.",AI4;AI4-02,statistical_analysis;confidence_intervals,library,R,https://github.com/tystan/deltacomp,,GPL-2.0,compositional-data;statistics;confidence-intervals;r-package
625,HyperLearn,High-performance machine learning algorithms library,"HyperLearn is a machine learning library designed for speed and efficiency, offering algorithms that are significantly faster and use less memory than traditional implementations. It aims to accelerate data science and scientific computing tasks on modern hardware.",AI4,machine_learning;optimization,library,Jupyter Notebook,https://github.com/unslothai/hyperlearn,,Apache-2.0,machine-learning;performance;optimization;acceleration
626,KAIROS Scoring,Scoring and analysis software for KAIROS evaluation,Software developed by NIST for the scoring and analysis of the Knowledge Directed Artificial Intelligence Reasoning Over Schemas (KAIROS) program. It provides tools for evaluating AI reasoning capabilities against defined schemas.,AI4;AI4-02,evaluation;scoring,solver,Python,https://github.com/usnistgov/KAIROS,,NOASSERTION,evaluation;nist;ai-reasoning;scoring
627,Errudite,Interactive tool for scalable and reproducible error analysis,Errudite is an interactive tool designed for scalable and reproducible error analysis in NLP models. It allows researchers to group and analyze error patterns to better understand model failure modes.,AI4;AI4-02,error_analysis;evaluation,tool,Python,https://github.com/uwdata/errudite,,GPL-2.0,nlp;error-analysis;visualization;debugging
628,ulab,Numpy-like fast vector module for MicroPython,ulab is a numpy-like fast vector module written in C for MicroPython and CircuitPython. It enables efficient numerical and scientific computing on embedded systems and microcontrollers.,AI4,numerical_computing;embedded_computing,library,C,https://github.com/v923z/micropython-ulab,https://micropython-ulab.readthedocs.io/,MIT,micropython;numpy;numerical-computing;embedded
629,open-rag-eval,RAG evaluation tool without golden answers,open-rag-eval is a library for evaluating Retrieval-Augmented Generation (RAG) systems without the need for ground truth 'golden answers'. It provides metrics to assess the quality of retrieved context and generated responses.,AI4;AI4-02,evaluation;metrics;rag,library,Python,https://github.com/vectara/open-rag-eval,,Apache-2.0,rag;evaluation;llm;metrics
630,HyPhy,Hypothesis testing using Phylogenies,"HyPhy (Hypothesis Testing using Phylogenies) is a software package for the analysis of genetic sequences using techniques from phylogenetics, molecular evolution, and machine learning. It is widely used for detecting natural selection and evolutionary modeling.",AI4;Bio,phylogenetics;hypothesis_testing;evolutionary_analysis,solver,HyPhy,https://github.com/veg/hyphy,http://hyphy.org/,NOASSERTION,bioinformatics;phylogenetics;evolution;hypothesis-testing
631,RC-FIAP,Platform for seismic vulnerability evaluation of reinforced concrete frames,"RC-FIAP is an open virtual platform for evaluating the seismic vulnerability of reinforced concrete frames. Built on OpenSeesPy, it facilitates performance-based earthquake engineering, risk assessment, and fragility analysis of structural archetypes.",AI4;CivilEng,simulation;risk_assessment;structural_analysis,platform,Python,https://github.com/vfceball/RC-FIAP,,MIT,seismic-analysis;civil-engineering;opensees;simulation
632,PyUMLS_Similarity,Similarity metrics for UMLS concepts,PyUMLS_Similarity is a package that computes various similarity metrics between concepts in the Unified Medical Language System (UMLS). It serves as an interface to UMLS and supports biomedical informatics research.,AI4;Bio,similarity_calculation;metrics;biomedical_informatics,library,Perl,https://github.com/victormurcia/PyUMLS_Similarity,,MIT,umls;similarity-metrics;biomedical;perl
633,marginaleffects,"R package for model predictions, comparisons, and hypothesis tests","marginaleffects is an R package to compute and plot predictions, slopes, marginal means, and comparisons for over 100 classes of statistical and ML models. It supports linear and non-linear hypothesis tests and uncertainty estimation.",AI4;AI4-02,statistical_analysis;hypothesis_testing;inference,library,R,https://github.com/vincentarelbundock/marginaleffects,https://vincentarelbundock.github.io/marginaleffects/,NOASSERTION,statistics;r-package;hypothesis-testing;marginal-effects
634,MagnetLoss-PyTorch,PyTorch implementation of Magnet Loss for deep metric learning,"A PyTorch implementation of Magnet Loss, a deep metric learning technique. This repository provides a reusable component for training models with advanced metric learning objectives.",AI4;AI4-02,metric_learning;loss_function,library,Python,https://github.com/vithursant/MagnetLoss-PyTorch,,MIT,pytorch;metric-learning;loss-function;deep-learning
635,Hemm,Holistic evaluation library for multi-modal generative models,Hemm is a library for the holistic evaluation of multi-modal generative models. It integrates with Weave to provide comprehensive metrics and tracking for generative AI research.,AI4;AI4-02,evaluation;metrics;multimodal_ai,library,Python,https://github.com/wandb/Hemm,,Apache-2.0,evaluation;generative-ai;multimodal;metrics
636,pretty-print-confusion-matrix,Utility for plotting confusion matrices in Python,"A Python utility to plot aesthetically pleasing confusion matrices using seaborn and matplotlib, similar to Matlab's style. It aids in the visualization of classification model performance metrics.",AI4;AI4-02,visualization;metrics_visualization,library,Python,https://github.com/wcipriano/pretty-print-confusion-matrix,,Apache-2.0,confusion-matrix;visualization;python;matplotlib
637,statannot,Statistical annotations for seaborn boxplots,Statannot is a Python package that adds statistical significance annotations (p-values) to existing boxplots generated by seaborn. It helps in visualizing the results of statistical tests directly on data plots.,AI4;AI4-02,visualization;statistical_test,library,Jupyter Notebook,https://github.com/webermarcolivier/statannot,,MIT,statistics;visualization;seaborn;p-value
638,LangKit,Open-source toolkit for monitoring and evaluating Large Language Models (LLMs),"A comprehensive toolkit designed for monitoring LLMs by extracting signals from prompts and responses. It provides metrics for text quality, relevance, sentiment analysis, and safety, ensuring the security and reliability of LLM applications.",AI4;AI4-02,monitoring;text_quality_metrics;sentiment_analysis,library,Jupyter Notebook,https://github.com/whylabs/langkit,,Apache-2.0,llm;observability;nlp;metrics
639,neleval,Evaluation and error analysis tool for Named Entity Linking (NEL) systems,A tool designed to evaluate entity disambiguation and named entity linking systems. It facilitates error analysis and provides standard metrics to assess the performance of NEL models.,AI4;AI4-02,entity_linking;evaluation;error_analysis,library,Python,https://github.com/wikilinks/neleval,,Apache-2.0,nel;nlp;evaluation;disambiguation
640,wmt-format-tools,Tools for formatting WMT hypothesis and test sets,A collection of utilities for processing and formatting data for the Workshop on Machine Translation (WMT). It handles XML conversion and standardization of hypothesis and test sets for translation benchmarks.,AI4;AI4-01,data_formatting;benchmark_preparation,workflow,Hare,https://github.com/wmt-conference/wmt-format-tools,,Apache-2.0,wmt;machine-translation;xml;formatting
641,bm25s,Fast Python implementation of BM25 algorithm for lexical search,"A high-performance implementation of the BM25 ranking function using Numpy, Numba, and Scipy. It is designed for fast lexical search and information retrieval tasks in Python.",AI4;AI4-02,information_retrieval;ranking;search,library,Python,https://github.com/xhluca/bm25s,,MIT,bm25;search;ranking;numba
642,AB3DMOT,3D Multi-Object Tracking baseline and evaluation metrics,An official implementation providing a baseline for 3D Multi-Object Tracking (MOT) along with new evaluation metrics. It serves as a standard benchmark tool for autonomous driving perception tasks.,AI4;AI4-02,object_tracking;benchmark;evaluation,solver,Python,https://github.com/xinshuoweng/AB3DMOT,,NOASSERTION,3d-tracking;autonomous-driving;metrics;benchmark
643,MLmetrics,Collection of Machine Learning evaluation metrics for R,"An R package that provides a comprehensive set of standard evaluation metrics for machine learning tasks, including classification and regression performance indicators.",AI4;AI4-02,model_evaluation;metrics,library,R,https://github.com/yanyachen/MLmetrics,,GPL-2.0,r;machine-learning;evaluation;metrics
644,summary-reward-no-reference,Reference-free metric for measuring text summary quality,"A Python implementation of a reference-free metric for evaluating the quality of text summaries. The metric is learned from human ratings, allowing for quality assessment without ground truth references.",AI4;AI4-02,text_summarization;evaluation_metric,library,Python,https://github.com/yg211/summary-reward-no-reference,,Apache-2.0,nlp;summarization;metrics;reference-free
645,AlignScore,Metric for factual consistency evaluation in text generation,"An implementation of AlignScore, a metric designed to evaluate the factual consistency of generated text against source information, addressing hallucination issues in NLP models.",AI4;AI4-02,factual_consistency;evaluation_metric;hallucination_detection,library,Python,https://github.com/yuh-zha/AlignScore,,MIT,nlp;consistency;metrics;hallucination
646,ashpy,TensorFlow 2.0 library for distributed training and evaluation,"A library built on TensorFlow 2.0 that facilitates distributed training, evaluation, model selection, and fast prototyping of deep learning models.",AI4;AI4-02,model_training;evaluation;model_selection,library,Python,https://github.com/zurutech/ashpy,,Apache-2.0,tensorflow;training;evaluation;deep-learning
647,GraphRAG Agent Framework,"Integrated framework for GraphRAG construction, search, and custom evaluation","A comprehensive framework integrating GraphRAG, LightRAG, and Neo4j for knowledge graph construction and search, featuring a custom evaluation framework for assessing GraphRAG performance.",AI4;AI4-03,evaluation;rag_framework,framework,Python,https://github.com/1517005260/graph-rag-agent,,MIT,rag;graph-rag;evaluation-framework
648,lm-evaluation,Evaluation suite for large-scale language models,"A suite of tools and metrics for evaluating the performance of large-scale language models, developed by AI21 Labs.",AI4;AI4-03,model_evaluation;nlp_metrics,library,Python,https://github.com/AI21Labs/lm-evaluation,,Apache-2.0,llm;evaluation;nlp
649,AutoQuant,"Automation framework for ML, forecasting, and model evaluation","An R framework for automating machine learning tasks, time series forecasting, model evaluation, and interpretation.",AI4;AI4-03,automl;forecasting;model_evaluation,framework,R,https://github.com/AdrianAntico/AutoQuant,,AGPL-3.0,automl;forecasting;evaluation
650,Agenta,Open-source LLMOps platform for prompt management and evaluation,"A platform for LLM operations that includes features for prompt playground, management, LLM evaluation, and observability.",AI4;AI4-03,llmops;model_evaluation;prompt_engineering,platform,Python,https://github.com/Agenta-AI/agenta,https://agenta.ai,NOASSERTION,llmops;evaluation;observability
651,Bjontegaard Metric,Calculation tool for Bjontegaard metric (BD-PSNR and BD-rate),"A Python implementation for calculating the Bjontegaard metric, including BD-PSNR and BD-rate, commonly used for evaluating video compression performance.",AI4;AI4-03,metric_calculation;video_compression_eval,library,Python,https://github.com/Anserw/Bjontegaard_metric,,None,video-coding;metrics;bd-rate
652,cell-eval,Evaluation suite for perturbation prediction models,"A comprehensive suite for evaluating models designed to predict cellular perturbations, developed by Arc Institute.",AI4;AI4-03,bioinformatics_eval;perturbation_prediction,library,Python,https://github.com/ArcInstitute/cell-eval,,MIT,biology;perturbation;evaluation
653,BigDataBench MicroBenchmark,Micro-benchmark suite for Big Data systems,A micro-benchmark suite from BigDataBench V5.0 for evaluating the performance of big data systems and components.,AI4;AI4-03,benchmarking;system_evaluation,solver,Java,https://github.com/BenchCouncil/BigDataBench_V5.0_BigData_MicroBenchmark,http://www.benchcouncil.org/,Apache-2.0,big-data;benchmark;microbenchmark
654,Ethereum Economic Model,Dynamical-systems model of Ethereum validator economics,A modular dynamical-systems model for simulating and analyzing the economics of Ethereum validators.,AI4;AI4-03,economic_simulation;system_modeling,solver,Jupyter Notebook,https://github.com/CADLabs/ethereum-economic-model,,GPL-3.0,ethereum;economics;simulation
655,Gadget,Benchmark harness for streaming state stores,A benchmark harness designed for the systematic and robust evaluation of streaming state stores in data systems.,AI4;AI4-03,benchmarking;streaming_systems,solver,C++,https://github.com/CASP-Systems-BU/Gadget,,Apache-2.0,streaming;benchmark;state-store
656,CS-Eval,Evaluation suite for cybersecurity models,A comprehensive evaluation suite for assessing the cybersecurity capabilities of fundamental models or large language models.,AI4;AI4-03,model_evaluation;cybersecurity_eval,framework,,https://github.com/CS-EVAL/CS-Eval,,MIT,cybersecurity;llm;evaluation
657,Matcha (VariationAnalysis),Framework for training and evaluating genomic variation models,"The Matcha framework, part of the VariationAnalysis project, used to train and evaluate deep learning models for calling genomic variations.",AI4;AI4-03,genomics_eval;model_training,framework,Java,https://github.com/CampagneLaboratory/variationanalysis,,NOASSERTION,genomics;deep-learning;variation-calling
658,SURE,Library for assessing synthetic tabular data utility and privacy,An open-source Python library for evaluating the utility and privacy performance of tabular synthetic datasets.,AI4;AI4-03,data_evaluation;synthetic_data,library,Python,https://github.com/Clearbox-AI/SURE,,Apache-2.0,synthetic-data;evaluation;privacy
659,AI Agents Reality Check,Mathematical benchmark for AI agent performance,A benchmark suite that exposes the performance gap between real agents and LLM wrappers through rigorous multi-dimensional evaluation and statistical validation.,AI4;AI4-03,agent_evaluation;benchmarking,solver,Python,https://github.com/Cre4T3Tiv3/ai-agents-reality-check,,Apache-2.0,ai-agents;benchmark;evaluation
660,DMind Benchmark,Benchmark for LLMs on blockchain and Web3 knowledge,"A comprehensive framework for evaluating large language models on their knowledge of blockchain, cryptocurrency, and Web3 domains.",AI4;AI4-03,domain_evaluation;llm_benchmark,framework,Python,https://github.com/DMindAI/DMind-Benchmark,,None,blockchain;llm;benchmark
661,Benchmarking Big Streams Systems,Benchmark suite for big streaming systems,An extension of Yahoo!'s benchmarking suite designed for evaluating the performance of big streaming data systems.,AI4;AI4-03,benchmarking;streaming_systems,solver,Java,https://github.com/DataSystemsGroupUT/Benchmarking-Big-Streams-Systems,,Apache-2.0,streaming;benchmark;big-data
662,DreamLayer,Benchmarking and evaluation automation tool for diffusion models,"A tool designed to benchmark diffusion models faster by automating evaluations, seed management, and metric calculation to ensure reproducible results.",AI4;AI4-03,model_evaluation;benchmarking;reproducibility,solver,Python,https://github.com/DreamLayer-AI/DreamLayer,,GPL-3.0,diffusion-models;benchmarking;evaluation-harness
663,E3SM,Energy Exascale Earth System Model,The Energy Exascale Earth System Model (E3SM) is a state-of-the-art earth system modeling project designed to simulate the earth's climate system at high resolution.,AI4;Earth Science,climate_modeling;simulation,solver,Fortran,https://github.com/E3SM-Project/E3SM,https://e3sm.org/,NOASSERTION,earth-system-model;climate-simulation;exascale
664,lm-evaluation-harness,Few-shot evaluation framework for language models,"A framework for few-shot evaluation of language models, providing a unified interface to benchmark models on a wide variety of tasks.",AI4;AI4-03,model_evaluation;benchmarking,library,Python,https://github.com/EleutherAI/lm-evaluation-harness,,MIT,llm;evaluation;few-shot;nlp
665,fdsvismap,Visibility verification tool for fire safety assessment,"A tool for waypoint-based verification of visibility within the scope of performance-based fire safety assessment, likely interfacing with Fire Dynamics Simulator (FDS) data.",AI4;Physics,safety_assessment;simulation_analysis,solver,Python,https://github.com/FireDynamics/fdsvismap,,MIT,fire-dynamics;safety-engineering;visibility-analysis
666,llm-benchmarker-suite,Leaderboard and benchmarking suite for LLM evaluations,"A suite designed for benchmarking Large Language Models, providing tools to run evaluations and generate leaderboard rankings.",AI4;AI4-03,model_evaluation;benchmarking,solver,Python,https://github.com/FormulaMonks/llm-benchmarker-suite,,MIT,llm;leaderboard;benchmarking
667,LLMZoo,"Data, models, and evaluation benchmarks for LLMs","A project providing a collection of data, models, and evaluation benchmarks specifically for Large Language Models, facilitating comparative analysis.",AI4;AI4-03,model_evaluation;dataset_management,dataset,Python,https://github.com/FreedomIntelligence/LLMZoo,,Apache-2.0,llm;benchmark;model-zoo
668,gee_landcover_metrics,Land cover metrics calculation for Google Earth Engine,"A tool for calculating landscape metrics on land cover data within the Google Earth Engine platform, supporting spatial analysis in earth sciences.",AI4;Earth Science,spatial_analysis;metrics_calculation,library,JavaScript,https://github.com/Helmholtz-UFZ/gee_landcover_metrics,,MIT,google-earth-engine;land-cover;landscape-metrics
669,UHGEval,Evaluation framework for hallucination in LLMs,"A user-friendly evaluation framework and benchmark suite (including HaluEval, HalluQA) designed to assess hallucination generation in Large Language Models.",AI4;AI4-03,hallucination_detection;model_evaluation,solver,Python,https://github.com/IAAR-Shanghai/UHGEval,,Apache-2.0,hallucination;llm-evaluation;benchmark
670,GraFiTe,Platform for tracking domain-specific model issues,A platform to track and manage domain-specific model issues for continuous evaluation of Large Language Models.,AI4;AI4-03,issue_tracking;continuous_evaluation,platform,TypeScript,https://github.com/IBM/grafite,,Apache-2.0,llm-evaluation;model-governance;issue-tracking
671,FakeFinder,Framework for evaluating deepfake detection models,"A modular framework for evaluating various deepfake detection models, providing tools to assess model performance against manipulated media.",AI4;AI4-03,deepfake_detection;model_evaluation,solver,Python,https://github.com/IQTLabs/FakeFinder,,Apache-2.0,deepfake;evaluation-framework;media-forensics
672,CGM_Performance_Assessment,Statistical performance assessment for CGM systems,"A collection of software packages for the statistical performance assessment of Continuous Glucose Monitoring (CGM) systems, used in medical data analysis.",AI4;Life Science,statistical_analysis;performance_assessment,library,Python,https://github.com/IfDTUlm/CGM_Performance_Assessment,,MIT,cgm;diabetes-technology;statistical-analysis
673,Deepmark,Testing environment for LLM assessment,A testing environment enabling the assessment of language models (LLMs) on task-specific metrics and custom data for predictable performance.,AI4;AI4-03,model_evaluation;performance_testing,platform,PHP,https://github.com/IngestAI/deepmark,,AGPL-3.0,llm-testing;metrics;evaluation
674,Factorio Learning Environment,Environment for evaluating LLMs in Factorio,"A non-saturating, open-ended environment designed for evaluating Large Language Models and Reinforcement Learning agents within the Factorio game simulation.",AI4;AI4-03,rl_environment;agent_evaluation,solver,Python,https://github.com/JackHopkins/factorio-learning-environment,,NOASSERTION,reinforcement-learning;factorio;benchmark-environment
675,ModelMetrics,R package for rapid calculation of model metrics,An R library designed to facilitate the rapid calculation of various statistical model performance metrics.,AI4;Statistics,statistical_analysis;metrics_calculation,library,R,https://github.com/JackStat/ModelMetrics,,None,r-package;statistics;model-evaluation
676,DetectionMetrics,Evaluation tool for perception models,A tool designed to unify and streamline the evaluation of perception (object detection) models across different frameworks and datasets.,AI4;Computer Vision,object_detection;model_evaluation,solver,Python,https://github.com/JdeRobot/DetectionMetrics,,GPL-3.0,computer-vision;object-detection;metrics
677,MixEval,Evaluation suite for MixEval benchmark,"The official evaluation suite and dynamic data release for the MixEval benchmark, designed for assessing language model performance.",AI4;AI4-03,model_evaluation;benchmarking,solver,Python,https://github.com/JinjieNi/MixEval,,None,llm;benchmark;evaluation-suite
678,LLM-Writing-Assessment-Psychometric-Framework,Psychometric framework for evaluating LLMs as raters,"A repository and framework for evaluating large language models when used as raters in large-scale writing assessments, focusing on reliability and validity from a psychometric perspective.",AI4;Social Science,psychometrics;model_evaluation;reliability_analysis,solver,Python,https://github.com/John-Wang-0809/LLM-Writing-Assessment-Psychometric-Framework,,MIT,psychometrics;llm-evaluation;automated-scoring
679,multihop-edit-eval,Fine-grained evaluation framework for multi-hop knowledge editing in LLMs,A specialized evaluation framework designed to assess the performance of Large Language Models in multi-hop knowledge editing tasks. It provides metrics and datasets to measure how well models can update their knowledge base across connected facts.,AI4;AI4-03,model_evaluation;knowledge_editing_assessment,solver,Python,https://github.com/KUNLP/multihop-edit-eval,,None,llm-evaluation;knowledge-editing;multi-hop-reasoning
680,q-evaluation-harness,Evaluation framework for LLMs on Q/kdb+ code generation tasks,An open-source framework developed by KX Systems to evaluate the performance of Large Language Models specifically on generating Q/kdb+ code. It serves as a domain-specific benchmark harness for financial time-series database languages.,AI4;AI4-03,code_generation_evaluation;llm_benchmark,solver,Python,https://github.com/KxSystems/q-evaluation-harness,,MIT,q;kdb+;llm-evaluation;code-generation
681,aac-metrics,Metrics library for evaluating Automated Audio Captioning systems,A Python library designed for PyTorch that implements various metrics for evaluating Automated Audio Captioning (AAC) systems. It provides a standardized way to measure the quality of generated audio captions against reference annotations.,AI4;AI4-03,audio_captioning_evaluation;metrics_calculation,library,Python,https://github.com/Labbeti/aac-metrics,,MIT,audio-captioning;evaluation-metrics;pytorch
682,eurybia,Model drift monitoring and data validation library,A Python library designed to monitor machine learning model drift over time and secure model deployment through rigorous data validation. It helps in maintaining the reliability of AI systems in production environments.,AI4;AI4-03,model_monitoring;drift_detection;data_validation,library,Jupyter Notebook,https://github.com/MAIF/eurybia,https://eurybia.readthedocs.io/,Apache-2.0,model-drift;data-validation;mlops
683,VideoEval,Benchmark suite for evaluation of Video Foundation Models,A comprehensive benchmark suite designed for the low-cost and efficient evaluation of Video Foundation Models. It provides a set of tasks and metrics to assess the capabilities of video understanding and generation models.,AI4;AI4-03,video_model_evaluation;benchmark_suite,solver,Python,https://github.com/MCG-NJU/VideoEval,,None,video-understanding;foundation-models;benchmark
684,MHKiT-MATLAB,Marine and Hydrokinetic Toolkit (MATLAB),"A MATLAB toolkit providing standardized data processing, visualization, quality control, and resource assessment tools for the marine renewable energy community. It supports wave, tidal, and river energy research.",AI4;AI4-03,data_processing;resource_assessment;quality_control,library,MATLAB,https://github.com/MHKiT-Software/MHKiT-MATLAB,https://mhkit-software.github.io/MHKiT/,BSD-3-Clause,marine-energy;hydrokinetic;data-analysis
685,MHKiT-Python,Marine and Hydrokinetic Toolkit (Python),"A Python toolkit providing standardized data processing, visualization, quality control, and resource assessment tools for the marine renewable energy community. It serves as the Python counterpart to MHKiT-MATLAB.",AI4;AI4-03,data_processing;resource_assessment;quality_control,library,Python,https://github.com/MHKiT-Software/MHKiT-Python,https://mhkit-software.github.io/MHKiT/,BSD-3-Clause,marine-energy;hydrokinetic;data-analysis
686,AutoRAG,AutoML-style framework for RAG evaluation and optimization,An open-source framework designed to evaluate and optimize Retrieval-Augmented Generation (RAG) pipelines. It applies AutoML principles to automatically find the best RAG configuration for a given dataset and use case.,AI4;AI4-03,rag_evaluation;pipeline_optimization;automl,framework,Python,https://github.com/Marker-Inc-Korea/AutoRAG,https://docs.autorag.io/,Apache-2.0,rag;evaluation;automl;optimization
687,ContainerInception (Gerber),Generalized Easy Reproducible Bioinformatics Environment wRapper,A tool developed during NCBI Hackathons to facilitate reproducible bioinformatics research by wrapping environments in containers. It aims to simplify the creation and sharing of reproducible scientific workflows.,AI4;AI4-03,reproducibility;workflow_management;containerization,workflow,Python,https://github.com/NCBI-Hackathons/ContainerInception,,MIT,bioinformatics;reproducibility;docker
688,OpenOA,Framework for assessing wind plant performance,An open-source framework developed by NREL for assessing wind plant performance using operational assessment (OA) methodologies. It provides data structures and analysis methods for processing time-series data from wind plants.,AI4;AI4-03,performance_assessment;data_analysis;wind_energy,framework,Jupyter Notebook,https://github.com/NREL/OpenOA,https://openoa.readthedocs.io/,BSD-3-Clause,wind-energy;operational-assessment;nrel
689,compute-eval,Evaluation framework for LLM-based CUDA code generation,A framework by NVIDIA designed to generate and evaluate CUDA code produced by Large Language Models. It provides tools to assess the correctness and performance of AI-generated GPU kernels.,AI4;AI4-03,code_generation_evaluation;cuda_benchmarking,solver,Python,https://github.com/NVIDIA/compute-eval,,NOASSERTION,cuda;llm;code-generation;evaluation
690,atropos,LLM Reinforcement Learning Environments framework,A framework for collecting and evaluating Large Language Model (LLM) trajectories through diverse environments. It serves as a testbed for Reinforcement Learning with LLMs.,AI4;AI4-03,rlhf_evaluation;trajectory_collection;environment_simulation,framework,Python,https://github.com/NousResearch/atropos,,MIT,llm;reinforcement-learning;evaluation-environment
691,gptables,Tool for writing consistently formatted statistical tables,"A Python wrapper around XlsxWriter developed by the Office for National Statistics (ONS) to produce consistently formatted statistical tables in Excel, ensuring data reporting standards.",AI4;AI4-03,data_reporting;statistical_formatting,library,Python,https://github.com/ONSdigital/gptables,https://gptables.readthedocs.io/,NOASSERTION,statistics;reporting;excel;ons
692,OneIG-Benchmark,Fine-grained evaluation benchmark for Text-to-Image models,"A comprehensive benchmark framework designed for the fine-grained evaluation of Text-to-Image (T2I) models. It assesses dimensions such as subject-element alignment, text rendering precision, reasoning, stylization, and diversity.",AI4;AI4-03,t2i_evaluation;image_generation_benchmark,framework,Python,https://github.com/OneIG-Bench/OneIG-Benchmark,,None,text-to-image;benchmark;evaluation
693,llm-colosseum,Game-based evaluation harness for LLMs,A unique benchmarking tool that evaluates Large Language Models by having them compete in the Street Fighter 3 game. It tests the models' ability to make real-time decisions and strategize in a dynamic environment.,AI4;AI4-03,agent_evaluation;game_benchmark,solver,Jupyter Notebook,https://github.com/OpenGenerativeAI/llm-colosseum,,MIT,llm-agent;game-benchmark;evaluation
694,Paddle Continuous Evaluation,Continuous evaluation platform for PaddlePaddle framework,A macro continuous evaluation platform designed for the PaddlePaddle deep learning framework to monitor model performance and regression.,AI Toolchain;Deep Learning,continuous_evaluation;model_regression_testing,platform,Python,https://github.com/PaddlePaddle/continuous_evaluation,,None,paddlepaddle;ci;evaluation;regression
695,RAG-evaluation-harnesses,Evaluation suite for Retrieval-Augmented Generation (RAG),A specialized evaluation harness designed to assess the performance of Retrieval-Augmented Generation systems.,AI4;NLP,rag_evaluation;metrics,library,Python,https://github.com/RulinShao/RAG-evaluation-harnesses,,MIT,rag;evaluation;nlp
696,RuSentEval,Probing suite for Russian language models,A benchmark and probing suite for evaluating the linguistic capabilities of Russian embedding and language models.,AI4;NLP,probing;model_evaluation;linguistics,library,Python,https://github.com/RussianNLP/RuSentEval,,Apache-2.0,russian-nlp;probing-tasks;evaluation
697,MAP (Mapping Accessibility),Urban network modeling and accessibility metric calculation tool,A software package for creating urban network models and calculating cumulative accessibility metrics and spatial justice indicators for urban planning.,Urban Planning;Spatial Science,network_modeling;accessibility_analysis;spatial_metrics,library,Jupyter Notebook,https://github.com/RuthJNelson/MAP-Mapping-Accessibility-for-Ethically-Informed-Urban-Planning,,MIT,urban-planning;accessibility;network-analysis
698,Langtrace,Observability and evaluation tool for LLM applications,"An Open Telemetry based tool for tracing, evaluating, and monitoring LLM applications, supporting various LLMs and vector databases.",AI4;MLOps,observability;tracing;model_evaluation,platform,TypeScript,https://github.com/Scale3-Labs/langtrace,,AGPL-3.0,llm-ops;observability;evaluation;opentelemetry
699,ScienceEval,Evaluation suite for scientific foundation models,An open-source evaluation suite specifically designed for assessing the capabilities of ScienceOne Base models and other scientific LLMs.,AI4S;Scientific AI,model_evaluation;scientific_reasoning,library,Python,https://github.com/ScienceOne-AI/ScienceEval,,Apache-2.0,science-llm;evaluation;benchmark
700,Gorilla,Benchmark and training framework for LLM function calling,A framework and benchmark for training and evaluating Large Language Models on their ability to perform function calls (tool use).,AI4;NLP,function_calling;model_evaluation;training,library,Python,https://github.com/ShishirPatil/gorilla,,Apache-2.0,llm;function-calling;benchmark;tool-use
701,TransformerQuant,Framework for deep learning models in quantitative trading,A framework for training and evaluating deep learning models specifically for the quantitative trading domain.,Quantitative Finance;AI4,model_training;model_evaluation;quantitative_analysis,library,Python,https://github.com/StateOfTheArt-quant/transformerquant,,Apache-2.0,quantitative-trading;deep-learning;evaluation
702,CellSPA,Cell Segmentation Performance Assessment tool,A tool for assessing the performance of cell segmentation algorithms in biological imaging.,Bioinformatics;Imaging,segmentation_evaluation;image_analysis,library,R,https://github.com/SydneyBioX/CellSPA,,None,cell-segmentation;evaluation;bioinformatics
703,HPCPerfStats,HPC resource-usage monitoring and analysis package,"An automated resource-usage monitoring and analysis package for High Performance Computing (HPC) clusters, supporting scientific computing infrastructure.",HPC;Scientific Computing,performance_monitoring;resource_analysis,tool,C,https://github.com/TACC/HPCPerfStats,,LGPL-2.1,hpc;monitoring;performance-analysis
704,AgentBench,Comprehensive benchmark to evaluate LLMs as Agents,A benchmark suite designed to evaluate the capabilities of Large Language Models acting as autonomous agents across various environments.,AI4;NLP,agent_evaluation;benchmarking,library,Python,https://github.com/THUDM/AgentBench,,Apache-2.0,llm-agent;benchmark;evaluation
705,AICGSecEval,AI-generated code security evaluation benchmark,A repository-level benchmark for evaluating the security of code generated by Artificial Intelligence models.,AI4;Software Security,security_evaluation;code_generation_benchmarking,library,Python,https://github.com/Tencent/AICGSecEval,,None,ai-security;code-generation;evaluation
706,Afrobench Eval Suite,LLM evaluation leaderboard for African Languages,An evaluation suite and leaderboard for assessing Large Language Model performance on African languages.,AI4;NLP,multilingual_evaluation;benchmarking,library,Jupyter Notebook,https://github.com/The-African-Research-Collective/afrobench-eval-suite,,Apache-2.0,african-languages;nlp;evaluation
707,TimeCopilot,GenAI Forecasting Agent for time series analysis,"An agent-based tool leveraging LLMs and Time Series Foundation Models for forecasting, cross-validation, and anomaly detection in domains like finance and energy.",AI4S;Time Series Analysis,forecasting;anomaly_detection;cross_validation,solver,Python,https://github.com/TimeCopilot/timecopilot,,MIT,time-series;forecasting;llm-agent;anomaly-detection
708,Inspect,Framework for large language model evaluations,An open-source framework for creating and running evaluations for Large Language Models (LLMs).,AI4;NLP,model_evaluation;benchmarking,library,Python,https://github.com/UKGovernmentBEIS/inspect_ai,,MIT,llm;evaluation;framework
709,VMEvalKit,Evaluation framework for reasoning capabilities in foundational video models,"A comprehensive framework designed to evaluate the reasoning abilities of video foundational models, providing metrics and protocols for assessing model performance in understanding complex video content.",AI4;AI4-03,model_evaluation;reasoning_assessment,solver,Python,https://github.com/Video-Reason/VMEvalKit,,Apache-2.0,video-understanding;evaluation-framework;reasoning
710,owl-eval,Evaluation harness for diffusion world models,"A specialized evaluation harness designed to assess the performance and capabilities of diffusion-based world models, facilitating reproducible benchmarking in generative modeling research.",AI4;AI4-03,model_evaluation;diffusion_models,solver,TypeScript,https://github.com/Wayfarer-Labs/owl-eval,,MIT,evaluation-harness;diffusion-models;world-models
711,OmniBenchmark,Benchmark for evaluating pre-trained vision models and contrastive learning,"A benchmark suite and framework for evaluating pre-trained computer vision models, featuring a supervised contrastive learning framework to assess model robustness and transferability.",AI4;AI4-03,model_evaluation;computer_vision,solver,Python,https://github.com/ZhangYuanhan-AI/OmniBenchmark,,None,benchmark;pre-trained-models;contrastive-learning
712,PertEval,Evaluation suite for transcriptomic perturbation effect prediction models,"A specialized evaluation suite for assessing models that predict transcriptomic perturbation effects, including support for single-cell foundation models, aiding in computational biology research.",AI4;AI4-03,model_evaluation;computational_biology,solver,Python,https://github.com/aaronwtr/PertEval,,MIT,transcriptomics;perturbation-prediction;single-cell
713,indic_eval,Evaluation suite for assessing Indic LLMs across diverse tasks,"A lightweight evaluation suite tailored for benchmarking Large Language Models on Indic language tasks, facilitating the assessment of multilingual and low-resource language models.",AI4;AI4-03,model_evaluation;nlp,solver,Python,https://github.com/adithya-s-k/indic_eval,,MIT,indic-languages;llm-evaluation;nlp-benchmark
714,OLMo-Eval,Evaluation suite for Open Language Models (OLMo),"A comprehensive evaluation suite developed by AllenAI for benchmarking Large Language Models, specifically supporting the OLMo ecosystem and general LLM performance assessment.",AI4;AI4-03,model_evaluation;llm_benchmark,solver,Python,https://github.com/allenai/OLMo-Eval,,Apache-2.0,llm;evaluation;allenai
715,deepfabric,"Platform for dataset curation, training, and evaluation of AI models","An integrated framework designed to streamline the lifecycle of AI model development, including high-quality dataset curation, model training, and evaluation, facilitating reproducible AI research.",AI4;AI4-03,workflow_automation;model_evaluation,platform,Python,https://github.com/always-further/deepfabric,,Apache-2.0,mlops;dataset-curation;evaluation-pipeline
716,Apache Liminal,Workflow orchestration for automating machine learning pipelines,"An Apache incubator project that provides a domain-specific language to build, orchestrate, and operationalize machine learning workflows, bridging the gap between experimentation and production inference.",AI4;AI4-03,workflow_automation;pipeline_orchestration,workflow,Python,https://github.com/apache/incubator-liminal,,Apache-2.0,mlops;workflow;apache-airflow
717,rcaaqs,R package for calculating air quality metrics based on Canadian standards,"An R package developed by the British Columbia Government to facilitate the calculation of air quality metrics (PM2.5, Ozone, NO2, SO2) according to the Canadian Ambient Air Quality Standards (CAAQS). It aids in environmental science data processing and regulatory reporting.",Environmental Science;Atmospheric Science,data_processing;metrics_calculation,library,R,https://github.com/bcgov/rcaaqs,,Apache-2.0,air-quality;environmental-metrics;r-package
718,ASAP-AES Metrics,Evaluation metrics for Automated Essay Scoring (Quadratic Weighted Kappa),"Provides the reference implementation of evaluation metrics, specifically the Quadratic Weighted Kappa, used for the Automated Student Assessment Prize (ASAP) Automated Essay Scoring competition. Useful for evaluating NLP models in scoring tasks.",AI4;AI4-03,metrics_calculation;model_evaluation,library,Python,https://github.com/benhamner/ASAP-AES,,BSD-2-Clause,nlp;evaluation-metric;kappa
719,survcomp,Performance assessment and comparison for survival analysis models,"An R package providing functions to assess and compare the performance of risk prediction models in survival analysis, widely used in biostatistics and medical research (e.g., cancer prognosis).",Biostatistics;Medical Science,model_evaluation;statistical_analysis,library,C++,https://github.com/bhklab/survcomp,https://www.bioconductor.org/packages/release/bioc/html/survcomp.html,NOASSERTION,survival-analysis;bioconductor;risk-prediction
720,BigCode Evaluation Harness,Evaluation framework for code generation language models,"A framework for the evaluation of autoregressive code generation language models, supporting various coding tasks and metrics. Developed by the BigCode project to standardize code LLM assessment.",AI4;AI4-03,model_evaluation;benchmark,workflow,Python,https://github.com/bigcode-project/bigcode-evaluation-harness,,Apache-2.0,code-generation;llm-evaluation;benchmark
721,Realistic SSL Evaluation,Benchmark suite for realistic evaluation of Semi-Supervised Learning,"A benchmark suite and codebase for evaluating Deep Semi-Supervised Learning (SSL) algorithms under realistic conditions, developed by Google Brain research.",AI4;AI4-03,model_evaluation;benchmark,workflow,Python,https://github.com/brain-research/realistic-ssl-evaluation,,Apache-2.0,semi-supervised-learning;benchmark;deep-learning
722,BytevalKit-Emb,Modular evaluation framework for embedding models,"A modular framework developed by ByteDance for evaluating embedding models, supporting automated performance assessment across multiple task types and standardized processes.",AI4;AI4-03,model_evaluation;benchmark,workflow,Python,https://github.com/bytedance/BytevalKit-Emb,,Apache-2.0,embedding-models;evaluation-framework;nlp
723,TMU,Library implementing Tsetlin Machine algorithms for logic-based AI,"A library implementing the Tsetlin Machine and its variants (Coalesced, Convolutional, etc.) for interpretable pattern recognition and logic-based machine learning, with CUDA support.",AI4;Machine Learning,modeling;solver,solver,Python,https://github.com/cair/tmu,,MIT,tsetlin-machine;logic-learning;interpretable-ai
724,XLM-T,Framework for evaluating multilingual language models on Twitter data,"A framework and repository for training and evaluating multilingual language models specifically on Twitter data, enabling consistent benchmarking in social media NLP tasks.",AI4;AI4-03,model_evaluation;benchmark,workflow,Jupyter Notebook,https://github.com/cardiffnlp/xlm-t,,Apache-2.0,nlp;multilingual;twitter;evaluation
725,Yet Another Applied LLM Benchmark,Applied benchmark for evaluating LLMs on practical coding and reasoning tasks,"A benchmark suite developed by Nicholas Carlini to evaluate Large Language Models on a set of practical, applied questions and coding tasks, serving as a personal but influential baseline for model capability assessment.",AI4;AI4-03,model_evaluation;benchmark,dataset,Python,https://github.com/carlini/yet-another-applied-llm-benchmark,,GPL-3.0,llm;benchmark;reasoning
726,Opik,"Platform for evaluating, debugging, and monitoring LLM applications","Opik is a comprehensive platform designed for the evaluation and monitoring of Large Language Model (LLM) applications, RAG systems, and agentic workflows. It provides tracing capabilities, automated evaluation metrics, and dashboards to ensure production readiness of AI models.",AI4;AI4-03,model_evaluation;monitoring;tracing,platform,Python,https://github.com/comet-ml/opik,,Apache-2.0,llm-evaluation;observability;rag;debugging
727,Compl-AI,Compliance-centered evaluation framework for Generative AI models,"Compl-AI is an open-source framework focused on evaluating Generative AI models against compliance standards. It enables the assessment of models for technical and ethical compliance, providing a structured approach to AI safety and regulation adherence.",AI4;AI4-03,model_evaluation;compliance_testing;ai_safety,framework,Python,https://github.com/compl-ai/compl-ai,,Apache-2.0,generative-ai;compliance;evaluation;safety
728,DeepEval,Unit testing and evaluation framework for LLMs,"DeepEval is an evaluation framework designed to unit test Large Language Models (LLMs). It offers a suite of metrics to assess RAG pipelines and agents, facilitating continuous integration and regression testing for AI application development.",AI4;AI4-03,model_evaluation;unit_testing;rag_evaluation,framework,Python,https://github.com/confident-ai/deepeval,https://docs.confident-ai.com,Apache-2.0,llm;testing;metrics;rag
729,LogiEval,Benchmark suite for testing logical reasoning in prompt-based models,LogiEval is a benchmark suite designed to evaluate the logical reasoning capabilities of prompt-based AI models. It provides a set of tasks and metrics to assess how well models perform on logic-intensive problems.,AI4;AI4-03,model_evaluation;benchmarking;logical_reasoning,dataset,Python,https://github.com/csitfun/LogiEval,,None,benchmark;logical-reasoning;llm;prompting
730,Bisheng,Open LLM DevOps and evaluation platform for enterprise AI,"Bisheng is a comprehensive platform for developing and managing LLM applications. It includes features for evaluation, dataset management, supervised fine-tuning (SFT), and RAG workflows, serving as a unified environment for AI model lifecycle management.",AI4;AI4-03,model_management;evaluation;workflow_orchestration,platform,TypeScript,https://github.com/dataelement/bisheng,,Apache-2.0,llmops;evaluation;rag;fine-tuning
731,Deepchecks,Continuous validation and testing for ML models and data,"Deepchecks is a holistic open-source solution for testing and validating machine learning models and data. It supports the entire ML lifecycle from research to production, offering checks for data integrity, model performance, and distribution drift.",AI4;AI4-03,model_validation;data_quality_control;drift_detection,framework,Python,https://github.com/deepchecks/deepchecks,https://docs.deepchecks.com,NOASSERTION,ml-testing;validation;data-quality;model-monitoring
732,Ollama Grid Search,Desktop application to evaluate and compare LLM models,Ollama Grid Search is a multi-platform desktop tool that facilitates the evaluation and comparison of various Large Language Models (LLMs) managed via Ollama. It allows users to run grid searches over prompts and model parameters to assess performance.,AI4;AI4-03,model_evaluation;parameter_tuning;comparison,solver,TypeScript,https://github.com/dezoito/ollama-grid-search,,MIT,llm;grid-search;evaluation;ollama
733,Docling Eval,Evaluation framework for document processing models,Docling Eval is a framework designed to evaluate the performance of document processing models and services. It provides metrics and workflows to assess the accuracy of information extraction and document layout analysis.,AI4;AI4-03,model_evaluation;document_processing;ocr_evaluation,framework,Python,https://github.com/docling-project/docling-eval,,MIT,document-ai;evaluation;ocr;layout-analysis
734,Uni-Dock-Benchmarks,Benchmark datasets for molecular docking systems,A curated collection of datasets and benchmarking tests specifically for evaluating the performance and accuracy of the Uni-Dock molecular docking system. It serves as a standard for comparing docking results in drug discovery research.,AI4;AI4-03,benchmarking;molecular_docking;drug_discovery,dataset,Python,https://github.com/dptech-corp/Uni-Dock-Benchmarks,,Apache-2.0,molecular-docking;benchmark;drug-discovery;bioinformatics
735,Encord Active,Active learning toolkit for model evaluation and data curation,"Encord Active is an open-source toolkit for testing, validating, and evaluating computer vision models. It focuses on data-centric AI, helping users prioritize data for labeling, detect errors, and analyze model performance through actionable metrics.",AI4;AI4-03,model_evaluation;active_learning;data_curation,toolkit,Python,https://github.com/encord-team/encord-active,https://docs.encord.com/active/,Apache-2.0,computer-vision;active-learning;evaluation;data-centric-ai
736,Text-to-Image Eval,Evaluation metrics for text-to-image generation models,"A toolkit for evaluating text-to-image and zero-shot image classification models (e.g., CLIP, SigLIP). It implements metrics such as Zero-shot accuracy, Linear Probe, and Image retrieval to assess model quality and alignment.",AI4;AI4-03,model_evaluation;image_generation;zero_shot_classification,library,Jupyter Notebook,https://github.com/encord-team/text-to-image-eval,,Apache-2.0,text-to-image;evaluation;clip;metrics
737,OBR,Runner for OpenFOAM benchmarks,"OBR (OpenFOAM Benchmark Runner) is a tool designed to automate the execution and management of benchmarks for OpenFOAM, a popular computational fluid dynamics (CFD) software.",AI4;AI4-03,simulation_runner;benchmarking,workflow,Python,https://github.com/exasim-project/OBR,,BSD-3-Clause,openfoam;cfd;benchmark;hpc
738,LLM Speedrunner,Automated benchmark for LLM agents in language modeling innovation,"The Automated LLM Speedrunning Benchmark measures the capability of LLM agents to reproduce previous innovations and discover new ones in the field of language modeling, serving as a metric for agentic research capabilities.",AI4;AI4-03,agent_evaluation;benchmarking,solver,Jupyter Notebook,https://github.com/facebookresearch/llm-speedrunner,,NOASSERTION,llm-agents;benchmark;research-automation
739,Video Transformers,Fine-tuning framework for HuggingFace video classification models,"A lightweight library designed to simplify the fine-tuning process of video classification models from the HuggingFace ecosystem, providing easy-to-use interfaces for training and evaluation.",AI4;AI4-03,model_finetuning;video_classification,library,Python,https://github.com/fcakyon/video-transformers,,MIT,video-classification;transformers;fine-tuning
740,divraster,R package for calculating diversity metrics on rasterized data,"divraster is an R package that provides functions to calculate various diversity metrics (e.g., alpha, beta diversity) directly on raster data, commonly used in ecology and biogeography.",AI4;AI4-03,data_analysis;diversity_metrics,library,R,https://github.com/flaviomoc/divraster,,GPL-3.0,ecology;raster;diversity-metrics;r-package
741,js-quantities,JavaScript library for quantity calculation and unit conversion,"A library to handle physical quantities and unit conversions in JavaScript, useful for scientific data processing and frontend scientific visualizations.",AI4;AI4-03,unit_conversion;data_processing,library,JavaScript,https://github.com/gentooboontoo/js-quantities,,MIT,unit-conversion;physics;quantities
742,Google Agent Development Toolkit (Go),Go toolkit for building and evaluating AI agents,"An open-source toolkit that provides components and infrastructure for building, evaluating, and deploying AI agents, facilitating research and development in agentic AI.",AI4;AI4-03,agent_evaluation;agent_development,library,Go,https://github.com/google/adk-go,,Apache-2.0,ai-agents;evaluation;toolkit
743,Google Agent Development Toolkit (Java),Java toolkit for building and evaluating AI agents,"An open-source toolkit that provides components and infrastructure for building, evaluating, and deploying AI agents using Java.",AI4;AI4-03,agent_evaluation;agent_development,library,Java,https://github.com/google/adk-java,,Apache-2.0,ai-agents;evaluation;toolkit
744,Google Agent Development Toolkit (Python),Python toolkit for building and evaluating AI agents,"An open-source toolkit that provides components and infrastructure for building, evaluating, and deploying AI agents using Python, widely used in AI research.",AI4;AI4-03,agent_evaluation;agent_development,library,Python,https://github.com/google/adk-python,,Apache-2.0,ai-agents;evaluation;toolkit
745,Gromit,Decentralized systems benchmarking and experiment runner framework,"Gromit is a framework designed to automate the deployment, execution, and benchmarking of decentralized systems experiments, facilitating reproducible research in distributed computing.",AI4;AI4-03,experiment_runner;system_benchmarking,workflow,Python,https://github.com/grimadas/gromit,,None,distributed-systems;benchmarking;experiment-runner
746,haddock-runner,Runner for large scale HADDOCK biomolecular simulations,"A utility to automate and manage large-scale docking simulations using HADDOCK, enabling high-throughput structural biology research.",AI4;AI4-03,simulation_runner;molecular_docking,workflow,Go,https://github.com/haddocking/haddock-runner,,Apache-2.0,structural-biology;docking;haddock;simulation
747,C-Eval,Comprehensive Chinese evaluation suite for foundation models,C-Eval is a comprehensive evaluation suite designed to assess the advanced knowledge and reasoning abilities of foundation models in a Chinese context.,AI4;AI4-03,model_evaluation;benchmark_suite,dataset,Python,https://github.com/hkust-nlp/ceval,https://cevalbenchmark.com/,MIT,llm;benchmark;chinese;evaluation
748,HMOG Dataset,Multimodal dataset for evaluating continuous authentication performance,"A multimodal dataset designed for evaluating continuous authentication performance on smartphones, capturing various sensor data to benchmark security models.",AI4;AI4-03,dataset;security_evaluation,dataset,,https://github.com/hmog-dataset/hmog,,None,dataset;authentication;biometrics;mobile
749,Lighteval,All-in-one toolkit for evaluating LLMs across multiple backends,"A comprehensive library for evaluating Large Language Models (LLMs) on various benchmarks and tasks. It supports multiple backends and provides a unified interface for assessing model performance, making it a critical component in the AI model development and evaluation lifecycle.",AI4;AI4-03,model_evaluation;benchmarking,library,Python,https://github.com/huggingface/lighteval,,MIT,llm;evaluation;nlp;benchmarking
750,RAVEN,"Probabilistic risk analysis, validation, and uncertainty quantification framework","A flexible framework for probabilistic risk analysis, uncertainty quantification, parameter optimization, and model reduction. It is designed to perform parametric and stochastic analysis of complex system codes, widely used in nuclear engineering and other safety-critical scientific domains.",AI4;AI4-03,uncertainty_quantification;risk_analysis;parameter_optimization,workflow,Python,https://github.com/idaholab/raven,https://raven.inl.gov,Apache-2.0,uncertainty-quantification;risk-analysis;simulation;model-validation
751,Probatus,SHAP-based validation toolkit for linear and tree-based models,"A Python library for validating binary, multiclass, and regression models using SHAP (SHapley Additive exPlanations) values. It provides tools for feature selection, model analysis, and ensuring model robustness in scientific and financial ML applications.",AI4;AI4-03,model_validation;feature_selection;interpretability,library,Python,https://github.com/ing-bank/probatus,https://ing-bank.github.io/probatus/,MIT,shap;model-validation;machine-learning;feature-selection
752,ReadabilityMetrics,Library and service for computing text readability metrics,"A tool that calculates various readability metrics (e.g., ARI, Coleman-Liau, Flesch-Kincaid) for text data. While implemented as a service, the underlying logic serves as a library for linguistic analysis and text quality assessment in NLP research.",AI4;AI4-03,text_analysis;metric_calculation,library,Java,https://github.com/ipeirotis/ReadabilityMetrics,,Apache-2.0,nlp;readability;text-analysis;metrics
753,CML,Continuous Machine Learning (CML) for CI/CD of ML experiments,"An open-source library for implementing Continuous Integration/Continuous Delivery (CI/CD) in machine learning projects. It enables researchers to automate model training, evaluation, and report generation (e.g., plots, metrics) directly within pull requests, facilitating reproducible research and regression testing.",AI4;AI4-03,experiment_tracking;ci_regression;reproducibility,workflow,JavaScript,https://github.com/iterative/cml,https://cml.dev,Apache-2.0,mlops;ci-cd;reproducibility;experiment-tracking
754,Matbench Discovery,Evaluation framework for ML models in high-throughput materials discovery,"A benchmark and evaluation framework designed to simulate high-throughput materials discovery using machine learning models. It assesses the ability of models to predict stable materials and guide experimental synthesis, serving as a critical tool for materials informatics.",AI4;AI4-03,materials_discovery;model_evaluation;benchmarking,workflow,Python,https://github.com/janosh/matbench-discovery,https://matbench-discovery.materialsproject.org,MIT,materials-science;machine-learning;benchmarking;discovery
755,Runcharter,Automated run chart analysis for faceted data displays,"An R package for automating the creation and analysis of run charts, which are used for quality control and performance monitoring in healthcare and scientific processes. It supports identifying trends and shifts in data across multiple metrics or locations.",AI4;AI4-03,statistical_analysis;quality_control;visualization,library,R,https://github.com/johnmackintosh/runcharter,,GPL-3.0,r;statistics;quality-improvement;visualization
756,Eva,Evaluation framework for oncology foundation models,"A specialized framework for evaluating foundation models in the context of oncology. It provides metrics and workflows to assess the performance of AI models on cancer-related tasks, facilitating the development of reliable medical AI tools.",AI4;AI4-03,model_evaluation;medical_ai;oncology,workflow,Python,https://github.com/kaiko-ai/eva,,Apache-2.0,oncology;foundation-models;evaluation;medical-imaging
757,VSDFLOW,Automated RTL-to-GDS flow for semiconductor design,"An automated Electronic Design Automation (EDA) toolchain that converts Register Transfer Level (RTL) designs to GDSII layout. It integrates synthesis, placement, routing, and timing analysis tools, enabling hardware engineers and researchers to produce physical chip designs from logic descriptions.",AI4;AI4-03,chip_design;eda;synthesis,workflow,Verilog,https://github.com/kunalg123/vsdflow,,Apache-2.0,eda;vlsi;rtl-to-gds;hardware-design
758,Langfuse,"Open source LLM engineering platform for observability, metrics, and evaluations","A comprehensive platform for LLM engineering that includes tools for tracing, dataset management, and running evaluations on model outputs. It supports the lifecycle of developing and refining LLM applications through rigorous metrics and observability.",AI4;AI4-03,model_evaluation;observability;dataset_management,platform,TypeScript,https://github.com/langfuse/langfuse,https://langfuse.com/docs,NOASSERTION,llm-ops;evaluation;observability;metrics
759,LangWatch,"LLM Ops platform for analytics, evaluations, and prompt optimization","A platform designed for monitoring and evaluating Large Language Models (LLMs). It provides capabilities for tracing execution, analyzing performance analytics, managing datasets, and optimizing prompts through systematic evaluation.",AI4;AI4-03,model_evaluation;analytics;prompt_optimization,platform,TypeScript,https://github.com/langwatch/langwatch,https://docs.langwatch.ai,NOASSERTION,llm-ops;analytics;evaluation
760,Latitude LLM,Open-source prompt engineering and evaluation platform,"A platform focused on the engineering and refinement of prompts for Large Language Models. It allows users to build, evaluate, and iterate on prompts using AI-assisted feedback and performance metrics.",AI4;AI4-03,prompt_engineering;model_evaluation,platform,TypeScript,https://github.com/latitude-dev/latitude-llm,https://docs.latitude.so,LGPL-3.0,prompt-engineering;evaluation;llm
761,Les Audits Affaires Eval Harness,CLI for benchmarking French LLMs in business law tasks,"A lightweight Python command-line interface designed to benchmark and test French Large Language Models specifically on business law tasks, including action determination, deadline analysis, document processing, cost estimation, and risk assessment.",AI4;AI4-03,domain_specific_evaluation;legal_benchmarking,solver,Python,https://github.com/legml-ai/les-audits-affaires-eval-harness,,MIT,legal-ai;benchmark;llm-evaluation;french-nlp
762,BioMonTools,R tools for biomonitoring and bioassessment metric calculation,"A suite of tools for calculating metrics related to biomonitoring and bioassessment, specifically for benthic macroinvertebrates, fish, and periphyton. It aids in the analysis of ecological data for environmental health assessment.",AI4;AI4-03,metric_calculation;bioassessment;ecological_analysis,library,R,https://github.com/leppott/BioMonTools,,NOASSERTION,biomonitoring;ecology;r-package;metrics
763,S3Eval,"Synthetic, Scalable and Systematic Evaluation Suite for LLMs","An evaluation suite presented at NAACL 2024 designed to assess Large Language Models. It focuses on providing a synthetic, scalable, and systematic approach to benchmarking LLM performance across various tasks.",AI4;AI4-03,model_evaluation;benchmarking,solver,Python,https://github.com/lfy79001/S3Eval,,None,llm-evaluation;benchmark;nlp
764,LinearityIQA,Norm-in-Norm Loss implementation for Image Quality Assessment,"Implementation of the Norm-in-Norm Loss function for Image Quality Assessment (IQA), as presented at ACM MM 2020. It provides a method for evaluating image quality with faster convergence and better performance compared to standard metrics.",AI4;AI4-03,image_quality_assessment;loss_function,library,Python,https://github.com/lidq92/LinearityIQA,,None,computer-vision;iqa;image-quality;metric
765,Arena-Hard-Auto,Automatic benchmark for Large Language Models,"An automated benchmarking tool for Large Language Models, designed to replicate the difficulty and discrimination power of the Chatbot Arena. It provides a pipeline for evaluating models against hard prompts using judge models.",AI4;AI4-03,model_benchmarking;automated_evaluation,solver,Python,https://github.com/lmarena/arena-hard-auto,,Apache-2.0,llm-benchmark;evaluation;arena-hard
766,LEP-Hybrid-Visual-Odometry,Real-time monocular Hybrid Visual Odometry system,"A C++ implementation of a hybrid visual odometry formulation that combines indirect (feature-based) and direct methods. It uses lines, edges, and points (LEP) for robust tracking and mapping, serving as a tool for robotics and computer vision research.",AI4;AI4-03,visual_odometry;slam;robotics_navigation,solver,CMake,https://github.com/maazmb/LEP-Hybrid-Visual-Odometry,,GPL-3.0,slam;visual-odometry;computer-vision;robotics
767,FixEval,Dataset and test suite for competitive programming bug fixing,A dataset and evaluation framework for automated program repair in the context of competitive programming. It emphasizes execution-based evaluation over match-based metrics to accurately assess bug-fixing capabilities of models.,AI4;AI4-03,program_repair_evaluation;code_generation,dataset,Python,https://github.com/mahimanzum/FixEval,,MIT,automated-program-repair;benchmark;dataset
768,llm_eval_suite,Tool to evaluate LLMs using Ollama and Hugging Face datasets,"A Python-based utility to facilitate the evaluation of Large Language Models. It integrates with Ollama for model inference and Hugging Face for dataset retrieval, allowing for streamlined performance testing.",AI4;AI4-03,model_evaluation;inference_benchmarking,solver,Python,https://github.com/majesticio/llm_eval_suite,,None,llm;evaluation;ollama;huggingface
769,saliency-faithfulness-eval,Tests to assess attention faithfulness for explainability,A suite of tests designed to evaluate the faithfulness of saliency maps and attention mechanisms in explainable AI (XAI). It helps researchers verify if the explanations provided by models accurately reflect their decision-making process.,AI4;AI4-03,xai_evaluation;saliency_analysis,library,Python,https://github.com/matteo-rizzo/saliency-faithfulness-eval,,None,explainable-ai;saliency-maps;evaluation;faithfulness
770,Evalite,TypeScript library for evaluating LLM-powered applications,A lightweight TypeScript library designed to help developers and researchers evaluate the performance of LLM-powered applications. It provides a structured way to define and run evaluations on model outputs.,AI4;AI4-03,model_evaluation;app_testing,library,TypeScript,https://github.com/mattpocock/evalite,,MIT,llm-evaluation;typescript;testing
771,CVRR-Evaluation-Suite,Complex Video Reasoning and Robustness Evaluation Suite for Video-LMMs,"An evaluation suite for assessing Video Large Multimodal Models (Video-LMMs). It focuses on complex reasoning tasks and robustness, providing a benchmark to measure model performance in challenging video understanding scenarios.",AI4;AI4-03,video_lmm_evaluation;multimodal_benchmarking,solver,Python,https://github.com/mbzuai-oryx/CVRR-Evaluation-Suite,,CC-BY-4.0,video-lmm;benchmark;robustness;reasoning
772,HEAL-T,PPG-based Heart Rate and IBI estimation method,A MATLAB and Bash based implementation for estimating Interbeat-interval (IBI) and Heart-rate (HR) from artifactual Blood Volume Pulse (BVP) signals during physical exercise. It serves as a tool for physiological signal processing.,AI4;AI4-03,signal_processing;physiological_metrics,solver,MATLAB,https://github.com/meiyor/HEAL-T-AN-EFFICIENT-PPG-BASED-HEART-RATE-AND-IBI-ESTIMATION-METHOD-DURING-PHYSICAL-EXERCISE,,NOASSERTION,ppg;heart-rate;signal-processing;biomedical
773,MSMARCO-Conversational-Search,Dataset and evaluation paradigm for conversational search,A dataset and evaluation framework designed to study conversational search behavior. It provides artificial sessions and a methodology to evaluate model performance on real user behavior without compromising privacy.,AI4;AI4-03,dataset_creation;conversational_search_evaluation,dataset,Python,https://github.com/microsoft/MSMARCO-Conversational-Search,,MIT,conversational-search;dataset;evaluation;ir
774,Prompty,"Tool to create, manage, debug, and evaluate LLM prompts","An asset class and toolset for managing LLM prompts. It facilitates the creation, debugging, and evaluation of prompts, enhancing observability and portability for AI application development.",AI4;AI4-03,prompt_management;prompt_evaluation,tool,Python,https://github.com/microsoft/prompty,,MIT,prompt-engineering;llm-ops;evaluation
775,minerl-wrappers,Standardized wrappers for reproducibility in MineRL environment,A collection of wrappers for the MineRL environment to standardize code and ensure reproducibility in Reinforcement Learning experiments involving Minecraft.,AI4;AI4-03,rl_environment_wrapper;reproducibility,library,Python,https://github.com/minerl-wrappers/minerl-wrappers,,GPL-3.0,reinforcement-learning;minerl;reproducibility
776,ModelBench,Safety benchmarks for AI models with detailed reporting,"A tool from MLCommons for running safety benchmarks against AI models. It generates detailed reports on model performance regarding safety metrics, facilitating standardized safety assessments.",AI4;AI4-03,ai_safety_benchmarking;model_evaluation,solver,Python,https://github.com/mlcommons/modelbench,,Apache-2.0,ai-safety;benchmark;mlcommons
777,FSEB,Energy-based Monthly Simulation of Land Surface Temperature,Code for modeling Land Surface Temperature (LST) using Surface Energy Balance (SEB) principles. It allows for the simulation of environmental factors and evapotranspiration at a monthly temporal scale.,AI4;AI4-03,environmental_modeling;surface_energy_balance,solver,Python,https://github.com/mnasserimn/FSEB,,None,remote-sensing;land-surface-temperature;modeling
778,Prognostic Algorithm Package,Framework for model-based prognostics and remaining useful life computation of engineering systems,"A Python framework for model-based prognostics that provides algorithms for state estimation, prediction, and uncertainty propagation. It allows for the rapid development and comparative study of prognostics solutions for engineering components and systems.",AI4;AI4-03,prognostics;state_estimation;uncertainty_propagation,library,Python,https://github.com/nasa/prog_algs,,NOASSERTION,nasa;prognostics;engineering-systems
779,Evals,Framework for evaluating LLMs and an open-source registry of benchmarks,A framework developed by OpenAI for evaluating Large Language Models (LLMs) and LLM systems. It includes an open-source registry of benchmarks to test model capabilities and ensure quality.,AI4;AI4-03,model_evaluation;benchmark_registry,framework,Python,https://github.com/openai/evals,,NOASSERTION,llm;openai;benchmark
780,pChem,Modification-centric assessment tool for chemoproteomic probe performance,"A tool for the assessment of chemoproteomic probes, focusing on modification-centric performance evaluation. It aids in the analysis of probe efficiency and specificity in proteomic studies.",AI4,chemoproteomics;probe_assessment;data_analysis,tool,Python,https://github.com/pFindStudio/pChem,,Apache-2.0,proteomics;chemistry;bioinformatics
781,Japanese LM Financial Evaluation Harness,Evaluation harness for Japanese language models in the financial domain,A specialized evaluation harness designed for assessing the performance of Japanese Language Models within the financial domain. It provides benchmarks and metrics tailored to financial texts and tasks.,AI4;AI4-03,model_evaluation;domain_specific_benchmarking,harness,Shell,https://github.com/pfnet-research/japanese-lm-fin-harness,,MIT,nlp;finance;japanese
782,InstructEval,Evaluation suite for instruction selection methods in NLP,"A systematic evaluation suite designed to assess instruction selection methods for Large Language Models (LLMs), supporting the reproduction of findings from NAACL 2024.",AI4;AI4-03;Natural Language Processing,model_evaluation;instruction_tuning_assessment,solver,Jupyter Notebook,https://github.com/princeton-nlp/InstructEval,,None,nlp;llm-evaluation;instruction-following;benchmark
783,skore,Python library for ML model evaluation and reporting,"An open-source library that accelerates machine learning model development by providing automated evaluation reports, methodological guidance, and cross-validation analysis.",AI4;AI4-03;Machine Learning,model_evaluation;reporting;cross_validation,library,Python,https://github.com/probabl-ai/skore,,MIT,machine-learning;evaluation-metrics;data-science;reporting
784,etalon,Performance evaluation harness for LLM serving systems,"A harness designed to benchmark and evaluate the performance (latency, throughput) of Large Language Model serving systems.",AI4;AI4-03;Machine Learning Systems,performance_benchmarking;llm_serving,solver,Python,https://github.com/project-etalon/etalon,,Apache-2.0,llm;benchmarking;performance;serving
785,prometheus-eval,LLM response evaluation tool using Prometheus and GPT-4,"A library for evaluating Large Language Model responses, leveraging Prometheus models and GPT-4 as judges for automated assessment.",AI4;AI4-03;Natural Language Processing,llm_evaluation;automated_grading,library,Python,https://github.com/prometheus-eval/prometheus-eval,,Apache-2.0,llm;evaluation;prometheus;gpt-4
786,promptfoo,CLI tool for testing and evaluating LLM prompts and agents,"A tool for testing prompts, agents, and RAG systems, supporting red teaming, vulnerability scanning, and performance comparison across multiple LLM providers.",AI4;AI4-03;Natural Language Processing,prompt_engineering;model_evaluation;red_teaming,solver,TypeScript,https://github.com/promptfoo/promptfoo,https://www.promptfoo.dev,MIT,llm;testing;prompt-engineering;red-teaming
787,epm,R package for eco-phylogenetic metrics calculation,"An R package designed for calculating taxonomic, phenotypic, and phylogenetic metrics across spatial grid cells for ecological research.",AI4;AI4-03;Ecology;Phylogenetics,metric_calculation;spatial_analysis;biodiversity_metrics,library,R,https://github.com/ptitle/epm,,GPL-3.0,ecology;phylogenetics;spatial-analysis;r-package
788,speciesRaster,R package for species indexing and community metric calculation,"A tool for indexing species by grid cell and calculating community ecology metrics, serving as a companion to spatial ecological analysis.",AI4;AI4-03;Ecology,community_ecology;metric_calculation,library,R,https://github.com/ptitle/speciesRaster,,None,ecology;biodiversity;r-package
789,aeromancy,Framework for reproducible AI and ML experiments,"A framework designed to facilitate reproducible artificial intelligence and machine learning workflows, likely with a focus on environmental or atmospheric sciences given the organization name (quant-aq).",AI4;AI4-03;Machine Learning,reproducibility;experiment_tracking,workflow,Python,https://github.com/quant-aq/aeromancy,,Apache-2.0,reproducibility;machine-learning;framework
790,RagaAI-Catalyst,SDK for Agent AI observability and evaluation,"A Python SDK providing a framework for observability, monitoring, and evaluation of AI agents, including tracing and debugging capabilities for multi-agent systems.",AI4;AI4-03;Artificial Intelligence,agent_evaluation;observability;monitoring,library,Python,https://github.com/raga-ai-hub/RagaAI-Catalyst,,Apache-2.0,ai-agents;observability;evaluation;sdk
791,crossfit,GPU-accelerated metric calculation library,"A library from RAPIDS AI for calculating metrics, optimized for GPU execution, typically used in recommender systems or ranking tasks.",AI4;AI4-03;Data Science,metric_calculation;gpu_acceleration,library,Python,https://github.com/rapidsai/crossfit,,Apache-2.0,rapids;gpu;metrics;ranking
792,tensorflow-qnd,Command framework for TensorFlow model training and evaluation,"A framework to simplify the creation of TensorFlow commands for training, evaluating, and inferencing with machine learning models.",AI4;AI4-03;Machine Learning,model_training;model_evaluation,workflow,Python,https://github.com/raviqqe/tensorflow-qnd,,Unlicense,tensorflow;cli;training;inference
793,continuous-eval,Data-driven evaluation framework for LLM applications,"A framework for the continuous, data-driven evaluation of Large Language Model (LLM) powered applications, focusing on pipeline performance.",AI4;AI4-03;Natural Language Processing,llm_evaluation;pipeline_assessment,library,Python,https://github.com/relari-ai/continuous-eval,,Apache-2.0,llm;evaluation;rag;metrics
794,auto-evaluator,Evaluation tool for LLM QA chains,"A tool designed to evaluate Question Answering (QA) chains built with Large Language Models, automating the assessment of response quality.",AI4;AI4-03;Natural Language Processing,qa_evaluation;llm_assessment,solver,Python,https://github.com/rlancemartin/auto-evaluator,,None,llm;qa;evaluation;langchain
795,nanopore_assembly_and_polishing_assessment,Pipeline for assessing Nanopore assembly and polishing,"Automated pipelines for evaluating the performance of genome assembly and polishing tools specifically for Nanopore sequencing data, including visualization.",AI4;AI4-03;Bioinformatics;Genomics,assembly_evaluation;quality_assessment;nanopore_sequencing,workflow,Python,https://github.com/rlorigro/nanopore_assembly_and_polishing_assessment,,MIT,genomics;nanopore;assembly;evaluation
796,TestSuiteEval,Semantic evaluation tool for Text-to-SQL models,"Implements the semantic evaluation method for Text-to-SQL tasks using a distilled test suite, as proposed in EMNLP 2020.",AI4;AI4-03;Natural Language Processing,model_evaluation;text_to_sql;semantic_parsing,solver,Python,https://github.com/ruiqi-zhong/TestSuiteEval,,None,nlp;text-to-sql;evaluation;emnlp
797,auto_reports,Dashboard for ocean model skill assessment,"An interactive dashboard application for assessing the skill of oceanographic models, specifically for tidal analysis and regional performance evaluation.",AI4;AI4-03;Oceanography,model_assessment;tidal_analysis;visualization,platform,Python,https://github.com/seareport/auto_reports,,EUPL-1.2,oceanography;model-evaluation;dashboard;tides
798,bjontegaard2,Calculation of Bjontegaard metrics for video coding,"A MATLAB implementation for calculating Bjontegaard metrics (BD-Rate), a standard method for evaluating the coding efficiency of video compression algorithms.",AI4;AI4-03;Signal Processing;Video Compression,metric_calculation;performance_evaluation,library,MATLAB,https://github.com/serge-m/bjontegaard2,,None,video-coding;metrics;matlab;compression
799,SigOpt EvalSet,Benchmark suite of test functions for black-box optimization strategies,"A collection of test functions and benchmarks designed to evaluate and compare the performance of various black-box optimization algorithms, facilitating research in hyperparameter tuning and optimization.",AI4;AI4-03,optimization_evaluation;benchmarking,library,Python,https://github.com/sigopt/evalset,,MIT,optimization;black-box;benchmarking
800,pg_landmetrics,PostgreSQL extension for landscape metrics calculations,"A PostGIS extension that enables the calculation of landscape metrics directly within a database environment, supporting spatial analysis for ecology and geography.",AI4;AI4-03,spatial_analysis;landscape_metrics,library,TSQL,https://github.com/siose-innova/pg_landmetrics,,GPL-3.0,gis;landscape-ecology;postgis
801,dismay,R package for calculating distance metrics on matrices,"An R library providing efficient implementations for calculating various distance metrics on matrices, commonly used in statistical analysis and bioinformatics.",AI4;AI4-03,statistical_analysis;distance_metrics,library,R,https://github.com/skinnider/dismay,,MIT,r-package;statistics;matrices
802,Open Supply Chains,Tool for modeling and analyzing supply chain sustainability metrics,"An open-source codebase for visualizing and analyzing supply chains, including modules for calculating evaluation metrics such as carbon footprints, supporting research in sustainable operations.",AI4;AI4-03,sustainability_analysis;supply_chain_modeling,platform,PHP,https://github.com/supplychainstudies/OpenSupplyChains,,None,supply-chain;sustainability;carbon-footprint
803,BIG-Bench Hard,Challenging subset of BIG-Bench tasks for LLM evaluation,"A benchmark suite focusing on tasks from BIG-Bench where language models previously struggled, used to evaluate the reasoning capabilities of Chain-of-Thought prompting.",AI4;AI4-03,llm_evaluation;reasoning_benchmark,dataset,,https://github.com/suzgunmirac/BIG-Bench-Hard,,MIT,llm;benchmark;chain-of-thought
804,Test Suite SQL Eval,Semantic evaluation harness for Text-to-SQL models,"A testing suite and evaluation framework for Text-to-SQL systems, using distilled test suites to provide semantic correctness evaluation beyond simple string matching.",AI4;AI4-03,model_evaluation;text_to_sql,library,Python,https://github.com/taoyds/test-suite-sql-eval,,Apache-2.0,nlp;sql;evaluation
805,AlpacaEval,Automatic evaluator for instruction-following language models,"An LLM-based automatic evaluation framework designed to simulate human evaluation of instruction-following models, providing a fast and low-cost alternative to human annotation.",AI4;AI4-03,llm_evaluation;instruction_following,library,Jupyter Notebook,https://github.com/tatsu-lab/alpaca_eval,https://tatsu-lab.github.io/alpaca_eval/,Apache-2.0,llm;evaluation;automation
806,TensorZero,"Unified stack for LLM engineering, optimization, and evaluation","An industrial-grade platform for building LLM applications that integrates gateway services with observability, optimization, and evaluation workflows to improve model performance.",AI4;AI4-03,llmops;model_evaluation;optimization,platform,Rust,https://github.com/tensorzero/tensorzero,https://www.tensorzero.com/docs,Apache-2.0,llmops;evaluation;gateway
807,hlb-gpt,Minimalistic and fast researcher's toolbench for training and evaluating GPT models,"A hackable and highly optimized toolbench designed for researchers to train and evaluate GPT-style models. It features extremely fast training speeds (e.g., <100 seconds for wikitext-103 on A100) and a minimalistic codebase to facilitate experimentation and scaling.",AI4;AI4-03,model_training;model_evaluation,library,Python,https://github.com/tysam-code/hlb-gpt,,Apache-2.0,gpt;llm-training;research-toolbench;optimization
808,aibench-llm-endpoints,Metric collection runner for LLM inference endpoints,A runner tool designed to collect and report performance metrics from various LLM inference endpoints. It serves as a component of the Unify Hub ecosystem for benchmarking and monitoring LLM service quality.,AI4;AI4-03,inference_benchmarking;metric_collection,solver,Python,https://github.com/unifyai/aibench-llm-endpoints,,Apache-2.0,llm-inference;benchmarking;metrics;unify-hub
809,Gale,PyTorch framework for reproducible deep learning experiments,"Gale is a framework built on PyTorch designed to facilitate reproducible deep learning experiments. It provides structure and utilities to standardize experimental workflows, ensuring consistency in model training and evaluation.",AI4;AI4-03,experiment_management;reproducibility,framework,Python,https://github.com/v7labs/Gale,,LGPL-3.0,pytorch;reproducibility;deep-learning;experiment-framework
810,whisper-finetune,Tool for fine-tuning and evaluating Whisper ASR models,A comprehensive tool for fine-tuning OpenAI's Whisper models on custom or Hugging Face datasets. It includes functionality for evaluating the performance of fine-tuned models for Automatic Speech Recognition (ASR) tasks.,AI4;AI4-03,model_finetuning;model_evaluation,solver,Python,https://github.com/vasistalodagala/whisper-finetune,,MIT,whisper;asr;fine-tuning;speech-recognition
811,Tiny QA Benchmark++,Micro-benchmark suite and CI-ready eval harness for LLM smoke-testing,A lightweight benchmark suite and evaluation harness designed for fast smoke-testing and regression detection in Large Language Models. It includes a generator CLI and supports on-demand multilingual synthetic packs.,AI4;AI4-03,benchmark;regression_testing,solver,Python,https://github.com/vincentkoc/tiny_qa_benchmark_pp,,Apache-2.0,llm-benchmark;regression-testing;ci-cd;qa
812,caption-eval,Automated metric evaluation tool for image captions,"A Python-based tool for evaluating sentence and image captions using standard automated metrics such as BLEU, METEOR, ROUGE, CIDEr, and SPICE. It facilitates the quantitative assessment of captioning models.",AI4;AI4-03,caption_evaluation;metric_calculation,library,Python,https://github.com/vsubhashini/caption-eval,,None,nlp-evaluation;image-captioning;metrics;bleu
813,BVQA_Benchmark,Benchmark for Blind Video Quality Assessment on User Generated Content,"A resource list and performance benchmark for Blind Video Quality Assessment (BVQA) models, specifically targeting User Generated Content (UGC) datasets. It supports the evaluation and comparison of video quality assessment algorithms.",AI4;AI4-03,video_quality_assessment;benchmark,dataset,Python,https://github.com/vztu/BVQA_Benchmark,,MIT,video-quality;bvqa;benchmark;ugc
814,RMBench-2022,Benchmark for robotic manipulation reinforcement learning,RMBench is a benchmark suite for robotic manipulation tasks involving high-dimensional continuous action and state spaces. It evaluates reinforcement learning algorithms that use pixel-based observations.,AI4;AI4-03,robotics_benchmark;reinforcement_learning,dataset,Python,https://github.com/xiangyanfei212/RMBench-2022,,None,robotics;reinforcement-learning;benchmark;manipulation
815,thoughtful-agents,Framework for building and evaluating proactive LLM agents,"A Python framework for constructing proactive LLM agents that simulate human-like cognitive processes. It enables agents to generate and evaluate thoughts in parallel with conversations, facilitating the development of more autonomous AI systems.",AI4;AI4-03,agent_framework;cognitive_simulation,framework,Python,https://github.com/xybruceliu/thoughtful-agents,,Apache-2.0,llm-agents;cognitive-architecture;proactive-agents
816,AC-EVAL,Ancient Chinese evaluation suite for Large Language Models,AC-EVAL is a specialized benchmark suite designed to evaluate the performance of Large Language Models (LLMs) on tasks involving Ancient Chinese. It serves as a resource for assessing model capabilities in historical language understanding.,AI4;AI4-03,benchmark;nlp_evaluation,dataset,Python,https://github.com/yuting-wei/AC-EVAL,,MIT,ancient-chinese;llm-benchmark;nlp
817,fastmri-reproducible-benchmark,Reproducible benchmark methods for MRI reconstruction on fastMRI dataset,A repository providing reproducible methods and benchmarks for MRI reconstruction using the fastMRI dataset. It includes implementations of models like XPDNet to facilitate comparison and advancement in medical image reconstruction.,AI4;AI4-03,mri_reconstruction;benchmark,solver,Jupyter Notebook,https://github.com/zaccharieramzi/fastmri-reproducible-benchmark,,MIT,mri-reconstruction;fastmri;medical-imaging;benchmark
818,ZeroEntropy Evals,Evaluation suite for benchmarking retrievers and rerankers,"An evaluation suite developed by ZeroEntropy to benchmark the performance of various information retrieval components, specifically retrievers and rerankers. It aids in selecting optimal components for search and RAG systems.",AI4;AI4-03,retrieval_benchmarking;reranker_evaluation,library,Python,https://github.com/zeroentropy-ai/evals,,None,retrieval;reranking;benchmark;search
819,Arline Benchmarks,Benchmarking platform for quantum circuit mapping and compression algorithms,"A platform designed to benchmark various quantum circuit mapping and compression algorithms against each other on a list of predefined hardware types and target circuit classes, facilitating evaluation in quantum computing research.",AI4;AI4-04,benchmarking;quantum_circuit_optimization;algorithm_evaluation,platform,Python,https://github.com/ArlineQ/arline_benchmarks,,AGPL-3.0,quantum-computing;benchmarking;circuit-mapping
820,rd-cdm,Ontology-based rare disease common data model,"A repository for the ontology-based rare disease common data model (RD-CDM) that harmonizes international registries, FHIR, and Phenopackets to support rare disease research and data interoperability.",AI4;AI4-04,data_harmonization;ontology_mapping;rare_disease_registry,library,Python,https://github.com/BIH-CEI/rd-cdm,,MIT,rare-disease;common-data-model;fhir;phenopackets
821,CTF for Science Framework,Benchmarking framework for dynamic system modeling methods,A modular and extensible platform designed for benchmarking modeling methods on dynamic systems. It supports the evaluation and comparison of models for systems like ordinary differential equations (ODEs) and partial differential equations (PDEs) using standardized datasets and metrics.,AI4;AI4-04,benchmarking;ode_modeling;pde_modeling;model_evaluation,library,Jupyter Notebook,https://github.com/CTF-for-Science/ctf4science,,MIT,benchmarking;dynamic-systems;ode;pde
822,EvalAI,Open source platform for evaluating and comparing AI algorithms,"An open-source platform for hosting AI challenges and evaluating state-of-the-art AI algorithms. It provides a standardized environment for benchmarking models across various tasks, including scientific datasets and competitions.",AI4;AI4-04,leaderboard;model_evaluation;benchmarking_platform,platform,Python,https://github.com/Cloud-CV/EvalAI,https://eval.ai/,NOASSERTION,benchmarking;leaderboard;ai-challenges;evaluation
823,DagsHub Client,Client libraries for DagsHub scientific collaboration platform,"Python client for DagsHub that facilitates scientific experiment tracking, data versioning, and collaboration. It integrates with MLflow and DVC to provide a unified interface for managing research projects.",AI4;AI4-04,experiment_tracking;data_versioning;collaboration,library,Python,https://github.com/DagsHub/client,https://dagshub.com/docs/,MIT,experiment-tracking;mlflow;dvc;data-science
824,Fast Data Science (fds),CLI for version controlling data and code in scientific projects,A command-line tool that wraps Git and DVC (Data Version Control) to streamline the versioning of code and large datasets in data science and scientific research workflows.,AI4;AI4-04,data_versioning;version_control;workflow_management,solver,Python,https://github.com/DagsHub/fds,,MIT,dvc;git;data-versioning;cli
825,OpenVectorFormat,Data format for scanner-based laser-processing jobs,"A library and data format description for storing scanner-based laser-processing jobs, including vector-geometry data, processing parameters, and metadata, used in manufacturing science and material processing.",AI4,data_format;manufacturing_process;laser_processing,library,Starlark,https://github.com/Digital-Production-Aachen/OpenVectorFormat,,MIT,laser-processing;vector-data;manufacturing;file-format
826,GeoMet Data Registry,Registry system for managing and serving meteorological data via OGC standards,"A system to manage access to Environment and Climate Change Canada's Meteorological Service of Canada (MSC) open data, including raw numerical weather prediction (NWP) model data layers and weather radar mosaics, via Open Geospatial Consortium (OGC) standards.",Meteorology;Earth Science;Data Management,data_registry;data_serving;geospatial_data,platform,Python,https://github.com/ECCC-MSC/geomet-data-registry,,NOASSERTION,meteorology;ogc;wms;data-registry
827,Otter,Multi-modal model based on OpenFlamingo for instruction following,"A multi-modal model based on OpenFlamingo, trained on MIMIC-IT, showcasing improved instruction-following and in-context learning ability for visual-language tasks.",Computer Vision;Multimodal AI,multimodal_modeling;instruction_following,solver,Python,https://github.com/EvolvingLMMs-Lab/Otter,,MIT,multimodal;openflamingo;vlm
828,precisionFDA,Cloud-based platform for benchmarking NGS analysis pipelines,"A cloud-based platform that provides an environment where the community can test, pilot, and benchmark new approaches to validating their next-generation sequencing (NGS) analysis pipelines.",Bioinformatics;Genomics;AI4-04,benchmarking;pipeline_validation;ngs_analysis,platform,TypeScript,https://github.com/FDA/precisionFDA,https://precision.fda.gov/,CC0-1.0,ngs;benchmarking;genomics;fda
829,SUPIR,Model for photo-realistic image restoration,A deep learning model aiming at developing practical algorithms for photo-realistic image restoration in the wild.,Computer Vision;Image Processing,image_restoration;super_resolution,solver,Python,https://github.com/Fanghua-Yu/SUPIR,https://suppixel.ai/,NOASSERTION,image-restoration;deep-learning;restoration
830,FastTrackML,High-performance experiment tracking server compatible with MLflow,"An experiment tracking server focused on speed and scalability, designed to be compatible with the MLflow client for tracking machine learning experiments.",AI4;AI4-04;MLOps,experiment_tracking;model_management,platform,Go,https://github.com/G-Research/fasttrackml,,Apache-2.0,mlflow;experiment-tracking;mlops
831,kedro-mlflow,Kedro plugin for MLflow integration,"A plugin for the Kedro framework that integrates MLflow capabilities, enabling machine learning model versioning, packaging, and experiment tracking within Kedro projects.",AI4;AI4-04;MLOps,experiment_tracking;pipeline_versioning,library,Python,https://github.com/Galileo-Galilei/kedro-mlflow,https://kedro-mlflow.readthedocs.io/,Apache-2.0,kedro;mlflow;experiment-tracking
832,Graph Learning Indexer (GLI),Benchmarking platform for graph learning,"A contributor-friendly and metadata-rich platform for graph learning benchmarks, facilitating dataloading, benchmarking, and tagging of graph datasets and models.",AI4;AI4-04;Graph Learning,benchmarking;dataset_management;graph_learning,platform,Python,https://github.com/Graph-Learning-Benchmarks/gli,,MIT,graph-learning;benchmark;gnn
833,DVC (Deep Video Compression),End-to-end deep video compression framework,"An end-to-end deep learning framework for video compression, providing a model implementation for efficient video coding.",Computer Vision;Video Processing,video_compression;neural_compression,solver,Python,https://github.com/GuoLusjtu/DVC,,MIT,video-compression;deep-learning;cvpr
834,LH-VLN,Benchmark and platform for Long-Horizon Vision-Language Navigation,"A platform and benchmark suite for Long-Horizon Vision-Language Navigation (LH-VLN), designed to evaluate agents in complex navigation tasks.",AI4;AI4-04;Robotics;Computer Vision,benchmarking;vision_language_navigation,platform,Jupyter Notebook,https://github.com/HCPLab-SYSU/LH-VLN,,None,vln;benchmark;navigation
835,Snowman,Data matching benchmark platform,A benchmarking platform designed for evaluating and comparing data matching (entity resolution) algorithms.,AI4;AI4-04;Data Science,benchmarking;data_matching;entity_resolution,platform,TypeScript,https://github.com/HPI-Information-Systems/snowman,,MIT,benchmark;data-matching;entity-resolution
836,Fast-AgingGAN,Deep learning model for face aging,"A deep learning model designed to simulate aging effects on face images, running efficiently on GPUs.",Computer Vision;Image Synthesis,image_synthesis;face_aging,solver,Python,https://github.com/HasnainRaz/Fast-AgingGAN,,MIT,gan;face-aging;deep-learning
837,Fast-SRGAN,Fast Super Resolution GAN for video upsampling,"A fast deep learning model (SRGAN) to upsample low-resolution videos to high resolution, optimized for speed.",Computer Vision;Image Processing,super_resolution;video_upsampling,solver,Python,https://github.com/HasnainRaz/Fast-SRGAN,,MIT,srgan;super-resolution;upsampling
838,CMF (Common Metadata Framework),Library for tracking metadata and lineage in ML pipelines,"A library to collect, store, and query metadata associated with ML pipelines, tracking lineage for artifacts and executions in distributed AI workflows.",AI4;AI4-04;MLOps,metadata_tracking;lineage_tracking;experiment_logging,library,Python,https://github.com/HewlettPackard/cmf,,Apache-2.0,metadata;mlops;lineage
839,FakeSV,Multimodal benchmark for fake news detection,A multimodal benchmark dataset and platform with rich social context for fake news detection on short video platforms.,AI4;AI4-04;Social Computing,benchmarking;fake_news_detection,dataset,Python,https://github.com/ICTMCG/FakeSV,,None,benchmark;fake-news;multimodal
840,Sacred,Experiment configuration and tracking tool,"A tool to help configure, organize, log, and reproduce computational experiments, widely used in machine learning research.",AI4;AI4-04;MLOps,experiment_tracking;reproducibility;configuration_management,library,Python,https://github.com/IDSIA/sacred,https://sacred.readthedocs.io/,MIT,experiment-tracking;reproducibility;logging
841,INCF EEG Database,Portal for managing and sharing EEG/ERP data,"A portal enabling researchers to store, update, download, and search data and metadata from EEG/ERP experiments.",Neuroscience;Data Management,data_sharing;eeg_database;metadata_management,platform,Java,https://github.com/INCF/eeg-database,,None,eeg;neuroscience;database
842,Incense,Interactive data retrieval for Sacred experiments,"A library to interactively retrieve and analyze data from experiments logged with the Sacred tool, facilitating experiment visualization and comparison.",AI4;AI4-04;MLOps,experiment_analysis;data_retrieval,library,Python,https://github.com/JarnoRFB/incense,https://incense.readthedocs.io/,MIT,sacred;experiment-analysis;visualization
843,MLJModels.jl,Model registry for the MLJ framework in Julia,The model registry and tools for model queries and code loading for the MLJ (Machine Learning in Julia) framework.,AI4;AI4-04;Machine Learning,model_registry;model_loading,library,Julia,https://github.com/JuliaAI/MLJModels.jl,,MIT,julia;mlj;model-registry
844,Snowboy,DNN-based hotword detection library,"A customizable hotword detection engine based on deep neural networks, widely used in robotics and AI applications for audio processing.",Audio Processing;AI,hotword_detection;audio_analysis,library,C++,https://github.com/Kitt-AI/snowboy,https://snowboy.kitt.ai/,NOASSERTION,audio;speech-recognition;hotword
845,METR Eval Analysis,DVC pipeline for analyzing AI model evaluation data,"A public repository containing METR's data version control (DVC) pipeline designed for the analysis of evaluation data, facilitating reproducible research in model evaluation.",AI4;AI4-04,evaluation_analysis;reproducibility,workflow,Python,https://github.com/METR/eval-analysis-public,,None,dvc;evaluation;pipeline
846,MLReef,Collaboration workspace for Machine Learning experiment tracking,"A collaboration platform for machine learning that provides experiment tracking, model management, and reproducibility features to streamline the ML lifecycle.",AI4;AI4-04,experiment_tracking;collaboration,platform,Kotlin,https://github.com/MLReef/mlreef,,NOASSERTION,mlops;experiment-tracking;collaboration
847,OpenCaptchaWorld,Benchmark for evaluating MLLM agents via CAPTCHA puzzles,A web-based benchmark and platform designed to evaluate the visual reasoning and interaction capabilities of Multimodal LLM agents through diverse CAPTCHA puzzles.,AI4;AI4-04,benchmarking;visual_reasoning,dataset,JavaScript,https://github.com/MetaAgentX/OpenCaptchaWorld,,None,benchmark;mllm;captcha
848,MetaBox,Benchmarking platform for Meta-Black-Box Optimization,"A comprehensive benchmarking platform designed for evaluating Meta-Black-Box Optimization algorithms, facilitating comparison and analysis of optimization strategies.",AI4;AI4-04,benchmarking;optimization,platform,Python,https://github.com/MetaEvo/MetaBox,,BSD-3-Clause,optimization;benchmark;black-box
849,PipelineX,ML pipeline builder integrating Kedro and MLflow,"A Python package that simplifies building machine learning pipelines for experimentation by integrating frameworks like Kedro and MLflow, focusing on developer efficiency.",AI4;AI4-04,pipeline_orchestration;experiment_tracking,library,Python,https://github.com/Minyus/pipelinex,,NOASSERTION,pipeline;mlflow;kedro
850,MiroFlow,Open-source deep research agent for reproducible benchmarks,A fully open-source deep research agent framework designed to achieve reproducible state-of-the-art performance on various AI benchmarks like GAIA and FutureX.,AI4;AI4-04,research_agent;reproducibility,solver,Python,https://github.com/MiroMindAI/MiroFlow,,Apache-2.0,agent;benchmark;reproducibility
851,CoolGraph,Library for simplifying Graph Neural Networks (GNN) usage,A machine learning library designed to make Graph Neural Networks (GNNs) easier to implement and use for scientific and industrial graph learning tasks.,AI4,graph_learning;modeling,library,Jupyter Notebook,https://github.com/MobileTeleSystems/CoolGraph,,MIT,gnn;graph-learning;deep-learning
852,ModelChimp,Experiment tracking for deep learning projects,"An experiment tracking tool tailored for machine and deep learning projects, allowing researchers to log, visualize, and compare model experiments.",AI4;AI4-04,experiment_tracking;visualization,platform,JavaScript,https://github.com/ModelChimp/modelchimp,,BSD-2-Clause,experiment-tracking;visualization;deep-learning
853,NVIDIA DeepLearningExamples,State-of-the-art deep learning scripts for reproduction and benchmarking,"A comprehensive collection of state-of-the-art deep learning model implementations optimized for NVIDIA GPUs, serving as a primary resource for performance benchmarking and experimental reproduction.",AI4;AI4-04,model_reproduction;benchmarking,solver,Jupyter Notebook,https://github.com/NVIDIA/DeepLearningExamples,,None,benchmark;reproducibility;nvidia
854,DANCE,Deep learning library and benchmark for single-cell analysis,"DANCE is a deep learning library and benchmark platform specifically designed for single-cell analysis in computational biology, providing tools for clustering, imputation, and integration.",AI4;AI4-04,single_cell_analysis;benchmarking,library,Python,https://github.com/OmicsML/dance,,BSD-2-Clause,single-cell;benchmark;deep-learning
855,What-If Tool,Interactive interface for probing and analyzing ML models,"A visual interface designed to help researchers and practitioners analyze machine learning models, understand their behavior, and assess fairness and performance across different data slices.",AI4;AI4-04,model_interpretation;analysis,platform,HTML,https://github.com/PAIR-code/what-if-tool,https://pair-code.github.io/what-if-tool/,Apache-2.0,visualization;fairness;interpretability
856,Open-Sora-Plan,Open source reproduction of Sora text-to-video model,"An open-source project aiming to reproduce the Sora text-to-video model, providing training scripts, model architectures, and inference tools for the research community.",AI4;AI4-04,video_generation;model_reproduction,solver,Python,https://github.com/PKU-YuanGroup/Open-Sora-Plan,,MIT,text-to-video;reproduction;sora
857,mousetrap-os,Mouse-tracking plugin for OpenSesame psychological experiments,A plugin for the OpenSesame experiment builder that enables mouse-tracking data collection for psychological and behavioral research.,AI4;AI4-04,data_collection;behavioral_tracking,library,Python,https://github.com/PascalKieslich/mousetrap-os,,GPL-3.0,psychology;mouse-tracking;opensesame;behavioral-science
858,PLIP,Pathology Language and Image Pre-Training foundation model,"A vision and language foundation model for pathology AI, fine-tuned from CLIP to extract visual and language features from pathology images and text descriptions.",AI4;AI4-01,feature_extraction;image_analysis,solver,Python,https://github.com/PathologyFoundation/plip,,None,pathology;foundation-model;medical-imaging;clip
859,ProteoBench,Community-curated benchmarks for proteomics data analysis,"An open and collaborative platform for continuous, easy, and controlled comparison of proteomics data analysis workflows and pipelines.",AI4;AI4-04,benchmarking;proteomics_analysis,platform,Jupyter Notebook,https://github.com/Proteobench/ProteoBench,https://proteobench.cubimed.rub.de/,Apache-2.0,proteomics;benchmark;bioinformatics;data-analysis
860,topsacred,Viewer and toolset for Sacred experiment databases,"A collection of functions and tools to interact with and visualize data stored by Sacred, a tool for configuring and organizing computational experiments.",AI4;AI4-04,experiment_tracking;visualization,library,Jupyter Notebook,https://github.com/Qwlouse/topsacred,,MIT,sacred;experiment-tracking;reproducibility
861,rocHPCG,HPCG benchmark for ROCm platform,"Implementation of the High Performance Conjugate Gradients (HPCG) benchmark, designed to measure performance of HPC systems, optimized for the ROCm platform.",AI4;AI4-04,benchmarking;performance_analysis,solver,C++,https://github.com/ROCm/rocHPCG,,BSD-3-Clause,hpc;benchmark;rocm;gpu-computing
862,RagView,Unified evaluation platform for RAG methods,"A platform to benchmark different Retrieval-Augmented Generation (RAG) methods on specific datasets, facilitating comparative analysis of AI models.",AI4;AI4-04,benchmarking;model_evaluation,platform,Python,https://github.com/RagView/RagView,,None,rag;benchmark;nlp;evaluation
863,RapidFire AI,Toolchain for rapid AI model customization,"A framework facilitating rapid AI customization ranging from Retrieval-Augmented Generation (RAG) to Fine-Tuning, supporting AI research workflows.",AI4;AI4-04,model_customization;fine_tuning,workflow,JavaScript,https://github.com/RapidFireAI/rapidfireai,,Apache-2.0,rag;fine-tuning;ai-workflow
864,RoboVerse,Benchmark and platform for robot learning,"A unified platform, dataset, and benchmark designed for scalable and generalizable robot learning research, enabling simulation and evaluation of robotic agents.",AI4;AI4-04,benchmarking;simulation,platform,Python,https://github.com/RoboVerseOrg/RoboVerse,,Apache-2.0,robotics;benchmark;simulation;embodied-ai
865,SREGym,Benchmark platform for SRE AI agents,"An AI-native platform for benchmarking Site Reliability Engineering (SRE) agents, evaluating their performance in managing software reliability tasks.",AI4;AI4-04,benchmarking;agent_evaluation,platform,Python,https://github.com/SREGym/SREGym,,MIT,sre;ai-agents;benchmark;reliability-engineering
866,QA-Board,Experiment tracker and visualization tool,"An experiment tracker to organize, visualize, compare, and share algorithm runs, designed to remove toil from R&D and performance tuning.",AI4;AI4-04,experiment_tracking;visualization,platform,JavaScript,https://github.com/Samsung/qaboard,,Apache-2.0,experiment-tracking;visualization;r-and-d
867,SpeechIO Leaderboard,Benchmarking platform for Automatic Speech Recognition,A robust and comprehensive benchmarking platform for evaluating Automatic Speech Recognition (ASR) systems.,AI4;AI4-04,benchmarking;model_evaluation,platform,Python,https://github.com/SpeechColab/Leaderboard,,None,asr;speech-recognition;benchmark;leaderboard
868,tensorboard-aggregator,Tool to aggregate TensorBoard runs,"A utility to aggregate multiple TensorBoard runs into new summary or CSV files, facilitating the analysis of multiple experiments.",AI4;AI4-04,experiment_tracking;data_aggregation,library,Python,https://github.com/Spenhouet/tensorboard-aggregator,,MIT,tensorboard;experiment-analysis;visualization
869,BEHAVIOR-1K,Benchmark platform for Embodied AI,"A platform for accelerating Embodied AI research, providing a benchmark for evaluating agents on human-centric activities in simulation.",AI4;AI4-04,benchmarking;simulation,platform,Python,https://github.com/StanfordVL/BEHAVIOR-1K,,None,embodied-ai;benchmark;simulation;robotics
870,RaySAR,3D Synthetic Aperture Radar (SAR) simulator,"A 3D synthetic aperture radar (SAR) simulator that generates SAR image layers from detailed 3D object models, enabling localization and signal analysis.",AI4;AI4-01,simulation;data_generation,solver,C++,https://github.com/StefanJAuer/RaySAR,,None,sar;radar;simulation;remote-sensing
871,FHIRFLARE-IG-Toolkit,Toolkit for managing FHIR Implementation Guides,"A comprehensive web app for managing FHIR Implementation Guides (IGs), including import, validation, processing, and conversion features for medical informatics.",AI4;AI4-01,data_processing;standardization,platform,HTML,https://github.com/Sudo-JHare/FHIRFLARE-IG-Toolkit,,NOASSERTION,fhir;medical-informatics;healthcare-data;interoperability
872,SwanLab,AI training tracking and visualization tool,An open-source AI training tracking and visualization tool that integrates with popular frameworks like PyTorch and Transformers to monitor experiments.,AI4;AI4-04,experiment_tracking;visualization,platform,Python,https://github.com/SwanHubX/SwanLab,,Apache-2.0,experiment-tracking;visualization;mlops;training-monitoring
873,FedScale,Federated Learning benchmarking platform,A scalable and extensible open-source platform for benchmarking and deploying federated learning (FL) algorithms and systems.,AI4;AI4-04,benchmarking;federated_learning,platform,Python,https://github.com/SymbioticLab/FedScale,,Apache-2.0,federated-learning;benchmark;distributed-systems
874,tensorboard_logger,Library to log TensorBoard events without TensorFlow dependency,"A Python library that allows users to log metrics, images, and histograms to TensorBoard event files without needing to install the full TensorFlow framework. It is useful for lightweight experiment tracking in PyTorch or other non-TF environments.",AI4;AI4-04,experiment_tracking;visualization,library,Python,https://github.com/TeamHG-Memex/tensorboard_logger,,MIT,tensorboard;logging;visualization
875,AI-Infra-Guard,Comprehensive AI Red Teaming and safety evaluation platform,"A platform developed by Tencent Zhuque Lab for AI Red Teaming, focusing on evaluating the safety, security, and robustness of AI models through adversarial testing and vulnerability scanning.",AI4;AI4-04,model_evaluation;red_teaming;safety_testing,platform,Python,https://github.com/Tencent/AI-Infra-Guard,,NOASSERTION,red-teaming;ai-safety;security
876,GFPGAN,Practical algorithm for real-world face restoration,A blind face restoration algorithm that leverages a Generative Facial Prior (GFP) to restore low-quality face images. It is widely used for image enhancement and preprocessing in computer vision research.,Computer Vision;Image Processing,image_restoration;face_enhancement,solver,Python,https://github.com/TencentARC/GFPGAN,,NOASSERTION,gan;face-restoration;super-resolution
877,LLMstudio,Framework and UI for LLM fine-tuning and prompt engineering,"A framework designed to manage the lifecycle of Large Language Model (LLM) applications, offering tools for prompt engineering, fine-tuning, and evaluating models, bridging the gap between experimentation and production.",AI4;AI4-04,llm_finetuning;prompt_engineering,platform,Python,https://github.com/TensorOpsAI/LLMstudio,,MPL-2.0,llm;finetuning;mlops
878,Local-LLM-Comparison-Colab-UI,WebUI for benchmarking and comparing local LLMs,"A Colab-compatible WebUI tool designed to run and compare the performance and outputs of various Large Language Models (LLMs) on consumer hardware, facilitating model evaluation and selection.",AI4;AI4-04,model_benchmarking;model_comparison,workflow,Jupyter Notebook,https://github.com/Troyanovsky/Local-LLM-Comparison-Colab-UI,,None,llm;benchmarking;colab
879,CPM-1-Generate,Inference code for Chinese Pre-Trained Language Models (CPM-LM),"Provides the generation code and model interface for CPM-1, a large-scale Chinese pre-trained language model, enabling researchers to perform text generation and downstream NLP tasks.",NLP;AI Models,text_generation;language_modeling,solver,Python,https://github.com/TsinghuaAI/CPM-1-Generate,,MIT,nlp;chinese-llm;text-generation
880,ModelDB,Open source system for ML model versioning and experiment management,"A system to manage machine learning models, including versioning models, tracking metadata, and managing experiments. It helps researchers ensure reproducibility and track the history of model development.",AI4;AI4-04,experiment_management;model_versioning,platform,Java,https://github.com/VertaAI/modeldb,https://www.verta.ai/,Apache-2.0,mlops;experiment-tracking;model-registry
881,VevestaX,Lightweight library for tracking ML experiments and EDA,"A Python library designed to track machine learning experiments and exploratory data analysis (EDA) with minimal code changes, automatically logging features, parameters, and performance metrics.",AI4;AI4-04,experiment_tracking;eda_logging,library,Jupyter Notebook,https://github.com/Vevesta/VevestaX,,Apache-2.0,experiment-tracking;eda;mlops
882,NeuroNLP2,Deep neural models library for core NLP tasks,"A PyTorch-based library implementing deep neural network models for various core Natural Language Processing tasks, serving as a toolkit for NLP research and experimentation.",NLP,nlp_modeling;sequence_labeling,library,Python,https://github.com/XuezheMax/NeuroNLP2,,GPL-3.0,nlp;pytorch;neural-networks
883,Xwin-LM,Models and methods for LLM alignment and reproducibility,"Provides a suite of Large Language Models and alignment methodologies (like RLHF) focused on stability and reproducibility, serving as a resource for research into LLM alignment techniques.",AI4;AI Models,llm_alignment;rlhf,solver,Python,https://github.com/Xwin-LM/Xwin-LM,,None,llm;alignment;rlhf
884,Geochemistrypi,Automated machine learning framework for geochemistry,"An open-source Python framework designed for data-driven discovery in geochemistry. It automates the machine learning pipeline for analyzing geochemical data, supporting tasks like classification, regression, and clustering in geological contexts.",AI4S;Geochemistry,geochemical_analysis;scientific_discovery,framework,Python,https://github.com/ZJUEarthData/Geochemistrypi,,MIT,geochemistry;automl;ai4science
885,ACTS,Toolkit for charged particle track reconstruction in high energy physics,"An experiment-independent toolkit for track reconstruction in high energy physics experiments. It provides modern, thread-safe implementations of tracking algorithms and geometry handling for particle physics research.",Physics;High Energy Physics,track_reconstruction;particle_physics_simulation,library,C++,https://github.com/acts-project/acts,https://acts.readthedocs.io/,MPL-2.0,physics;tracking;cern
886,OlliePy,Library for ML experiment evaluation and data exploration,A Python package that assists data scientists in exploring data and evaluating machine learning experiments by generating web-based reports and dashboards for analysis.,AI4;AI4-04,experiment_evaluation;eda,library,Python,https://github.com/ahmed-mohamed-sn/olliePy,,MIT,data-science;visualization;evaluation
887,Aim,Open-source experiment tracker for machine learning metadata,"Aim is an easy-to-use and supercharged open-source experiment tracker that logs training runs, hyperparameters, and metrics, providing a UI for comparison and visualization of ML experiments.",AI4;AI4-04,experiment_tracking;metrics_logging;visualization,platform,Python,https://github.com/aimhubio/aim,https://aimstack.io/,Apache-2.0,experiment-tracking;mlops;visualization;metadata
888,aimlflow,Integration tool to sync MLflow runs with Aim,"A synchronization tool that allows users to track and visualize MLflow experiments using the Aim UI, bridging the gap between the two experiment tracking ecosystems.",AI4;AI4-04,experiment_tracking;interoperability,library,Python,https://github.com/aimhubio/aimlflow,,Apache-2.0,mlflow;aim;integration;experiment-tracking
889,Mandala,Experiment tracking framework with integrated persistence logic,"Mandala is a framework that integrates experiment tracking directly with Python's persistence logic, enabling automatic memoization and queryable storage of computational graph results for data science projects.",AI4;AI4-04,experiment_tracking;data_persistence;reproducibility,library,Python,https://github.com/amakelov/mandala,https://amakelov.github.io/mandala/,Apache-2.0,experiment-tracking;memoization;data-management
890,MLflow Export Import,Tools to export and import MLflow experiments and models,"A utility set for copying MLflow objects (experiments, runs, registered models) from one MLflow tracking server to another, facilitating data migration and backup in research workflows.",AI4;AI4-04,experiment_management;data_migration,library,Python,https://github.com/amesar/mlflow-export-import,,Apache-2.0,mlflow;migration;experiment-management
891,Grand,Scalable and interoperable Python graph library,"Grand allows using familiar Python graph APIs (like NetworkX) on scalable backends (SQL, graph databases), enabling analysis of large-scale scientific network data that exceeds memory limits.",AI4;Data Science,graph_analysis;data_processing,library,Python,https://github.com/aplbrain/grand,https://grand.readthedocs.io/,Apache-2.0,graph-theory;network-analysis;scalability
892,Labwatch,Hyperparameter optimization extension for Sacred,"Labwatch connects the Sacred experiment tracking tool with various hyperparameter optimization libraries, enabling automated search and logging of optimal parameters in ML experiments.",AI4;AI4-04,hyperparameter_optimization;experiment_tracking,library,Python,https://github.com/automl/labwatch,,MIT,hpo;sacred;optimization
893,Ogama,OpenGazeAndMouseAnalyzer for eye and mouse tracking data,"Ogama is a software application for recording and analyzing eye- and mouse-tracking data from experimental setups, providing features for fixation detection, heatmaps, and statistical analysis.",Neuroscience;HCI,data_analysis;behavioral_tracking;visualization,solver,C#,https://github.com/avosskuehler/ogama,http://www.ogama.net/,GPL-3.0,eye-tracking;data-analysis;experiment-control
894,SageMaker Entrypoint Utilities,Utilities for SageMaker training entrypoints,"A library providing logging handlers, progress bar management, and hyperparameter parsing utilities to simplify the creation of robust training entrypoint scripts for scientific ML workflows on SageMaker.",AI4;AI4-04,workflow_utility;logging,library,Python,https://github.com/aws-samples/amazon-sagemaker-entrypoint-utilities,,MIT-0,sagemaker;utilities;logging
895,Foundation Model Benchmarking Tool,Benchmarking tool for foundation models on AWS,"A tool to benchmark the performance (latency, throughput, cost) of foundation models across various instance types and serving stacks, facilitating model selection for research and production.",AI4;AI4-04,benchmarking;performance_evaluation,solver,Jupyter Notebook,https://github.com/aws-samples/foundation-model-benchmarking-tool,,MIT-0,benchmarking;foundation-models;llm
896,ML Lineage Helper,Wrapper for SageMaker ML Lineage Tracking,"A helper library that extends SageMaker's lineage tracking capabilities to cover end-to-end ML lifecycles, including feature store groups and artifact queries, ensuring reproducibility and auditability.",AI4;AI4-04,lineage_tracking;provenance,library,Python,https://github.com/aws-samples/ml-lineage-helper,,Apache-2.0,lineage;sagemaker;reproducibility
897,AWS SDK for pandas,Pandas integration for AWS data services,"An open-source Python library (formerly AWS Wrangler) that extends Pandas to easily connect with AWS data services like Athena, Glue, and Redshift, facilitating data processing for scientific workflows.",AI4;Data Science,data_processing;data_integration,library,Python,https://github.com/aws/aws-sdk-pandas,https://aws-sdk-pandas.readthedocs.io/,Apache-2.0,pandas;etl;data-engineering
898,Graph Notebook,Jupyter notebook extension for graph databases,"A library extending Jupyter notebooks to integrate with graph databases (Gremlin, SPARQL, openCypher), providing visualization and query capabilities for graph data analysis.",AI4;Data Science,visualization;data_analysis;graph_querying,library,Jupyter Notebook,https://github.com/aws/graph-notebook,https://graph-notebook.readthedocs.io/,Apache-2.0,graph-visualization;jupyter;sparql
899,SageMaker Experiments,Experiment tracking SDK for Amazon SageMaker,"A Python SDK for tracking, organizing, and comparing machine learning experiments, runs, and metrics within the Amazon SageMaker environment.",AI4;AI4-04,experiment_tracking;metrics_logging,library,Python,https://github.com/aws/sagemaker-experiments,https://sagemaker-experiments.readthedocs.io/,Apache-2.0,experiment-tracking;sagemaker;mlops
900,MXBoard,Logging MXNet data for visualization in TensorBoard,"A Python package that provides APIs for logging MXNet data for visualization in TensorBoard, enabling monitoring of training metrics and model performance.",AI4;AI4-04,visualization;experiment_tracking,library,Python,https://github.com/awslabs/mxboard,,Apache-2.0,mxnet;tensorboard;visualization;logging
901,Cloud Detection Model,Cloud Detection Model for Sentinel-2 Imagery,"A semantic segmentation model and tool for detecting clouds in Sentinel-2 satellite imagery, supporting earth observation data processing.",AI4;AI4-01,image_segmentation;remote_sensing;data_processing,solver,Python,https://github.com/azavea/cloud-model,,MIT,sentinel-2;cloud-detection;remote-sensing;satellite-imagery
902,ml_board,Machine learning dashboard for hyperparameter and log visualization,"A machine learning dashboard that displays hyperparameter settings alongside visualizations and logs, designed to track the scientist's thoughts and experiment progress.",AI4;AI4-04,experiment_tracking;visualization;dashboard,platform,Python,https://github.com/bbli/ml_board,,MIT,dashboard;machine-learning;visualization;experiment-tracking
903,calkit,Project management and reproducibility tool for research,"A tool for simplified version control, environment management, and single-button reproducible pipelines specifically designed for research projects.",AI4;AI4-04,reproducibility;version_control;pipeline_management,workflow,Python,https://github.com/calkit/calkit,,MIT,research-ops;reproducibility;version-control;pipeline
904,Neptune,Trajectory planning framework for multiple robots,"A trajectory planning framework for multiple robots, supporting scientific modeling and control theory applications in robotics.",AI4;AI4-01,trajectory_planning;robotics;control_theory,solver,C++,https://github.com/caomuqing/neptune,,BSD-3-Clause,robotics;trajectory-planning;multi-agent
905,Anserini,Lucene toolkit for reproducible information retrieval research,"A Lucene-based toolkit built to support reproducible information retrieval research, providing standard indexing and retrieval capabilities for scientific evaluation.",AI4;AI4-04,information_retrieval;indexing;evaluation,library,Java,https://github.com/castorini/anserini,http://anserini.io/,Apache-2.0,information-retrieval;lucene;reproducibility;search
906,Pyserini,Python toolkit for reproducible information retrieval research,"A Python toolkit for reproducible information retrieval research that supports sparse and dense representations, serving as a Python interface to Anserini.",AI4;AI4-04,information_retrieval;dense_retrieval;evaluation,library,Python,https://github.com/castorini/pyserini,,Apache-2.0,information-retrieval;python;search;bm25
907,Catalyst,Accelerated deep learning R&D framework,"A PyTorch framework for accelerated deep learning research and development, focusing on reproducibility, rapid experimentation, and codebase reuse.",AI4;AI4-04,model_training;experiment_management;deep_learning,workflow,Python,https://github.com/catalyst-team/catalyst,https://catalyst-team.github.io/catalyst/,Apache-2.0,pytorch;deep-learning;framework;reproducibility
908,chaiNNer,Node-based image processing GUI for chaining tasks,"A node-based image processing GUI aimed at making chaining image processing tasks easy and customizable, supporting AI upscaling and programmatic image manipulation workflows.",AI4;AI4-01,image_processing;workflow_automation;upscaling,workflow,Python,https://github.com/chaiNNer-org/chaiNNer,,GPL-3.0,image-processing;gui;node-based;workflow
909,jupyterlab_tensorboard,Tensorboard extension for JupyterLab,"A JupyterLab extension that facilitates the integration of TensorBoard, allowing for seamless visualization of machine learning experiments within the Jupyter environment.",AI4;AI4-04,visualization;experiment_tracking;ide_extension,platform,TypeScript,https://github.com/chesterli29/jupyterlab_tensorboard,,MIT,jupyterlab;tensorboard;visualization;extension
910,Sacredboard,Dashboard for Sacred experiment tracking,"A web-based dashboard for Sacred, enabling users to monitor, access, and analyze past machine learning experiments and their configurations.",AI4;AI4-04,experiment_tracking;visualization;dashboard,platform,Python,https://github.com/chovanecm/sacredboard,,MIT,sacred;dashboard;experiment-tracking;machine-learning
911,Beholder,TensorBoard plugin for visualizing arbitrary tensors,"A TensorBoard plugin that allows for the visualization of arbitrary tensors as video overlays during network training, aiding in model debugging and interpretation.",AI4;AI4-04,visualization;model_debugging;tensorboard_plugin,library,Python,https://github.com/chrisranderson/beholder,,NOASSERTION,tensorboard;visualization;deep-learning;plugin
912,ClearML,MLOps platform for experiment management and orchestration,"An integrated MLOps platform that provides experiment management, data management, pipeline orchestration, and model serving to streamline AI workflows.",AI4;AI4-04,experiment_tracking;mlops;orchestration;data_management,platform,Python,https://github.com/clearml/clearml,https://clear.ml/docs/,Apache-2.0,mlops;experiment-tracking;orchestration;reproducibility
913,ClearML Agent,Orchestration agent for ClearML,"The agent component for ClearML that handles the execution, scheduling, and orchestration of machine learning experiments and pipelines.",AI4;AI4-04,orchestration;scheduling;mlops,service,Python,https://github.com/clearml/clearml-agent,https://clear.ml/docs/,Apache-2.0,mlops;agent;orchestration;scheduler
914,ClearML Server,Backend infrastructure for the ClearML MLOps and experiment tracking platform,"ClearML Server is the backend service for the ClearML platform, providing experiment tracking, model management, and orchestration capabilities. It serves as the central repository for logging metrics, artifacts, and execution details of machine learning experiments.",AI4;AI4-04,experiment_tracking;model_management;mlops,platform,Python,https://github.com/clearml/clearml-server,https://clear.ml/docs,NOASSERTION,experiment-tracking;mlops;model-management
915,benchmark_VAE,Unified PyTorch implementation and benchmarking framework for Variational Autoencoders,A comprehensive library for implementing and benchmarking various Variational Autoencoder (VAE) models. It provides a unified interface to reproduce results and compare performance across different VAE architectures on standard datasets.,AI4;AI4-04,model_benchmarking;reproducibility;generative_models,library,Python,https://github.com/clementchadebec/benchmark_VAE,https://github.com/clementchadebec/benchmark_VAE,Apache-2.0,vae;benchmarking;pytorch;generative-ai
916,Codabench,Flexible and reproducible benchmarking platform for AI competitions and tasks,"Codabench is a framework for creating and hosting benchmarks and competitions. It allows researchers to define tasks, metrics, and datasets to evaluate algorithms in a reproducible manner, supporting the organization of scientific challenges.",AI4;AI4-04,benchmarking_platform;competition_hosting;evaluation,platform,Python,https://github.com/codalab/codabench,https://github.com/codalab/codabench/wiki,Apache-2.0,benchmarking;competitions;reproducibility
917,WA-Testing-Tool,Evaluation and testing scripts for Watson Assistant models,A set of tools to perform K-fold validation and blind testing on Watson Assistant workspaces. It generates precision curves and other metrics to evaluate the performance of NLU models.,AI4;AI4-04,model_evaluation;validation;metrics_calculation,solver,Jupyter Notebook,https://github.com/cognitive-catalyst/WA-Testing-Tool,,Apache-2.0,nlp;evaluation;watson-assistant;metrics
918,sna-js,JavaScript library for egocentric social network analysis metrics,A library to visualize and calculate relevant metrics for egocentric social network analysis (SNA) using adjacency matrices. It supports the computation of network statistics useful in social science research.,AI4;AI4-04,network_analysis;metrics_calculation,library,JavaScript,https://github.com/craigtutterow/sna-js,,GPL-2.0,social-network-analysis;metrics;visualization
919,CK-Crowdtuning,Crowdsourcing extension for Collective Knowledge benchmarking workflows,"An extension for the Collective Knowledge (CK) framework that enables crowdsourcing of experiments, such as performance benchmarking and auto-tuning of machine learning models across diverse hardware platforms.",AI4;AI4-04,benchmarking;auto_tuning;crowdsourcing,workflow,Python,https://github.com/ctuning/ck-crowdtuning,http://cKnowledge.org,BSD-3-Clause,benchmarking;crowdsourcing;collective-knowledge;optimization
920,The Omega Project,Constraint-based tools for compiler dependence analysis and transformation,"A collection of tools and libraries (Omega Library, Omega Calculator, Omega Test) for manipulating sets of affine constraints and performing dependence analysis. These tools are fundamental for research in compiler optimization and polyhedral models.",AI4;AI4-04,dependence_analysis;constraint_solving;compiler_optimization,solver,C,https://github.com/davewathaverford/the-omega-project,,BSD-2-Clause,compiler;dependence-analysis;polyhedral-model;constraints
921,gtsummary,R package for creating presentation-ready data summary and analytic tables,"An R package designed to create publication-ready analytical and summary tables. It automates the reporting of regression models, descriptive statistics, and other scientific data analysis results.",AI4;AI4-04,reporting;statistical_summary;visualization,library,R,https://github.com/ddsjoberg/gtsummary,http://www.danieldsjoberg.com/gtsummary/,NOASSERTION,r;statistics;reporting;tables
922,Determined,Deep learning training platform with integrated experiment tracking,"Determined is an open-source deep learning training platform that handles distributed training, hyperparameter tuning, experiment tracking, and resource management. It enables researchers to track and reproduce experiments efficiently.",AI4;AI4-04,experiment_tracking;hyperparameter_tuning;resource_management,platform,Go,https://github.com/determined-ai/determined,https://docs.determined.ai/,Apache-2.0,deep-learning;experiment-tracking;distributed-training
923,machine-learning-interactive-visualization,Interactive visualization tools for ML model evaluation metrics,"A collection of notebooks and tools for interactively visualizing machine learning evaluation metrics (like ROC curves, precision-recall, etc.) to aid in model analysis and understanding.",AI4;AI4-04,visualization;metrics_evaluation,library,Jupyter Notebook,https://github.com/dhaitz/machine-learning-interactive-visualization,,NOASSERTION,visualization;metrics;machine-learning
924,NLG-Metricverse,End-to-end library for evaluating Natural Language Generation,"A comprehensive library for evaluating Natural Language Generation (NLG) models. It provides a unified interface to access a wide range of evaluation metrics, facilitating the comparison and benchmarking of NLG systems.",AI4;AI4-04,nlg_evaluation;metrics_calculation;benchmarking,library,Python,https://github.com/disi-unibo-nlp/nlg-metricverse,https://nlg-metricverse.readthedocs.io/,MIT,nlg;evaluation;metrics;nlp
925,PromptSite,Version control and tracking system for LLM prompts,"PromptSite is a lightweight package for managing, versioning, and tracking LLM prompts. It helps researchers experiment with and debug prompts by maintaining a history of changes and their effects.",AI4;AI4-04,prompt_tracking;version_control;experiment_management,library,Python,https://github.com/dkuang1980/promptsite,,Apache-2.0,prompt-engineering;version-control;llm
926,TensorBoard,Visualization toolkit for machine learning experimentation,"A suite of visualization tools to understand and debug machine learning models, enabling tracking of metrics like loss and accuracy, visualizing the model graph, and projecting embeddings.",AI4;AI4-04,experiment_tracking;visualization,platform,Python,https://github.com/dmlc/tensorboard,https://www.tensorflow.org/tensorboard,Apache-2.0,visualization;deep-learning;experiment-tracking
927,Open LLM Leaderboard Report,Visualization report generator for Open LLM Leaderboard,"A tool to generate weekly visualization reports of Open LLM model performance based on various metrics, aiding in the tracking and comparison of large language models.",AI4;AI4-04,leaderboard_visualization;performance_tracking,workflow,Python,https://github.com/dsdanielpark/open-llm-leaderboard-report,,MIT,llm;leaderboard;visualization
928,MET (Model Evaluation Tools),Verification and validation tools for meteorological models,"A suite of tools for evaluating the performance of meteorological models, providing state-of-the-art verification methods and metrics.",AI4;AI4-04,model_evaluation;meteorology,solver,C++,https://github.com/dtcenter/MET,https://dtcenter.org/community-code/model-evaluation-tools-met,Apache-2.0,meteorology;verification;model-evaluation
929,mltraq,Experiment tracking and collaboration tool for ML,"A library to track and collaborate on Machine Learning and AI experiments, supporting logging of parameters, metrics, and artifacts.",AI4;AI4-04,experiment_tracking;collaboration,library,Jupyter Notebook,https://github.com/elehcimd/mltraq,,BSD-3-Clause,experiment-tracking;mlops;logging
930,isaaclab_rl,RL training library for Isaac Lab robotics simulation,"A library for training robotic agents in Isaac Lab using PPO, featuring built-in hyperparameter optimisation and extensive logging capabilities.",AI4,reinforcement_learning;robotics_simulation,library,Python,https://github.com/elle-miller/isaaclab_rl,,BSD-3-Clause,robotics;reinforcement-learning;simulation
931,tf-faster-rcnn,TensorFlow implementation of Faster R-CNN,"A widely used implementation of Faster R-CNN for object detection in TensorFlow, serving as a baseline and tool for computer vision research.",AI4,object_detection;computer_vision,solver,Python,https://github.com/endernewton/tf-faster-rcnn,,MIT,object-detection;faster-rcnn;tensorflow
932,NdLinear,PyTorch module for model compression,"A drop-in PyTorch module that shrinks models with no accuracy loss, supporting export to various frameworks like ONNX and TensorRT.",AI4,model_compression;optimization,library,Python,https://github.com/ensemble-core/NdLinear,https://app.ensemblecore.ai/signup,Apache-2.0,pytorch;model-compression;optimization
933,PyGaze,Toolbox for eye tracking experiments,"An open-source, cross-platform toolbox for minimal-effort programming of eye tracking experiments, supporting various eye trackers.",AI4,eye_tracking;experiment_control,library,Python,https://github.com/esdalmaijer/PyGaze,http://www.pygaze.org/,GPL-3.0,eye-tracking;psychophysics;experiment
934,papermill-mlflow,Integration for Papermill and MLflow,A tool to integrate Jupyter notebooks (via Papermill) with MLflow for data science experimentation and tracking.,AI4;AI4-04,experiment_tracking;workflow_automation,library,Jupyter Notebook,https://github.com/eugeneyan/papermill-mlflow,,None,mlflow;papermill;experiment-tracking
935,ieee118_power_flow_data,Data pipeline for IEEE-118 power flow cases,"A data pipeline to build power flow cases for the IEEE-118 power system, facilitating power systems research and simulation.",AI4,data_generation;power_systems,workflow,Jupyter Notebook,https://github.com/evgenytsydenov/ieee118_power_flow_data,,NOASSERTION,power-systems;ieee-118;data-pipeline
936,maggot,Lightweight experiment tracking library,"A lightweight Python library designed to help keep track of numerical experiments, managing configurations and results.",AI4;AI4-04,experiment_tracking;configuration_management,library,Python,https://github.com/ex4sperans/maggot,,Apache-2.0,experiment-tracking;reproducibility;python
937,hermit,Hermetic sandbox for reproducible execution,"A tool that launches Linux programs in a hermetically isolated sandbox to ensure deterministic and repeatable behavior, useful for reproducible research artifacts and debugging.",AI4;AI4-04,reproducibility;execution_control,platform,Rust,https://github.com/facebookexperimental/hermit,,NOASSERTION,reproducibility;sandbox;determinism
938,PySlowFast,Video understanding codebase,"A codebase for reproducing state-of-the-art video models, providing implementations of SlowFast and other video classification algorithms.",AI4,video_understanding;model_training,library,Python,https://github.com/facebookresearch/SlowFast,,Apache-2.0,video-understanding;computer-vision;slowfast
939,Meta Agents Research Environments,Platform for evaluating AI agents,"A comprehensive platform designed to evaluate AI agents in dynamic, realistic scenarios, supporting evolving environments for agent adaptation research.",AI4;AI4-04,agent_evaluation;benchmark_environment,platform,Python,https://github.com/facebookresearch/meta-agents-research-environments,,MIT,ai-agents;evaluation;benchmark
940,param,Benchmark for recommendation and AI models,"A repository for development of micro-benchmarks and end-to-end networks for evaluation of training and inference platforms, specifically for recommendation systems.",AI4;AI4-04,benchmarking;recommendation_systems,dataset,Python,https://github.com/facebookresearch/param,,MIT,benchmark;recommendation;performance
941,keras-ocr,Packaged CRAFT and CRNN for OCR,A flexible and packaged version of the CRAFT text detector and Keras CRNN recognition model for optical character recognition tasks.,AI4,ocr;text_detection,library,Python,https://github.com/faustomorales/keras-ocr,,MIT,ocr;keras;computer-vision
942,yolov5-pip,Packaged version of YOLOv5,"A pip-installable package of the Ultralytics YOLOv5 object detection model, facilitating easy integration into Python workflows.",AI4,object_detection;computer_vision,library,Python,https://github.com/fcakyon/yolov5-pip,,GPL-3.0,yolov5;object-detection;pip-package
943,FaceRank,Face ranking using CNN,"A tool that uses a Convolutional Neural Network (CNN) based on TensorFlow/Keras to rank faces, providing a scoring mechanism.",AI4,face_analysis;ranking,solver,Python,https://github.com/fendouai/FaceRank,,GPL-3.0,face-ranking;cnn;tensorflow
944,FFMetrics,Video quality metrics visualization,"A tool to visualize video quality metrics such as PSNR, SSIM, and VMAF calculated by FFmpeg, aiding in video compression analysis.",AI4;AI4-04,video_quality_assessment;metrics_visualization,solver,,https://github.com/fifonik/FFMetrics,,None,video-quality;psnr;ssim
945,FlagPerf,AI chip benchmarking platform,"An open-source software platform for benchmarking AI chips, providing a standardized way to evaluate hardware performance for AI workloads.",AI4;AI4-04,hardware_benchmarking;performance_evaluation,platform,Python,https://github.com/flagos-ai/FlagPerf,,Apache-2.0,benchmark;ai-chip;hardware
946,GTO (Git Tag Ops),Git-based registry for versioning machine learning models and artifacts,"A tool that turns a Git repository into an artifact or model registry using Git tags. It allows data scientists to register, version, and manage the lifecycle of models and datasets directly within their version control system.",AI4;AI4-04,model_registry;artifact_versioning,tool,Python,https://github.com/iterative/gto,https://iterative.github.io/gto/,Apache-2.0,mlops;git;model-registry;version-control
947,tbparse,Parser for loading TensorBoard event logs into Pandas DataFrames,"A library designed to parse TensorBoard event logs (tfevents) and convert them into Pandas DataFrames. This facilitates custom analysis, statistical processing, and publication-quality plotting of experiment metrics outside of the TensorBoard UI.",AI4;AI4-04,experiment_analysis;log_parsing,library,Python,https://github.com/j3soon/tbparse,https://tbparse.readthedocs.io/,Apache-2.0,tensorboard;pandas;visualization;experiment-tracking
948,jeelizPupillometry,Browser-based real-time pupillometry library using computer vision,A JavaScript library that uses WebGL and deep learning to perform real-time pupillometry (measurement of pupil size) via a webcam. It is used in cognitive science and psychology experiments to measure physiological responses.,AI4;AI4-01,biometric_measurement;data_acquisition,library,JavaScript,https://github.com/jeeliz/jeelizPupillometry,,Apache-2.0,pupillometry;computer-vision;neuroscience;webgl
949,clip-image-sorter,Tool for semantic image sorting and filtering using CLIP,A browser-based tool that utilizes OpenAI's CLIP model to sort and organize local image datasets based on text prompts. It serves as a data processing utility for curating and filtering image datasets for scientific or ML purposes.,AI4;AI4-01,data_filtering;dataset_curation,tool,HTML,https://github.com/josephrocca/clip-image-sorter,https://josephrocca.github.io/clip-image-sorter/,MIT,clip;dataset-management;computer-vision;data-cleaning
950,OpenJSCAD,Programmatic 3D modeling tool for parametric design,"A set of modular tools for creating parametric 2D and 3D designs using JavaScript code. It provides a reproducible method for generating 3D models, useful for designing scientific apparatus, hardware components, and physical visualizations.",AI4;AI4-01,scientific_modeling;hardware_design,tool,JavaScript,https://github.com/jscad/OpenJSCAD.org,https://openjscad.xyz/,MIT,cad;3d-modeling;parametric-design;visualization
951,Kedro,Framework for reproducible data science pipelines,"A development workflow framework that applies software engineering best practices to data science code. It helps structure data pipelines to be reproducible, modular, and maintainable, facilitating experiment tracking and collaboration.",AI4;AI4-04,workflow_management;reproducibility,framework,Python,https://github.com/kedro-org/kedro,https://kedro.org,Apache-2.0,pipeline;data-science;reproducibility;workflow
952,Kedro-Viz,Visualizer for Kedro data science pipelines,A visualization tool for Kedro pipelines that displays the data lineage and structure of data science experiments. It helps researchers understand complex workflows and track the flow of data transformations.,AI4;AI4-04,pipeline_visualization;experiment_tracking,tool,JavaScript,https://github.com/kedro-org/kedro-viz,https://kedro.org,Apache-2.0,visualization;data-lineage;pipeline;mlops
953,Hera,Real-time metrics dashboard for Keras models,"A tool designed to train and evaluate Keras models while streaming metrics to a browser-based dashboard. It provides real-time visualization of experiment performance, aiding in the monitoring and debugging of deep learning training processes.",AI4;AI4-04,experiment_monitoring;metrics_visualization,tool,JavaScript,https://github.com/keplr-io/hera,,MIT,keras;dashboard;visualization;deep-learning
954,KitOps,Packaging and versioning tool for AI/ML model artifacts,"A DevOps tool that packages AI/ML models, datasets, code, and configurations into OCI (Open Container Initiative) artifacts (ModelKits). It facilitates the versioning, sharing, and deployment of scientific models in a standardized format.",AI4;AI4-04,model_packaging;artifact_management,tool,Go,https://github.com/kitops-ml/kitops,https://kitops.ml/,Apache-2.0,mlops;oci;model-packaging;reproducibility
955,Klever Model Registry,Cloud-native registry for ML model management,"A tool for managing the lifecycle of machine learning models, providing a registry to index, version, and store model metadata. It supports the tracking of model lineage and facilitates collaboration in ML research and production.",AI4;AI4-04,model_registry;metadata_tracking,service,Go,https://github.com/kleveross/klever-model-registry,,Apache-2.0,mlops;model-management;kubernetes;registry
956,Kubeflow Model Registry,Centralized model registry for the Kubeflow ecosystem,"A component of the Kubeflow project that provides a central interface for indexing and managing ML models, versions, and artifact metadata. It bridges the gap between experimentation and production by tracking model lineage.",AI4;AI4-04,model_registry;experiment_tracking,platform,Go,https://github.com/kubeflow/model-registry,https://www.kubeflow.org/,Apache-2.0,kubeflow;mlops;model-management;metadata
957,LabML,Mobile monitoring tool for deep learning training,A library and platform that allows researchers to monitor deep learning model training metrics and hardware usage (GPU/CPU) remotely via mobile devices. It assists in tracking long-running experiments.,AI4;AI4-04,experiment_monitoring;remote_tracking,library,Python,https://github.com/labmlai/labml,https://labml.ai/,MIT,monitoring;deep-learning;mobile;experiment-tracking
958,LangEvals,Platform for benchmarking and evaluating LLMs,"A tool that aggregates various language model evaluators into a single interface. It provides standard metrics and guardrails to benchmark LLM models and pipelines, facilitating the scientific evaluation of NLP models.",AI4;AI4-04,model_evaluation;benchmarking,library,Jupyter Notebook,https://github.com/langwatch/langevals,,MIT,llm;evaluation;benchmark;nlp
959,tensorboardX,TensorBoard logging adapter for PyTorch and other frameworks,"A library that allows researchers using PyTorch, Chainer, MXNet, and other frameworks to write events and metrics to TensorBoard files. It enables the visualization of training curves, embeddings, and other scientific experiment data.",AI4;AI4-04,experiment_logging;visualization,library,Python,https://github.com/lanpa/tensorboardX,https://tensorboardx.readthedocs.io/,MIT,pytorch;tensorboard;visualization;logging
960,Layer SDK,Metadata store SDK for production ML pipelines,"The SDK for Layer, a metadata store designed to track machine learning experiments, models, and datasets. It enables reproducibility and collaboration by managing the metadata of scientific workflows.",AI4;AI4-04,metadata_management;experiment_tracking,library,Python,https://github.com/layerai-archive/sdk,https://layer.ai/,Apache-2.0,mlops;metadata;experiment-tracking;reproducibility
961,VALL-E (PyTorch),PyTorch implementation of VALL-E zero-shot text-to-speech model,"An open-source implementation of the VALL-E text-to-speech model, capable of zero-shot speech synthesis and voice cloning, serving as a solver for audio generation research.",AI4,speech_synthesis;generative_modeling,solver,Python,https://github.com/lifeiteng/vall-e,https://lifeiteng.github.io/valle/index.html,Apache-2.0,text-to-speech;pytorch;deep-learning;audio-generation
962,Minetorch,Lightweight PyTorch wrapper for deep learning training loops,"A minimalist library designed to simplify the training process of deep learning models in PyTorch, providing abstractions for training loops and experiment management.",AI4,model_training;experiment_management,library,Python,https://github.com/louis-she/minetorch,,MIT,pytorch;training-loop;deep-learning
963,simple_GRPO,Minimal implementation of GRPO algorithm for LLM reasoning,"A simplified implementation of the Group Relative Policy Optimization (GRPO) algorithm, designed for reproducing reasoning capabilities in Large Language Models (LLMs) similar to DeepSeek-R1.",AI4,reinforcement_learning;llm_alignment,solver,Python,https://github.com/lsdefine/simple_GRPO,,Apache-2.0,grpo;llm;reinforcement-learning;reasoning
964,jupyter_tensorboard,Jupyter Notebook extension for starting TensorBoard,"A tool that integrates TensorBoard directly into the Jupyter Notebook interface, facilitating the tracking and visualization of machine learning experiments within the interactive coding environment.",AI4;AI4-04,experiment_tracking;visualization,platform,Python,https://github.com/lspvic/jupyter_tensorboard,,MIT,tensorboard;jupyter;visualization;tracking
965,gaze_tracker,Eye gaze tracking system using OpenCV,"A computer vision-based application that estimates eye gaze direction to control a mouse pointer, serving as a tool for human-computer interaction research and behavioral analysis.",AI4,gaze_estimation;computer_vision,solver,Python,https://github.com/luca-ant/gaze_tracker,,GPL-3.0,opencv;eye-tracking;gaze-estimation
966,ema-pytorch,Exponential Moving Average (EMA) wrapper for PyTorch models,"A utility library for maintaining an Exponential Moving Average of model parameters during training, which is a common technique in deep learning to improve model generalization and stability.",AI4,model_optimization;training_utility,library,Python,https://github.com/lucidrains/ema-pytorch,,MIT,pytorch;ema;deep-learning
967,µBench,Microservice benchmark for cloud/edge computing platforms,A benchmarking tool designed to evaluate the performance of cloud and edge computing platforms by running customizable dummy microservice applications on Kubernetes.,AI4;AI4-04,benchmarking;system_performance,solver,Python,https://github.com/mSvcBench/muBench,,NOASSERTION,benchmarking;kubernetes;edge-computing;microservices
968,marimo,Reactive Python notebook for reproducible research,"A next-generation reactive notebook for Python that ensures reproducibility by executing as a script, supporting SQL queries, and providing a modern AI-native editing environment for scientific experiments.",AI4,scientific_computing;reproducibility;visualization,platform,Python,https://github.com/marimo-team/marimo,https://marimo.io,Apache-2.0,notebook;reproducibility;python;data-science
969,Banks,Jinja-based LLM prompt engineering and management tool,"A tool for managing and generating LLM prompts using a Jinja-based template language, supporting metadata management, versioning, and storage, facilitating systematic prompt engineering research.",AI4,prompt_engineering;llm_interaction,library,Python,https://github.com/masci/banks,,MIT,prompt-engineering;llm;jinja;templates
970,plf_nanotimer,Cross-platform C++ microsecond-precision timer for benchmarking,"A simple, low-overhead C++ timer class designed for microsecond-precision benchmarking across different platforms, useful for performance analysis in scientific computing.",AI4;AI4-04,benchmarking;performance_analysis,library,C++,https://github.com/mattreecebentley/plf_nanotimer,http://plflib.org/nanotimer.htm,Zlib,benchmarking;timer;cpp;performance
971,rl-bh-environment,Reinforcement learning environment for bullet hell games,"A custom reinforcement learning environment simulating a bullet hell game (Sacred Curry Shooter), providing an interface for training and testing RL agents in dynamic environments.",AI4,reinforcement_learning;simulation,solver,Python,https://github.com/michael-pacheco/rl-bh-environment,,None,reinforcement-learning;gym-environment;simulation
972,SacredBrowser,GUI for browsing Sacred experiment results,"A graphical user interface for browsing, filtering, and analyzing machine learning experiment results stored in MongoDB by the Sacred experiment management framework.",AI4;AI4-04,experiment_tracking;visualization,platform,Python,https://github.com/michaelwand/SacredBrowser,,MIT,sacred;experiment-tracking;visualization;mongodb
973,Windows Agent Arena,Scalable OS platform for benchmarking multi-modal AI agents,"A scalable operating system platform designed for testing and benchmarking multi-modal AI agents, providing a realistic environment for evaluating agent performance on Windows tasks.",AI4;AI4-04,benchmarking;agent_evaluation,platform,Python,https://github.com/microsoft/WindowsAgentArena,,MIT,ai-agent;benchmarking;multimodal;evaluation
974,Coyote,Tool for testing concurrent C# code and reproducing bugs,"A library and tool designed for the systematic testing of concurrent C# code, enabling deterministic reproduction of concurrency bugs, widely used in systems research and verification.",AI4,software_verification;concurrency_testing,solver,C#,https://github.com/microsoft/coyote,https://microsoft.github.io/coyote/,NOASSERTION,testing;concurrency;verification;dotnet
975,Project Malmo,AI experimentation platform built on Minecraft,"A platform for Artificial Intelligence experimentation and research built on top of Minecraft, providing a rich, complex environment for testing reinforcement learning and multi-agent systems.",AI4,reinforcement_learning;simulation;ai_environment,platform,Java,https://github.com/microsoft/malmo,https://microsoft.github.io/malmo/,MIT,minecraft;reinforcement-learning;ai-platform;simulation
976,Qlib,AI-oriented quantitative investment platform,"An AI-oriented quantitative investment platform that covers the entire chain of quantitative research, including data processing, model training (RL/Supervised), and backtesting, empowering AI-driven financial modeling.",AI4,quantitative_finance;market_modeling;backtesting,platform,Python,https://github.com/microsoft/qlib,https://qlib.readthedocs.io/,MIT,quantitative-finance;machine-learning;backtesting;investment
977,Responsible AI Toolbox Tracker,JupyterLab extension for tracking Responsible AI mitigations,"A JupyterLab extension designed to track, manage, and compare Responsible AI experiments and mitigations, facilitating the evaluation of model fairness and safety.",AI4;AI4-04,responsible_ai;experiment_tracking;model_evaluation,platform,TypeScript,https://github.com/microsoft/responsible-ai-toolbox-tracker,,MIT,responsible-ai;jupyterlab-extension;tracking;fairness
978,MedPerf,Open benchmarking platform for medical AI using Federated Evaluation,"MedPerf is an open benchmarking platform designed specifically for medical artificial intelligence. It enables the evaluation of AI models on diverse, real-world medical datasets without requiring data to leave the data owner's premises (Federated Evaluation), ensuring privacy and compliance while providing robust performance metrics.",AI4;AI4-04;Medical AI,benchmarking;federated_evaluation;model_validation,platform,Python,https://github.com/mlcommons/medperf,https://medperf.org,Apache-2.0,medical-ai;benchmarking;federated-learning;healthcare
979,MLflow,Open source platform for the machine learning lifecycle,"MLflow is a comprehensive platform for managing the end-to-end machine learning lifecycle. It includes components for experiment tracking (logging parameters, metrics, and artifacts), model packaging, model registry, and deployment, widely used in scientific research for reproducibility and experiment management.",AI4;AI4-04;MLOps,experiment_tracking;model_registry;reproducibility,platform,Python,https://github.com/mlflow/mlflow,https://mlflow.org/docs/latest/index.html,Apache-2.0,experiment-tracking;model-management;mlops;reproducibility
980,Neptune Client,Client library for Neptune experiment tracking and model registry,"The Neptune Client is the Python interface for Neptune, a metadata store for MLOps. It allows researchers to log, organize, and compare machine learning experiments, including metrics, hyperparameters, and model artifacts, facilitating reproducibility in scientific computing.",AI4;AI4-04;MLOps,experiment_tracking;metadata_logging;reproducibility,library,Python,https://github.com/neptune-ai/neptune-client,https://docs.neptune.ai/,Apache-2.0,experiment-tracking;mlops;metadata-store
981,ONNX Registry,Intelligent Component Registry web service for ONNX models,"A web service designed to act as a registry for managing and retrieving Spiking Neural Networks (SNN), Deep Neural Networks (DNN), and ML models stored in the ONNX format. It supports the tracking and versioning of model components in neuromorphic and IoT research contexts.",AI4;AI4-04;Neuromorphic Computing,model_registry;component_management;model_tracking,service,Python,https://github.com/neurom-iot/onnx-registry,,MIT,onnx;model-registry;neuromorphic;iot
982,Cosmos-Predict2.5,World Foundation Models for video-based future state prediction and simulation,Cosmos-Predict2.5 is a suite of World Foundation Models (WFMs) designed to simulate and predict future states of the world through video generation. It serves as a scientific simulation tool for physical world dynamics.,AI4;Computer Vision;Simulation,simulation;video_generation;future_prediction,solver,Python,https://github.com/nvidia-cosmos/cosmos-predict2.5,,Apache-2.0,world-model;simulation;video-generation;foundation-model
983,Onepanel,"End-to-end computer vision platform for annotation, training, and workflow automation","Onepanel is a unified platform for computer vision workflows, integrating data labeling, model training, hyperparameter tuning, and pipeline automation. It supports reproducible research by managing the entire lifecycle of CV models.",AI4;Computer Vision;MLOps,workflow_automation;annotation;training_management,platform,Go,https://github.com/onepanelio/onepanel,https://docs.onepanel.ai,Apache-2.0,computer-vision;mlops;workflow;annotation
984,GenAIStudio,Low-code platform for constructing and benchmarking GenAI applications,"GenAI Studio provides a visual interface and toolkit for building, evaluating, and benchmarking Generative AI applications. It facilitates the assessment of model performance and the creation of deployable AI packages.",AI4;AI4-04;Generative AI,evaluation;benchmarking;application_building,platform,JavaScript,https://github.com/opea-project/GenAIStudio,,Apache-2.0,genai;evaluation;benchmark;low-code
985,Amphion,"Toolkit for audio, music, and speech generation research","Amphion is a toolkit designed to support reproducible research in audio, music, and speech generation. It provides implementations of various generation models and tools for processing audio data, aiming to lower the barrier for entry in audio AI research.",AI4;Audio;Speech Synthesis,audio_generation;speech_synthesis;music_generation,library,Python,https://github.com/open-mmlab/Amphion,,MIT,audio-generation;speech-synthesis;music-generation;reproducibility
986,modelstore,Library for versioning and exporting machine learning models,"modelstore is a Python library that facilitates the versioning, export, and storage of machine learning models to various storage providers. It helps in tracking model artifacts and maintaining a registry of trained models.",AI4;AI4-04;MLOps,model_versioning;artifact_management;model_registry,library,Python,https://github.com/operatorai/modelstore,https://modelstore.readthedocs.io,Apache-2.0,model-versioning;mlops;artifact-tracking
987,mlogger,Lightweight logger for machine learning experiments,mlogger is a simple and lightweight logging utility specifically designed for machine learning. It helps researchers track metrics and experiment progress without the overhead of heavy frameworks.,AI4;AI4-04,experiment_logging;metrics_tracking,library,Python,https://github.com/oval-group/mlogger,,MIT,logging;experiment-tracking;ml-tools
988,Android HCI Extractor,Multimodal Human-Computer Interaction extractor for Android experiments,"A tool for extracting, monitoring, and tracking multimodal user interactions on Android devices. It is designed for use in scientific experiments to record user behavior data for HCI research and analysis.",HCI;Data Collection,data_extraction;user_monitoring;interaction_tracking,tool,Java,https://github.com/pedromateo/android-hci-extractor-AHE11,,None,hci;android;data-collection;interaction-logging
989,PathBench,Benchmarking platform for path planning algorithms,PathBench is a platform for benchmarking both classic and learning-based path planning algorithms. It provides a standardized environment to evaluate and compare the performance of navigation algorithms in robotics and AI.,AI4;Robotics;AI4-04,benchmarking;algorithm_evaluation;path_planning,platform,Python,https://github.com/perfectly-balanced/PathBench,,NOASSERTION,path-planning;benchmark;robotics;navigation
990,Clean-Offline-RLHF,Benchmark suite for Offline Reinforcement Learning with Human Feedback,"This repository implements the Uni-RLHF benchmark suite, providing a platform for evaluating offline Reinforcement Learning with Human Feedback (RLHF) algorithms. It supports reproducible research in alignment and reinforcement learning.",AI4;AI4-04;RLHF,benchmarking;algorithm_evaluation;rlhf,platform,Python,https://github.com/pickxiguapi/Clean-Offline-RLHF,,MIT,rlhf;benchmark;reinforcement-learning;alignment
991,Uni-RLHF-Platform,Universal platform and benchmark suite for Reinforcement Learning with Human Feedback (RLHF),"A comprehensive platform and benchmark suite designed for Reinforcement Learning with Diverse Human Feedback (RLHF). It supports the evaluation and development of RLHF algorithms, providing a standardized environment for research and comparison.",AI4;AI4-04,benchmarking;reinforcement_learning;human_feedback,platform,Python,https://github.com/pickxiguapi/Uni-RLHF-Platform,,MIT,rlhf;benchmark;reinforcement-learning
992,ploomber-engine,"Toolbox for testing, tracking, and debugging Jupyter notebooks","A toolkit designed to enhance Jupyter notebooks for scientific workflows. It includes features for experiment tracking, debugging, profiling, and testing notebooks, facilitating reproducible research and development in interactive environments.",AI4;AI4-04,experiment_tracking;reproducibility;debugging,library,Python,https://github.com/ploomber/ploomber-engine,https://ploomber-engine.readthedocs.io/,BSD-3-Clause,jupyter;experiment-tracking;profiling
993,sklearn-evaluation,Machine learning model evaluation and experiment tracking library,"A library that simplifies machine learning model evaluation by providing tools for generating plots, tables, and HTML reports. It also includes features for experiment tracking and analysis within Jupyter notebooks, aiding in the comparison and selection of models.",AI4;AI4-04,model_evaluation;experiment_tracking;visualization,library,Python,https://github.com/ploomber/sklearn-evaluation,https://sklearn-evaluation.ploomber.io/,Apache-2.0,evaluation;metrics;visualization
994,Polyaxon,MLOps platform for managing and orchestrating the machine learning lifecycle,"A platform for reproducible and scalable machine learning and deep learning. It provides tools for experiment tracking, hyperparameter tuning, and workflow orchestration, allowing researchers to manage the entire ML lifecycle.",AI4;AI4-04,experiment_tracking;workflow_orchestration;mlops,platform,Python,https://github.com/polyaxon/polyaxon,https://polyaxon.com/docs/,Apache-2.0,mlops;experiment-tracking;orchestration
995,TraceML,"Engine for ML/Data tracking, visualization, and drift detection","A library and engine dedicated to tracking machine learning experiments, visualizing data and model performance, explaining models, and detecting drift. It serves as the tracking component of the Polyaxon ecosystem but can be used for logging and analysis.",AI4;AI4-04,experiment_tracking;visualization;drift_detection,library,Python,https://github.com/polyaxon/traceml,https://polyaxon.com/docs/traceml/,Apache-2.0,tracking;visualization;logging
996,Promptify,Prompt engineering and versioning toolkit for LLMs,"A toolkit for prompt engineering, versioning, and structuring outputs from Large Language Models (LLMs). It facilitates the management of prompts as scientific artifacts, enabling reproducible interactions with NLP models.",AI4;AI4-04,prompt_engineering;versioning;nlp,library,Jupyter Notebook,https://github.com/promptslab/Promptify,,Apache-2.0,prompt-engineering;llm;versioning
997,EconML,Automated Learning and Intelligence for Causation and Economics,"A Python package that applies machine learning techniques to estimate heterogeneous treatment effects from observational data via causal inference. It implements orthogonal machine learning algorithms to measure causal effects, bridging econometrics and machine learning.",AI4,causal_inference;econometrics;estimation,library,Jupyter Notebook,https://github.com/py-why/EconML,https://econml.azurewebsites.net/,NOASSERTION,causal-inference;machine-learning;economics
998,pseudo-3d-pytorch,PyTorch implementation of Pseudo-3D Residual Networks (P-3D),"A PyTorch implementation of Pseudo-3D Residual Networks (P-3D) for spatiotemporal feature learning in videos. It includes support for pretrained models, enabling reproduction of results and application to video analysis tasks.",AI4;AI4-04,model_implementation;reproduction;video_analysis,library,Python,https://github.com/qijiezhao/pseudo-3d-pytorch,,MIT,pytorch;p3d;video-recognition
999,rl-experiments,Configuration and tracking for Reinforcement Learning experiments,"A repository dedicated to tracking and reproducing Reinforcement Learning (RL) experiments using Ray RLlib. It serves as a benchmark reference, containing configurations and scripts to reproduce state-of-the-art results.",AI4;AI4-04,reproduction;benchmarking;reinforcement_learning,dataset,,https://github.com/ray-project/rl-experiments,,Apache-2.0,reinforcement-learning;reproducibility;benchmarks
1000,FuxiCTR,Configurable and reproducible library for CTR prediction,"A configurable, tunable, and reproducible library designed for Click-Through Rate (CTR) prediction tasks. It provides a standardized environment for benchmarking various CTR models and ensuring experimental reproducibility.",AI4;AI4-04,benchmarking;ctr_prediction;reproducibility,library,Python,https://github.com/reczoo/FuxiCTR,https://fuxictr.github.io/,Apache-2.0,ctr-prediction;benchmark;recommender-systems
1001,garage,Toolkit for reproducible reinforcement learning research,A toolkit for developing and evaluating reinforcement learning algorithms with a focus on reproducibility. It provides a consistent interface for running experiments and benchmarking RL methods.,AI4;AI4-04,reproducibility;reinforcement_learning;benchmarking,library,Python,https://github.com/rlworkgroup/garage,https://garage.readthedocs.io/,MIT,reinforcement-learning;reproducibility;toolkit
1002,drake,Pipeline toolkit for reproducibility in R,"An R-focused pipeline toolkit designed for reproducibility and high-performance computing. It manages data analysis workflows, tracks dependencies, and ensures that results are up-to-date and reproducible.",AI4;AI4-04,workflow_management;reproducibility;pipeline,workflow,R,https://github.com/ropensci/drake,https://docs.ropensci.org/drake/,GPL-3.0,r;reproducibility;pipeline
1003,greadability,Graph layout readability metrics library,"A JavaScript library for calculating readability metrics for graph layouts. It provides quantitative measures to evaluate the quality of graph visualizations, supporting research in information visualization and graph theory.",AI4;AI4-04,metrics;graph_visualization;evaluation,library,JavaScript,https://github.com/rpgove/greadability,http://rpgove.github.io/greadability/,BSD-3-Clause,graph-layout;metrics;visualization
1004,vetiver-r,"MLOps tool for versioning, sharing, deploying, and monitoring models in R","Vetiver provides fluent tooling to version, share, deploy, and monitor machine learning models. It is designed to handle the lifecycle of models, ensuring they can be reliably tracked and maintained in production or research environments.",AI4;AI4-04,model_management;tracking;deployment,library,R,https://github.com/rstudio/vetiver-r,https://vetiver.rstudio.com/,MIT,mlops;model-monitoring;r-stats;model-deployment
1005,llm-data-annotation,Framework using LLMs for data annotation and iterative active learning,A framework that leverages Large Language Models (like GPT-3.5) to perform data annotation and model enhancement. It combines human expertise with LLMs and employs Iterative Active Learning and CleanLab (Confident Learning) to improve dataset quality.,AI4;AI4-04,data_annotation;active_learning;dataset_curation,workflow,Python,https://github.com/saran9991/llm-data-annotation,,MIT,llm;data-annotation;active-learning;cleanlab
1006,DVCP-TE,Simulation model of the Tennessee Eastman chemical process for security research,The Damn Vulnerable Chemical Process (DVCP) - Tennessee Eastman is a simulation environment designed for research into cyber-physical systems security. It provides a realistic model of a chemical process to generate data for attack detection and control system analysis.,AI4;AI4-04,simulation;data_generation;security_benchmark,dataset,HTML,https://github.com/satejnik/DVCP-TE,,BSD-3-Clause,simulation;chemical-process;ics-security;benchmark
1007,Semantic-Shapes,Semantic segmentation pipeline for custom image annotation,A pipeline designed to facilitate custom image annotation for semantic segmentation tasks. It aids in the processing and preparation of image datasets for computer vision research.,AI4;AI4-04,image_annotation;segmentation;data_processing,workflow,Jupyter Notebook,https://github.com/seth814/Semantic-Shapes,,MIT,semantic-segmentation;annotation;computer-vision
1008,EyeLoop,Python-based eye-tracking system for dynamic closed-loop experiments,"EyeLoop is a specialized eye-tracking tool tailored for dynamic, closed-loop experiments in neuroscience and psychology. It runs on consumer-grade hardware and provides a platform for behavioral data acquisition.",AI4;AI4-04,data_acquisition;behavioral_tracking;neuroscience,platform,Python,https://github.com/simonarvin/eyeloop,https://github.com/simonarvin/eyeloop,GPL-3.0,eye-tracking;neuroscience;psychophysics;experiment-control
1009,Snakemake,Workflow management system for reproducible data analysis,Snakemake is a workflow management system that aims to reduce the complexity of creating facsimiles of data analysis steps. It is widely used in bioinformatics and data science to ensure reproducibility and scalability of scientific pipelines.,AI4;AI4-04,workflow_management;reproducibility;pipeline_orchestration,workflow,Python,https://github.com/snakemake/snakemake,https://snakemake.readthedocs.io,MIT,workflow;bioinformatics;reproducibility;pipeline
1010,Whitebox,E2E ML monitoring platform with edge capabilities,"Whitebox is an open-source machine learning monitoring platform designed to track model performance and data drift, specifically supporting edge capabilities and Kubernetes integration.",AI4;AI4-04,model_monitoring;tracking;mlops,platform,Python,https://github.com/squaredev-io/whitebox,https://whitebox.squaredev.io,MIT,ml-monitoring;observability;kubernetes;edge-ai
1011,Julia-LLM-Leaderboard,Benchmarking platform for LLM code generation in Julia,A platform and toolkit for evaluating and comparing the ability of Large Language Models to generate syntactically correct Julia code. It includes structured tests and automated evaluation pipelines.,AI4;AI4-04,benchmarking;leaderboard;code_generation_eval,platform,HTML,https://github.com/svilupp/Julia-LLM-Leaderboard,https://svilupp.github.io/Julia-LLM-Leaderboard/,MIT,julia;llm-leaderboard;benchmarking;code-generation
1012,tensorboardcolab,Utility to run TensorBoard within Google Colab,"A library designed to facilitate the use of TensorBoard, a standard visualization tool for machine learning experiments, directly within the Google Colab environment.",AI4;AI4-04,visualization;experiment_tracking,library,Python,https://github.com/taomanwai/tensorboardcolab,,MIT,tensorboard;colab;visualization;ml-tools
1013,envd,Reproducible development environment manager for AI/Data Science,envd is a command-line tool that creates reproducible development environments for AI and data science. It abstracts away Dockerfile complexity to ensure consistent environments for research and development.,AI4;AI4-04,reproducibility;environment_management;infrastructure,solver,Go,https://github.com/tensorchord/envd,https://envd.tensorchord.ai,Apache-2.0,reproducibility;dev-environment;data-science;docker-wrapper
1014,Iterate,Benchmarking and hyperparameter optimization tool for TerraTorch,"A tool designed for benchmarking and hyper-parameter optimization within the TerraTorch ecosystem. It integrates MLFlow for experiment logging, Optuna for optimization, and Ray for parallel execution.",AI4;AI4-04,benchmarking;hyperparameter_optimization,workflow,Jupyter Notebook,https://github.com/terrastackai/iterate,,Apache-2.0,benchmarking;hpo;mlflow
1015,Testground,Platform for benchmarking and simulating distributed systems,"A platform for testing, benchmarking, and simulating distributed and p2p systems at scale. It allows researchers and developers to verify protocols and systems under various network conditions.",AI4;AI4-04,benchmarking;simulation,platform,Go,https://github.com/testground/testground,https://docs.testground.ai,NOASSERTION,benchmarking;distributed-systems;simulation
1016,Crayon,Language-agnostic interface for TensorBoard visualization,"A tool that provides a language-agnostic interface to TensorBoard, allowing users to log and visualize experiment data from any programming language.",AI4;AI4-04,experiment_visualization;logging,library,Python,https://github.com/torrvision/crayon,,MIT,tensorboard;visualization;interface
1017,Torch-Fidelity,High-fidelity performance metrics for generative models,"A PyTorch library for calculating high-fidelity performance metrics for generative models, such as Inception Score (IS) and Fréchet Inception Distance (FID), ensuring reproducible evaluation.",AI4;AI4-04,model_evaluation;metrics_calculation,library,Python,https://github.com/toshas/torch-fidelity,,NOASSERTION,generative-models;metrics;fid
1018,DVC,Data Version Control for machine learning projects,"An open-source tool for data management and machine learning experiment tracking. It handles large files, data sets, and machine learning models, making projects reproducible and shareable.",AI4;AI4-04,data_versioning;experiment_tracking,workflow,Python,https://github.com/treeverse/dvc,https://dvc.org,Apache-2.0,data-versioning;mlops;reproducibility
1019,DVCLive,Library for logging ML metrics and tracking experiments,"A Python library for logging machine learning metrics, parameters, and model artifacts. It integrates with DVC to provide experiment tracking capabilities.",AI4;AI4-04,experiment_tracking;metrics_logging,library,Python,https://github.com/treeverse/dvclive,https://dvc.org/doc/dvclive,Apache-2.0,logging;tracking;mlops
1020,ATOM,Automated Tool for Optimized Modelling,"A Python package for fast exploration and experimentation of machine learning pipelines. It automates data cleaning, feature engineering, and model selection.",AI4;AI4-01,automl;pipeline_optimization,library,HTML,https://github.com/tvdboom/ATOM,https://tvdboom.github.io/ATOM/,MIT,automl;data-science;optimization
1021,Uncertainty Toolbox,Toolbox for predictive uncertainty quantification and metrics,"A Python toolbox for predictive uncertainty quantification, calibration, metrics, and visualization. It helps researchers evaluate the reliability of machine learning model predictions.",AI4;AI4-04,uncertainty_quantification;model_evaluation,library,Python,https://github.com/uncertainty-toolbox/uncertainty-toolbox,https://uncertainty-toolbox.github.io/,MIT,uncertainty;calibration;metrics
1022,TVault,Lightweight local registry for comparing PyTorch models,"A tool to quickly compare PyTorch models in a local, lightweight registry. It facilitates tracking model versions and performance metrics during development.",AI4;AI4-04,model_registry;model_comparison,library,Python,https://github.com/vessl-ai/tvault,,MIT,pytorch;model-management;tracking
1023,vSwarm-u,Serverless benchmark suite integrated with gem5,"A framework that integrates the serverless benchmark suite vSwarm with gem5, enabling research into system and microarchitecture for serverless computing.",AI4;AI4-04,benchmarking;system_simulation,workflow,Python,https://github.com/vhive-serverless/vSwarm-u,,MIT,benchmarking;serverless;gem5
1024,OpenCorr,Digital Image and Volume Correlation Library,"A C++ library for Digital Image Correlation (DIC) and Digital Volume Correlation (DVC), used for measuring deformation and strain in materials science and engineering.",AI4;AI4-11,image_analysis;strain_measurement,library,C++,https://github.com/vincentjzy/OpenCorr,,MPL-2.0,dic;dvc;mechanics
1025,Omniboard,Web-based dashboard for Sacred experiment tracking,"A web dashboard for visualizing experiments tracked with Sacred. It connects to the MongoDB database used by Sacred to display metrics, configuration, and artifacts.",AI4;AI4-04,experiment_visualization;dashboard,platform,JavaScript,https://github.com/vivekratnavel/omniboard,,MIT,sacred;visualization;dashboard
1026,SD-Extension-System-Info,Benchmarking and system info for Stable Diffusion,An extension for Stable Diffusion web UIs that provides system information and standardized benchmarking capabilities to evaluate generation performance.,AI4;AI4-04,benchmarking;performance_profiling,solver,Python,https://github.com/vladmandic/sd-extension-system-info,,MIT,stable-diffusion;benchmarking;system-info
1027,Deepkit ML,Real-time machine learning development and tracking suite,"A collaborative open-source machine learning development tool and training suite. It offers experiment execution, real-time tracking, debugging, and project management features.",AI4;AI4-04,experiment_tracking;mlops,platform,TypeScript,https://github.com/voided-org/deepkit-ml,https://deepkit.io,MIT,mlops;tracking;debugging
1028,PytorchAutoDrive,Segmentation and lane detection models with benchmarking,"A repository providing implementations of various segmentation and lane detection models for autonomous driving. It includes tools for fast training, visualization, benchmarking, and deployment.",AI4;AI4-01,benchmarking;model_training,library,Python,https://github.com/voldemortX/pytorch-auto-drive,,BSD-3-Clause,autonomous-driving;segmentation;benchmarking
1029,pref_voting,Python library for simulating and analyzing preferential voting methods,"A Python package designed to study and run elections using various preferential voting methods, including graded and cardinal voting systems, suitable for social choice theory research.",Social Science;Game Theory,simulation;social_choice_analysis,library,Python,https://github.com/voting-tools/pref_voting,,MIT,voting-systems;social-choice;simulation
1030,HiddenLayer,Neural network graph visualization and training metrics library,"A lightweight library for visualizing neural network graphs and tracking training metrics for PyTorch, TensorFlow, and Keras models.",AI4;AI4-04,visualization;training_monitoring,library,Python,https://github.com/waleedka/hiddenlayer,,MIT,visualization;neural-networks;training-metrics
1031,Weights & Biases,Developer platform for experiment tracking and model management,"A comprehensive MLOps platform for tracking experiments, visualizing results, managing model versions, and collaborating on machine learning projects.",AI4;AI4-04,experiment_tracking;model_management,platform,Python,https://github.com/wandb/wandb,https://wandb.ai,MIT,experiment-tracking;mlops;visualization
1032,MuZero General,General implementation of the MuZero reinforcement learning algorithm,"A readable and modular implementation of the MuZero algorithm, serving as a research baseline and tool for reinforcement learning experiments.",AI4;Reinforcement Learning,modeling;reinforcement_learning,solver,Python,https://github.com/werner-duvaud/muzero-general,,MIT,muzero;reinforcement-learning;ai-model
1033,Test Tube,Library for experiment logging and hyperparameter search,A Python library designed to easily log deep learning experiments and parallelize hyperparameter search across computing clusters.,AI4;AI4-04,experiment_logging;hyperparameter_optimization,library,Python,https://github.com/williamFalcon/test-tube,,MIT,experiment-logging;hpo;deep-learning
1034,pytorch-fcn,PyTorch implementation of Fully Convolutional Networks,"A reproducible implementation of Fully Convolutional Networks (FCN) for semantic segmentation, serving as a baseline tool for computer vision research.",Computer Vision,semantic_segmentation;modeling,solver,Python,https://github.com/wkentaro/pytorch-fcn,,MIT,fcn;semantic-segmentation;pytorch
1035,tensorflow-plot,Utility for using Matplotlib within TensorFlow computation graphs,"A library that allows wrapping Matplotlib plots as TensorFlow operations, enabling visualization of training progress directly within TensorBoard.",AI4;AI4-04,visualization;monitoring,library,Python,https://github.com/wookayin/tensorflow-plot,,MIT,tensorflow;matplotlib;visualization
1036,MultiKernelBench,Multi-platform benchmark for kernel generation,"A benchmark suite designed to evaluate the performance of kernel generation across multiple hardware platforms, aiding in system performance analysis.",AI Systems;High Performance Computing,benchmarking;performance_analysis,library,Python,https://github.com/wzzll123/MultiKernelBench,,NOASSERTION,benchmarking;kernel-generation;hpc
1037,Real-ESRGAN,Practical algorithms for general image and video restoration,"A tool implementing Real-ESRGAN for enhancing and restoring images and videos, widely used for data preprocessing and quality improvement in vision tasks.",Computer Vision,image_restoration;super_resolution,solver,Python,https://github.com/xinntao/Real-ESRGAN,,BSD-3-Clause,super-resolution;image-restoration;gan
1038,MME,"Platform for logging, replaying, and benchmarking LLM calls","A plug-and-play platform designed to log, replay, and benchmark Large Language Model (LLM) API calls, facilitating evaluation and optimization of LLM applications.",AI4;AI4-04,benchmarking;logging;llm_evaluation,library,CSS,https://github.com/xuzeyu91/MME,,MIT,llm;benchmarking;logging
1039,Arrakis,Library for mechanistic interpretability experiments,"A library designed to conduct, track, and visualize mechanistic interpretability experiments for neural networks, aiding in understanding model behavior.",AI4;AI Interpretability,interpretability;analysis,library,Jupyter Notebook,https://github.com/yash-srivastava19/arrakis,,None,interpretability;mechanistic-interpretability;visualization
1040,nerf-pytorch,PyTorch implementation of Neural Radiance Fields (NeRF),"A widely used PyTorch implementation of NeRF for synthesizing novel views of complex scenes, serving as a standard tool for 3D vision research.",Computer Vision;Graphics,novel_view_synthesis;modeling,solver,Python,https://github.com/yenchenlin/nerf-pytorch,,MIT,nerf;3d-vision;rendering
1041,friendly-stable-audio-tools,Tools for training and using stable audio generative models,"A refactored and updated library for working with audio/music generative models, facilitating research and application of stable audio generation.",Audio Processing;AI4,audio_generation;modeling,library,Python,https://github.com/yukara-ikemiya/friendly-stable-audio-tools,,MIT,generative-audio;stable-audio;deep-learning
1042,TF2DeepFloorplan,Deep FloorPlan Recognition tool with multi-task network,"A TensorFlow 2 implementation for floorplan recognition using a multi-task network, including deployment and visualization tools like TensorBoard and TFLite support.",Computer Vision,floorplan_recognition;modeling,solver,Python,https://github.com/zcemycl/TF2DeepFloorplan,,GPL-3.0,floorplan-recognition;computer-vision;tensorflow
1043,ZnTrack,"Interface to create, run, and benchmark DVC pipelines in Python","A Python interface for DVC (Data Version Control) that allows creating, visualizing, and benchmarking data science pipelines directly from Python or Jupyter notebooks.",AI4;AI4-04,pipeline_management;experiment_tracking;benchmarking,workflow,Python,https://github.com/zincware/ZnTrack,,Apache-2.0,dvc;pipeline;reproducibility
1044,pytorch-generative-model-collections,Collection of generative model implementations in PyTorch,"A comprehensive library containing implementations of various generative models (GANs, VAEs, etc.) in PyTorch, serving as a reference and toolbox for generative modeling research.",AI4;Computer Vision,generative_modeling;modeling,library,Python,https://github.com/znxlwm/pytorch-generative-model-collections,,None,generative-models;gan;pytorch
1045,SkySense-O,Open-world remote sensing interpretation model,"A vision-centric visual-language model aggregated with CLIP and SAM, designed for open-world remote sensing interpretation tasks.",Remote Sensing;AI4,remote_sensing_interpretation;modeling,solver,Python,https://github.com/zqcrafts/SkySense-O,,Apache-2.0,remote-sensing;vlm;clip
1046,DiaHalu,Dialogue-level hallucination evaluation benchmark for LLMs,"A benchmark dataset and evaluation framework designed to assess hallucination in Large Language Models at the dialogue level, focusing on multi-turn interactions.",AI4;AI4-05,hallucination_evaluation;safety_benchmarking,dataset,Python,https://github.com/141forever/DiaHalu,,None,hallucination;llm-evaluation;benchmark
1047,AgentPoison,Red-teaming tool for LLM Agents via backdoor poisoning,A red-teaming framework that evaluates the robustness of LLM agents by injecting backdoors into their memory or knowledge base to trigger targeted misbehaviors.,AI4;AI4-05,red_teaming;adversarial_attack;agent_security,solver,Python,https://github.com/AI-secure/AgentPoison,,MIT,red-teaming;llm-agent;backdoor-attack
1048,UDora,Unified red teaming framework against LLM Agents,A comprehensive framework for conducting red teaming operations against Large Language Model agents to identify security vulnerabilities and safety risks.,AI4;AI4-05,red_teaming;safety_evaluation,framework,Python,https://github.com/AI-secure/UDora,,None,red-teaming;llm-agent;safety-framework
1049,aixploit,Exploitation toolkit for LLM vulnerabilities,A toolkit designed for red teams and penetration testers to identify and exploit vulnerabilities in Large Language Model solutions.,AI4;AI4-05,penetration_testing;vulnerability_scanning,solver,Python,https://github.com/AINTRUST-AI/aixploit,,GPL-3.0,exploit;llm-security;red-teaming
1050,hallucinogen,Benchmark for evaluating hallucinations in LVLMs,A benchmark suite specifically designed to evaluate and quantify hallucination phenomena in Large Visual Language Models.,AI4;AI4-05,hallucination_evaluation;multimodal_benchmarking,dataset,Python,https://github.com/AikyamLab/hallucinogen,,MIT,lvlm;hallucination;benchmark
1051,Octopus-Family,Multi-dimensional safety assessment suite for AI models,A comprehensive safety testing suite developed by Alibaba-AAIG that provides multi-faceted probing to evaluate the safety and robustness of AI models.,AI4;AI4-05,safety_assessment;robustness_testing,framework,,https://github.com/Alibaba-AAIG/Octopus-Family,,None,safety-evaluation;robustness;testing-suite
1052,CognitiveLens,Analytics tool for human-AI alignment visualization,"A Streamlit-based analytics tool that visualizes fairness, calibration, and interpretability metrics to explore alignment between human and AI decisions.",AI4;AI4-05,alignment_visualization;fairness_auditing,application,Python,https://github.com/AmirhosseinHonardoust/Cognitivelens-AI-Human-Comparison,,MIT,visualization;alignment;fairness
1053,POPE,Evaluation method for object hallucination in LVLMs,A benchmark and evaluation method (Polling-based Object Probing Evaluation) for assessing object hallucination issues in Large Vision-Language Models.,AI4;AI4-05,hallucination_evaluation;object_detection_check,solver,Python,https://github.com/AoiDragon/POPE,,MIT,lvlm;hallucination;evaluation-metric
1054,PREPER,Dataset for safety evaluation of AI perception systems,A dataset specifically constructed to evaluate the safety and robustness of AI perception systems under various conditions.,AI4;AI4-05,safety_evaluation;perception_benchmarking,dataset,Python,https://github.com/AsymptoticAI/PREPER,,NOASSERTION,dataset;safety;perception
1055,Counterfit,CLI for assessing security of ML models,"A command-line automation tool by Azure for assessing the security of machine learning models, enabling red teaming and vulnerability scanning.",AI4;AI4-05,security_assessment;vulnerability_scanning,platform,Python,https://github.com/Azure/counterfit,,MIT,security;ml-assessment;cli
1056,ALERT,Benchmark for assessing LLM safety via red teaming,A comprehensive benchmark designed to assess the safety of Large Language Models through simulated red teaming attacks and scenarios.,AI4;AI4-05,safety_benchmarking;red_teaming,dataset,Python,https://github.com/Babelscape/ALERT,,NOASSERTION,benchmark;llm-safety;red-teaming
1057,SafeWatch,Safety-policy following video guardrail model,"An efficient video guardrail model designed to enforce safety policies with transparent explanations, serving as a tool for content moderation and safety alignment.",AI4;AI4-05,content_moderation;video_safety,solver,Python,https://github.com/BillChan226/SafeWatch,,MIT,guardrail;video-safety;explainability
1058,LitterBox,Secure sandbox for malware analysis with LLM integration,"A secure sandbox environment designed for red teamers to test payloads, featuring integration with LLM agents for enhanced analysis capabilities.",AI4;AI4-05,malware_analysis;red_teaming_environment,platform,YARA,https://github.com/BlackSnufkin/LitterBox,,GPL-3.0,sandbox;malware-analysis;llm-agent
1059,advertorch,Toolbox for adversarial robustness research,"A Python toolbox for adversarial robustness research, providing implementations of attacks, defenses, and robust training methods for PyTorch.",AI4;AI4-05,adversarial_robustness;attack_simulation,library,Jupyter Notebook,https://github.com/BorealisAI/advertorch,,LGPL-3.0,adversarial-attacks;robustness;pytorch
1060,Bud Runtime,Inference stack for AI deployment and optimization,"A comprehensive inference stack and runtime environment for deploying, optimizing, and scaling compound AI systems.",AI4,inference_optimization;model_deployment,platform,Python,https://github.com/BudEcosystem/bud-runtime,,AGPL-3.0,inference;deployment;optimization
1061,agent-attack,Adversarial robustness benchmark for multimodal agents,A framework and benchmark for dissecting and evaluating the adversarial robustness of multimodal language model agents.,AI4;AI4-05,robustness_benchmarking;adversarial_attack,solver,Python,https://github.com/ChenWu98/agent-attack,,MIT,multimodal-agent;robustness;benchmark
1062,VidHalluc,Benchmark for temporal hallucinations in video LLMs,A benchmark designed to evaluate temporal hallucinations in Multimodal Large Language Models specifically for video understanding tasks.,AI4;AI4-05,hallucination_evaluation;video_understanding,dataset,Python,https://github.com/CyL97/VidHalluc,,None,video-llm;hallucination;benchmark
1063,CMM,Benchmark for evaluating hallucinations across modalities,"The Curse of Multi-Modalities (CMM) is a benchmark for evaluating hallucinations in Large Multimodal Models across language, visual, and audio modalities.",AI4;AI4-05,hallucination_evaluation;multimodal_benchmarking,dataset,Python,https://github.com/DAMO-NLP-SG/CMM,,None,multimodal;hallucination;benchmark
1064,ToxiCN,Benchmark for Chinese toxic language detection,"A fine-grained benchmark and resource for detecting toxic language in Chinese, including a hierarchical taxonomy and dataset.",AI4;AI4-05,toxicity_detection;safety_benchmarking,dataset,Python,https://github.com/DUT-lujunyu/ToxiCN,,None,toxicity;chinese-nlp;benchmark
1065,AISecurity,AI Firewall for protecting LLMs,"A security tool acting as an AI firewall with multiple detection engines to protect Large Language Models from jailbreaks, injections, and adversarial attacks.",AI4;AI4-05,security_defense;attack_detection,service,HTML,https://github.com/DmitrL-dev/AISecurity,,NOASSERTION,firewall;llm-security;jailbreak-detection
1066,DiaHalu (ECNU),Dialogue-level hallucination evaluation benchmark,A benchmark for evaluating dialogue-level hallucinations in LLMs (Mirror/Fork of the main DiaHalu project).,AI4;AI4-05,hallucination_evaluation,dataset,,https://github.com/ECNU-ICALK/DiaHalu,,None,hallucination;benchmark
1067,Robust3DOD,Benchmark and robustness evaluation toolkit for LiDAR-based 3D object detectors,"A comprehensive study and toolkit for evaluating the robustness of LiDAR-based 3D object detectors against adversarial attacks, providing benchmark datasets and attack implementations.",AI4;AI4-05,robustness_evaluation;adversarial_attack;benchmarking,library,Python,https://github.com/Eaphan/Robust3DOD,,Apache-2.0,3d-object-detection;adversarial-robustness;lidar;autonomous-driving
1068,FAIR Metrics,"Reference implementation of FAIR (Findable, Accessible, Interoperable, Reusable) metrics","A Ruby-based library implementing the metrics defined by the FAIR Metrics Group to evaluate the FAIRness of digital resources, supporting scientific data management and quality control.",AI4;AI4-05,data_quality_control;fair_evaluation,library,Ruby,https://github.com/FAIRMetrics/Metrics,http://fairmetrics.org,MIT,fair-principles;data-quality;metrics
1069,LRV-Instruction,Dataset and instruction tuning method for mitigating hallucination in LMMs,A framework and dataset designed to evaluate and mitigate hallucinations in Large Multi-Modal Models (LMMs) via robust instruction tuning.,AI4;AI4-05,hallucination_mitigation;instruction_tuning;dataset_generation,dataset,Python,https://github.com/FuxiaoLiu/LRV-Instruction,,BSD-3-Clause,hallucination;multimodal-models;instruction-tuning
1070,Giskard Client,Python client for the Giskard AI testing platform,"The official API client for interacting with the Giskard platform, enabling programmatic testing, monitoring, and evaluation of AI models.",AI4;AI4-05,model_testing;api_client,library,Python,https://github.com/Giskard-AI/giskard-client,https://docs.giskard.ai,Apache-2.0,ai-testing;mlops;client
1071,Giskard Hub SDK,SDK for enterprise LLM agent testing and red teaming,"A software development kit for the Giskard Hub, facilitating collaborative testing, continuous red teaming, and evaluation of LLM agents in enterprise environments.",AI4;AI4-05,red_teaming;agent_testing,library,Jupyter Notebook,https://github.com/Giskard-AI/giskard-hub,https://docs.giskard.ai,Apache-2.0,llm-testing;red-teaming;sdk
1072,Giskard Vision,Evaluation and testing library for Computer Vision models,A specialized extension of the Giskard framework dedicated to testing and evaluating computer vision AI systems for robustness and correctness.,AI4;AI4-05,vision_evaluation;robustness_testing,library,Python,https://github.com/Giskard-AI/giskard-vision,https://docs.giskard.ai,Apache-2.0,computer-vision;testing;evaluation
1073,Phare,Benchmark for evaluating LLM security and safety dimensions,"A benchmark suite designed to evaluate Large Language Models across key security and safety dimensions, providing standardized metrics for model comparison.",AI4;AI4-05,safety_benchmarking;security_evaluation,dataset,Python,https://github.com/Giskard-AI/phare,,None,llm-benchmark;safety;security
1074,Giskard Prompt Injections,Dataset of prompt injections for LLM security scanning,A curated collection of prompt injection attacks used by the Giskard Scan tool to test the vulnerability of Large Language Models to adversarial inputs.,AI4;AI4-05,adversarial_attack;security_scanning,dataset,Python,https://github.com/Giskard-AI/prompt-injections,,MIT,prompt-injection;llm-security;dataset
1075,ChatGPT Evaluation,Multitask benchmark for evaluating ChatGPT on reasoning and hallucination,"A repository containing test samples and scripts for a multitask, multilingual, and multimodal evaluation of ChatGPT, focusing on reasoning capabilities, hallucination, and interactivity.",AI4;AI4-05,model_benchmarking;hallucination_evaluation,dataset,Python,https://github.com/HLTCHKUST/chatgpt-evaluation,,None,chatgpt;evaluation;hallucination;multilingual
1076,Smoothing Adversarial,Reference implementation for Randomized Smoothing robustness certification,"The official implementation of 'Provably Robust Deep Learning via Adversarially Trained Smoothed Classifiers', serving as a standard solver for certifying the robustness of deep learning models.",AI4;AI4-05,robustness_certification;adversarial_training,solver,Python,https://github.com/Hadisalman/smoothing-adversarial,,MIT,randomized-smoothing;certified-robustness;adversarial-defense
1077,TrustLLM,Comprehensive benchmark for trustworthiness in Large Language Models,"A benchmark toolkit evaluating LLM trustworthiness across multiple dimensions including truthfulness, safety, fairness, robustness, privacy, and machine ethics.",AI4;AI4-05,trustworthiness_evaluation;safety_benchmarking,library,Python,https://github.com/HowieHwong/TrustLLM,https://trustllmbenchmark.github.io/TrustLLM-Website/,MIT,llm;trustworthiness;benchmark;safety
1078,UHGEval-dataset,Pipeline for creating hallucination evaluation datasets,"The data generation pipeline used to create the UHGEval hallucination dataset, supporting the creation of custom evaluation benchmarks for LLMs.",AI4;AI4-05,dataset_generation;hallucination_research,workflow,Python,https://github.com/IAAR-Shanghai/UHGEval-dataset,,None,dataset-creation;hallucination;llm
1079,HEART,Hardened Extension of the Adversarial Robustness Toolbox,A library extending the Adversarial Robustness Toolbox (ART) to support the assessment of adversarial AI vulnerabilities specifically in Test & Evaluation workflows.,AI4;AI4-05,vulnerability_assessment;adversarial_robustness,library,Jupyter Notebook,https://github.com/IBM/heart-library,,MIT,adversarial-robustness;security;testing
1080,Infosys Responsible AI Toolkit,"Toolkit for AI safety, security, fairness, and explainability","A comprehensive toolkit incorporating features for safety, security, explainability, fairness, bias detection, and hallucination detection to ensure trustworthy AI solutions.",AI4;AI4-05,responsible_ai;bias_detection;hallucination_detection,workflow,Python,https://github.com/Infosys/Infosys-Responsible-AI-Toolkit,,MIT,responsible-ai;fairness;explainability;security
1081,LAION-SAFETY,Toolbox for NSFW and toxicity detection in datasets,"An open toolbox providing models and scripts for detecting NSFW content and toxicity, primarily used for filtering and cleaning large-scale datasets like LAION.",AI4;AI4-05,data_filtering;toxicity_detection;nsfw_detection,library,Jupyter Notebook,https://github.com/LAION-AI/LAION-SAFETY,,None,safety;content-moderation;dataset-cleaning
1082,LLMs-Finetuning-Safety,Resources for demonstrating safety guardrail breaches via fine-tuning,A research toolkit and dataset demonstrating how fine-tuning on a small number of adversarial examples can compromise the safety guardrails of LLMs like GPT-3.5 Turbo.,AI4;AI4-05,safety_evaluation;adversarial_attack;jailbreaking,dataset,Python,https://github.com/LLM-Tuning-Safety/LLMs-Finetuning-Safety,,MIT,jailbreak;fine-tuning;llm-safety
1083,fair-test,Library to build and deploy FAIR metrics tests APIs,"A Python library facilitating the creation and deployment of FAIR (Findable, Accessible, Interoperable, Reusable) metrics tests, compatible with FAIR evaluation services.",AI4;AI4-05,fair_evaluation;data_management,library,Python,https://github.com/MaastrichtU-IDS/fair-test,,MIT,fair-principles;metrics;api
1084,CIFAR10 Challenge,Benchmark challenge for adversarial robustness on CIFAR10,A benchmark and challenge framework designed to evaluate and compare the adversarial robustness of neural networks on the CIFAR10 dataset.,AI4;AI4-05,robustness_benchmarking;adversarial_defense,dataset,Python,https://github.com/MadryLab/cifar10_challenge,,MIT,cifar10;adversarial-robustness;benchmark
1085,MNIST Challenge,Benchmark challenge for adversarial robustness on MNIST,A benchmark and challenge framework designed to evaluate and compare the adversarial robustness of neural networks on the MNIST dataset.,AI4;AI4-05,robustness_benchmarking;adversarial_defense,dataset,Python,https://github.com/MadryLab/mnist_challenge,,MIT,mnist;adversarial-robustness;benchmark
1086,Robustness,Library for training and evaluating robust neural networks,"A comprehensive library for experimenting with, training, and evaluating neural networks with a specific focus on adversarial robustness, providing standard implementations of adversarial training.",AI4;AI4-05,adversarial_training;robustness_evaluation,library,Jupyter Notebook,https://github.com/MadryLab/robustness,https://robustness.readthedocs.io/,MIT,adversarial-training;deep-learning;robustness
1087,HaluMem,Hallucination evaluation benchmark for agent memory systems,HaluMem is an operation-level hallucination evaluation benchmark specifically designed for agent memory systems. It provides a framework to assess and detect hallucinations in the memory retrieval and utilization processes of AI agents.,AI4;AI4-05,hallucination_detection;benchmark,dataset,Python,https://github.com/MemTensor/HaluMem,,None,hallucination;agent-memory;benchmark
1088,Dingo,"Comprehensive AI quality evaluation tool for data, models, and applications","Dingo is a comprehensive evaluation tool designed to assess the quality of AI data, models, and applications. It supports various evaluation metrics and scenarios to ensure the reliability and performance of AI systems.",AI4;AI4-05,quality_evaluation;model_assessment,solver,JavaScript,https://github.com/MigoXLab/dingo,,Apache-2.0,evaluation;quality-assurance;ai-testing
1089,Gandalf LLM Pentester,Automated red-teaming toolkit for stress-testing LLM defenses,Gandalf LLM Pentester is an automated toolkit designed for red-teaming Large Language Models. It focuses on stress-testing LLM defenses through vector attacks and provides insights into potential vulnerabilities and alignment failures.,AI4;AI4-05,red_teaming;adversarial_attack,solver,Jupyter Notebook,https://github.com/MrMoshkovitz/gandalf-llm-pentester,,None,red-teaming;llm-security;penetration-testing
1090,Summarization Eval,Reference-free automatic summarization evaluation with hallucination detection,"This tool provides a reference-free method for evaluating automatic summarization. It includes features for detecting potential hallucinations in generated summaries, aiming to improve the reliability of summarization metrics without requiring ground truth references.",AI4;AI4-05,hallucination_detection;summarization_evaluation,library,Python,https://github.com/Muhtasham/summarization-eval,,None,summarization;evaluation;hallucination
1091,NRP,Self-supervised approach for adversarial robustness in vision models,NRP (Neural Representation Purification) is an implementation of a self-supervised approach for enhancing adversarial robustness. It focuses on purifying adversarial perturbations from input images to protect vision models against attacks.,AI4;AI4-05,adversarial_defense;robustness,solver,Python,https://github.com/Muzammal-Naseer/NRP,,MIT,adversarial-robustness;computer-vision;self-supervised-learning
1092,Hallu-PI,Benchmark for evaluating hallucination in Multi-modal LLMs with perturbed inputs,Hallu-PI is a benchmark and dataset designed to evaluate hallucinations in Multi-modal Large Language Models (MLLMs). It specifically focuses on scenarios involving perturbed inputs to assess the robustness and faithfulness of model generations.,AI4;AI4-05,hallucination_evaluation;multimodal_benchmark,dataset,,https://github.com/NJUNLP/Hallu-PI,,MIT,hallucination;mllm;benchmark
1093,NeMo Guardrails,Toolkit for adding programmable guardrails to LLM-based systems,NeMo Guardrails is an open-source toolkit that allows developers to add programmable guardrails to Large Language Model (LLM) based conversational systems. It helps ensure that the models behave within defined safety and topical boundaries.,AI4;AI4-05,safety_guardrails;alignment,library,Python,https://github.com/NVIDIA-NeMo/Guardrails,https://github.com/NVIDIA-NeMo/Guardrails,None,guardrails;llm-safety;dialogue-systems
1094,garak,Vulnerability scanner for Large Language Models,"garak is a vulnerability scanner specifically designed for Large Language Models (LLMs). It probes LLMs for a wide range of weaknesses, including hallucination, data leakage, prompt injection, and toxicity, acting as an automated red-teaming tool.",AI4;AI4-05,vulnerability_scanning;red_teaming,solver,Python,https://github.com/NVIDIA/garak,https://garak.readthedocs.io/,Apache-2.0,llm-security;vulnerability-scanner;red-teaming
1095,ToXCL,Unified framework for toxic speech detection and explanation,"ToXCL is a framework designed for the detection and explanation of toxic speech. It integrates methods to identify toxic content and provide explanations for the detection, facilitating research into interpretable safety measures for NLP models.",AI4;AI4-05,toxicity_detection;explainable_ai,library,Python,https://github.com/NhatHoang2002/ToXCL,,None,toxicity-detection;nlp;explainability
1096,HalluQA,Benchmark for evaluating hallucinations in Chinese Large Language Models,HalluQA is a dataset and evaluation framework specifically tailored for assessing hallucinations in Chinese Large Language Models. It provides a set of questions and evaluation scripts to measure the factual correctness and hallucination rates of models.,AI4;AI4-05,hallucination_evaluation;benchmark,dataset,Python,https://github.com/OpenMOSS/HalluQA,,Apache-2.0,hallucination;chinese-llm;benchmark
1097,SafeVLA,Safety alignment framework for Vision-Language-Action models,SafeVLA is a framework for the safety alignment of Vision-Language-Action (VLA) models via constrained learning. It addresses safety concerns in embodied AI agents by enforcing constraints during the learning process.,AI4;AI4-05,safety_alignment;embodied_ai,library,Python,https://github.com/PKU-Alignment/SafeVLA,,None,safety-alignment;vla;constrained-learning
1098,BeaverTails,Dataset collection for safety alignment in Large Language Models,BeaverTails is a comprehensive collection of datasets designed to support research on safety alignment in Large Language Models (LLMs). It includes data for training and evaluating models on helpfulness and harmlessness.,AI4;AI4-05,safety_alignment;dataset,dataset,Makefile,https://github.com/PKU-Alignment/beavertails,,Apache-2.0,safety-alignment;rlhf;dataset
1099,Safe-RLHF,Library for constrained value alignment via Safe Reinforcement Learning,"Safe-RLHF is a library that implements Safe Reinforcement Learning from Human Feedback. It enables the alignment of LLMs with human values while enforcing safety constraints, decoupling helpfulness and harmlessness objectives.",AI4;AI4-05,safety_alignment;rlhf,library,Python,https://github.com/PKU-Alignment/safe-rlhf,,Apache-2.0,rlhf;safety;alignment
1100,SafeSora,Human preference dataset for safety alignment in text-to-video generation,SafeSora is a dataset of human preferences designed for safety alignment research in text-to-video generation models. It aims to enhance the helpfulness and harmlessness of Large Vision Models (LVMs) by providing safety-oriented preference data.,AI4;AI4-05,safety_alignment;text-to-video,dataset,Python,https://github.com/PKU-Alignment/safe-sora,,None,text-to-video;safety;preference-dataset
1101,FCMI,Deep Fair Clustering via Maximizing and Minimizing Mutual Information,FCMI is a PyTorch implementation of a Deep Fair Clustering algorithm. It utilizes mutual information maximization and minimization to achieve clustering results that are fair with respect to sensitive attributes.,AI4;AI4-05,fairness;clustering,solver,Python,https://github.com/PengxinZeng/2023-CVPR-FCMI,,Apache-2.0,fair-clustering;mutual-information;fairness
1102,SafeWorld,Geo-Diverse Safety Alignment framework,"SafeWorld is a framework and dataset for Geo-Diverse Safety Alignment. It addresses the cultural and geographical variations in safety standards for LLMs, enabling the evaluation and improvement of model alignment across different global contexts.",AI4;AI4-05,safety_alignment;cultural_bias,dataset,,https://github.com/PlusLabNLP/SafeWorld,,None,safety;geo-diversity;alignment
1103,ASTRA,Adversarial attack framework for AI safety competitions,ASTRA is an adversarial attack framework developed for AI safety competitions. It includes methods for generating effective adversarial prompts and attacks to evaluate the robustness of AI models against red-teaming efforts.,AI4;AI4-05,adversarial_attack;red_teaming,solver,Python,https://github.com/PurCL/ASTRA,,MIT,adversarial-attack;ai-safety;red-teaming
1104,Decepticon,Autonomous Multi-Agent Based Red Team Testing Service,Decepticon is an autonomous red-teaming service that utilizes a multi-agent system to test AI models. It simulates various attack vectors and interactions to uncover vulnerabilities in LLM deployments.,AI4;AI4-05,red_teaming;multi_agent_simulation,platform,Python,https://github.com/PurpleAILAB/Decepticon,,Apache-2.0,red-teaming;multi-agent;security-testing
1105,Reevaluating NLP Adversarial Examples,Code for reevaluating adversarial examples in Natural Language Processing,This repository contains the code and resources for reevaluating the effectiveness and validity of adversarial examples in NLP. It provides tools to analyze and benchmark different adversarial attack methods.,AI4;AI4-05,adversarial_evaluation;nlp_robustness,library,Jupyter Notebook,https://github.com/QData/Reevaluating-NLP-Adversarial-Examples,,None,adversarial-examples;nlp;evaluation
1106,TextAttack,"Framework for adversarial attacks, data augmentation, and model training in NLP","TextAttack is a Python framework for adversarial attacks, data augmentation, and adversarial training in NLP. It allows researchers to easily construct attacks, benchmark model robustness, and improve model performance through augmentation.",AI4;AI4-05,adversarial_attack;data_augmentation,library,Python,https://github.com/QData/TextAttack,https://textattack.readthedocs.io/en/master/,MIT,adversarial-attacks;nlp;robustness
1107,TextAttack-A2T,Tools for improving adversarial training of NLP models,TextAttack-A2T provides implementations for improving adversarial training in NLP models. It builds upon TextAttack to offer specific methods for enhancing model robustness against textual adversarial examples.,AI4;AI4-05,adversarial_training;robustness,library,Python,https://github.com/QData/TextAttack-A2T,,MIT,adversarial-training;nlp;robustness
1108,TextAttack Search Benchmark,Benchmark for search algorithms in generating NLP adversarial examples,This repository benchmarks various search algorithms used for generating adversarial examples in NLP. It provides a comparative analysis of search strategies within the context of adversarial attacks.,AI4;AI4-05,adversarial_benchmark;search_algorithms,dataset,Jupyter Notebook,https://github.com/QData/TextAttack-Search-Benchmark,,None,benchmark;adversarial-search;nlp
1109,Qwen3Guard,Multilingual guardrail model series for AI safety,"Qwen3Guard is a series of multilingual guardrail models designed to ensure the safety of AI interactions. It detects and mitigates unsafe content across multiple languages, serving as a safety layer for LLM deployments.",AI4;AI4-05,safety_guardrails;content_moderation,solver,Python,https://github.com/QwenLM/Qwen3Guard,,None,guardrails;multilingual;safety-model
1110,HaluEval,Large-scale hallucination evaluation benchmark for LLMs,HaluEval is a large-scale benchmark designed to evaluate hallucinations in Large Language Models. It includes a diverse set of generated and human-annotated samples to assess the factual consistency and hallucination tendencies of LLMs.,AI4;AI4-05,hallucination_evaluation;benchmark,dataset,Python,https://github.com/RUCAIBox/HaluEval,,MIT,hallucination;benchmark;llm-evaluation
1111,RedTeamingforLLMs,Framework for executing positive red-teaming experiments on LLMs,This framework is designed for conducting positive red-teaming experiments on Large Language Models. It provides a structure for testing model behaviors and identifying failure modes in a controlled environment.,AI4;AI4-05,red_teaming;safety_testing,library,Python,https://github.com/RedTeamingforLLMs/RedTeamingforLLMs,,MIT,red-teaming;llm;experiment-framework
1112,HalluDetect,Token probability approach for detecting hallucinations in LLM generation,HalluDetect implements a method for detecting hallucinations in LLM generations using token probability features. It uses logistic regression and MLP classifiers trained on features extracted from the text to identify hallucinated content.,AI4;AI4-05,hallucination_detection;uncertainty_estimation,solver,Jupyter Notebook,https://github.com/Rivas-AI/HalluDetect,,MIT,hallucination-detection;token-probability;llm
1113,CipherChat,Framework to evaluate generalization capability of safety alignment for LLMs,"CipherChat is a framework designed to evaluate the generalization of safety alignment in LLMs, particularly in the context of cipher-based or encrypted conversations. It tests whether safety mechanisms hold up under non-standard input formats.",AI4;AI4-05,safety_evaluation;generalization_testing,library,Python,https://github.com/RobustNLP/CipherChat,,MIT,safety-alignment;llm;evaluation
1114,LangBiTe,Bias Tester framework for Large Language Models,LangBiTe is a framework for testing bias in Large Language Models. It provides a structured approach to generate test cases and evaluate models for various types of social biases.,AI4;AI4-05,bias_detection;fairness_evaluation,library,Python,https://github.com/SOM-Research/LangBiTe,,MIT,bias-testing;llm;fairness
1115,PRISM,Robust VLM Alignment with Principled Reasoning,PRISM is a framework for the robust alignment of Vision-Language Models (VLMs). It incorporates principled reasoning to improve the safety and reliability of multimodal systems.,AI4;AI4-05,safety_alignment;vlm_robustness,library,Python,https://github.com/SaFo-Lab/PRISM,,MIT,vlm;alignment;robustness
1116,LLM Testlab,Comprehensive testing tool for Large Language Models,"LLM Testlab is a tool designed for the comprehensive testing of Large Language Models. It likely includes features for evaluating performance, safety, and other quality metrics of LLMs.",AI4;AI4-05,model_testing;quality_assurance,solver,Python,https://github.com/Saivineeth147/llm-testlab,,MIT,llm-testing;evaluation;qa
1117,RedesignAutonomy,AI safety evaluation framework for LLM-assisted software engineering,"RedesignAutonomy is a safety evaluation framework specifically for LLM-assisted software engineering. It assesses risks such as security flaws, overtrust, and misinterpretation in code generated by AI.",AI4;AI4-05,safety_evaluation;code_generation_security,library,Python,https://github.com/Satyamkumarnavneet/RedesignAutonomy,,MIT,ai-safety;software-engineering;risk-assessment
1118,giskardpy,Core library for constraint- and optimization-based robot motion control,"giskardpy is the core Python library of the Giskard framework, designed for robot motion control using constraint-based and optimization-based methods. It allows for the specification and execution of complex robot behaviors.",AI4;Robotics,motion_control;trajectory_optimization,library,Python,https://github.com/SemRoCo/giskardpy,,LGPL-3.0,robotics;motion-control;optimization
1119,Face-Robustness-Benchmark,Adversarial robustness evaluation library for face recognition,A comprehensive library and benchmark designed to evaluate the adversarial robustness of face recognition models against various attack methods.,AI4;AI4-05,robustness_evaluation;adversarial_attack,library,Python,https://github.com/ShawnXYang/Face-Robustness-Benchmark,,Apache-2.0,face-recognition;adversarial-robustness;benchmark
1120,Graph Robustness Benchmark (GRB),Scalable benchmark for evaluating graph machine learning robustness,"A unified, modular, and reproducible benchmark framework for evaluating the adversarial robustness of Graph Machine Learning (GML) models against various attacks.",AI4;AI4-05,robustness_evaluation;graph_learning,library,Python,https://github.com/THUDM/grb,https://grb.readthedocs.io,MIT,graph-neural-networks;adversarial-robustness;benchmark
1121,Trust & Safety Evals,Reference stack for AI model trust and safety evaluation,"A project by The AI Alliance defining a reference stack for AI model and system evaluation, providing benchmarks and tools for assessing trust and safety.",AI4;AI4-05,safety_evaluation;benchmarking,workflow,Makefile,https://github.com/The-AI-Alliance/trust-safety-evals,,None,ai-safety;evaluation;benchmarks
1122,AI Fairness 360 (AIF360),Fairness metrics and bias mitigation library,"A comprehensive open-source toolkit containing metrics to check for unwanted bias in datasets and machine learning models, and algorithms to mitigate such bias.",AI4;AI4-05,bias_mitigation;fairness_evaluation,library,Python,https://github.com/Trusted-AI/AIF360,https://aif360.mybluemix.net/,Apache-2.0,fairness;bias-mitigation;machine-learning
1123,Adversarial Robustness Toolbox (ART),Python library for machine learning security and robustness,"A Python library for machine learning security, providing tools for evasion, poisoning, extraction, and inference attacks, as well as defenses and robustness certification.",AI4;AI4-05,robustness_evaluation;adversarial_defense,library,Python,https://github.com/Trusted-AI/adversarial-robustness-toolbox,https://adversarial-robustness-toolbox.readthedocs.io/,MIT,adversarial-ml;security;robustness
1124,AdvBox,Adversarial example generation and robustness benchmarking toolbox,"A toolbox to generate adversarial examples that fool neural networks across multiple frameworks (PaddlePaddle, PyTorch, etc.) and benchmark the robustness of machine learning models.",AI4;AI4-05,adversarial_attack;robustness_benchmarking,library,Jupyter Notebook,https://github.com/advboxes/AdvBox,,Apache-2.0,adversarial-examples;robustness;paddlepaddle
1125,PromptInject,Framework for evaluating LLM robustness to adversarial prompt attacks,A modular framework that assembles prompts to provide a quantitative analysis of the robustness of Large Language Models (LLMs) to adversarial prompt injection attacks.,AI4;AI4-05,prompt_injection;robustness_evaluation,library,Python,https://github.com/agencyenterprise/PromptInject,,MIT,llm;prompt-injection;security
1126,Moonshot,Modular framework for evaluating and red-teaming LLM applications,"Moonshot is a tool designed to evaluate and red-team Large Language Model (LLM) applications. It provides a modular architecture to test for safety, security, and performance issues.",AI4;AI4-05,red_teaming;safety_evaluation;llm_testing,library,Python,https://github.com/aiverify-foundation/moonshot,,Apache-2.0,red-teaming;llm-evaluation;ai-safety
1127,TurboFuzzLLM,Mutation-based fuzzing tool for jailbreaking Large Language Models,"TurboFuzzLLM is a tool that enhances mutation-based fuzzing techniques to effectively jailbreak Large Language Models (LLMs), aiding in safety testing and red teaming.",AI4;AI4-05,fuzzing;jailbreaking;red_teaming,solver,Python,https://github.com/amazon-science/TurboFuzzLLM,,Apache-2.0,fuzzing;llm-safety;jailbreak
1128,last_layer,High-performance library for LLM prompt injection and jailbreak detection,"last_layer is an ultra-fast, low-latency Python library designed to detect prompt injections and jailbreak attempts in Large Language Models.",AI4;AI4-05,prompt_injection_detection;safety_monitoring,library,Python,https://github.com/arekusandr/last_layer,,MIT,prompt-injection;jailbreak-detection;security
1129,Arthur Bench,Evaluation tool for comparing and testing LLMs,"Arthur Bench is an open-source tool for evaluating Large Language Models (LLMs) to compare performance across different models, prompts, and hyperparameters.",AI4;AI4-05,llm_evaluation;model_comparison,library,TypeScript,https://github.com/arthur-ai/bench,https://docs.arthur.ai/bench,MIT,llm-eval;benchmarking;observability
1130,Fairness.jl,Julia toolkit for fairness metrics and bias mitigation,Fairness.jl is a Julia library providing a collection of fairness metrics and bias mitigation algorithms for machine learning models.,AI4;AI4-05,bias_mitigation;fairness_evaluation,library,Julia,https://github.com/ashryaagr/Fairness.jl,,MIT,fairness;bias;julia
1131,MLIP Arena,Benchmark platform for machine learning interatomic potentials,"MLIP Arena is a fair and transparent benchmark framework for evaluating machine learning interatomic potentials (MLIPs), going beyond basic error metrics to assess physical properties.",AI4;AI4-04,model_benchmarking;interatomic_potentials,workflow,Jupyter Notebook,https://github.com/atomind-ai/mlip-arena,,Apache-2.0,mlip;materials-science;benchmarking
1132,FaithScore,Evaluation metric for hallucinations in Large Vision-Language Models,"FaithScore is a tool for fine-grained evaluation of hallucinations in Large Vision-Language Models (LVLMs), assessing the faithfulness of generated text to the visual input.",AI4;AI4-05,hallucination_detection;vlm_evaluation,library,Python,https://github.com/bcdnlp/FAITHSCORE,,MIT,hallucination;vlm;evaluation-metric
1133,nn_robust_attacks,Library of robust evasion attacks against neural networks,A foundational library implementing robust evasion attacks (including the Carlini & Wagner attack) to evaluate the adversarial robustness of neural networks.,AI4;AI4-05,adversarial_attack;robustness_evaluation,library,Python,https://github.com/carlini/nn_robust_attacks,,BSD-2-Clause,adversarial-attacks;robustness;neural-networks
1134,RedEval,Library for red-teaming LLM applications,RedEval is a Python library designed for red-teaming LLM applications using other LLMs to generate adversarial inputs and evaluate safety.,AI4;AI4-05,red_teaming;safety_evaluation,library,Python,https://github.com/chziakas/redeval,,Apache-2.0,red-teaming;llm;security
1135,DeepTeam,Framework for red teaming LLM systems,"DeepTeam is a framework designed to red team LLMs and LLM systems, automating the process of finding vulnerabilities and safety issues.",AI4;AI4-05,red_teaming;vulnerability_scanning,library,Python,https://github.com/confident-ai/deepteam,,Apache-2.0,red-teaming;llm-security;automation
1136,LangFair,Library for LLM bias and fairness assessment,"LangFair is a Python library for conducting use-case level assessments of bias and fairness in Large Language Models (LLMs), providing metrics and evaluation tools.",AI4;AI4-05,bias_assessment;fairness_evaluation,library,Python,https://github.com/cvs-health/langfair,,NOASSERTION,fairness;bias;llm
1137,VerifyML,Toolkit for responsible AI workflows and model verification,"VerifyML is an open-source toolkit designed to help implement responsible AI workflows, enabling model verification, documentation, and fairness checks.",AI4;AI4-05,responsible_ai;model_verification;documentation,library,Python,https://github.com/cylynx/verifyml,https://verifyml.com,Apache-2.0,responsible-ai;governance;fairness
1138,PHUDGE,Phi-3 based scalable judge for LLM evaluation and hallucination detection,"A framework using Phi-3 as a scalable judge to evaluate Large Language Models (LLMs). It includes tools and methods for detecting hallucinations, grading responses, and performing evaluations with or without custom rubrics and reference answers.",AI4;AI4-05,hallucination_detection;llm_evaluation,solver,Jupyter Notebook,https://github.com/deshwalmahesh/PHUDGE,,None,llm-judge;hallucination;evaluation
1139,AIRTBench,Benchmark for measuring autonomous AI red teaming capabilities,"A code repository for AIRTBench, designed to evaluate and measure the capabilities of autonomous AI red teaming agents in language models, focusing on safety and robustness testing.",AI4;AI4-05,red_teaming;safety_benchmark,dataset,Jupyter Notebook,https://github.com/dreadnode/AIRTBench-Code,,Apache-2.0,red-teaming;benchmark;llm-safety
1140,Aequitas,Open-source bias auditing and fair machine learning toolkit,A toolkit for auditing bias and fairness in machine learning models. It enables developers and researchers to evaluate models for various bias metrics and visualize the results to ensure equitable outcomes.,AI4;AI4-05,bias_auditing;fairness_evaluation,library,Python,https://github.com/dssg/aequitas,http://aequitas.dssg.io,MIT,fairness;bias-audit;machine-learning
1141,Robust-Semantic-Segmentation,Dynamic divide-and-conquer adversarial training for robust segmentation,Implementation of the Dynamic Divide-and-Conquer Adversarial Training (DDCAT) method to improve the robustness of semantic segmentation models against adversarial attacks.,AI4;AI4-05,adversarial_training;robust_segmentation,solver,Python,https://github.com/dvlab-research/Robust-Semantic-Segmentation,,None,adversarial-defense;semantic-segmentation;robustness
1142,ChatProtect,"Evaluation, detection, and mitigation of self-contradictory hallucinations in LLMs",Code implementation for detecting and mitigating self-contradictory hallucinations in Large Language Models. It provides a framework for evaluating consistency and improving model reliability.,AI4;AI4-05,hallucination_detection;consistency_evaluation,solver,Python,https://github.com/eth-sri/ChatProtect,,Apache-2.0,hallucination;llm;reliability
1143,DiffAI,Certifiable defense against adversarial examples via provable robustness training,A system for training neural networks to be provably robust against adversarial examples. It implements differentiable abstract interpretation to certify the robustness of deep learning models.,AI4;AI4-05,robustness_verification;adversarial_defense,library,Python,https://github.com/eth-sri/diffai,,MIT,certified-robustness;adversarial-defense;abstract-interpretation
1144,RigorLLM,Resilient guardrails for LLMs against undesired content,"Implementation of RigorLLM, a framework for creating resilient guardrails to prevent Large Language Models from generating undesired or harmful content, enhancing safety in deployment.",AI4;AI4-05,safety_guardrail;content_moderation,solver,Python,https://github.com/eurekayuan/RigorLLM,,None,guardrails;llm-safety;moderation
1145,ImageNet-Adversarial-Training,State-of-the-art adversarial training for ImageNet classifiers,A repository providing code and pre-trained models for adversarially robust ImageNet classifiers. It serves as a benchmark and toolkit for researching adversarial robustness in large-scale computer vision.,AI4;AI4-05,adversarial_training;robust_model,solver,Python,https://github.com/facebookresearch/ImageNet-Adversarial-Training,,NOASSERTION,adversarial-robustness;imagenet;computer-vision
1146,Fairlearn,Toolkit to assess and improve fairness of machine learning models,A Python package that empowers developers of artificial intelligence systems to assess their systems' fairness and mitigate any observed unfairness issues. It offers metrics for evaluation and algorithms for bias mitigation.,AI4;AI4-05,fairness_assessment;bias_mitigation,library,Python,https://github.com/fairlearn/fairlearn,https://fairlearn.org,MIT,fairness;machine-learning;bias-mitigation
1147,Jurity,Fairness and evaluation library for AI systems,A Python library for evaluating the fairness of AI models. It provides a set of metrics and tools to detect bias and ensure compliance with fairness standards in machine learning applications.,AI4;AI4-05,fairness_evaluation;bias_detection,library,Python,https://github.com/fidelity/jurity,,Apache-2.0,fairness;evaluation;metrics
1148,AutoAttack,Ensemble of diverse parameter-free attacks for robust evaluation,A standard benchmark tool for reliably evaluating the adversarial robustness of machine learning models. It utilizes an ensemble of parameter-free attacks to provide a rigorous assessment of model security.,AI4;AI4-05,adversarial_attack;robustness_evaluation,solver,Python,https://github.com/fra31/auto-attack,,MIT,adversarial-robustness;benchmark;attack-ensemble
1149,Booster,Defense against harmful fine-tuning attacks on LLMs,"Implementation of the Booster method, designed to tackle harmful fine-tuning attacks on Large Language Models by attenuating harmful perturbations, thereby enhancing model safety.",AI4;AI4-05,alignment_defense;fine_tuning_safety,solver,Shell,https://github.com/git-disl/Booster,,Apache-2.0,llm-safety;fine-tuning;defense
1150,Lisa,Lazy safety alignment for LLMs against harmful fine-tuning,"Code for the 'Lazy Safety Alignment' (Lisa) method, which protects Large Language Models against harmful fine-tuning attacks by employing strategic alignment techniques.",AI4;AI4-05,alignment_defense;fine_tuning_safety,solver,Python,https://github.com/git-disl/Lisa,,Apache-2.0,safety-alignment;llm;defense
1151,Safety-Tax,Evaluation of trade-offs between safety alignment and reasoning in LLMs,"A tool/codebase for analyzing the 'Safety Tax', quantifying how safety alignment processes may impact the reasoning capabilities of Large Language Models.",AI4;AI4-05,safety_alignment_evaluation;model_performance_analysis,solver,Python,https://github.com/git-disl/Safety-Tax,,Apache-2.0,safety-alignment;reasoning;evaluation
1152,Virus,Harmful fine-tuning attack framework for bypassing LLM guardrails,"Implementation of the 'Virus' attack method, used for red teaming and evaluating the vulnerability of Large Language Models to harmful fine-tuning that bypasses guardrail moderation.",AI4;AI4-05,adversarial_attack;red_teaming,solver,Python,https://github.com/git-disl/Virus,,Apache-2.0,red-teaming;jailbreak;fine-tuning
1153,RobNets,Neural Architecture Search for robust architectures against adversarial attacks,A framework combining Neural Architecture Search (NAS) with adversarial robustness to discover network architectures that are inherently more resistant to adversarial attacks.,AI4;AI4-05,robust_architecture_search;adversarial_defense,solver,Python,https://github.com/gmh14/RobNets,,MIT,nas;robustness;architecture-search
1154,RETVec,"Efficient, multilingual, and adversarially-robust text vectorizer",RETVec (Resilient and Efficient Text Vectorizer) is a text processing layer designed to be robust against adversarial attacks (like typos and character perturbations) while remaining efficient and multilingual.,AI4;AI4-05,robust_text_vectorization;adversarial_defense,library,Jupyter Notebook,https://github.com/google-research/retvec,,Apache-2.0,text-vectorization;robustness;nlp
1155,Guardrails,Framework for adding data validation and safety guardrails to LLMs,"A Python package that lets users add structure, type checking, and quality assurance to the outputs of large language models. It enforces safety policies and validates LLM responses against defined specifications.",AI4;AI4-05,safety_guardrail;output_validation,library,Python,https://github.com/guardrails-ai/guardrails,https://www.guardrailsai.com/docs/,Apache-2.0,guardrails;validation;llm-safety
1156,Gyroscopic Diagnostics,AI safety diagnostics and alignment evaluation lab,A toolkit for diagnosing AI safety issues and evaluating alignment. It provides methods to assess how well AI models adhere to intended safety guidelines and alignment principles.,AI4;AI4-05,alignment_evaluation;safety_diagnostics,solver,Python,https://github.com/gyrogovernance/diagnostics,,MIT,ai-safety;alignment;diagnostics
1157,HolisticAI,Open-source tool to assess and improve AI trustworthiness,"A library designed to assess the trustworthiness of AI systems across multiple dimensions including bias, efficacy, robustness, and explainability. It helps in auditing and mitigating risks in AI deployments.",AI4;AI4-05,trustworthiness_assessment;bias_mitigation,library,Jupyter Notebook,https://github.com/holistic-ai/holisticai,https://holisticai.readthedocs.io,Apache-2.0,trustworthiness;audit;risk-management
1158,SaLoRA,Safety-Alignment Preserved Low-Rank Adaptation for LLMs,"Implementation of SaLoRA, a method for fine-tuning Large Language Models using Low-Rank Adaptation (LoRA) while preserving safety alignment, preventing the degradation of safety features during adaptation.",AI4;AI4-05,safety_alignment;peft_safety,solver,Jupyter Notebook,https://github.com/homles11/SaLoRA,,None,lora;safety-alignment;fine-tuning
1159,Circular Bias Detection,Statistical framework for detecting circular reasoning bias in AI evaluation,"A comprehensive statistical framework designed to detect circular reasoning bias in the evaluation of AI algorithms, ensuring that evaluation metrics do not unfairly favor certain models due to data leakage or self-reinforcing patterns.",AI4;AI4-05,bias_detection;evaluation_framework,solver,Python,https://github.com/hongping-zh/circular-bias-detection,,MIT,bias-detection;circular-reasoning;evaluation
1160,CROWN-IBP,Certified defense against adversarial examples using CROWN and IBP,A certified defense framework for neural networks that uses CROWN (bound propagation) and IBP (Interval Bound Propagation) to verify robustness against adversarial examples and train provably robust models.,AI4;AI4-05,robustness_verification;certified_defense,library,Python,https://github.com/huanzhang12/CROWN-IBP,,BSD-2-Clause,certified-robustness;verification;neural-networks
1161,AISploit,Package for red teaming and exploiting LLM AI solutions,A Python package designed to assist red teams and penetration testers in identifying vulnerabilities in Large Language Model (LLM) solutions. It provides tools for exploiting and testing the security of AI deployments.,AI4;AI4-05,red_teaming;adversarial_exploitation,library,Python,https://github.com/hupe1980/aisploit,,MIT,red-teaming;exploit;llm-security
1162,Adversarial Robustness PyTorch,PyTorch implementation of adversarial training and robustness methods,An unofficial but widely used implementation of key DeepMind papers on adversarial training and data augmentation for improving adversarial robustness in PyTorch.,AI4;AI4-05,adversarial_training;robustness_defense,library,Python,https://github.com/imrahulr/adversarial_robustness_pytorch,,MIT,adversarial-training;pytorch;robustness
1163,PatchGuard,Provably robust defense against adversarial patches,"Code for PatchGuard, a defense mechanism that provides provable robustness against adversarial patch attacks by utilizing small receptive fields and masking techniques in vision models.",AI4;AI4-05,adversarial_defense;robust_vision,solver,Python,https://github.com/inspire-group/PatchGuard,,MIT,adversarial-patch;defense;computer-vision
1164,HYDRA,Pruning technique for creating adversarially robust neural networks,"A PyTorch implementation of HYDRA (Pruning Adversarially Robust Neural Networks), a technique to compress neural networks while maintaining their robustness against adversarial attacks. It provides code for training, pruning, and evaluating robust models.",AI4;AI4-05,adversarial_robustness;model_compression,library,Python,https://github.com/inspire-group/hydra,https://arxiv.org/abs/2002.10509,None,adversarial-robustness;pruning;neural-network-compression
1165,IMMUNE,Inference-time alignment defense against jailbreaks in Multi-modal LLMs,Official implementation of the CVPR 2025 paper 'IMMUNE'. It provides a defense mechanism to improve the safety of Multi-modal Large Language Models (MLLMs) against jailbreak attacks during inference time.,AI4;AI4-05,jailbreak_defense;safety_alignment,library,Python,https://github.com/itsvaibhav01/Immune,,None,jailbreak-defense;mllm;safety-alignment
1166,PhD (Prompted Hallucination Dataset),Large-scale visual hallucination evaluation dataset for LVLMs,"A ChatGPT-Prompted Visual hallucination Evaluation Dataset (PhD) for Large Vision-Language Models. It features over 100,000 data samples with extensive contextual descriptions and counterintuitive images to evaluate hallucination.",AI4;AI4-05,hallucination_evaluation;dataset,dataset,,https://github.com/jiazhen-code/PhD,,None,hallucination;lvlm;benchmark
1167,ToLD-Br,Toxic language detection dataset for Brazilian Portuguese,A dataset and analysis code for toxic language detection in Brazilian Portuguese social media posts. It supports multilingual analysis and benchmarking of toxicity detection models.,AI4;AI4-05,toxicity_detection;dataset,dataset,Jupyter Notebook,https://github.com/joaoaleite/ToLD-Br,,NOASSERTION,toxicity-detection;nlp;portuguese
1168,AMBER,Multi-dimensional benchmark for multi-modal hallucination evaluation,"An LLM-free, multi-dimensional benchmark designed to evaluate hallucinations in Multi-modal Large Language Models (MLLMs). It covers various types of hallucinations and provides a standardized evaluation framework.",AI4;AI4-05,hallucination_evaluation;benchmark,dataset,Python,https://github.com/junyangwang0410/AMBER,,Apache-2.0,hallucination;mllm;benchmark
1169,ToxVidLM,Dataset for toxicity detection in code-mixed Hinglish video content,"Code and datasets from the ACL 2024 paper focusing on toxicity detection in code-mixed Hinglish (Hindi-English) video content, addressing multimodal toxicity challenges.",AI4;AI4-05,toxicity_detection;dataset,dataset,Python,https://github.com/justaguyalways/ToxVidLM_ACL_2024,,MIT,toxicity;multimodal;hinglish
1170,MobileSafetyBench,Benchmark for evaluating safety of autonomous agents in mobile device control,"A benchmark for evaluating the safety of autonomous agents when controlling mobile devices, presented at AAAI 2026 AI Alignment Track. It assesses risks associated with agentic actions on mobile platforms.",AI4;AI4-05,safety_evaluation;agent_safety,dataset,Jupyter Notebook,https://github.com/jylee425/mobilesafetybench,,Apache-2.0,agent-safety;benchmark;mobile-agents
1171,AVHBench,Cross-modal hallucination evaluation for Audio-Visual LLMs,"Official repository for the ICLR 2025 paper 'AVHBench'. It is a benchmark designed to evaluate hallucinations in Audio-Visual Large Language Models, focusing on cross-modal inconsistencies.",AI4;AI4-05,hallucination_evaluation;benchmark,dataset,Python,https://github.com/kaist-ami/AVHBench,,None,audio-visual;hallucination;benchmark
1172,BEAF,Evaluation method for hallucination in Vision-Language Models,Official repository for the ECCV 2024 paper 'BEAF'. It proposes a method to evaluate hallucinations in Vision-Language Models by observing Before-After changes in visual inputs.,AI4;AI4-05,hallucination_evaluation;methodology,library,Python,https://github.com/kaist-ami/BEAF,,None,hallucination;vlm;evaluation
1173,Kereva Scanner,Code scanner for detecting issues in prompts and LLM calls,"A static analysis tool and scanner designed to check for security issues, PII leakage, and safety concerns in LLM prompts and API calls within codebases.",AI4;AI4-05,safety_scanning;prompt_security,solver,Python,https://github.com/kereva-dev/kereva-scanner,,Apache-2.0,security-scanner;llm-safety;pii-detection
1174,fairness,R package for computing and visualizing fair ML metrics,"An R package that provides functions to compute and visualize various fairness metrics for machine learning models, helping researchers evaluate algorithmic bias.",AI4;AI4-05,fairness_evaluation;metrics,library,R,https://github.com/kozodoi/fairness,,NOASSERTION,fairness;r-package;bias-evaluation
1175,convex_adversarial,Method for training provably robust neural networks,"A Python library implementing methods for training neural networks that are provably robust to adversarial attacks, using convex relaxation techniques.",AI4;AI4-05,adversarial_robustness;robust_training,library,Python,https://github.com/locuslab/convex_adversarial,,MIT,adversarial-robustness;verification;neural-networks
1176,smoothing,Randomized smoothing for provable adversarial robustness,"Code for 'Provable adversarial robustness at ImageNet scale', implementing randomized smoothing techniques to certify the robustness of deep learning models against adversarial perturbations.",AI4;AI4-05,adversarial_robustness;certified_robustness,library,Python,https://github.com/locuslab/smoothing,,None,randomized-smoothing;robustness;certification
1177,Square Attack,Query-efficient black-box adversarial attack,"Implementation of Square Attack, a score-based black-box adversarial attack that does not require gradient information, suitable for evaluating model robustness.",AI4;AI4-05,adversarial_attack;robustness_evaluation,library,Python,https://github.com/max-andr/square-attack,https://arxiv.org/abs/1912.00049,BSD-3-Clause,adversarial-attack;black-box;robustness
1178,FairBench,Comprehensive benchmark framework for exploring and evaluating AI fairness,FairBench is a comprehensive framework designed for the exploration and evaluation of fairness in AI models. It provides a suite of tools and metrics to assess bias and ensure equitable outcomes across different demographic groups in machine learning applications.,AI4;AI4-05,fairness_evaluation;bias_detection,library,Jupyter Notebook,https://github.com/mever-team/FairBench,,NOASSERTION,fairness;bias;benchmark;ai-ethics
1179,AI Agent Evals,Evaluation framework for AI agents using model-as-judge and safety metrics,"A tool designed to evaluate AI agent applications, focusing on performance, content safety, and mathematical metrics. It utilizes a 'model as the judge' approach to assess the quality and safety of agent outputs, suitable for integration into development workflows.",AI4;AI4-05,agent_evaluation;safety_metrics;model_as_judge,workflow,Python,https://github.com/microsoft/ai-agent-evals,,MIT,ai-agents;evaluation;safety;metrics
1180,R-Bench,Benchmark for evaluating relationship hallucinations in Large Vision-Language Models,R-Bench is a benchmark specifically designed to evaluate and analyze relationship hallucinations in Large Vision-Language Models (LVLMs). It provides a dataset and evaluation scripts to assess how well models perceive and describe relationships between objects in images.,AI4;AI4-05,hallucination_detection;vision_language_evaluation,dataset,Python,https://github.com/mrwu-mac/R-Bench,,Apache-2.0,hallucination;lvlm;benchmark;computer-vision
1181,Agentic Security,Vulnerability scanner and red teaming toolkit for Agentic LLMs,"Agentic Security is a comprehensive vulnerability scanner and red teaming toolkit designed for Agentic Large Language Models. It helps identify security flaws, safety issues, and potential exploits in AI agents through automated testing and evaluation.",AI4;AI4-05,red_teaming;vulnerability_scanning;safety_evaluation,solver,Python,https://github.com/msoedov/agentic_security,,Apache-2.0,red-teaming;llm-security;vulnerability-scanner;ai-safety
1182,OpenGuardrails,Open-source customizable AI guardrails for inference pipeline security,"OpenGuardrails is an open-source framework for implementing customizable guardrails in AI applications. It protects the AI inference pipeline by scanning prompts, models, agents, and outputs, allowing users to define custom scanners and security policies.",AI4;AI4-05,safety_guardrails;inference_security;input_output_scanning,library,Python,https://github.com/openguardrails/openguardrails,,NOASSERTION,guardrails;ai-security;inference-protection
1183,HAI Guardrails,TypeScript library providing safety guards for LLM applications,HAI Guardrails is a TypeScript library that provides a set of guardrails for Large Language Model (LLM) applications. It helps developers implement safety checks and content filtering mechanisms to ensure responsible AI usage.,AI4;AI4-05,safety_guardrails;content_filtering,library,TypeScript,https://github.com/presidio-oss/hai-guardrails,,NOASSERTION,guardrails;llm;typescript;safety
1184,NoMIRACL,Multilingual hallucination evaluation dataset for RAG robustness,"NoMIRACL is a dataset designed to evaluate the robustness of Large Language Models (LLMs) in Retrieval-Augmented Generation (RAG) settings. It focuses on hallucination detection across 18 languages, specifically targeting scenarios with first-stage retrieval errors.",AI4;AI4-05,hallucination_evaluation;rag_robustness;multilingual_benchmark,dataset,Python,https://github.com/project-miracl/nomiracl,,Apache-2.0,hallucination;rag;multilingual;dataset
1185,Promptfoo,"CLI tool for testing, red teaming, and evaluating LLM prompts and agents","Promptfoo is a command-line tool and library for evaluating Large Language Models (LLMs). It supports testing prompts, agents, and RAG pipelines, offering features for AI red teaming, pentesting, and vulnerability scanning with declarative configuration.",AI4;AI4-05,red_teaming;prompt_evaluation;vulnerability_scanning,solver,TypeScript,https://github.com/promptfoo/promptfoo,https://www.promptfoo.dev/,MIT,red-teaming;llm-testing;prompt-engineering;evaluation
1186,Hallucination Index,Ranking and evaluation initiative for LLM hallucination propensity,The Hallucination Index is an initiative to evaluate and rank popular Large Language Models (LLMs) based on their propensity to hallucinate across various task types. It provides benchmarks and data to help users choose reliable models.,AI4;AI4-05,hallucination_ranking;model_evaluation,dataset,Python,https://github.com/rungalileo/hallucination-index,,NOASSERTION,hallucination;leaderboard;llm-ranking
1187,OpenAgentSafety,Framework for evaluating AI agent safety in realistic environments,"OpenAgentSafety is a framework designed to evaluate the safety of AI agents within realistic environments. It provides methodologies and tools to assess how agents behave in complex scenarios, ensuring they operate safely and reliably.",AI4;AI4-05,agent_safety;environment_simulation;behavioral_evaluation,framework,Python,https://github.com/sani903/OpenAgentSafety,,MIT,ai-agents;safety;evaluation;simulation
1188,frAI,Open-source toolkit for responsible AI with scanning and reporting capabilities,"frAI is an open-source toolkit for responsible AI that includes a CLI and SDK. It allows users to scan code, collect evidence, and generate model cards, risk files, and evaluations, facilitating compliance and transparency in AI development.",AI4;AI4-05,responsible_ai;model_card_generation;risk_assessment,toolkit,JavaScript,https://github.com/sebuzdugan/frai,,MIT,responsible-ai;compliance;model-cards;risk-management
1189,STATE-ToxiCN,Benchmark for span-level target-aware toxicity extraction in Chinese hate speech,"A benchmark dataset and evaluation framework designed for span-level target-aware toxicity extraction in Chinese hate speech detection, supporting research in safety and bias evaluation.",AI4;AI4-05,toxicity_detection;benchmark_dataset,dataset,Jupyter Notebook,https://github.com/shenmeyemeifashengguo/STATE-ToxiCN,,None,toxicity-detection;chinese-nlp;benchmark
1190,LLM-Detector-Robustness,Red teaming framework for language model detectors,"Code implementation for red teaming language model detectors using other language models, focusing on evaluating the robustness of AI-generated text detectors.",AI4;AI4-05,red_teaming;robustness_evaluation,solver,Python,https://github.com/shizhouxing/LLM-Detector-Robustness,,BSD-3-Clause,red-teaming;llm-detection;robustness
1191,Red-Teaming-Language-Models,Implementation of red teaming language models with language models,"A re-implementation of the 'Red Teaming Language Models with Language Models' paper, providing tools to generate adversarial test cases for evaluating LLM safety.",AI4;AI4-05,red_teaming;safety_evaluation,solver,Python,https://github.com/shreyansh26/Red-Teaming-Language-Models-with-Language-Models,,None,red-teaming;llm;adversarial-generation
1192,SIUO,Cross-modality safety alignment framework,"A framework for cross-modality safety alignment in multimodal models, providing methods to evaluate and improve safety across different data modalities.",AI4;AI4-05,safety_alignment;multimodal_evaluation,solver,HTML,https://github.com/sinwang20/SIUO,,None,safety-alignment;multimodal;llm
1193,Kov.jl,Black-box red teaming of LLMs using MDPs,A Julia package for black-box red teaming and jailbreaking of large language models using Markov Decision Processes (MDPs) to discover failure modes.,AI4;AI4-05,red_teaming;jailbreaking,library,Julia,https://github.com/sisl/Kov.jl,,MIT,red-teaming;julia;mdp;jailbreak
1194,toxic-comments-detection-in-russian,Toxic comments detection model for Russian language,"A tool and model for detecting toxic comments specifically in the Russian language, useful for content moderation and safety evaluation.",AI4;AI4-05,toxicity_detection,solver,Python,https://github.com/sismetanin/toxic-comments-detection-in-russian,,Apache-2.0,toxicity-detection;russian-nlp;bert
1195,FairClassifier,Neural Network fairness evaluation package,An open source package that evaluates the fairness of a Neural Network using the p% rule fairness metric.,AI4;AI4-05,fairness_evaluation,library,Python,https://github.com/suraz09/FairClassifier,,GPL-3.0,fairness;neural-networks;metrics
1196,Re-Align,Alignment framework to mitigate hallucinations in VLMs,A novel alignment framework that leverages image retrieval to mitigate hallucinations in Vision Language Models (VLMs).,AI4;AI4-05,hallucination_mitigation;alignment,solver,Python,https://github.com/taco-group/Re-Align,,Apache-2.0,vlm;hallucination;alignment
1197,rGAN,Label-Noise Robust Generative Adversarial Networks,"Implementation of rGAN, a Generative Adversarial Network designed to be robust against label noise.",AI4;AI4-05,robust_modeling;generative_models,solver,Python,https://github.com/takuhirok/rGAN,,MIT,gan;robustness;label-noise
1198,ToxiBenchCN,Benchmark for multimodal toxic Chinese detection,"A benchmark and taxonomy for exploring multimodal challenges in toxic Chinese detection, providing data and evaluation standards.",AI4;AI4-05,toxicity_detection;multimodal_benchmark,dataset,Python,https://github.com/thomasyyyoung/ToxiBenchCN,,MIT,toxicity;multimodal;benchmark;chinese
1199,AISafetyLab,"Comprehensive framework for AI safety attack, defense, and evaluation","A comprehensive framework covering safety attack, defense, and evaluation for AI systems, facilitating research in AI safety.",AI4;AI4-05,safety_evaluation;adversarial_attack;defense,platform,Python,https://github.com/thu-coai/AISafetyLab,,MIT,ai-safety;evaluation;attack-defense
1200,cotk,Toolkit for fast development and fair evaluation of text generation,Conversational Toolkit (cotk) is an open-source toolkit designed for fast development and fair evaluation of text generation models.,AI4;AI4-05,text_generation_evaluation;fairness,library,Python,https://github.com/thu-coai/cotk,,Apache-2.0,text-generation;evaluation;nlp
1201,MMTrustEval,Toolbox for benchmarking trustworthiness of multimodal LLMs,"A toolbox for benchmarking the trustworthiness of multimodal large language models, covering various safety and reliability dimensions.",AI4;AI4-05,trustworthiness_evaluation;multimodal_benchmarking,library,Python,https://github.com/thu-ml/MMTrustEval,,CC-BY-SA-4.0,multimodal;trustworthiness;benchmark
1202,STAIR,Safety alignment with introspective reasoning,Codebase for improving safety alignment in language models using introspective reasoning techniques.,AI4;AI4-05,safety_alignment;reasoning,solver,Python,https://github.com/thu-ml/STAIR,,MIT,safety-alignment;llm;reasoning
1203,ares,Library for benchmarking adversarial robustness,A Python library for adversarial machine learning focusing on benchmarking adversarial robustness of models.,AI4;AI4-05,adversarial_robustness;benchmarking,library,Python,https://github.com/thu-ml/ares,,Apache-2.0,adversarial-ml;robustness;benchmark
1204,understanding-fast-adv-training,Implementation of Fast Adversarial Training,Code for understanding and improving fast adversarial training methods to enhance model robustness.,AI4;AI4-05,adversarial_training;robustness,solver,Python,https://github.com/tml-epfl/understanding-fast-adv-training,,None,adversarial-training;robustness
1205,redteam-ai-benchmark,Benchmark for evaluating uncensored LLMs for offensive security,A benchmark suite designed to evaluate uncensored Large Language Models in the context of offensive security and red teaming.,AI4;AI4-05,red_teaming;security_evaluation,dataset,Python,https://github.com/toxy4ny/redteam-ai-benchmark,,MIT,red-teaming;llm;security
1206,trulens,Evaluation and tracking for LLM experiments,"A library for evaluating and tracking Large Language Model (LLM) experiments, providing feedback functions to assess relevance, groundedness, and other metrics.",AI4;AI4-05,llm_evaluation;experiment_tracking,library,Python,https://github.com/truera/trulens,https://www.trulens.org/,MIT,llm-evaluation;observability;metrics
1207,image-crop-analysis,Analysis tools for image cropping fairness,"Code and tools for analyzing fairness metrics in image cropping algorithms, specifically addressing representation and bias.",AI4;AI4-05,fairness_analysis;bias_detection,solver,Jupyter Notebook,https://github.com/twitter-research/image-crop-analysis,,Apache-2.0,fairness;image-processing;bias
1208,armory,Adversarial Robustness Evaluation Test Bed,"A test bed for evaluating the adversarial robustness of machine learning models, providing a standardized environment for testing defenses.",AI4;AI4-05,robustness_evaluation;adversarial_defense,platform,Python,https://github.com/twosixlabs/armory,,MIT,adversarial-robustness;evaluation;testbed
1209,Metric-Fairness,Evaluation of social bias in text generation metrics,Tools and code for analyzing social bias in language model-based metrics (like BERTScore) for text generation.,AI4;AI4-05,metric_evaluation;bias_analysis,solver,Jupyter Notebook,https://github.com/txsun1997/Metric-Fairness,,MIT,fairness;metrics;bias
1210,Quantus,Explainable AI toolkit for evaluating neural network explanations,Quantus is an eXplainable AI (XAI) toolkit designed for the responsible evaluation of neural network explanations using various metrics.,AI4;AI4-05,xai_evaluation;interpretability,library,Jupyter Notebook,https://github.com/understandable-machine-intelligence-lab/Quantus,,NOASSERTION,xai;evaluation;interpretability
1211,Oversight,Modular LLM Red Teaming and Vulnerability Research Framework,"A modular framework for reverse engineering, red teaming, and vulnerability research on Large Language Models.",AI4;AI4-05,red_teaming;vulnerability_research,framework,Python,https://github.com/user1342/Oversight,,GPL-3.0,red-teaming;llm;security
1212,configurable-safety-tuning,Safety tuning of LMs with synthetic preference data,Code and data for configurable safety tuning of language models using synthetic preference data to align models with safety guidelines.,AI4;AI4-05,safety_tuning;alignment,solver,Python,https://github.com/vicgalle/configurable-safety-tuning,,MIT,safety-tuning;alignment;synthetic-data
1213,Trojan-Activation-Attack,Trojan attack on LLMs using activation steering,"Implementation of Trojan Activation Attack, a method to attack Large Language Models using activation steering to bypass safety alignment.",AI4;AI4-05,adversarial_attack;safety_evaluation,solver,Python,https://github.com/wang2226/Trojan-Activation-Attack,,None,trojan-attack;llm;activation-steering
1214,CHALE,Controlled Hallucination-Evaluation Dataset,A dataset designed for the controlled evaluation of hallucinations in Question-Answering systems.,AI4;AI4-05,hallucination_evaluation;dataset,dataset,Python,https://github.com/weijiaheng/CHALE,,NOASSERTION,hallucination;dataset;qa
1215,circle-guard-bench,Benchmark for evaluating LLM guard systems,"A benchmark for evaluating the protection capabilities of large language model (LLM) guard systems, including guardrails and safeguards.",AI4;AI4-05,safety_evaluation;guardrails_benchmarking,dataset,Python,https://github.com/whitecircle-ai/circle-guard-bench,,Apache-2.0,guardrails;benchmark;llm-security
1216,drug_det_ro,Toxic and narcotic medication detection with rotated object detector,"Source code for detecting toxic and narcotic medications using rotated object detectors, applicable in medical safety and monitoring.",AI4;AI4-05,object_detection;safety_monitoring,solver,Python,https://github.com/woodywff/drug_det_ro,,GPL-3.0,object-detection;medical-safety;rotated-bbox
1217,Tri-HE,Unified triplet-level hallucination evaluation for LVLMs,Code and data for evaluating hallucinations in Large Vision-Language Models (LVLMs) at the triplet level.,AI4;AI4-05,hallucination_evaluation;vlm,solver,Python,https://github.com/wujunjie1998/Tri-HE,,None,hallucination;vlm;evaluation
1218,Adversarial_Long-Tail,Adversarial robustness under long-tailed distribution,PyTorch implementation for improving adversarial robustness in scenarios with long-tailed data distributions.,AI4;AI4-05,adversarial_robustness;long_tail_learning,solver,Python,https://github.com/wutong16/Adversarial_Long-Tail,,None,robustness;long-tail;adversarial-training
1219,veiled-toxicity-detection,Detection of veiled toxicity in speech,Tools and methods for fortifying toxic speech detectors against veiled or subtle toxicity.,AI4;AI4-05,toxicity_detection;robustness,solver,Jupyter Notebook,https://github.com/xhan77/veiled-toxicity-detection,,None,toxicity;nlp;safety

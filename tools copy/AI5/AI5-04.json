{
  "generated_at": "2025-12-16T16:01:56.032834+08:00",
  "metadata": {
    "leaf_cluster": {
      "leaf_cluster_id": "AI5",
      "leaf_cluster_name": "科研代理-工具调用与工作流编排生态",
      "domain": "AI Toolchain",
      "typical_objects": "tools/workflows",
      "task_chain": "规划→调用→追踪→回放→评测→治理",
      "tool_form": "agent框架 + tracing + registry"
    },
    "unit": {
      "unit_id": "AI5-04",
      "unit_name": "追踪/回放/评测（tracing/eval/regression）",
      "target_scale": "200–450",
      "coverage_tools": "tracing、eval harness、tests"
    },
    "search": {
      "target_candidates": 450,
      "queries": [
        "[GH] LangSmith",
        "[GH] Weave",
        "[GH] Helicone",
        "[GH] Giskard",
        "[GH] TruLens",
        "[GH] Promptfoo",
        "[GH] DeepEval",
        "[GH] Ragas",
        "[GH] Arize Phoenix",
        "[GH] Langfuse",
        "[GH] llm observability",
        "[GH] agent tracing",
        "[GH] llm evaluation",
        "[GH] rag evaluation",
        "[GH] eval harness",
        "[GH] prompt testing",
        "[GH] agent benchmark",
        "[GH] workflow replay",
        "[GH] opentelemetry llm",
        "[GH] model regression",
        "[GH] langchain tracing",
        "[GH] llamaindex callbacks",
        "[GH] agent debugging",
        "[GH] hallucination detection",
        "[WEB] llm agent observability tools github",
        "[WEB] open source llm evaluation framework github",
        "[WEB] rag pipeline tracing and monitoring github",
        "[WEB] prompt engineering regression testing github",
        "[WEB] ai agent workflow replay github"
      ],
      "total_candidates": 1302,
      "tool_candidates": 743,
      "final_tools": 244
    }
  },
  "tools": [
    {
      "name": "GraphRAG Agent",
      "one_line_profile": "Integrated framework for Knowledge Graph construction and RAG with custom evaluation",
      "detailed_description": "A comprehensive RAG solution that integrates GraphRAG, LightRAG, and Neo4j-llm-graph-builder for knowledge graph construction and search. It includes DeepSearch integration for reasoning and a custom evaluation framework specifically designed for GraphRAG systems.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "rag_evaluation",
        "knowledge_graph_construction",
        "workflow_orchestration"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/1517005260/graph-rag-agent",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "graph-rag",
        "evaluation",
        "neo4j"
      ],
      "id": 1
    },
    {
      "name": "Tuui",
      "one_line_profile": "Desktop MCP client for cross-vendor LLM API orchestration",
      "detailed_description": "A desktop client implementing the Model Context Protocol (MCP) designed to facilitate tool unitary utility integration. It accelerates AI adoption by enabling cross-vendor LLM API orchestration, serving as a platform for managing and utilizing AI agents and tools.",
      "domains": [
        "AI5"
      ],
      "subtask_category": [
        "agent_orchestration",
        "api_management"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/AI-QL/tuui",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "mcp",
        "llm-orchestration",
        "agent-client"
      ],
      "id": 2
    },
    {
      "name": "Aigne Framework",
      "one_line_profile": "Typescript-first AI Agent framework for composable LLM applications",
      "detailed_description": "A functional and composable AI Agent framework designed for building real-world LLM applications. It prioritizes TypeScript support and provides structures for creating and managing intelligent agents.",
      "domains": [
        "AI5"
      ],
      "subtask_category": [
        "agent_framework",
        "workflow_orchestration"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/AIGNE-io/aigne-framework",
      "help_website": [],
      "license": null,
      "tags": [
        "agent-framework",
        "typescript",
        "llm-apps"
      ],
      "id": 3
    },
    {
      "name": "VideoGen-Eval",
      "one_line_profile": "Agent-based system for evaluating video generation models",
      "detailed_description": "An agent-based system designed to evaluate video generation models. It provides a framework for assessing the quality and performance of AI-generated videos, serving as a specialized evaluation tool in the generative AI domain.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "video_generation_assessment"
      ],
      "application_level": "solver",
      "primary_language": null,
      "repo_url": "https://github.com/AILab-CVC/VideoGen-Eval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "evaluation",
        "video-generation",
        "agent-based"
      ],
      "id": 4
    },
    {
      "name": "MCP-Bench",
      "one_line_profile": "Benchmark for tool-using LLM agents via MCP servers",
      "detailed_description": "A benchmarking tool designed to evaluate tool-using LLM agents on complex real-world tasks. It utilizes MCP (Model Context Protocol) servers to standardize the interaction and assessment of agent performance.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "agent_benchmarking",
        "performance_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/Accenture/mcp-bench",
      "help_website": [],
      "license": null,
      "tags": [
        "benchmark",
        "mcp",
        "agent-evaluation"
      ],
      "id": 5
    },
    {
      "name": "AutoQuant",
      "one_line_profile": "Automation framework for ML, forecasting, and model evaluation",
      "detailed_description": "A comprehensive automation framework in R for machine learning and forecasting. It includes modules for model evaluation, interpretation, and automated workflow management, suitable for quantitative analysis and data science tasks.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "automl",
        "model_evaluation",
        "forecasting"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/AdrianAntico/AutoQuant",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "automl",
        "forecasting",
        "model-interpretation",
        "r"
      ],
      "id": 6
    },
    {
      "name": "AgentField",
      "one_line_profile": "Kubernetes-like orchestration platform for AI Agents",
      "detailed_description": "An infrastructure tool that enables building and running AI agents like microservices. It provides scalability, observability, and identity management, acting as a container orchestration layer specifically for AI agents.",
      "domains": [
        "AI5"
      ],
      "subtask_category": [
        "agent_orchestration",
        "infrastructure"
      ],
      "application_level": "platform",
      "primary_language": "Go",
      "repo_url": "https://github.com/Agent-Field/agentfield",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "kubernetes",
        "agent-orchestration",
        "microservices"
      ],
      "id": 7
    },
    {
      "name": "AgentOps",
      "one_line_profile": "Observability and evaluation SDK for AI agents",
      "detailed_description": "A Python SDK for monitoring, benchmarking, and tracking costs of AI agents. It integrates with major agent frameworks (CrewAI, LangChain, etc.) to provide comprehensive observability, session replays, and performance metrics for agent development.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "agent_observability",
        "tracing",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/AgentOps-AI/agentops",
      "help_website": [
        "https://agentops.ai"
      ],
      "license": "MIT",
      "tags": [
        "observability",
        "monitoring",
        "llm-cost",
        "agent-evaluation"
      ],
      "id": 8
    },
    {
      "name": "TokenCost",
      "one_line_profile": "Token price estimation utility for LLMs",
      "detailed_description": "A utility library for estimating token costs across 400+ Large Language Models. It serves as a support tool for LLMOps and agent cost management/tracking.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "cost_estimation",
        "resource_tracking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/AgentOps-AI/tokencost",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "token-counting",
        "cost-estimation",
        "llmops"
      ],
      "id": 9
    },
    {
      "name": "Agenta",
      "one_line_profile": "Open-source LLMOps platform for prompt management and evaluation",
      "detailed_description": "An LLMOps platform that provides tools for prompt engineering, management, LLM evaluation, and observability. It allows developers to iterate on prompts and evaluate model performance in a unified interface.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "prompt_management",
        "llm_evaluation",
        "observability"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/Agenta-AI/agenta",
      "help_website": [
        "https://agenta.ai"
      ],
      "license": null,
      "tags": [
        "llmops",
        "prompt-engineering",
        "evaluation"
      ],
      "id": 10
    },
    {
      "name": "AgenticGoKit",
      "one_line_profile": "Agentic AI framework in Go with observability",
      "detailed_description": "An open-source framework in Go for building and orchestrating intelligent agents. It supports multi-agent workflows, MCP tool discovery, and includes production-grade observability features.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "agent_framework",
        "observability",
        "workflow_orchestration"
      ],
      "application_level": "library",
      "primary_language": "Go",
      "repo_url": "https://github.com/AgenticGoKit/AgenticGoKit",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "go",
        "agent-framework",
        "observability"
      ],
      "id": 11
    },
    {
      "name": "OmniSearch",
      "one_line_profile": "Benchmark for Multimodal RAG with adaptive planning agents",
      "detailed_description": "A repository for benchmarking Multimodal Retrieval Augmented Generation (RAG) systems. It features a Dynamic VQA dataset and a self-adaptive planning agent to evaluate retrieval and generation capabilities across modalities.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "rag_benchmarking",
        "multimodal_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/Alibaba-NLP/OmniSearch",
      "help_website": [],
      "license": null,
      "tags": [
        "benchmark",
        "multimodal-rag",
        "vqa"
      ],
      "id": 12
    },
    {
      "name": "OpenInference",
      "one_line_profile": "OpenTelemetry instrumentation standard for AI observability",
      "detailed_description": "Provides OpenTelemetry instrumentation for AI observability, defining a standard for tracing and monitoring LLM and agentic applications. It enables consistent data collection across various AI frameworks.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "tracing",
        "observability",
        "instrumentation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Arize-ai/openinference",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "opentelemetry",
        "observability",
        "tracing",
        "standard"
      ],
      "id": 13
    },
    {
      "name": "DeepResearch Bench",
      "one_line_profile": "Benchmark for evaluating Deep Research Agents",
      "detailed_description": "A comprehensive benchmark specifically designed for assessing the performance of Deep Research Agents. It provides datasets and evaluation metrics to measure the capabilities of agents in conducting deep, multi-step research tasks.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "agent_benchmarking",
        "research_capability_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/Ayanami0730/deep_research_bench",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "benchmark",
        "deep-research",
        "agent-evaluation"
      ],
      "id": 14
    },
    {
      "name": "ArtKit",
      "one_line_profile": "Automated prompt-based testing and evaluation for Gen AI",
      "detailed_description": "A toolkit for automated prompt-based testing and evaluation of Generative AI applications. It facilitates the creation of test suites and evaluation pipelines to ensure the quality and reliability of Gen AI outputs.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "prompt_testing",
        "model_evaluation",
        "quality_assurance"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/BCG-X-Official/artkit",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "testing",
        "evaluation",
        "gen-ai",
        "prompt-engineering"
      ],
      "id": 15
    },
    {
      "name": "Ko-LM-Evaluation-Harness",
      "one_line_profile": "Evaluation harness for Korean language models",
      "detailed_description": "A specialized fork of the lm-evaluation-harness tailored for evaluating Korean Large Language Models. It includes datasets and metrics specific to the Korean language context.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "linguistic_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Beomi/ko-lm-evaluation-harness",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "evaluation",
        "korean-llm",
        "benchmark"
      ],
      "id": 16
    },
    {
      "name": "RAG Logger",
      "one_line_profile": "Lightweight logging tool for RAG applications",
      "detailed_description": "An open-source logging tool specifically designed for Retrieval-Augmented Generation (RAG) applications. It serves as a lightweight alternative to complex observability platforms, focusing on capturing RAG-specific events and data flows.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "logging",
        "tracing",
        "rag_observability"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Brandon-c-tech/RAG-logger",
      "help_website": [],
      "license": null,
      "tags": [
        "logging",
        "rag",
        "observability"
      ],
      "id": 17
    },
    {
      "name": "DAComp",
      "one_line_profile": "Benchmark for Data Agents across the data intelligence lifecycle",
      "detailed_description": "A benchmarking suite for evaluating Data Agents across the full data intelligence lifecycle. It assesses the capabilities of agents in handling data processing, analysis, and management tasks.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "agent_benchmarking",
        "data_agent_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/ByteDance-Seed/DAComp",
      "help_website": [],
      "license": null,
      "tags": [
        "benchmark",
        "data-agents",
        "evaluation"
      ],
      "id": 18
    },
    {
      "name": "RAGEval",
      "one_line_profile": "Automated evaluation system for RAG applications",
      "detailed_description": "A one-stop solution for the automated evaluation of RAG systems. It provides tools and metrics to assess the performance of retrieval and generation components in RAG pipelines.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "rag_evaluation",
        "automated_testing"
      ],
      "application_level": "solver",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/BytePioneer-AI/RAGEval",
      "help_website": [],
      "license": null,
      "tags": [
        "rag",
        "evaluation",
        "automation"
      ],
      "id": 19
    },
    {
      "name": "Rankify",
      "one_line_profile": "Comprehensive Python toolkit for Retrieval, Re-Ranking, and RAG",
      "detailed_description": "A toolkit integrating pre-retrieved benchmark datasets and supporting multiple retrieval techniques, reranking models, and RAG methods for scientific information retrieval and processing.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "retrieval",
        "reranking",
        "rag"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/DataScienceUIBK/Rankify",
      "help_website": [],
      "license": null,
      "tags": [
        "rag",
        "retrieval",
        "reranking"
      ],
      "id": 20
    },
    {
      "name": "XRAG",
      "one_line_profile": "Benchmark for evaluating foundational component modules in RAG",
      "detailed_description": "A benchmarking framework designed to examine the core components of Advanced Retrieval-Augmented Generation systems, facilitating evaluation of retrieval and generation modules.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "benchmarking",
        "evaluation",
        "rag"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/DocAILab/XRAG",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "benchmark",
        "evaluation"
      ],
      "id": 21
    },
    {
      "name": "LM Evaluation Harness",
      "one_line_profile": "Framework for few-shot evaluation of language models",
      "detailed_description": "A framework for few-shot evaluation of language models, widely used for benchmarking LLM performance across various tasks including scientific reasoning.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/EleutherAI/lm-evaluation-harness",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "evaluation",
        "benchmark"
      ],
      "id": 22
    },
    {
      "name": "JamAIBase",
      "one_line_profile": "Collaborative spreadsheet interface for building and evaluating AI pipelines",
      "detailed_description": "A tool that combines spreadsheet UI with LLM pipeline building capabilities, allowing users to experiment with prompts, chain cells, and evaluate LLM responses in real-time.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "workflow_orchestration",
        "evaluation",
        "prompt_engineering"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/EmbeddedLLM/JamAIBase",
      "help_website": [
        "https://jamaibase.com"
      ],
      "license": "Apache-2.0",
      "tags": [
        "spreadsheet",
        "llm-ops",
        "evaluation"
      ],
      "id": 23
    },
    {
      "name": "EmbodiedBench",
      "one_line_profile": "Benchmark to evaluate Multimodal LLMs as embodied agents",
      "detailed_description": "A comprehensive benchmark designed to evaluate Multimodal Large Language Models (MLLMs) in the context of embodied agents, testing their ability to interact with environments.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "benchmarking",
        "evaluation",
        "embodied_ai"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/EmbodiedBench/EmbodiedBench",
      "help_website": [],
      "license": null,
      "tags": [
        "embodied-ai",
        "benchmark",
        "mllm"
      ],
      "id": 24
    },
    {
      "name": "spacecutter",
      "one_line_profile": "Ordinal regression models implementation in PyTorch",
      "detailed_description": "A library for implementing ordinal regression models in PyTorch, useful for scientific data analysis where outcome variables are ordered categories.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "regression",
        "statistical_modeling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/EthanRosenthal/spacecutter",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ordinal-regression",
        "pytorch",
        "statistics"
      ],
      "id": 25
    },
    {
      "name": "QueryWeaver",
      "one_line_profile": "Text-to-SQL tool using graph-powered schema understanding",
      "detailed_description": "An open-source Text2SQL tool that transforms natural language into SQL, facilitating data retrieval from scientific databases via agentic interfaces.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "text_to_sql",
        "query_generation",
        "data_retrieval"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/FalkorDB/QueryWeaver",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "text2sql",
        "graph-database",
        "nlp"
      ],
      "id": 26
    },
    {
      "name": "FixedEffectModels.jl",
      "one_line_profile": "Fast estimation of linear models with fixed effects and IV in Julia",
      "detailed_description": "A Julia package for fast estimation of linear models with high-dimensional categorical variables and instrumental variables, widely used in econometrics and statistical analysis.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "regression",
        "statistical_modeling"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/FixedEffects/FixedEffectModels.jl",
      "help_website": [],
      "license": null,
      "tags": [
        "julia",
        "econometrics",
        "statistics"
      ],
      "id": 27
    },
    {
      "name": "LLMZoo",
      "one_line_profile": "Data, models, and evaluation benchmarks for large language models",
      "detailed_description": "A project providing data, models, and evaluation benchmarks for LLMs, supporting the development and assessment of language models.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "evaluation",
        "benchmarking",
        "model_hosting"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/FreedomIntelligence/LLMZoo",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "benchmark",
        "dataset"
      ],
      "id": 28
    },
    {
      "name": "Giskard",
      "one_line_profile": "Open-source testing and evaluation library for LLM agents and AI models",
      "detailed_description": "A comprehensive testing and evaluation library for AI models, including LLMs, focusing on quality, security, and performance assessment.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "evaluation",
        "testing",
        "hallucination_detection"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Giskard-AI/giskard-oss",
      "help_website": [
        "https://docs.giskard.ai"
      ],
      "license": "Apache-2.0",
      "tags": [
        "ai-testing",
        "evaluation",
        "llm-ops"
      ],
      "id": 29
    },
    {
      "name": "Giskard Vision",
      "one_line_profile": "Evaluation and testing library for Computer Vision AI systems",
      "detailed_description": "An extension of the Giskard framework specifically designed for evaluating and testing computer vision models.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "evaluation",
        "computer_vision",
        "testing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Giskard-AI/giskard-vision",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "computer-vision",
        "evaluation",
        "testing"
      ],
      "id": 30
    },
    {
      "name": "Giskard Prompt Injections",
      "one_line_profile": "Collection of prompt injection payloads for security testing of LLMs",
      "detailed_description": "A dataset/resource containing prompt injection payloads used for red-teaming and security evaluation of Large Language Models.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "security_testing",
        "evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/Giskard-AI/prompt-injections",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "security",
        "prompt-injection",
        "red-teaming"
      ],
      "id": 31
    },
    {
      "name": "GraphRAG-Benchmark",
      "one_line_profile": "Benchmark and dataset for evaluating GraphRAG models",
      "detailed_description": "A comprehensive benchmark and dataset specifically designed for evaluating Retrieval-Augmented Generation models that utilize graph structures.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "benchmarking",
        "evaluation",
        "graph_rag"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/GraphRAG-Bench/GraphRAG-Benchmark",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "graph-rag",
        "benchmark",
        "evaluation"
      ],
      "id": 32
    },
    {
      "name": "LightRAG",
      "one_line_profile": "Simple and fast Retrieval-Augmented Generation framework",
      "detailed_description": "A framework for implementing Retrieval-Augmented Generation (RAG) systems, optimizing for simplicity and speed in information retrieval tasks.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "rag",
        "retrieval",
        "generation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/HKUDS/LightRAG",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "retrieval",
        "llm"
      ],
      "id": 33
    },
    {
      "name": "GLM.jl",
      "one_line_profile": "Generalized linear models library for Julia",
      "detailed_description": "A Julia package for fitting generalized linear models (GLM), including linear, logistic, and Poisson regression, which are fundamental for scientific statistical analysis.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "statistical_analysis",
        "regression",
        "modeling"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/JuliaStats/GLM.jl",
      "help_website": [
        "https://juliastats.org/GLM.jl/stable/"
      ],
      "license": "MIT",
      "tags": [
        "statistics",
        "regression",
        "julia",
        "glm"
      ],
      "id": 34
    },
    {
      "name": "Weave.jl",
      "one_line_profile": "Scientific report generator and literate programming tool for Julia",
      "detailed_description": "A scientific report generator for Julia that enables literate programming, allowing researchers to weave code and documentation into reproducible scientific reports (HTML, PDF, LaTeX).",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "scientific_visualization",
        "reproducible_research",
        "report_generation"
      ],
      "application_level": "tool",
      "primary_language": "Julia",
      "repo_url": "https://github.com/JunoLab/Weave.jl",
      "help_website": [
        "http://weavejl.mpastell.com/stable/"
      ],
      "license": "MIT",
      "tags": [
        "literate-programming",
        "reproducibility",
        "julia",
        "reporting"
      ],
      "id": 35
    },
    {
      "name": "trame",
      "one_line_profile": "Python framework for building interactive scientific visualization applications",
      "detailed_description": "A framework by Kitware that enables the creation of interactive web-based applications for scientific visualization and data analysis, leveraging VTK and ParaView backends.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "scientific_visualization",
        "visual_analytics",
        "dashboarding"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/Kitware/trame",
      "help_website": [
        "https://kitware.github.io/trame/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "visualization",
        "vtk",
        "paraview",
        "web-app"
      ],
      "id": 36
    },
    {
      "name": "DeepTab",
      "one_line_profile": "Deep learning library for tabular data regression and classification",
      "detailed_description": "A Python package providing a suite of deep learning models (e.g., FT-Transformer, TabTransformer) specifically for tabular data analysis, enabling regression and classification tasks common in scientific experiments.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "tabular_regression",
        "classification",
        "data_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenTabular/DeepTab",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "tabular-data",
        "deep-learning",
        "regression",
        "classification"
      ],
      "id": 37
    },
    {
      "name": "GitTaskBench",
      "one_line_profile": "Repo-level benchmark for evaluating coding agents on real-world Git tasks",
      "detailed_description": "A benchmark designed to evaluate the capabilities of Large Language Model (LLM) based agents in performing real-world software development tasks within a Git repository context. It assesses repo understanding, environment setup, and bug fixing.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "agent_evaluation",
        "benchmark"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/QuantaAlpha/GitTaskBench",
      "help_website": [],
      "license": null,
      "tags": [
        "benchmark",
        "agents",
        "coding-agents",
        "llm-evaluation"
      ],
      "id": 38
    },
    {
      "name": "LLMBox",
      "one_line_profile": "Comprehensive library for LLM training and evaluation",
      "detailed_description": "A unified library for implementing, training, and evaluating Large Language Models (LLMs). It provides a pipeline for standardizing the training and comprehensive evaluation of models, supporting various datasets and metrics.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "model_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/RUCAIBox/LLMBox",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "evaluation",
        "training-pipeline",
        "benchmark"
      ],
      "id": 39
    },
    {
      "name": "RagView",
      "one_line_profile": "Unified evaluation platform for benchmarking RAG methods",
      "detailed_description": "A platform designed to evaluate and benchmark different Retrieval-Augmented Generation (RAG) methods on custom datasets, facilitating the comparison of SOTA results in specific data contexts.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "rag_evaluation",
        "benchmark"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/RagView/RagView",
      "help_website": [],
      "license": null,
      "tags": [
        "rag",
        "evaluation",
        "benchmark",
        "retrieval"
      ],
      "id": 40
    },
    {
      "name": "Whistleblower",
      "one_line_profile": "Offensive security tool for testing AI application prompt leakage",
      "detailed_description": "A security tool designed for AI engineers and researchers to test system prompt leakage and discover capabilities of LLM-based applications exposed via API, serving as a red-teaming utility.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "security_evaluation",
        "red_teaming"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Repello-AI/whistleblower",
      "help_website": [],
      "license": null,
      "tags": [
        "security",
        "llm",
        "red-teaming",
        "prompt-injection"
      ],
      "id": 41
    },
    {
      "name": "RAG-evaluation-harnesses",
      "one_line_profile": "Evaluation suite for Retrieval-Augmented Generation (RAG)",
      "detailed_description": "A comprehensive evaluation harness specifically designed for assessing the performance of Retrieval-Augmented Generation (RAG) systems.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "rag_evaluation",
        "benchmark"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/RulinShao/RAG-evaluation-harnesses",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "evaluation",
        "harness"
      ],
      "id": 42
    },
    {
      "name": "Agentic Web",
      "one_line_profile": "Framework for weaving the web with AI Agents",
      "detailed_description": "A research framework aimed at enabling AI agents to interact with and navigate the web, likely supporting the development and evaluation of web-based agents.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "agent_framework",
        "web_agents"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/SafeRL-Lab/agentic-web",
      "help_website": [],
      "license": null,
      "tags": [
        "agents",
        "web-navigation",
        "framework"
      ],
      "id": 43
    },
    {
      "name": "MCP-Universe",
      "one_line_profile": "Framework for developing and benchmarking AI agents",
      "detailed_description": "A comprehensive framework from Salesforce AI Research designed for the development, testing, and benchmarking of AI agents, facilitating reproducible agent research.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "agent_benchmarking",
        "agent_development"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/SalesforceAIResearch/MCP-Universe",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "agents",
        "benchmark",
        "multi-agent"
      ],
      "id": 44
    },
    {
      "name": "DomainMind",
      "one_line_profile": "RAG integration for local LLMs in specialized domains",
      "detailed_description": "A tool to enhance local Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) for answering domain-specific questions, specifically targeting fields like scientific research.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "rag",
        "domain_adaptation"
      ],
      "application_level": "application",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/SalmanFarizN/DomainMind",
      "help_website": [],
      "license": null,
      "tags": [
        "rag",
        "local-llm",
        "scientific-research",
        "knowledge-base"
      ],
      "id": 45
    },
    {
      "name": "AgentClinic",
      "one_line_profile": "Agent benchmark for medical diagnosis",
      "detailed_description": "A benchmark environment designed to evaluate the performance of AI agents in the context of medical diagnosis tasks, simulating clinical scenarios.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "agent_evaluation",
        "medical_diagnosis"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/SamuelSchmidgall/AgentClinic",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "medical",
        "benchmark",
        "agents",
        "healthcare"
      ],
      "id": 46
    },
    {
      "name": "Langtrace",
      "one_line_profile": "Open Telemetry based observability tool for LLM applications",
      "detailed_description": "An open-source observability tool providing real-time tracing, evaluations, and metrics for LLM applications, supporting popular frameworks and vector databases.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "tracing",
        "observability",
        "evaluation"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/Scale3-Labs/langtrace",
      "help_website": [
        "https://docs.langtrace.ai"
      ],
      "license": "AGPL-3.0",
      "tags": [
        "observability",
        "tracing",
        "llm",
        "opentelemetry"
      ],
      "id": 47
    },
    {
      "name": "Langtrace Python SDK",
      "one_line_profile": "Python SDK for Langtrace observability platform",
      "detailed_description": "The Python Software Development Kit (SDK) for integrating Langtrace observability and tracing capabilities into Python-based LLM applications.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "tracing",
        "observability"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Scale3-Labs/langtrace-python-sdk",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "sdk",
        "python",
        "tracing",
        "llm"
      ],
      "id": 48
    },
    {
      "name": "giskardpy",
      "one_line_profile": "Python library for constraint-based robot motion control",
      "detailed_description": "The core Python library of the Giskard framework, facilitating constraint- and optimization-based motion control for robotics.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "robotics_control",
        "motion_planning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/SemRoCo/giskardpy",
      "help_website": [],
      "license": "LGPL-3.0",
      "tags": [
        "robotics",
        "motion-control",
        "optimization"
      ],
      "id": 49
    },
    {
      "name": "AgentLab",
      "one_line_profile": "Framework for developing and benchmarking web agents",
      "detailed_description": "An open-source framework for developing, testing, and benchmarking web agents across diverse tasks, emphasizing scalability and reproducibility in agent research.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "agent_benchmarking",
        "agent_development"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/ServiceNow/AgentLab",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "agents",
        "benchmark",
        "web-agents"
      ],
      "id": 50
    },
    {
      "name": "SynthGenAI",
      "one_line_profile": "Package for generating synthetic datasets using LLMs",
      "detailed_description": "A Python package designed to facilitate the generation of synthetic datasets using Large Language Models (LLMs), useful for data augmentation in scientific and AI research.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "synthetic_data_generation",
        "data_augmentation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Shekswess/synthgenai",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "synthetic-data",
        "llm",
        "dataset-generation"
      ],
      "id": 51
    },
    {
      "name": "Gorilla",
      "one_line_profile": "Training and evaluating LLMs for function calls",
      "detailed_description": "A project focused on training and evaluating Large Language Models (LLMs) specifically for their ability to perform function calls (tool use), critical for agentic workflows.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "tool_use"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ShishirPatil/gorilla",
      "help_website": [
        "https://gorilla.cs.berkeley.edu/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "function-calling",
        "tool-use",
        "evaluation"
      ],
      "id": 52
    },
    {
      "name": "Auto-GPT-Benchmarks",
      "one_line_profile": "Benchmark for evaluating agent performance",
      "detailed_description": "A repository dedicated to benchmarking the performance of AI agents, providing a standardized way to assess agent capabilities regardless of their underlying architecture.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "agent_benchmarking",
        "performance_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/Significant-Gravitas/Auto-GPT-Benchmarks",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "benchmark",
        "agents",
        "auto-gpt"
      ],
      "id": 53
    },
    {
      "name": "Skywork",
      "one_line_profile": "Skywork model series with training and evaluation resources",
      "detailed_description": "Open-source release of the Skywork series models, including training data, evaluation datasets, and evaluation methods, supporting multilingual and code tasks.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "model_release"
      ],
      "application_level": "model",
      "primary_language": "Python",
      "repo_url": "https://github.com/SkyworkAI/Skywork",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "llm",
        "evaluation",
        "pretrained-models"
      ],
      "id": 54
    },
    {
      "name": "EmoLLM",
      "one_line_profile": "Mental health LLM framework with evaluation and RAG",
      "detailed_description": "A comprehensive framework for Mental Health Large Language Models, including pre-training, post-training, datasets, evaluation, and RAG capabilities.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "domain_specific_llm",
        "mental_health_analysis"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/SmartFlowAI/EmoLLM",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mental-health",
        "llm",
        "evaluation",
        "rag"
      ],
      "id": 55
    },
    {
      "name": "stargazer",
      "one_line_profile": "Python implementation of R stargazer for regression tables",
      "detailed_description": "A Python library that generates LaTeX, HTML, and ASCII tables for statistical regression models, porting the functionality of the popular R 'stargazer' package.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "statistical_reporting",
        "visualization"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/StatsReporting/stargazer",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "statistics",
        "regression",
        "reporting",
        "latex"
      ],
      "id": 56
    },
    {
      "name": "AppWorld",
      "one_line_profile": "Benchmark for interactive coding agents and function calling",
      "detailed_description": "A controllable world of apps and people designed for benchmarking function calling and interactive coding agents, serving as a resource for evaluating agent capabilities.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "agent_benchmarking",
        "function_calling"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/StonyBrookNLP/appworld",
      "help_website": [
        "https://appworld.github.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "benchmark",
        "agents",
        "interactive-coding"
      ],
      "id": 57
    },
    {
      "name": "AgentBench",
      "one_line_profile": "Comprehensive benchmark to evaluate LLMs as Agents",
      "detailed_description": "A comprehensive benchmark suite designed to evaluate Large Language Models (LLMs) acting as agents across various environments and tasks.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "agent_benchmarking",
        "llm_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/THUDM/AgentBench",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "benchmark",
        "agents",
        "llm"
      ],
      "id": 58
    },
    {
      "name": "AI-Infra-Guard",
      "one_line_profile": "AI Red Teaming platform",
      "detailed_description": "A comprehensive, intelligent AI Red Teaming platform developed to evaluate and secure AI infrastructure and models.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "security_evaluation",
        "red_teaming"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/Tencent/AI-Infra-Guard",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "red-teaming",
        "ai-security",
        "evaluation"
      ],
      "id": 59
    },
    {
      "name": "AICGSecEval",
      "one_line_profile": "AI-generated code security evaluation benchmark",
      "detailed_description": "A repository-level benchmark for evaluating the security of code generated by AI models, developed by Tencent.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "security_evaluation",
        "code_generation_benchmark"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/Tencent/AICGSecEval",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "security",
        "benchmark",
        "code-generation"
      ],
      "id": 60
    },
    {
      "name": "WeKnora",
      "one_line_profile": "LLM-powered framework for deep document understanding and RAG",
      "detailed_description": "A framework leveraging LLMs for deep document understanding, semantic retrieval, and context-aware answers using the RAG paradigm, suitable for knowledge-intensive tasks.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "rag",
        "document_understanding"
      ],
      "application_level": "framework",
      "primary_language": "Go",
      "repo_url": "https://github.com/Tencent/WeKnora",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "rag",
        "document-understanding",
        "llm"
      ],
      "id": 61
    },
    {
      "name": "ActionWeaver",
      "one_line_profile": "Library for easier LLM function calling",
      "detailed_description": "A Python library designed to simplify the implementation of function calling with Large Language Models (LLMs), facilitating the creation of tool-using agents.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "tool_use",
        "function_calling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/TengHu/ActionWeaver",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "function-calling",
        "agents"
      ],
      "id": 62
    },
    {
      "name": "TheAgentCompany",
      "one_line_profile": "Agent benchmark with simulated software company tasks",
      "detailed_description": "A benchmark environment that simulates a software company to evaluate AI agents on a variety of realistic tasks.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "agent_benchmarking",
        "task_simulation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/TheAgentCompany/TheAgentCompany",
      "help_website": [
        "https://the-agent-company.github.io/"
      ],
      "license": "MIT",
      "tags": [
        "benchmark",
        "agents",
        "simulation"
      ],
      "id": 63
    },
    {
      "name": "Tonic Validate",
      "one_line_profile": "Metrics library for evaluating Retrieval Augmented Generation (RAG) applications",
      "detailed_description": "A Python library providing metrics and tools to evaluate the quality of responses from RAG applications, essential for verifying the accuracy of scientific AI assistants.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "evaluation",
        "rag_metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/TonicAI/tonic_validate",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "evaluation",
        "metrics",
        "llm"
      ],
      "id": 64
    },
    {
      "name": "VoltAgent",
      "one_line_profile": "AI Agent Framework with built-in LLM Observability",
      "detailed_description": "A TypeScript framework for building AI agents that includes built-in observability features, facilitating the tracing and debugging of agentic workflows.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "tracing",
        "observability",
        "agent_framework"
      ],
      "application_level": "workflow",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/VoltAgent/voltagent",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "agent",
        "observability",
        "tracing",
        "llm"
      ],
      "id": 65
    },
    {
      "name": "DeepPHY",
      "one_line_profile": "Benchmarking Agentic VLMs on Physical Reasoning",
      "detailed_description": "A benchmark and evaluation harness for testing the physical reasoning capabilities of Vision-Language Models (VLMs), critical for scientific agents interacting with the physical world.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "benchmarking",
        "physical_reasoning",
        "evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/XinrunXu/DeepPHY",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "vlm",
        "physical-reasoning",
        "benchmark",
        "agent"
      ],
      "id": 66
    },
    {
      "name": "PromptWorks",
      "one_line_profile": "Prompt management and testing tool",
      "detailed_description": "A platform for managing, testing, and optimizing prompts for LLMs, supporting the evaluation phase of AI agent development.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "prompt_engineering",
        "testing",
        "evaluation"
      ],
      "application_level": "platform",
      "primary_language": "Vue",
      "repo_url": "https://github.com/YellowSeaa/PromptWorks",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "prompt-engineering",
        "testing",
        "llm"
      ],
      "id": 67
    },
    {
      "name": "math-evaluation-harness",
      "one_line_profile": "Toolkit for benchmarking LLMs on mathematical reasoning tasks",
      "detailed_description": "A simple toolkit designed to evaluate and benchmark Large Language Models on various mathematical reasoning datasets, essential for verifying scientific reasoning capabilities.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "benchmarking",
        "mathematical_reasoning",
        "evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ZubinGou/math-evaluation-harness",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "math",
        "reasoning",
        "benchmark",
        "llm"
      ],
      "id": 68
    },
    {
      "name": "AgentScope Studio",
      "one_line_profile": "Visualization toolkit for AgentScope multi-agent platform",
      "detailed_description": "A development-oriented visualization tool for the AgentScope framework, enabling developers to trace, visualize, and debug multi-agent workflows.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "visualization",
        "debugging",
        "agent_workflow"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/agentscope-ai/agentscope-studio",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "visualization",
        "multi-agent",
        "debugging"
      ],
      "id": 69
    },
    {
      "name": "BeyondLLM",
      "one_line_profile": "Framework to build, evaluate and observe LLM apps",
      "detailed_description": "A framework that simplifies the development of LLM applications with integrated capabilities for evaluation and observability (tracing), supporting reliable AI agent deployment.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "evaluation",
        "observability",
        "workflow_orchestration"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/aiplanethub/beyondllm",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "evaluation",
        "observability",
        "rag"
      ],
      "id": 70
    },
    {
      "name": "AutoRAG-Eval",
      "one_line_profile": "Automated Evaluation of Retrieval-Augmented Language Models",
      "detailed_description": "A tool for the automated evaluation of RAG systems, generating task-specific exams to measure performance, developed by Amazon Science.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "evaluation",
        "rag_benchmarking",
        "exam_generation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/amazon-science/auto-rag-eval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "evaluation",
        "benchmarking",
        "llm"
      ],
      "id": 71
    },
    {
      "name": "langfuse-mcp",
      "one_line_profile": "MCP server for Langfuse enabling AI agent tracing and observability",
      "detailed_description": "A Model Context Protocol (MCP) server implementation that connects AI agents to Langfuse, allowing for the querying of trace data to enhance debugging and observability within agentic workflows.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "tracing",
        "observability",
        "debugging"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/avivsinai/langfuse-mcp",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mcp",
        "langfuse",
        "observability",
        "tracing",
        "agent-debugging"
      ],
      "id": 72
    },
    {
      "name": "linearmodels",
      "one_line_profile": "Extended linear models for econometrics and statistics in Python",
      "detailed_description": "A Python library that extends statsmodels with additional linear models, specifically focusing on instrumental variable (IV) models, panel data models, and other advanced statistical estimation methods used in econometrics and scientific data analysis.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "statistical_analysis",
        "regression",
        "estimation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/bashtage/linearmodels",
      "help_website": [
        "https://bashtage.github.io/linearmodels/"
      ],
      "license": "NCSA",
      "tags": [
        "econometrics",
        "statistics",
        "regression",
        "panel-data",
        "instrumental-variables"
      ],
      "id": 73
    },
    {
      "name": "BEIR",
      "one_line_profile": "Heterogeneous benchmark for zero-shot information retrieval evaluation",
      "detailed_description": "A heterogeneous benchmark for Information Retrieval (IR) that enables the evaluation of retrieval models across diverse datasets and tasks. It serves as a standard evaluation harness for assessing the generalization capabilities of neural retrieval models.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "information_retrieval",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/beir-cellar/beir",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "information-retrieval",
        "benchmark",
        "evaluation",
        "zero-shot"
      ],
      "id": 74
    },
    {
      "name": "BigCode Evaluation Harness",
      "one_line_profile": "Framework for evaluating code generation language models",
      "detailed_description": "A comprehensive framework designed for the evaluation of autoregressive code generation language models. It supports multiple programming languages and tasks, enabling rigorous assessment of LLMs used for coding, which is a critical utility in scientific computing.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "code_generation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/bigcode-project/bigcode-evaluation-harness",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "evaluation",
        "llm",
        "code-generation",
        "benchmark"
      ],
      "id": 75
    },
    {
      "name": "BigCodeBench",
      "one_line_profile": "Benchmark for code generation towards AGI capabilities",
      "detailed_description": "A benchmark dataset and evaluation suite designed to assess the capabilities of code generation models on complex, practical programming tasks. It serves as a standard for evaluating the reasoning and coding proficiency of AI agents.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking",
        "code_generation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/bigcode-project/bigcodebench",
      "help_website": [
        "https://bigcodebench.github.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "benchmark",
        "code-generation",
        "evaluation",
        "llm"
      ],
      "id": 76
    },
    {
      "name": "Carla-ppo",
      "one_line_profile": "Reinforcement learning agent wrapper for CARLA simulator",
      "detailed_description": "A customized PPO-based agent framework for the CARLA autonomous driving simulator. It wraps CARLA in a Gym-like environment, facilitating reinforcement learning experiments and simulation control for autonomous systems research.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "simulation",
        "reinforcement_learning",
        "agent_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/bitsauce/Carla-ppo",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "carla",
        "reinforcement-learning",
        "simulation",
        "autonomous-driving",
        "ppo"
      ],
      "id": 77
    },
    {
      "name": "AutoAI",
      "one_line_profile": "Automated AI framework for regression and classification",
      "detailed_description": "A Python-based AutoML framework designed to automate the process of model search, hyper-parameter tuning, and code generation for regression and classification tasks on numerical data, facilitating scientific data analysis.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "automl",
        "regression",
        "classification",
        "data_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/blobcity/autoai",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "automl",
        "regression",
        "classification",
        "data-science"
      ],
      "id": 78
    },
    {
      "name": "CRAB",
      "one_line_profile": "Cross-environment agent benchmark for multimodal language models",
      "detailed_description": "A benchmark framework for evaluating Multimodal Language Model (MLM) agents across diverse environments. It provides a standardized way to assess agent performance in complex, cross-domain tasks, supporting the development of robust scientific agents.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking",
        "agent_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/camel-ai/crab",
      "help_website": [
        "https://crab.camel-ai.org/"
      ],
      "license": null,
      "tags": [
        "benchmark",
        "agent",
        "multimodal",
        "evaluation"
      ],
      "id": 79
    },
    {
      "name": "Yet Another Applied LLM Benchmark",
      "one_line_profile": "Benchmark for evaluating LLMs on applied reasoning tasks",
      "detailed_description": "A benchmark suite designed to evaluate Large Language Models on a set of practical, applied questions and problems, serving as a dataset for assessing model reasoning and problem-solving capabilities.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking",
        "reasoning"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/carlini/yet-another-applied-llm-benchmark",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "benchmark",
        "llm",
        "evaluation",
        "reasoning"
      ],
      "id": 80
    },
    {
      "name": "Opik",
      "one_line_profile": "Platform for debugging, evaluating, and monitoring LLM applications",
      "detailed_description": "An open-source platform designed to trace, evaluate, and monitor LLM applications, RAG systems, and agentic workflows. It provides tools for automated evaluation and observability, essential for building reliable scientific AI agents.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "tracing",
        "model_evaluation",
        "observability",
        "monitoring"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/comet-ml/opik",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-ops",
        "evaluation",
        "tracing",
        "observability",
        "rag"
      ],
      "id": 81
    },
    {
      "name": "DeepEval",
      "one_line_profile": "Evaluation framework for LLMs and RAG pipelines",
      "detailed_description": "A comprehensive evaluation framework for Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems. It provides metrics and test suites to assess model performance, hallucination, and alignment, supporting the validation of scientific AI applications.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "testing",
        "quality_control"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/confident-ai/deepeval",
      "help_website": [
        "https://docs.confident-ai.com/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "evaluation",
        "llm",
        "rag",
        "testing",
        "metrics"
      ],
      "id": 82
    },
    {
      "name": "Coze Loop",
      "one_line_profile": "AI agent optimization and lifecycle management platform",
      "detailed_description": "A platform for the optimization and lifecycle management of AI agents. It offers capabilities for debugging, evaluation, and monitoring, enabling the iterative improvement of agents used in complex workflows.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "agent_optimization",
        "debugging",
        "monitoring",
        "evaluation"
      ],
      "application_level": "platform",
      "primary_language": "Go",
      "repo_url": "https://github.com/coze-dev/coze-loop",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "agent-optimization",
        "debugging",
        "evaluation",
        "monitoring"
      ],
      "id": 83
    },
    {
      "name": "Coze Studio",
      "one_line_profile": "Visual development environment for AI agents",
      "detailed_description": "An all-in-one visual development platform for creating, debugging, and deploying AI agents. It simplifies the orchestration of tools and workflows, facilitating the construction of agent-based solutions.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "workflow_orchestration",
        "agent_development",
        "debugging"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/coze-dev/coze-studio",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "agent-development",
        "visual-programming",
        "ide",
        "workflow"
      ],
      "id": 84
    },
    {
      "name": "UQLM",
      "one_line_profile": "Uncertainty quantification for language model hallucination detection",
      "detailed_description": "A Python package for Uncertainty Quantification (UQ) in Language Models, specifically designed to detect hallucinations. It provides statistical methods to estimate confidence and reliability in model outputs, a critical aspect for scientific AI.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "uncertainty_quantification",
        "hallucination_detection",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/cvs-health/uqlm",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "uncertainty-quantification",
        "hallucination-detection",
        "llm",
        "reliability"
      ],
      "id": 85
    },
    {
      "name": "AgentWatch",
      "one_line_profile": "Observability framework for AI agent interactions",
      "detailed_description": "An AI observability framework that provides insights into agent interactions. It enables monitoring, analysis, and optimization of AI-driven applications, supporting the tracing and evaluation of agent behaviors.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "observability",
        "monitoring",
        "tracing",
        "agent_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/cyberark/agentwatch",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "observability",
        "agent",
        "monitoring",
        "tracing"
      ],
      "id": 86
    },
    {
      "name": "finalfit",
      "one_line_profile": "R package for creating elegant regression results tables and plots",
      "detailed_description": "A library designed to streamline the process of generating final results tables and plots for regression models in R, commonly used in medical and epidemiological research for reporting.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "regression_analysis",
        "scientific_visualization"
      ],
      "application_level": "library",
      "primary_language": "HTML",
      "repo_url": "https://github.com/ewenharrison/finalfit",
      "help_website": [
        "https://finalfit.org/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "r",
        "regression",
        "statistics",
        "visualization"
      ],
      "id": 87
    },
    {
      "name": "BenchMARL",
      "one_line_profile": "Benchmarking library for Multi-Agent Reinforcement Learning (MARL)",
      "detailed_description": "A PyTorch-based library for benchmarking Multi-Agent Reinforcement Learning (MARL) algorithms, enabling systematic comparison of algorithms, tasks, and models with a focus on reproducibility.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "benchmarking",
        "reinforcement_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/BenchMARL",
      "help_website": [
        "https://facebookresearch.github.io/BenchMARL/"
      ],
      "license": "MIT",
      "tags": [
        "marl",
        "reinforcement-learning",
        "benchmark",
        "pytorch"
      ],
      "id": 88
    },
    {
      "name": "MLGym",
      "one_line_profile": "Framework and benchmark for advancing AI research agents",
      "detailed_description": "A framework designed to facilitate the development and benchmarking of AI agents, providing environments and tools to evaluate agent performance in research settings.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "benchmarking",
        "agent_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/MLGym",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "ai-agents",
        "benchmark",
        "gym"
      ],
      "id": 89
    },
    {
      "name": "Meta Agents Research Environments",
      "one_line_profile": "Platform for evaluating AI agents in dynamic, realistic scenarios",
      "detailed_description": "A comprehensive platform for evaluating AI agents in evolving environments where agents must adapt strategies based on new information, mirroring real-world challenges.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "agent_evaluation",
        "simulation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/meta-agents-research-environments",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ai-agents",
        "evaluation",
        "simulation"
      ],
      "id": 90
    },
    {
      "name": "SWEET-RL",
      "one_line_profile": "Benchmark for training multi-turn LLM agents on collaborative reasoning",
      "detailed_description": "A benchmark and research codebase for evaluating Large Language Model (LLM) agents on collaborative reasoning tasks, focusing on multi-turn interactions.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "benchmarking",
        "agent_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/sweet_rl",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "llm-agents",
        "benchmark",
        "reasoning"
      ],
      "id": 91
    },
    {
      "name": "rag-arena",
      "one_line_profile": "Open-source RAG evaluation tool using user feedback",
      "detailed_description": "A tool for evaluating Retrieval-Augmented Generation (RAG) systems by leveraging user feedback to compare and benchmark different RAG implementations.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "rag_evaluation",
        "benchmarking"
      ],
      "application_level": "application",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/firecrawl/rag-arena",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "evaluation",
        "arena"
      ],
      "id": 92
    },
    {
      "name": "DHARMa",
      "one_line_profile": "Residual diagnostics for hierarchical (multi-level) regression models",
      "detailed_description": "An R package providing simulation-based residual diagnostics for hierarchical regression models, useful for validating statistical models in ecology and other sciences.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "model_diagnostics",
        "regression_analysis"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/florianhartig/DHARMa",
      "help_website": [
        "https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html"
      ],
      "license": null,
      "tags": [
        "r",
        "statistics",
        "regression",
        "diagnostics"
      ],
      "id": 93
    },
    {
      "name": "traceAI",
      "one_line_profile": "Open Source AI Tracing Framework built on OpenTelemetry",
      "detailed_description": "A framework for tracing AI applications and frameworks, built on OpenTelemetry to provide observability into AI agent execution and performance.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "tracing",
        "observability"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/future-agi/traceAI",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "opentelemetry",
        "tracing",
        "ai-observability"
      ],
      "id": 94
    },
    {
      "name": "langsmith-evaluation-helper",
      "one_line_profile": "Configuration-based evaluation helper for LangSmith",
      "detailed_description": "A helper library that simplifies running evaluations on LangSmith by allowing users to define evaluation configurations in files.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "agent_evaluation",
        "workflow_automation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/gaudiy/langsmith-evaluation-helper",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "langsmith",
        "evaluation",
        "llm-ops"
      ],
      "id": 95
    },
    {
      "name": "langfuse-go (git-hulk)",
      "one_line_profile": "Go SDK for Langfuse AI observability platform",
      "detailed_description": "A Go client library for Langfuse, enabling Go-based AI applications to integrate with the Langfuse observability and evaluation platform.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "tracing",
        "observability"
      ],
      "application_level": "library",
      "primary_language": "Go",
      "repo_url": "https://github.com/git-hulk/langfuse-go",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "langfuse",
        "go",
        "observability"
      ],
      "id": 96
    },
    {
      "name": "mcp-dap-server",
      "one_line_profile": "MCP server for AI Agents to debug live programs via DAP",
      "detailed_description": "A Model Context Protocol (MCP) server that bridges AI agents with Debug Adapter Protocol (DAP) servers, allowing agents to debug code execution dynamically.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "debugging",
        "tool_calling"
      ],
      "application_level": "service",
      "primary_language": "Go",
      "repo_url": "https://github.com/go-delve/mcp-dap-server",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mcp",
        "dap",
        "debugging",
        "ai-agent"
      ],
      "id": 97
    },
    {
      "name": "golf",
      "one_line_profile": "Production-Ready MCP Server Framework for AI agents",
      "detailed_description": "A framework for building, deploying, and scaling Model Context Protocol (MCP) servers, providing infrastructure for secure AI agent tool usage and observability.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "tool_calling",
        "agent_infrastructure"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/golf-mcp/golf",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "mcp",
        "ai-agent",
        "infrastructure"
      ],
      "id": 98
    },
    {
      "name": "rageval",
      "one_line_profile": "Evaluation tools for Retrieval-augmented Generation (RAG) methods",
      "detailed_description": "A toolkit designed to evaluate Retrieval-Augmented Generation (RAG) pipelines, providing metrics and methods to assess retrieval and generation quality.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "rag_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/gomate-community/rageval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "evaluation",
        "metrics"
      ],
      "id": 99
    },
    {
      "name": "AndroidWorld",
      "one_line_profile": "Environment and benchmark for autonomous Android agents",
      "detailed_description": "A research environment and benchmark for evaluating autonomous agents that interact with the Android operating system, enabling reproducible agent research.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "benchmarking",
        "agent_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/google-research/android_world",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "android",
        "agent-benchmark",
        "autonomous-agents"
      ],
      "id": 100
    },
    {
      "name": "Agent Development Kit (Go)",
      "one_line_profile": "Go toolkit for building and evaluating AI agents",
      "detailed_description": "An open-source toolkit for building, evaluating, and deploying sophisticated AI agents in Go, offering flexibility and control over agent workflows.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "agent_development",
        "evaluation"
      ],
      "application_level": "framework",
      "primary_language": "Go",
      "repo_url": "https://github.com/google/adk-go",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ai-agent",
        "go",
        "framework"
      ],
      "id": 101
    },
    {
      "name": "Agent Development Kit (Java)",
      "one_line_profile": "Java toolkit for building and evaluating AI agents",
      "detailed_description": "An open-source toolkit for building, evaluating, and deploying sophisticated AI agents in Java.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "agent_development",
        "evaluation"
      ],
      "application_level": "framework",
      "primary_language": "Java",
      "repo_url": "https://github.com/google/adk-java",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ai-agent",
        "java",
        "framework"
      ],
      "id": 102
    },
    {
      "name": "Agent Development Kit (Python)",
      "one_line_profile": "Python toolkit for building and evaluating AI agents",
      "detailed_description": "An open-source toolkit for building, evaluating, and deploying sophisticated AI agents in Python.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "agent_development",
        "evaluation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/google/adk-python",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ai-agent",
        "python",
        "framework"
      ],
      "id": 103
    },
    {
      "name": "Agent Development Kit Web",
      "one_line_profile": "Developer UI for Agent Development Kit",
      "detailed_description": "A web-based user interface integrated with the Agent Development Kit (ADK) to facilitate agent development, debugging, and evaluation.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "debugging",
        "visualization"
      ],
      "application_level": "application",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/google/adk-web",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ai-agent",
        "ui",
        "debugging"
      ],
      "id": 104
    },
    {
      "name": "GPstuff",
      "one_line_profile": "Gaussian process models for Bayesian analysis",
      "detailed_description": "A toolbox for Gaussian process models, providing tools for Bayesian analysis and inference, widely used in machine learning and statistics.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "bayesian_analysis",
        "modeling"
      ],
      "application_level": "library",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/gpstuff-dev/gpstuff",
      "help_website": [
        "http://research.cs.aalto.fi/pml/software/gpstuff/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "gaussian-processes",
        "bayesian",
        "matlab",
        "statistics"
      ],
      "id": 105
    },
    {
      "name": "rms",
      "one_line_profile": "Regression Modeling Strategies",
      "detailed_description": "An R package by Frank Harrell for regression modeling, testing, estimation, validation, graphics, prediction, and typesetting of results.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "regression_analysis",
        "modeling"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/harrelfe/rms",
      "help_website": [
        "https://hbiostat.org/R/rms/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "r",
        "regression",
        "statistics",
        "modeling"
      ],
      "id": 106
    },
    {
      "name": "prompttools",
      "one_line_profile": "Tools for prompt testing and experimentation with LLMs and vector DBs",
      "detailed_description": "A suite of open-source tools for testing, experimenting with, and evaluating prompts across various LLMs and vector databases.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "prompt_evaluation",
        "experimentation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/hegelai/prompttools",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "prompt-engineering",
        "testing",
        "vector-db"
      ],
      "id": 107
    },
    {
      "name": "langfuse-go (henomis)",
      "one_line_profile": "Langfuse Go SDK",
      "detailed_description": "Another Go SDK implementation for the Langfuse AI observability platform, facilitating tracing and evaluation integration.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "tracing",
        "observability"
      ],
      "application_level": "library",
      "primary_language": "Go",
      "repo_url": "https://github.com/henomis/langfuse-go",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "langfuse",
        "go",
        "sdk"
      ],
      "id": 108
    },
    {
      "name": "Toolathlon",
      "one_line_profile": "Benchmark for Language Agents on diverse tasks",
      "detailed_description": "The Tool Decathlon: A benchmark designed to evaluate language agents on diverse, realistic, and long-horizon task execution capabilities.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "benchmarking",
        "agent_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/hkust-nlp/Toolathlon",
      "help_website": [],
      "license": null,
      "tags": [
        "benchmark",
        "language-agents",
        "tool-use"
      ],
      "id": 109
    },
    {
      "name": "LRAGE",
      "one_line_profile": "Framework for evaluating RAG pipelines in the legal domain",
      "detailed_description": "A specialized framework for evaluating Retrieval-Augmented Generation (RAG) pipelines, with specific adaptations for legal domain requirements.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "rag_evaluation",
        "domain_specific_eval"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/hoorangyee/LRAGE",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "legal-ai",
        "evaluation"
      ],
      "id": 110
    },
    {
      "name": "AI-model-comparison",
      "one_line_profile": "Web tool for side-by-side AI model response comparison",
      "detailed_description": "A web-based tool that allows users to compare responses from different Large Language Models (LLMs) side-by-side to evaluate performance and quality.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "comparison"
      ],
      "application_level": "application",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/hubhubgogo/AI-model-comparison",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "evaluation",
        "comparison-tool"
      ],
      "id": 111
    },
    {
      "name": "lighteval",
      "one_line_profile": "Toolkit for evaluating LLMs across multiple backends",
      "detailed_description": "A comprehensive toolkit by Hugging Face for evaluating Large Language Models (LLMs) across various backends, supporting a wide range of metrics and tasks.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "llm_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/huggingface/lighteval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "huggingface",
        "evaluation",
        "llm"
      ],
      "id": 112
    },
    {
      "name": "dify-eval",
      "one_line_profile": "Automated evaluation service based on Dify and Langfuse",
      "detailed_description": "An automated evaluation service that integrates Dify and Langfuse to provide continuous assessment of AI agent performance.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "agent_evaluation",
        "automation"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/hustyichi/dify-eval",
      "help_website": [],
      "license": null,
      "tags": [
        "dify",
        "langfuse",
        "evaluation"
      ],
      "id": 113
    },
    {
      "name": "ChainForge",
      "one_line_profile": "Visual programming environment for prompt engineering and evaluation",
      "detailed_description": "An open-source visual programming environment designed for battle-testing prompts to Large Language Models (LLMs). It allows users to conduct audit and evaluation of prompt robustness and model responses.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "prompt_engineering",
        "evaluation",
        "testing"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/ianarawjo/ChainForge",
      "help_website": [
        "https://chainforge.ai"
      ],
      "license": "MIT",
      "tags": [
        "prompt-engineering",
        "llm-evaluation",
        "visual-programming"
      ],
      "id": 114
    },
    {
      "name": "ViDoRe Benchmark",
      "one_line_profile": "Benchmark for Vision Document Retrieval evaluation",
      "detailed_description": "The Vision Document Retrieval (ViDoRe) Benchmark provides evaluation code and datasets for assessing the performance of vision-based document retrieval systems, specifically related to the ColPali paper.",
      "domains": [
        "AI5",
        "AI5-04",
        "Computer Vision"
      ],
      "subtask_category": [
        "benchmark",
        "retrieval_evaluation",
        "rag_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/illuin-tech/vidore-benchmark",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "benchmark",
        "document-retrieval",
        "rag"
      ],
      "id": 115
    },
    {
      "name": "Steer",
      "one_line_profile": "Active reliability layer for AI agents",
      "detailed_description": "A framework designed to act as a reliability layer for AI agents, providing capabilities to catch failures, teach fixes, and automate reliability in agentic workflows.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "agent_reliability",
        "error_handling",
        "workflow_automation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/imtt-dev/steer",
      "help_website": [],
      "license": null,
      "tags": [
        "ai-agents",
        "reliability",
        "observability"
      ],
      "id": 116
    },
    {
      "name": "Inference Gateway",
      "one_line_profile": "Unified gateway for multiple LLM providers",
      "detailed_description": "An open-source, cloud-native gateway that unifies access to multiple LLM providers (OpenAI, Anthropic, Ollama, etc.), facilitating model management and inference orchestration for AI applications.",
      "domains": [
        "AI5",
        "Infrastructure"
      ],
      "subtask_category": [
        "inference_serving",
        "model_gateway"
      ],
      "application_level": "service",
      "primary_language": "Go",
      "repo_url": "https://github.com/inference-gateway/inference-gateway",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm-gateway",
        "inference",
        "api-management"
      ],
      "id": 117
    },
    {
      "name": "Probatus",
      "one_line_profile": "SHAP-based validation toolkit for machine learning models",
      "detailed_description": "A Python library for the validation of binary classifiers, multi-class classifiers, and regression models using SHAP (SHapley Additive exPlanations) values to analyze feature importance and model behavior.",
      "domains": [
        "Data Science",
        "AI5-04"
      ],
      "subtask_category": [
        "model_validation",
        "feature_selection",
        "explainability"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ing-bank/probatus",
      "help_website": [
        "https://ing-bank.github.io/probatus/"
      ],
      "license": "MIT",
      "tags": [
        "shap",
        "model-validation",
        "machine-learning"
      ],
      "id": 118
    },
    {
      "name": "Prompt Forge",
      "one_line_profile": "Workbench for prompt engineering and evaluation",
      "detailed_description": "An AI prompt engineering workbench that allows users to craft, test, and systematically evaluate prompts with analysis tools, aiding in the optimization of LLM interactions.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "prompt_engineering",
        "evaluation",
        "testing"
      ],
      "application_level": "platform",
      "primary_language": "Go",
      "repo_url": "https://github.com/insaaniManav/prompt-forge",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "prompt-engineering",
        "llm",
        "workbench"
      ],
      "id": 119
    },
    {
      "name": "Invariant Explorer",
      "one_line_profile": "Tool for testing and inspecting AI Agent traces",
      "detailed_description": "A tool designed for testing, inspecting, and analyzing traces generated by AI agents, helping researchers and developers debug and understand agent behavior.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "tracing",
        "debugging",
        "agent_analysis"
      ],
      "application_level": "tool",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/invariantlabs-ai/explorer",
      "help_website": [
        "https://invariantlabs.ai"
      ],
      "license": "Apache-2.0",
      "tags": [
        "ai-agents",
        "tracing",
        "debugging"
      ],
      "id": 120
    },
    {
      "name": "Invariant Gateway",
      "one_line_profile": "LLM proxy for observing AI agent behavior",
      "detailed_description": "An LLM proxy service that enables observation and debugging of AI agent activities by intercepting and logging interactions with language models.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "observability",
        "tracing",
        "proxy"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/invariantlabs-ai/invariant-gateway",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-proxy",
        "observability",
        "ai-agents"
      ],
      "id": 121
    },
    {
      "name": "GemInsights",
      "one_line_profile": "Dataframe analysis tool using Gemini AI",
      "detailed_description": "A tool that leverages Gemini AI to analyze dataframes, offering insights and replacing traditional manual data analysis methods with AI-driven interpretation.",
      "domains": [
        "Data Science",
        "AI5"
      ],
      "subtask_category": [
        "data_analysis",
        "automated_insights"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/izam-mohammed/GemInsights",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "dataframe",
        "gemini-ai",
        "data-analysis"
      ],
      "id": 122
    },
    {
      "name": "RagRank",
      "one_line_profile": "Toolkit for evaluating RAG and LLM applications",
      "detailed_description": "A free LLM evaluation toolkit designed to assess the accuracy, context understanding, and tone of LLM applications, specifically focusing on RAG (Retrieval-Augmented Generation) systems.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "evaluation",
        "rag_assessment",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/izam-mohammed/ragrank",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "evaluation",
        "llm"
      ],
      "id": 123
    },
    {
      "name": "GraphRAG-Bench",
      "one_line_profile": "Benchmark for evaluating Graph Retrieval-Augmented Generation",
      "detailed_description": "A benchmark suite designed to evaluate Graph Retrieval-Augmented Generation (GraphRAG) systems, focusing on challenging domain-specific reasoning tasks.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "benchmark",
        "rag_evaluation",
        "graph_reasoning"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/jeremycp3/GraphRAG-Bench",
      "help_website": [],
      "license": null,
      "tags": [
        "graph-rag",
        "benchmark",
        "evaluation"
      ],
      "id": 124
    },
    {
      "name": "Ragas (Bioinformatics)",
      "one_line_profile": "R tools for single-cell subclustering analysis",
      "detailed_description": "A set of R tools for the analysis and visualization of single-cell subclustering data, aiding in bioinformatics research.",
      "domains": [
        "Bioinformatics",
        "Life Science"
      ],
      "subtask_category": [
        "single_cell_analysis",
        "clustering",
        "visualization"
      ],
      "application_level": "library",
      "primary_language": "HTML",
      "repo_url": "https://github.com/jig4003/Ragas",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "single-cell",
        "bioinformatics",
        "r"
      ],
      "id": 125
    },
    {
      "name": "Promptspot",
      "one_line_profile": "Tool for testing and managing prompts",
      "detailed_description": "A tool designed to simplify prompt testing, allowing users to organize and evaluate prompts for LLM applications.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "prompt_testing",
        "evaluation"
      ],
      "application_level": "tool",
      "primary_language": "Ruby",
      "repo_url": "https://github.com/jordanful/Promptspot",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "prompt-testing",
        "llm"
      ],
      "id": 126
    },
    {
      "name": "HaELM",
      "one_line_profile": "Automatic MLLM hallucination detection framework",
      "detailed_description": "A framework for automatically detecting hallucinations in Multimodal Large Language Models (MLLMs), providing a method to assess model reliability.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "hallucination_detection",
        "evaluation",
        "mllm"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/junyangwang0410/HaELM",
      "help_website": [],
      "license": null,
      "tags": [
        "hallucination-detection",
        "mllm",
        "evaluation"
      ],
      "id": 127
    },
    {
      "name": "llm-eval",
      "one_line_profile": "Platform for Large Language Model evaluation",
      "detailed_description": "An evaluation platform for Large Language Models supporting multiple benchmarks, custom datasets, and performance testing, including RAG evaluation capabilities.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "evaluation",
        "benchmark",
        "rag_assessment"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/justplus/llm-eval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm-evaluation",
        "benchmark",
        "rag"
      ],
      "id": 128
    },
    {
      "name": "AutoArena",
      "one_line_profile": "Automated head-to-head evaluation for LLMs and RAG",
      "detailed_description": "A tool for ranking LLMs, RAG systems, and prompts using automated head-to-head evaluation methodologies.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "evaluation",
        "ranking",
        "model_comparison"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/kolenaIO/autoarena",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-ranking",
        "evaluation",
        "rag"
      ],
      "id": 129
    },
    {
      "name": "BetterPrompt",
      "one_line_profile": "Test suite for LLM prompts",
      "detailed_description": "A test suite designed for validating and testing LLM prompts to ensure consistent and expected outputs.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "prompt_testing",
        "validation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/krrishdholakia/betterprompt",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "prompt-testing",
        "llm"
      ],
      "id": 130
    },
    {
      "name": "Claude Code Telemetry",
      "one_line_profile": "Telemetry bridge for Claude Code to Langfuse",
      "detailed_description": "A lightweight bridge tool that captures telemetry data from Claude Code and forwards it to Langfuse for visualization and analysis of AI coding agent behavior.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "telemetry",
        "tracing",
        "observability"
      ],
      "application_level": "tool",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/lainra/claude-code-telemetry",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "telemetry",
        "claude",
        "langfuse"
      ],
      "id": 131
    },
    {
      "name": "LangSmith Java SDK",
      "one_line_profile": "Java client library for the LangSmith LLM observability and evaluation platform",
      "detailed_description": "The official Java SDK for interacting with LangSmith, enabling developers to trace, evaluate, and debug LLM-powered applications and agents within Java environments.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "tracing",
        "evaluation"
      ],
      "application_level": "library",
      "primary_language": "Kotlin",
      "repo_url": "https://github.com/langchain-ai/langsmith-java",
      "help_website": [
        "https://docs.smith.langchain.com/"
      ],
      "license": "MIT",
      "tags": [
        "java",
        "sdk",
        "llm-ops",
        "observability"
      ],
      "id": 132
    },
    {
      "name": "LangSmith SDK",
      "one_line_profile": "Python client library for LangSmith observability and evaluation",
      "detailed_description": "The official Python SDK for LangSmith, providing interfaces to log traces, run evaluations, and manage datasets for LLM application development and regression testing.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "tracing",
        "evaluation",
        "dataset_management"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/langchain-ai/langsmith-sdk",
      "help_website": [
        "https://docs.smith.langchain.com/"
      ],
      "license": "MIT",
      "tags": [
        "python",
        "sdk",
        "llm-evaluation",
        "tracing"
      ],
      "id": 133
    },
    {
      "name": "Langfuse",
      "one_line_profile": "Open source LLM engineering platform for observability, metrics, and evaluation",
      "detailed_description": "A comprehensive platform for tracing, evaluating, and managing LLM applications. It provides tools for prompt management, dataset curation, and detailed analytics to improve agent performance and reliability.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "observability",
        "evaluation",
        "prompt_management"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/langfuse/langfuse",
      "help_website": [
        "https://langfuse.com/docs"
      ],
      "license": "MIT",
      "tags": [
        "llm-ops",
        "tracing",
        "evaluation",
        "analytics"
      ],
      "id": 134
    },
    {
      "name": "Langfuse Java SDK",
      "one_line_profile": "Java client for Langfuse API",
      "detailed_description": "The Java SDK for integrating Langfuse observability and evaluation capabilities into Java-based LLM applications.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "observability",
        "tracing"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/langfuse/langfuse-java",
      "help_website": [
        "https://langfuse.com/docs/sdk/java"
      ],
      "license": "MIT",
      "tags": [
        "java",
        "sdk",
        "observability"
      ],
      "id": 135
    },
    {
      "name": "Langfuse JS/TS SDK",
      "one_line_profile": "JavaScript/TypeScript SDK for Langfuse",
      "detailed_description": "The official JavaScript/TypeScript SDK for instrumenting LLM applications with Langfuse tracing and evaluation features.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "observability",
        "tracing"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/langfuse/langfuse-js",
      "help_website": [
        "https://langfuse.com/docs/sdk/typescript"
      ],
      "license": "MIT",
      "tags": [
        "typescript",
        "sdk",
        "observability"
      ],
      "id": 136
    },
    {
      "name": "Langfuse Python SDK",
      "one_line_profile": "Python SDK for Langfuse observability",
      "detailed_description": "The Python SDK for Langfuse, offering decorators and low-level clients to instrument LLM calls, manage prompts, and run evaluations.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "observability",
        "tracing",
        "evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/langfuse/langfuse-python",
      "help_website": [
        "https://langfuse.com/docs/sdk/python"
      ],
      "license": "MIT",
      "tags": [
        "python",
        "sdk",
        "observability"
      ],
      "id": 137
    },
    {
      "name": "Langfuse MCP Server",
      "one_line_profile": "Model Context Protocol server for Langfuse",
      "detailed_description": "An MCP server implementation that allows AI agents to directly access and manage Langfuse prompts and context, facilitating agentic workflows and self-management.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "prompt_management",
        "agent_integration"
      ],
      "application_level": "service",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/langfuse/mcp-server-langfuse",
      "help_website": [
        "https://langfuse.com/"
      ],
      "license": "MIT",
      "tags": [
        "mcp",
        "agent-tool",
        "prompt-engineering"
      ],
      "id": 138
    },
    {
      "name": "LangSmith4j SDK",
      "one_line_profile": "Java client for LangSmith API",
      "detailed_description": "A Java library providing access to the LangSmith API for tracing and evaluating LLM applications, serving as an alternative or experimental SDK for the Java ecosystem.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "tracing",
        "evaluation"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/langgraph4j/langsmith4j-sdk",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "java",
        "sdk",
        "langsmith"
      ],
      "id": 139
    },
    {
      "name": "LangWatch",
      "one_line_profile": "LLM Ops platform for traces, analytics, and evaluations",
      "detailed_description": "An open-source platform for monitoring and evaluating LLM applications. It features tools for tracing execution, analyzing costs and latency, and running evaluations to ensure model quality.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "observability",
        "evaluation",
        "analytics"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/langwatch/langwatch",
      "help_website": [
        "https://docs.langwatch.ai/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "llm-ops",
        "tracing",
        "evaluation"
      ],
      "id": 140
    },
    {
      "name": "Latitude LLM",
      "one_line_profile": "Prompt engineering and evaluation platform",
      "detailed_description": "An open-source platform designed to build, evaluate, and refine prompts using AI. It provides a collaborative environment for managing prompt versions and running evaluations against datasets.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "prompt_engineering",
        "evaluation"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/latitude-dev/latitude-llm",
      "help_website": [
        "https://docs.latitude.so/"
      ],
      "license": "LGPL-3.0",
      "tags": [
        "prompt-engineering",
        "evaluation",
        "collaboration"
      ],
      "id": 141
    },
    {
      "name": "Les Audits Affaires Eval Harness",
      "one_line_profile": "Evaluation harness for French legal LLMs",
      "detailed_description": "A lightweight Python CLI tool for benchmarking French Language Models specifically on business law tasks, evaluating metrics like actionability, delay estimation, and risk assessment.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "evaluation",
        "benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/legml-ai/les-audits-affaires-eval-harness",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "legal-tech",
        "evaluation",
        "llm-benchmark"
      ],
      "id": 142
    },
    {
      "name": "Codex Subagents MCP",
      "one_line_profile": "MCP server for orchestrating coding sub-agents",
      "detailed_description": "A tool that extends the Codex CLI with sub-agents (reviewer, debugger, security) via the Model Context Protocol, enabling isolated execution contexts for regression testing and code verification.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "agent_orchestration",
        "regression_testing"
      ],
      "application_level": "service",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/leonardsellem/codex-subagents-mcp",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mcp",
        "coding-agent",
        "debugging"
      ],
      "id": 143
    },
    {
      "name": "LLMKit",
      "one_line_profile": "Prompt management and evaluation toolkit",
      "detailed_description": "A toolkit and inference server for prompt management, versioning, testing, and evaluation. It offers an OpenAI-compatible API to streamline the development and assessment of LLM applications.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "prompt_management",
        "evaluation",
        "inference"
      ],
      "application_level": "platform",
      "primary_language": "Rust",
      "repo_url": "https://github.com/llmkit-ai/llmkit",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rust",
        "prompt-engineering",
        "evaluation"
      ],
      "id": 144
    },
    {
      "name": "RouteLLM",
      "one_line_profile": "Framework for serving and evaluating LLM routers",
      "detailed_description": "A framework designed to evaluate and serve LLM routers, enabling researchers to optimize the trade-off between cost and quality by dynamically selecting models based on query complexity.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "model_routing",
        "evaluation",
        "optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/lm-sys/RouteLLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-routing",
        "cost-optimization",
        "evaluation"
      ],
      "id": 145
    },
    {
      "name": "Logikon",
      "one_line_profile": "Tool for analyzing and scoring LLM reasoning traces",
      "detailed_description": "A Python tool for analyzing the reasoning capabilities of Large Language Models by scoring their execution traces, helping to evaluate logical consistency and inference quality.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "evaluation",
        "reasoning_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/logikon-ai/logikon",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "reasoning",
        "evaluation",
        "trace-analysis"
      ],
      "id": 146
    },
    {
      "name": "Agent Studio",
      "one_line_profile": "Environments and benchmarks for virtual agents",
      "detailed_description": "A comprehensive platform providing environments, tools, and benchmarks for evaluating general virtual agents. It supports the development and assessment of agents across various tasks and modalities.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "benchmarking",
        "agent_evaluation",
        "environment_simulation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/ltzheng/agent-studio",
      "help_website": [
        "https://arxiv.org/abs/2403.17918"
      ],
      "license": "AGPL-3.0",
      "tags": [
        "agent-benchmark",
        "virtual-environment",
        "evaluation"
      ],
      "id": 147
    },
    {
      "name": "FlowLens MCP Server",
      "one_line_profile": "MCP server for debugging and regression testing of coding agents",
      "detailed_description": "An open-source Model Context Protocol server that provides coding agents with browser context, enabling in-depth debugging and regression testing of web-based tasks and agent behaviors.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "debugging",
        "regression_testing",
        "agent_tool"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/magentic/flowlens-mcp-server",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "mcp",
        "debugging",
        "regression-testing"
      ],
      "id": 148
    },
    {
      "name": "AgC (Open Agentic Compute)",
      "one_line_profile": "Platform for deploying and orchestrating AI agents",
      "detailed_description": "An open-core platform designed as a compute substrate for deploying, running, and orchestrating AI agents at scale, providing necessary infrastructure for agentic workflows.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "agent_orchestration",
        "deployment"
      ],
      "application_level": "platform",
      "primary_language": "Kotlin",
      "repo_url": "https://github.com/masaic-ai-platform/AgC",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "agent-orchestration",
        "compute-platform"
      ],
      "id": 149
    },
    {
      "name": "Mastra",
      "one_line_profile": "TypeScript AI agent framework with observability",
      "detailed_description": "A TypeScript framework for building AI agents, featuring built-in support for RAG, observability, and workflow orchestration, compatible with multiple LLM providers.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "agent_framework",
        "observability",
        "rag"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/mastra-ai/mastra",
      "help_website": [
        "https://mastra.ai/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "typescript",
        "agent-framework",
        "observability"
      ],
      "id": 150
    },
    {
      "name": "Evalite",
      "one_line_profile": "Lightweight evaluation library for LLM apps in TypeScript",
      "detailed_description": "A TypeScript library for evaluating LLM-powered applications, providing tools to define metrics and run assessments on model outputs.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "evaluation"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/mattpocock/evalite",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "typescript",
        "evaluation",
        "llm"
      ],
      "id": 151
    },
    {
      "name": "CoNLI",
      "one_line_profile": "Framework for ungrounded hallucination detection and reduction in LLMs",
      "detailed_description": "CoNLI is a plug-and-play framework designed to detect and reduce ungrounded hallucinations in Large Language Models, enhancing the reliability of AI agents in scientific text generation.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "hallucination_detection",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/CoNLI_hallucination",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "hallucination",
        "llm-evaluation",
        "nlp"
      ],
      "id": 152
    },
    {
      "name": "HaDes",
      "one_line_profile": "Token-level reference-free hallucination detection for LLMs",
      "detailed_description": "HaDes provides a method for detecting hallucinations in generated text at the token level without requiring reference texts, useful for validating scientific content generated by AI.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "hallucination_detection",
        "quality_control"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/HaDes",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "hallucination-detection",
        "llm",
        "verification"
      ],
      "id": 153
    },
    {
      "name": "SmartPlay",
      "one_line_profile": "Benchmark suite for evaluating LLM agent capabilities via games",
      "detailed_description": "SmartPlay is a benchmark designed to test various capabilities of Large Language Models as agents, such as planning and reasoning, which are foundational for scientific agent development.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/SmartPlay",
      "help_website": [],
      "license": "CC-BY-4.0",
      "tags": [
        "benchmark",
        "llm-agents",
        "reasoning"
      ],
      "id": 154
    },
    {
      "name": "TaskWeaver",
      "one_line_profile": "Code-first agent framework for data analytics tasks",
      "detailed_description": "TaskWeaver is an agent framework that seamlessly plans and executes data analytics tasks, interpreting user requests and orchestrating code execution for data processing and analysis.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "data_analysis",
        "workflow_orchestration"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/TaskWeaver",
      "help_website": [
        "https://microsoft.github.io/TaskWeaver/"
      ],
      "license": "MIT",
      "tags": [
        "agent-framework",
        "data-analytics",
        "code-interpreter"
      ],
      "id": 155
    },
    {
      "name": "Windows Agent Arena",
      "one_line_profile": "Scalable platform for benchmarking multi-modal AI agents on OS environments",
      "detailed_description": "Windows Agent Arena (WAA) provides a scalable environment for testing and benchmarking multi-modal AI agents, enabling reproducible evaluation of agent performance in realistic OS interactions.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "benchmarking",
        "agent_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/WindowsAgentArena",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "benchmark",
        "multi-modal-agents",
        "evaluation"
      ],
      "id": 156
    },
    {
      "name": "micronaire",
      "one_line_profile": "RAG evaluation pipeline for Semantic Kernel applications",
      "detailed_description": "Micronaire is a tool designed to evaluate Retrieval-Augmented Generation (RAG) pipelines built with Semantic Kernel, assessing the quality and relevance of retrieved information and generated responses.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "rag_evaluation",
        "pipeline_optimization"
      ],
      "application_level": "library",
      "primary_language": "C#",
      "repo_url": "https://github.com/microsoft/micronaire",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "evaluation",
        "semantic-kernel"
      ],
      "id": 157
    },
    {
      "name": "Promptflow",
      "one_line_profile": "Development tool for building, testing, and evaluating LLM applications",
      "detailed_description": "Promptflow is a suite of development tools designed to streamline the development cycle of LLM-based applications, from prototyping and testing to evaluation and deployment.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "workflow_orchestration",
        "prompt_engineering",
        "evaluation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/promptflow",
      "help_website": [
        "https://microsoft.github.io/promptflow/"
      ],
      "license": "MIT",
      "tags": [
        "llmops",
        "workflow",
        "evaluation"
      ],
      "id": 158
    },
    {
      "name": "PromptPex",
      "one_line_profile": "Test generation tool for LLM prompts",
      "detailed_description": "PromptPex focuses on generating test cases for prompts, aiding in the systematic evaluation and refinement of prompts used in AI applications.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "prompt_testing",
        "test_generation"
      ],
      "application_level": "library",
      "primary_language": "TeX",
      "repo_url": "https://github.com/microsoft/promptpex",
      "help_website": [],
      "license": "CC-BY-4.0",
      "tags": [
        "prompt-engineering",
        "testing",
        "llm"
      ],
      "id": 159
    },
    {
      "name": "Prompty",
      "one_line_profile": "Asset class and format for managing and evaluating LLM prompts",
      "detailed_description": "Prompty provides a standardized format and tooling to create, manage, debug, and evaluate LLM prompts, enhancing observability and portability in AI development.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "prompt_management",
        "observability"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/prompty",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "prompt-engineering",
        "observability",
        "developer-tools"
      ],
      "id": 160
    },
    {
      "name": "RAG Experiment Accelerator",
      "one_line_profile": "Tool for conducting and evaluating RAG experiments",
      "detailed_description": "The RAG Experiment Accelerator facilitates the process of running experiments and evaluations on Retrieval-Augmented Generation patterns, helping optimize search and retrieval strategies.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "rag_optimization",
        "experimentation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/rag-experiment-accelerator",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "rag",
        "experimentation",
        "azure-search"
      ],
      "id": 161
    },
    {
      "name": "RAGTune",
      "one_line_profile": "Tool for tuning and evaluating RAG pipelines",
      "detailed_description": "RAGTune provides capabilities for the tuning and evaluation of Retrieval-Augmented Generation pipelines, aiming to automate the optimization of retrieval strategies.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "rag_tuning",
        "evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/misbahsy/RAGTune",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "optimization",
        "tuning"
      ],
      "id": 162
    },
    {
      "name": "TorchCP",
      "one_line_profile": "Conformal prediction toolbox for PyTorch deep learning models",
      "detailed_description": "TorchCP is a Python toolbox for conformal prediction, providing statistical guarantees of uncertainty for deep learning models, which is critical for scientific reliability.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "uncertainty_quantification",
        "statistical_inference"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ml-stat-Sustech/TorchCP",
      "help_website": [],
      "license": "LGPL-3.0",
      "tags": [
        "conformal-prediction",
        "uncertainty",
        "pytorch"
      ],
      "id": 163
    },
    {
      "name": "EvalScope",
      "one_line_profile": "Framework for large model evaluation and benchmarking",
      "detailed_description": "EvalScope is a customizable framework for evaluating and benchmarking large language models and multi-modal models, supporting efficient performance assessment.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/modelscope/evalscope",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "evaluation",
        "benchmark",
        "llm"
      ],
      "id": 164
    },
    {
      "name": "MyScale Telemetry",
      "one_line_profile": "Observability library for LLM applications",
      "detailed_description": "MyScale Telemetry provides open-source observability solutions for LLM applications, enabling tracing and monitoring of AI agent interactions.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "observability",
        "tracing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/myscale/myscale-telemetry",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "observability",
        "llm",
        "tracing"
      ],
      "id": 165
    },
    {
      "name": "Hallucination Probes",
      "one_line_profile": "Real-time detection of hallucinated entities in long-form generation",
      "detailed_description": "This tool provides probes for detecting hallucinated entities in real-time during long-form text generation, aiding in the verification of AI-generated content.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "hallucination_detection",
        "verification"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/obalcells/hallucination_probes",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "hallucination",
        "nlp",
        "detection"
      ],
      "id": 166
    },
    {
      "name": "LLM Proteomics Hallucination",
      "one_line_profile": "Evaluation framework for hallucination risks in proteomics LLMs",
      "detailed_description": "A systematic evaluation framework designed to detect and benchmark hallucination risks in Large Language Models specifically applied to clinical proteomics and mass spectrometry interpretation.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "domain_specific_evaluation",
        "hallucination_detection"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/olaflaitinen/llm-proteomics-hallucination",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "proteomics",
        "hallucination",
        "bioinformatics"
      ],
      "id": 167
    },
    {
      "name": "VLMEvalKit",
      "one_line_profile": "Evaluation toolkit for large multi-modality models",
      "detailed_description": "VLMEvalKit is an open-source toolkit for evaluating large multi-modality models (LMMs), supporting a wide range of models and benchmarks to assess performance across various tasks.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/open-compass/VLMEvalKit",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "vlm",
        "evaluation",
        "benchmark"
      ],
      "id": 168
    },
    {
      "name": "OpenCompass",
      "one_line_profile": "Comprehensive LLM evaluation platform",
      "detailed_description": "OpenCompass is a platform for evaluating Large Language Models across a wide range of datasets and capabilities, providing a standardized way to benchmark model performance.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/open-compass/opencompass",
      "help_website": [
        "https://opencompass.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm-evaluation",
        "benchmark",
        "nlp"
      ],
      "id": 169
    },
    {
      "name": "OpenAI Evals",
      "one_line_profile": "Framework for evaluating LLMs and LLM systems",
      "detailed_description": "Evals is a framework for evaluating Large Language Models and systems, providing a registry of benchmarks and tools to measure performance on various tasks.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/openai/evals",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "evaluation",
        "llm",
        "benchmark"
      ],
      "id": 170
    },
    {
      "name": "MLE-bench",
      "one_line_profile": "Benchmark for AI agents in machine learning engineering",
      "detailed_description": "MLE-bench is a benchmark designed to measure how well AI agents perform at machine learning engineering tasks, evaluating their ability to act as autonomous researchers/engineers.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "agent_evaluation",
        "benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/openai/mle-bench",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "benchmark",
        "ai-agents",
        "ml-engineering"
      ],
      "id": 171
    },
    {
      "name": "OHR-Bench",
      "one_line_profile": "Benchmark for evaluating OCR impact on RAG systems",
      "detailed_description": "OHR-Bench evaluates the cascading impact of Optical Character Recognition (OCR) quality on Retrieval-Augmented Generation (RAG) systems, critical for processing scientific literature.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "rag_evaluation",
        "ocr_benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/opendatalab/OHR-Bench",
      "help_website": [],
      "license": null,
      "tags": [
        "ocr",
        "rag",
        "benchmark"
      ],
      "id": 172
    },
    {
      "name": "OpenLIT",
      "one_line_profile": "OpenTelemetry-native observability platform for AI engineering",
      "detailed_description": "OpenLIT is an observability platform for AI engineering that provides monitoring, guardrails, and evaluations for LLM applications, integrating with OpenTelemetry.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "observability",
        "monitoring",
        "evaluation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/openlit/openlit",
      "help_website": [
        "https://docs.openlit.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "observability",
        "opentelemetry",
        "llmops"
      ],
      "id": 173
    },
    {
      "name": "Oumi",
      "one_line_profile": "Platform for fine-tuning, evaluating, and deploying open-source LLMs",
      "detailed_description": "Oumi is an open-source platform designed to simplify the lifecycle of Large Language Models (LLMs). It provides tools for fine-tuning, evaluating, and deploying models like Llama, Mistral, and others. It supports various training recipes and evaluation benchmarks, facilitating research into model performance and adaptation.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "fine_tuning"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/oumi-ai/oumi",
      "help_website": [
        "https://oumi.ai/docs"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "fine-tuning",
        "evaluation",
        "inference"
      ],
      "id": 174
    },
    {
      "name": "Voice Devtools",
      "one_line_profile": "Debugging and development tools for realtime voice agents",
      "detailed_description": "Voice Devtools provides a suite of utilities for developers building and debugging real-time voice AI agents. It allows for the inspection of audio streams, latency analysis, and interaction tracing, which are critical for optimizing the performance of voice-based AI systems.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "debugging",
        "tracing"
      ],
      "application_level": "solver",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/outspeed-ai/voice-devtools",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "voice-agent",
        "debugging",
        "realtime",
        "devtools"
      ],
      "id": 175
    },
    {
      "name": "SMAC",
      "one_line_profile": "The StarCraft Multi-Agent Challenge benchmark for reinforcement learning",
      "detailed_description": "SMAC (StarCraft Multi-Agent Challenge) is a benchmark environment for research in Multi-Agent Reinforcement Learning (MARL). It leverages the StarCraft II game engine to create complex scenarios requiring cooperative micromanagement, serving as a standard testbed for evaluating MARL algorithms.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "benchmark",
        "reinforcement_learning"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/oxwhirl/smac",
      "help_website": [
        "https://github.com/oxwhirl/smac"
      ],
      "license": "MIT",
      "tags": [
        "marl",
        "benchmark",
        "starcraft",
        "reinforcement-learning"
      ],
      "id": 176
    },
    {
      "name": "Pallma Guard",
      "one_line_profile": "Security and evaluation framework for AI agents",
      "detailed_description": "Pallma Guard provides lifecycle security tools for AI agents, including proactive red teaming, real-time threat detection, and automated remediation. It serves as an evaluation and hardening tool to ensure the robustness and safety of agentic systems.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "security_evaluation",
        "red_teaming"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/pallma-ai/pallma-guard",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ai-security",
        "red-teaming",
        "agent-evaluation"
      ],
      "id": 177
    },
    {
      "name": "MCP Server GDB",
      "one_line_profile": "Model Context Protocol server for GDB debugging",
      "detailed_description": "This tool implements a Model Context Protocol (MCP) server that exposes GDB (GNU Debugger) capabilities to AI agents. It enables LLMs to perform debugging tasks on C/C++ executables, facilitating automated code analysis and error correction in scientific computing workflows.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "debugging",
        "tool_calling"
      ],
      "application_level": "solver",
      "primary_language": "Rust",
      "repo_url": "https://github.com/pansila/mcp_server_gdb",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mcp",
        "gdb",
        "debugging",
        "agent-tool"
      ],
      "id": 178
    },
    {
      "name": "VideoHallucer",
      "one_line_profile": "Benchmark for hallucination detection in large video-language models",
      "detailed_description": "VideoHallucer is a comprehensive benchmark designed to evaluate and detect hallucinations in Large Video-Language Models (LVLMs). It provides datasets and evaluation metrics to assess the factual consistency of model outputs against video content.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "hallucination_detection",
        "benchmark"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/patrick-tssn/VideoHallucer",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "video-llm",
        "hallucination",
        "benchmark",
        "evaluation"
      ],
      "id": 179
    },
    {
      "name": "OpenTelemetry MCP Server",
      "one_line_profile": "MCP Server enabling LLMs to query OpenTelemetry data",
      "detailed_description": "This tool provides a Model Context Protocol (MCP) server that connects LLMs to OpenTelemetry data sources. It allows AI agents to retrieve and analyze telemetry data (traces, metrics, logs), facilitating automated observability and debugging of complex systems.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "observability",
        "tool_calling"
      ],
      "application_level": "solver",
      "primary_language": "Go",
      "repo_url": "https://github.com/pavolloffay/opentelemetry-mcp-server",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "opentelemetry",
        "mcp",
        "llm-agent",
        "observability"
      ],
      "id": 180
    },
    {
      "name": "Dify Promptfoo",
      "one_line_profile": "Integration tool to evaluate Dify assistants using Promptfoo",
      "detailed_description": "This utility bridges Dify (an LLM app development platform) and Promptfoo (an evaluation tool), enabling developers to run automated evaluations and red-teaming tests on Dify-based AI assistants.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "integration"
      ],
      "application_level": "solver",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/perzeuss/dify-promptfoo",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "dify",
        "promptfoo",
        "evaluation",
        "llm-ops"
      ],
      "id": 181
    },
    {
      "name": "Pezzo",
      "one_line_profile": "LLMOps platform for prompt management, observability, and evaluation",
      "detailed_description": "Pezzo is an open-source LLMOps platform that provides tools for prompt design, version control, and observability. It includes features for tracing LLM executions and troubleshooting issues, supporting the development and evaluation of reliable AI applications.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "llm_ops",
        "observability",
        "prompt_management"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/pezzolabs/pezzo",
      "help_website": [
        "https://docs.pezzo.ai"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llmops",
        "observability",
        "prompt-engineering",
        "evaluation"
      ],
      "id": 182
    },
    {
      "name": "SelfCheckGPT",
      "one_line_profile": "Zero-resource black-box hallucination detection for LLMs",
      "detailed_description": "SelfCheckGPT is a tool for detecting hallucinations in Generative Large Language Models. It uses a zero-resource, black-box approach to assess the factual consistency of generated text by checking for internal consistency across multiple sampled responses.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "hallucination_detection",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/potsawee/selfcheckgpt",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "hallucination",
        "llm",
        "evaluation",
        "consistency-check"
      ],
      "id": 183
    },
    {
      "name": "Promptimize",
      "one_line_profile": "Prompt engineering evaluation and testing toolkit",
      "detailed_description": "Promptimize is a toolkit designed for evaluating and testing prompt engineering workflows. It allows developers to systematically test prompts against various models and datasets to optimize performance and ensure reliability.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "prompt_evaluation",
        "testing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/preset-io/promptimize",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "prompt-engineering",
        "evaluation",
        "testing"
      ],
      "id": 184
    },
    {
      "name": "Prometheus Eval",
      "one_line_profile": "LLM response evaluation using Prometheus and GPT-4",
      "detailed_description": "Prometheus Eval is a library for evaluating Large Language Model responses. It leverages the Prometheus model (and others like GPT-4) to provide fine-grained feedback and scoring on generated text, aiding in the assessment of model quality.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "scoring"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/prometheus-eval/prometheus-eval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-evaluation",
        "prometheus",
        "feedback"
      ],
      "id": 185
    },
    {
      "name": "PS Fuzz",
      "one_line_profile": "System prompt hardening and security testing tool",
      "detailed_description": "PS Fuzz is a security tool designed to test and harden system prompts for GenAI applications. It performs fuzzing and red-teaming to identify vulnerabilities and improve the robustness of prompt defenses.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "security_testing",
        "prompt_hardening"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/prompt-security/ps-fuzz",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "security",
        "fuzzing",
        "prompt-injection",
        "red-teaming"
      ],
      "id": 186
    },
    {
      "name": "Promptfoo",
      "one_line_profile": "CLI tool for testing and evaluating LLM prompts and agents",
      "detailed_description": "Promptfoo is a CLI tool and library for testing, evaluating, and red-teaming LLM prompts, agents, and RAG pipelines. It supports comparing performance across multiple models (GPT, Claude, Llama) using declarative configuration files, making it essential for systematic AI research and development.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "red_teaming",
        "testing"
      ],
      "application_level": "solver",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/promptfoo/promptfoo",
      "help_website": [
        "https://www.promptfoo.dev/"
      ],
      "license": "MIT",
      "tags": [
        "llm-eval",
        "red-teaming",
        "prompt-testing",
        "cli"
      ],
      "id": 187
    },
    {
      "name": "VMAS",
      "one_line_profile": "Vectorized Multi-Agent Simulator for Reinforcement Learning",
      "detailed_description": "VMAS (Vectorized Multi-Agent Simulator) is a differentiable, vectorized physics engine and simulator designed for efficient benchmarking of Multi-Agent Reinforcement Learning (MARL) algorithms. It runs on PyTorch, enabling high-throughput simulation for research.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "simulation",
        "reinforcement_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/proroklab/VectorizedMultiAgentSimulator",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "marl",
        "simulator",
        "pytorch",
        "physics-engine"
      ],
      "id": 188
    },
    {
      "name": "RagaAI Catalyst",
      "one_line_profile": "SDK for Agent AI observability, monitoring, and evaluation",
      "detailed_description": "RagaAI Catalyst is a Python SDK and framework for the observability, monitoring, and evaluation of AI agents. It provides features for tracing LLM and tool interactions, debugging multi-agent systems, and analyzing execution graphs.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "observability",
        "agent_evaluation",
        "tracing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/raga-ai-hub/RagaAI-Catalyst",
      "help_website": [
        "https://docs.raga.ai"
      ],
      "license": "Apache-2.0",
      "tags": [
        "agent-observability",
        "llm-tracing",
        "evaluation"
      ],
      "id": 189
    },
    {
      "name": "Raga LLM Hub",
      "one_line_profile": "Framework for LLM evaluation, guardrails, and security",
      "detailed_description": "Raga LLM Hub is a framework designed for the evaluation and security of Large Language Models. It includes tools for setting up guardrails and assessing model performance against safety and quality metrics.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "security_guardrails"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/raga-ai-hub/raga-llm-hub",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "llm-eval",
        "guardrails",
        "security"
      ],
      "id": 190
    },
    {
      "name": "Agent-o-rama",
      "one_line_profile": "End-to-end LLM agent platform for Java and Clojure",
      "detailed_description": "Agent-o-rama is a platform for building, tracing, testing, and monitoring LLM agents in Java and Clojure. It provides integrated storage and deployment capabilities, facilitating the development and evaluation of agentic workflows.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "agent_platform",
        "tracing",
        "testing"
      ],
      "application_level": "platform",
      "primary_language": "Clojure",
      "repo_url": "https://github.com/redplanetlabs/agent-o-rama",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-agent",
        "clojure",
        "tracing",
        "platform"
      ],
      "id": 191
    },
    {
      "name": "Continuous Eval",
      "one_line_profile": "Data-driven evaluation framework for LLM applications",
      "detailed_description": "Continuous Eval is a framework for the systematic evaluation of LLM-powered applications. It provides metrics and pipelines to assess the quality of retrieval (RAG) and generation, enabling data-driven improvements in AI systems.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "rag_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/relari-ai/continuous-eval",
      "help_website": [
        "https://docs.relari.ai"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm-eval",
        "rag",
        "metrics"
      ],
      "id": 192
    },
    {
      "name": "Regenie",
      "one_line_profile": "Whole genome regression modelling for large-scale GWAS",
      "detailed_description": "Regenie is a C++ program for whole genome regression modeling. It is designed for large-scale genome-wide association studies (GWAS), capable of handling hundreds of thousands of samples and millions of variants efficiently.",
      "domains": [
        "AI5",
        "Bioinformatics"
      ],
      "subtask_category": [
        "regression_modeling",
        "gwas"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/rgcgithub/regenie",
      "help_website": [
        "https://rgcgithub.github.io/regenie/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "gwas",
        "genetics",
        "regression",
        "bioinformatics"
      ],
      "id": 193
    },
    {
      "name": "Auto Evaluator",
      "one_line_profile": "Evaluation tool for LLM QA chains",
      "detailed_description": "Auto Evaluator is a tool designed to assess the performance of LLM-based Question Answering (QA) chains. It automates the process of generating test cases and scoring responses, helping researchers optimize RAG pipelines.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "qa_evaluation",
        "rag_testing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/rlancemartin/auto-evaluator",
      "help_website": [],
      "license": null,
      "tags": [
        "llm-qa",
        "evaluation",
        "rag"
      ],
      "id": 194
    },
    {
      "name": "SISSO",
      "one_line_profile": "Data-driven symbolic regression method for materials science",
      "detailed_description": "A code implementing the SISSO (Sure Independence Screening and Sparsifying Operator) method, which combines symbolic regression and compressed sensing to discover accurate and interpretable physical models/descriptors from data.",
      "domains": [
        "Materials Science",
        "Physics",
        "AI4S"
      ],
      "subtask_category": [
        "symbolic_regression",
        "feature_selection",
        "scientific_modeling"
      ],
      "application_level": "solver",
      "primary_language": "Fortran",
      "repo_url": "https://github.com/rouyang2017/SISSO",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "symbolic-regression",
        "materials-science",
        "compressed-sensing"
      ],
      "id": 195
    },
    {
      "name": "sctransform",
      "one_line_profile": "Normalization and variance stabilization for single-cell UMI data",
      "detailed_description": "An R package for modeling single-cell UMI expression data using regularized negative binomial regression. It is widely used in single-cell genomics for data normalization and variance stabilization.",
      "domains": [
        "Bioinformatics",
        "Genomics"
      ],
      "subtask_category": [
        "normalization",
        "variance_stabilization",
        "single_cell_analysis"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/satijalab/sctransform",
      "help_website": [
        "https://github.com/satijalab/sctransform"
      ],
      "license": "GPL-3.0",
      "tags": [
        "single-cell",
        "normalization",
        "bioinformatics"
      ],
      "id": 196
    },
    {
      "name": "multiagent_mujoco",
      "one_line_profile": "Benchmark for Continuous Multi-Agent Robotic Control",
      "detailed_description": "A benchmark environment for continuous multi-agent robotic control based on OpenAI's MuJoCo physics engine. It enables research into multi-agent reinforcement learning in physically simulated environments.",
      "domains": [
        "Robotics",
        "Physics Simulation",
        "AI5"
      ],
      "subtask_category": [
        "simulation",
        "multi_agent_control",
        "benchmark"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/schroederdewitt/multiagent_mujoco",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "mujoco",
        "multi-agent",
        "robotics",
        "simulation"
      ],
      "id": 197
    },
    {
      "name": "OpenCE",
      "one_line_profile": "Toolkit for evaluating LLM context strategies",
      "detailed_description": "Open Context Engineering (OpenCE) is a community toolkit to implement, evaluate, and combine LLM context strategies such as RAG, ACE, and Compression. It supports the scientific evaluation of LLM performance in context-heavy tasks.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "evaluation",
        "context_engineering",
        "rag_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/sci-m-wang/OpenCE",
      "help_website": [],
      "license": null,
      "tags": [
        "llm-evaluation",
        "rag",
        "context-engineering"
      ],
      "id": 198
    },
    {
      "name": "frai",
      "one_line_profile": "Toolkit for responsible AI evaluation and documentation",
      "detailed_description": "An open-source toolkit for responsible AI that provides CLI and SDK tools to scan code, collect evidence, and generate model cards, risk files, and evaluations, supporting the governance of AI research.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "responsible_ai",
        "evaluation",
        "documentation"
      ],
      "application_level": "solver",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/sebuzdugan/frai",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "responsible-ai",
        "evaluation",
        "model-cards"
      ],
      "id": 199
    },
    {
      "name": "funcchain",
      "one_line_profile": "Pythonic framework for building cognitive systems",
      "detailed_description": "A pythonic framework for building cognitive systems and AI agents. It facilitates the creation of complex workflows and agentic behaviors, serving as a foundational tool for AI research agents.",
      "domains": [
        "AI5"
      ],
      "subtask_category": [
        "agent_framework",
        "workflow_orchestration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/shroominic/funcchain",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ai-agents",
        "pythonic",
        "cognitive-systems"
      ],
      "id": 200
    },
    {
      "name": "tau2-bench",
      "one_line_profile": "Benchmark for evaluating conversational agents",
      "detailed_description": "τ²-Bench is a benchmark for evaluating conversational agents in a dual-control environment, focusing on the agent's ability to handle complex interactions and control tasks.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "evaluation",
        "benchmark",
        "conversational_agents"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/sierra-research/tau2-bench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "benchmark",
        "conversational-agents",
        "evaluation"
      ],
      "id": 201
    },
    {
      "name": "sim",
      "one_line_profile": "Platform to build and deploy AI agent workflows",
      "detailed_description": "An open-source platform designed to build, deploy, and manage AI agent workflows. It provides the infrastructure for orchestrating complex agent interactions.",
      "domains": [
        "AI5"
      ],
      "subtask_category": [
        "workflow_orchestration",
        "agent_deployment",
        "platform"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/simstudioai/sim",
      "help_website": [
        "https://simstudio.ai"
      ],
      "license": "Apache-2.0",
      "tags": [
        "ai-agents",
        "workflow",
        "deployment"
      ],
      "id": 202
    },
    {
      "name": "SonAgent",
      "one_line_profile": "Self-repairing autonomous agent framework",
      "detailed_description": "A self-repairing autonomous agent framework that uses Large Language Models (LLM) for code generation, self-editing, and self-debugging, enabling resilient agentic workflows.",
      "domains": [
        "AI5"
      ],
      "subtask_category": [
        "autonomous_agent",
        "self_repair",
        "code_generation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/sonnhfit/SonAgent",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "autonomous-agents",
        "self-repair",
        "llm"
      ],
      "id": 203
    },
    {
      "name": "spring-ai-alibaba-admin",
      "one_line_profile": "Admin console for Spring AI Alibaba agents",
      "detailed_description": "A console supporting tracing, prompt engineering, and evaluation for AI agents developed with Spring AI Alibaba. It provides observability and management capabilities for agent workflows.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "tracing",
        "evaluation",
        "prompt_engineering"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/spring-ai-alibaba/spring-ai-alibaba-admin",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "observability",
        "tracing",
        "spring-ai"
      ],
      "id": 204
    },
    {
      "name": "rstanarm",
      "one_line_profile": "Bayesian applied regression modeling via Stan",
      "detailed_description": "An R package that provides an interface to Stan for Bayesian applied regression modeling. It allows users to specify models using standard R formula syntax and estimates them using Bayesian inference.",
      "domains": [
        "Statistics",
        "Data Science",
        "AI4S"
      ],
      "subtask_category": [
        "bayesian_inference",
        "regression_modeling",
        "statistical_analysis"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/stan-dev/rstanarm",
      "help_website": [
        "https://mc-stan.org/rstanarm/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "bayesian",
        "regression",
        "stan",
        "statistics"
      ],
      "id": 205
    },
    {
      "name": "HELM",
      "one_line_profile": "Holistic Evaluation of Language Models",
      "detailed_description": "A framework for the holistic evaluation of foundation models, including LLMs and multimodal models. It provides a standardized way to assess models across a wide range of metrics and scenarios.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking",
        "metrics_calculation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/stanford-crfm/helm",
      "help_website": [
        "https://crfm.stanford.edu/helm/latest/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm-evaluation",
        "benchmark",
        "foundation-models"
      ],
      "id": 206
    },
    {
      "name": "ARES",
      "one_line_profile": "Automated Evaluation of RAG Systems",
      "detailed_description": "A tool for the automated evaluation of Retrieval-Augmented Generation (RAG) systems. It helps researchers and developers assess the quality and accuracy of RAG pipelines.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "rag_evaluation",
        "automated_testing",
        "metrics"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/stanford-futuredata/ARES",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "evaluation",
        "automated-testing"
      ],
      "id": 207
    },
    {
      "name": "MedAgentBench",
      "one_line_profile": "Benchmark for Medical LLM Agents",
      "detailed_description": "A realistic virtual Electronic Health Record (EHR) environment designed to benchmark Medical LLM Agents. It evaluates the performance of agents in clinical scenarios.",
      "domains": [
        "AI5",
        "AI5-04",
        "Medicine"
      ],
      "subtask_category": [
        "benchmark",
        "medical_ai",
        "agent_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/stanfordmlgroup/MedAgentBench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "medical-ai",
        "benchmark",
        "ehr"
      ],
      "id": 208
    },
    {
      "name": "Universal_Head_3DMM",
      "one_line_profile": "Complete 3D morphable model of the human head",
      "detailed_description": "A project providing a comprehensive 3D Morphable Model (3DMM) of the human head, including code and models for generating and manipulating 3D head structures, useful in computer vision and graphics research.",
      "domains": [
        "Computer Vision",
        "Graphics",
        "AI4S"
      ],
      "subtask_category": [
        "3d_modeling",
        "morphable_model",
        "face_reconstruction"
      ],
      "application_level": "solver",
      "primary_language": null,
      "repo_url": "https://github.com/steliosploumpis/Universal_Head_3DMM",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "3dmm",
        "face-model",
        "computer-vision"
      ],
      "id": 209
    },
    {
      "name": "rag-genie",
      "one_line_profile": "LLM RAG prototype for testing and evaluation",
      "detailed_description": "A prototype tool to test and evaluate embeddings and chunk splitting strategies for RAG systems using Q&A and evaluations. It aids in optimizing RAG pipelines.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "rag_evaluation",
        "embedding_testing",
        "chunking_strategy"
      ],
      "application_level": "solver",
      "primary_language": "Java",
      "repo_url": "https://github.com/stephanj/rag-genie",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "evaluation",
        "embeddings"
      ],
      "id": 210
    },
    {
      "name": "betterprompt",
      "one_line_profile": "Test suite for LLM prompts",
      "detailed_description": "A testing framework designed specifically for LLM prompts. It allows developers to create test suites to verify the behavior and output quality of prompts before deployment.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "prompt_testing",
        "evaluation",
        "quality_assurance"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/stjordanis/betterprompt",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "prompt-engineering",
        "testing",
        "llm"
      ],
      "id": 211
    },
    {
      "name": "ggeffects",
      "one_line_profile": "R package for computing and visualizing estimated marginal means and effects from regression models",
      "detailed_description": "A comprehensive R package designed to compute, visualize, and interpret marginal effects, adjusted predictions, and estimated marginal means from a wide variety of regression models. It facilitates the interpretation of complex statistical models through visualization.",
      "domains": [
        "Statistics",
        "Data Visualization"
      ],
      "subtask_category": [
        "statistical_analysis",
        "visualization"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/strengejacke/ggeffects",
      "help_website": [
        "https://strengejacke.github.io/ggeffects/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "r",
        "statistics",
        "visualization",
        "regression",
        "marginal-effects"
      ],
      "id": 212
    },
    {
      "name": "AgentTrace",
      "one_line_profile": "Lightweight observability library to trace and evaluate agentic systems",
      "detailed_description": "A lightweight observability and evaluation library designed specifically for agentic systems. It allows researchers and developers to trace agent execution flows and evaluate performance metrics, facilitating the analysis of complex AI agent behaviors.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "observability",
        "evaluation",
        "tracing"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/tensorstax/agenttrace",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "observability",
        "agents",
        "evaluation",
        "tracing"
      ],
      "id": 213
    },
    {
      "name": "TensorZero",
      "one_line_profile": "Unified platform for LLM observability, optimization, and evaluation",
      "detailed_description": "An industrial-grade open-source stack for Large Language Model applications that unifies gateway functionality, observability, optimization, evaluation, and experimentation. It supports scientific workflows in model performance analysis and optimization.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "observability",
        "evaluation",
        "optimization",
        "experimentation"
      ],
      "application_level": "platform",
      "primary_language": "Rust",
      "repo_url": "https://github.com/tensorzero/tensorzero",
      "help_website": [
        "https://www.tensorzero.com/docs"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "observability",
        "evaluation",
        "optimization",
        "gateway"
      ],
      "id": 214
    },
    {
      "name": "Safety-Prompts",
      "one_line_profile": "Chinese safety prompts dataset for evaluating LLM safety",
      "detailed_description": "A comprehensive dataset of Chinese safety prompts designed for evaluating and improving the safety of Large Language Models. It covers various risk scenarios including synthetic, adversarial, and in-the-wild prompts.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "evaluation",
        "safety_testing",
        "dataset"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/thu-coai/Safety-Prompts",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "safety",
        "evaluation",
        "prompts",
        "chinese"
      ],
      "id": 215
    },
    {
      "name": "AgentQL",
      "one_line_profile": "Query language and tooling for AI agent web data extraction",
      "detailed_description": "A suite of tools enabling AI agents to interact with the web and extract data precisely. It features a specialized query language and Playwright integrations, facilitating scientific data collection and environment interaction for agents.",
      "domains": [
        "AI5",
        "Data Acquisition"
      ],
      "subtask_category": [
        "data_extraction",
        "web_automation",
        "query_language"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/tinyfish-io/agentql",
      "help_website": [
        "https://docs.agentql.com/"
      ],
      "license": "MIT",
      "tags": [
        "web-scraping",
        "agents",
        "data-extraction",
        "playwright"
      ],
      "id": 216
    },
    {
      "name": "caret",
      "one_line_profile": "Classification and Regression Training package for R",
      "detailed_description": "A comprehensive framework for building classification and regression models in R. It contains tools for data splitting, pre-processing, feature selection, model tuning using resampling, and variable importance estimation.",
      "domains": [
        "Machine Learning",
        "Statistics"
      ],
      "subtask_category": [
        "machine_learning",
        "model_training",
        "regression",
        "classification"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/topepo/caret",
      "help_website": [
        "http://topepo.github.io/caret/index.html"
      ],
      "license": "NOASSERTION",
      "tags": [
        "r",
        "machine-learning",
        "classification",
        "regression",
        "training"
      ],
      "id": 217
    },
    {
      "name": "go-openllmetry",
      "one_line_profile": "OpenTelemetry-based observability for LLM applications in Go",
      "detailed_description": "An open-source observability library for Large Language Model applications written in Go, based on OpenTelemetry standards. It enables tracing and monitoring of LLM interactions for performance analysis and evaluation.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "observability",
        "tracing",
        "monitoring"
      ],
      "application_level": "library",
      "primary_language": "Go",
      "repo_url": "https://github.com/traceloop/go-openllmetry",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "observability",
        "opentelemetry",
        "llm",
        "go",
        "tracing"
      ],
      "id": 218
    },
    {
      "name": "Traceloop Hub",
      "one_line_profile": "High-scale LLM gateway with built-in observability",
      "detailed_description": "A high-performance gateway for Large Language Models written in Rust, featuring built-in OpenTelemetry-based observability. It serves as infrastructure for managing, tracing, and analyzing LLM traffic in research and production environments.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "observability",
        "gateway",
        "tracing"
      ],
      "application_level": "platform",
      "primary_language": "Rust",
      "repo_url": "https://github.com/traceloop/hub",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "gateway",
        "observability",
        "rust",
        "opentelemetry"
      ],
      "id": 219
    },
    {
      "name": "OpenLLMetry",
      "one_line_profile": "Open-source observability for GenAI and LLM applications",
      "detailed_description": "A comprehensive observability SDK for Generative AI and Large Language Model applications, built on OpenTelemetry. It provides tracing, metrics, and evaluation capabilities to analyze model performance and behavior.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "observability",
        "tracing",
        "evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/traceloop/openllmetry",
      "help_website": [
        "https://www.traceloop.com/docs/openllmetry"
      ],
      "license": "Apache-2.0",
      "tags": [
        "observability",
        "opentelemetry",
        "llm",
        "genai",
        "tracing"
      ],
      "id": 220
    },
    {
      "name": "OpenLLMetry-JS",
      "one_line_profile": "OpenTelemetry-based observability for LLM applications in TypeScript",
      "detailed_description": "The TypeScript implementation of OpenLLMetry, providing open-source observability for Large Language Model applications based on OpenTelemetry standards. It facilitates tracing and performance analysis for JS/TS-based AI systems.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "observability",
        "tracing",
        "monitoring"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/traceloop/openllmetry-js",
      "help_website": [
        "https://www.traceloop.com/docs/openllmetry"
      ],
      "license": "Apache-2.0",
      "tags": [
        "observability",
        "opentelemetry",
        "llm",
        "typescript",
        "tracing"
      ],
      "id": 221
    },
    {
      "name": "OpenTelemetry MCP Server",
      "one_line_profile": "MCP server for querying OpenTelemetry traces by AI agents",
      "detailed_description": "A Model Context Protocol (MCP) server that enables AI agents to query and analyze distributed traces from various backends (Jaeger, Tempo, Traceloop). It allows agents to perform automated debugging and root cause analysis.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "trace_analysis",
        "automated_debugging",
        "observability"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/traceloop/opentelemetry-mcp-server",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "mcp",
        "opentelemetry",
        "agents",
        "tracing",
        "debugging"
      ],
      "id": 222
    },
    {
      "name": "TransformerLab",
      "one_line_profile": "Application for LLM and Diffusion model engineering and evaluation",
      "detailed_description": "An open-source application designed for advanced engineering of Large Language Models and Diffusion models. It supports workflows for interaction, training, fine-tuning, and evaluation of models on local hardware.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "model_training",
        "fine_tuning",
        "evaluation",
        "inference"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/transformerlab/transformerlab-app",
      "help_website": [
        "https://transformerlab.ai/"
      ],
      "license": "AGPL-3.0",
      "tags": [
        "llm",
        "diffusion",
        "fine-tuning",
        "evaluation",
        "training"
      ],
      "id": 223
    },
    {
      "name": "TruLens",
      "one_line_profile": "Evaluation and tracking for LLM experiments and AI agents",
      "detailed_description": "A tool for evaluating and tracking Large Language Model experiments and AI agents. It provides feedback functions to assess the quality of inputs, outputs, and intermediate steps, enabling systematic improvement of AI applications.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "evaluation",
        "experiment_tracking",
        "quality_assessment"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/truera/trulens",
      "help_website": [
        "https://www.trulens.org/"
      ],
      "license": "MIT",
      "tags": [
        "llm",
        "evaluation",
        "agents",
        "tracking",
        "observability"
      ],
      "id": 224
    },
    {
      "name": "CUA (Computer-Use Agents)",
      "one_line_profile": "Infrastructure and benchmarks for evaluating Computer-Use Agents",
      "detailed_description": "An open-source infrastructure providing sandboxes, SDKs, and benchmarks to train and evaluate AI agents capable of controlling full desktop environments (macOS, Linux, Windows). It facilitates rigorous testing of agentic capabilities.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "evaluation",
        "benchmarking",
        "training_environment"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/trycua/cua",
      "help_website": [
        "https://docs.trycua.com/"
      ],
      "license": "MIT",
      "tags": [
        "agents",
        "evaluation",
        "benchmarking",
        "computer-use",
        "sandbox"
      ],
      "id": 225
    },
    {
      "name": "Orbit",
      "one_line_profile": "Bayesian forecasting with object-oriented design",
      "detailed_description": "A Python package for Bayesian forecasting that uses an object-oriented design and probabilistic models. It provides a flexible interface for specifying and fitting Bayesian time series models for scientific and industrial forecasting tasks.",
      "domains": [
        "Statistics",
        "Time Series Analysis"
      ],
      "subtask_category": [
        "forecasting",
        "bayesian_modeling",
        "time_series"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/uber/orbit",
      "help_website": [
        "https://orbit-ml.readthedocs.io/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "forecasting",
        "bayesian",
        "time-series",
        "statistics",
        "python"
      ],
      "id": 226
    },
    {
      "name": "Unify",
      "one_line_profile": "Centralized platform for AI model observability and benchmarking",
      "detailed_description": "A platform designed for AI observability, providing tools to benchmark and monitor the performance of various AI models. It acts as a central hub for comparing model metrics and optimizing selection.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "observability",
        "benchmarking",
        "model_selection"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/unifyai/unify",
      "help_website": [
        "https://unify.ai/docs/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "observability",
        "benchmarking",
        "llm",
        "ai"
      ],
      "id": 227
    },
    {
      "name": "Jailjudge",
      "one_line_profile": "Benchmark for evaluating LLM safety against malicious prompts",
      "detailed_description": "A comprehensive evaluation benchmark for assessing the safety of Large Language Models. It includes a wide range of risk scenarios with complex malicious prompts (synthetic, adversarial, multi-language) and high-quality human-annotated datasets.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "evaluation",
        "safety_benchmarking",
        "dataset"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/usail-hkust/Jailjudge",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "safety",
        "benchmark",
        "evaluation",
        "adversarial"
      ],
      "id": 228
    },
    {
      "name": "Open RAG Eval",
      "one_line_profile": "Reference-free evaluation tool for RAG systems",
      "detailed_description": "A tool for evaluating Retrieval-Augmented Generation (RAG) systems without the need for 'golden answers' (ground truth). It utilizes various metrics to assess the quality of retrieved context and generated responses.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "evaluation",
        "rag_assessment",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/vectara/open-rag-eval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "evaluation",
        "llm",
        "metrics"
      ],
      "id": 229
    },
    {
      "name": "Ragas",
      "one_line_profile": "Evaluation framework for Retrieval Augmented Generation (RAG) pipelines",
      "detailed_description": "A framework that provides metrics and tools to evaluate the performance of RAG pipelines, focusing on retrieval and generation quality.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "evaluation",
        "quality_control"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/vibrantlabsai/ragas",
      "help_website": [
        "https://docs.ragas.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "evaluation",
        "llm",
        "metrics"
      ],
      "id": 230
    },
    {
      "name": "vllora",
      "one_line_profile": "Debugging and tracing tool for AI agents",
      "detailed_description": "A tool designed to help developers debug and trace the execution of AI agents, providing insights into their decision-making processes.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "tracing",
        "debugging"
      ],
      "application_level": "solver",
      "primary_language": "Rust",
      "repo_url": "https://github.com/vllora/vllora",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "ai-agent",
        "debugging",
        "tracing"
      ],
      "id": 231
    },
    {
      "name": "Weave",
      "one_line_profile": "Toolkit for tracking, tracing, and evaluating AI applications",
      "detailed_description": "A toolkit by Weights & Biases for developing, debugging, and evaluating LLM-based applications, offering tracing and versioning capabilities.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "tracing",
        "evaluation",
        "versioning"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/wandb/weave",
      "help_website": [
        "https://wandb.me/weave"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "tracing",
        "evaluation",
        "observability"
      ],
      "id": 232
    },
    {
      "name": "VisualWebArena",
      "one_line_profile": "Benchmark for evaluating multimodal agents on web tasks",
      "detailed_description": "A benchmark designed to evaluate the performance of multimodal agents in realistic web environments, assessing their ability to perform complex tasks.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "benchmarking",
        "evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/web-arena-x/visualwebarena",
      "help_website": [
        "https://jykoh.com/vwa"
      ],
      "license": "MIT",
      "tags": [
        "benchmark",
        "multimodal-agents",
        "web-agents"
      ],
      "id": 233
    },
    {
      "name": "LangKit",
      "one_line_profile": "Toolkit for monitoring and extracting signals from LLM text",
      "detailed_description": "An open-source toolkit for monitoring Large Language Models, extracting signals related to text quality, relevance, and safety from prompts and responses.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "monitoring",
        "quality_control",
        "safety_check"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/whylabs/langkit",
      "help_website": [
        "https://github.com/whylabs/langkit"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm-monitoring",
        "observability",
        "metrics"
      ],
      "id": 234
    },
    {
      "name": "AgentTrek",
      "one_line_profile": "Tool for synthesizing agent trajectories using web tutorials",
      "detailed_description": "A tool for synthesizing agent trajectories by guiding replay with web tutorials, useful for generating training data for agents.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "data_generation",
        "simulation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/xlang-ai/AgentTrek",
      "help_website": [],
      "license": null,
      "tags": [
        "agent-training",
        "trajectory-synthesis",
        "data-generation"
      ],
      "id": 235
    },
    {
      "name": "OSWorld",
      "one_line_profile": "Benchmark environment for multimodal agents in real computer OS",
      "detailed_description": "A benchmark for evaluating multimodal agents on open-ended tasks within real computer environments, providing a realistic OS interface.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "benchmarking",
        "evaluation",
        "simulation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/xlang-ai/OSWorld",
      "help_website": [
        "https://os-world.github.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "benchmark",
        "multimodal-agents",
        "os-environment"
      ],
      "id": 236
    },
    {
      "name": "OSWorld-G",
      "one_line_profile": "Framework for scaling computer-use grounding via UI decomposition",
      "detailed_description": "A framework focused on improving computer-use grounding for agents through UI decomposition and synthesis, aiding in agent training and evaluation.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "data_processing",
        "grounding"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/xlang-ai/OSWorld-G",
      "help_website": [],
      "license": null,
      "tags": [
        "ui-grounding",
        "agent-training",
        "computer-use"
      ],
      "id": 237
    },
    {
      "name": "OpenCUA",
      "one_line_profile": "Foundational framework and environment for computer-use agents",
      "detailed_description": "Open foundations for building and evaluating computer-use agents, providing necessary environments and tools.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "simulation",
        "benchmarking"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/xlang-ai/OpenCUA",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "computer-use-agents",
        "environment",
        "foundation"
      ],
      "id": 238
    },
    {
      "name": "ComfyBench",
      "one_line_profile": "Benchmark for evaluating LLM-based agents within ComfyUI workflows",
      "detailed_description": "A benchmark suite designed to evaluate LLM-based agents specifically in the context of ComfyUI for autonomously designing collaborative AI systems.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "benchmarking",
        "evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/xxyQwQ/ComfyBench",
      "help_website": [],
      "license": null,
      "tags": [
        "benchmark",
        "comfyui",
        "llm-agents"
      ],
      "id": 239
    },
    {
      "name": "MultiHop-RAG",
      "one_line_profile": "Dataset and benchmark for evaluating multi-hop retrieval-augmented generation",
      "detailed_description": "A dataset and benchmark designed to evaluate Retrieval-Augmented Generation (RAG) systems on multi-hop reasoning tasks across documents.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "evaluation",
        "benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/yixuantt/MultiHop-RAG",
      "help_website": [],
      "license": null,
      "tags": [
        "rag",
        "benchmark",
        "multi-hop-reasoning"
      ],
      "id": 240
    },
    {
      "name": "EasyLM",
      "one_line_profile": "Framework for pre-training, fine-tuning, and evaluating LLMs using JAX/Flax",
      "detailed_description": "A one-stop solution for Large Language Models (LLMs) that facilitates pre-training, fine-tuning, evaluating, and serving using JAX/Flax.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "training",
        "evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/young-geng/EasyLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "jax",
        "flax",
        "evaluation"
      ],
      "id": 241
    },
    {
      "name": "EasyDetect",
      "one_line_profile": "Framework for detecting hallucinations in LLMs",
      "detailed_description": "An easy-to-use framework designed to detect hallucinations in Large Language Models, aiding in the safety and reliability evaluation of AI models.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "evaluation",
        "safety_check"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/zjunlp/EasyDetect",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "hallucination-detection",
        "llm-eval",
        "safety"
      ],
      "id": 242
    },
    {
      "name": "FactCHD",
      "one_line_profile": "Benchmark for fact-conflicting hallucination detection",
      "detailed_description": "A benchmark specifically designed for evaluating Fact-Conflicting Hallucination Detection in LLMs.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "benchmarking",
        "evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/zjunlp/FactCHD",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "hallucination",
        "benchmark",
        "fact-checking"
      ],
      "id": 243
    },
    {
      "name": "WorfBench",
      "one_line_profile": "Benchmark for evaluating agentic workflow generation",
      "detailed_description": "A benchmark suite for evaluating the generation of agentic workflows, assessing the planning and execution capabilities of AI agents.",
      "domains": [
        "AI5",
        "AI5-04"
      ],
      "subtask_category": [
        "benchmarking",
        "evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/zjunlp/WorfBench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "agent-workflow",
        "benchmark",
        "planning"
      ],
      "id": 244
    }
  ]
}
{
  "generated_at": "2025-12-16T15:32:36.864324+08:00",
  "metadata": {
    "leaf_cluster": {
      "leaf_cluster_id": "X1",
      "leaf_cluster_name": "可信与不确定性量化生态",
      "domain": "Cross-domain",
      "typical_objects": "predictions",
      "task_chain": "UQ→校准→监控→审计→评测",
      "tool_form": "UQ/监控 + 评测"
    },
    "unit": {
      "unit_id": "X1-05",
      "unit_name": "可信评测与审计",
      "target_scale": "150–300",
      "coverage_tools": "auditing、fairness/safety"
    },
    "search": {
      "target_candidates": 300,
      "queries": [
        "[GH] Fairness Indicators",
        "[GH] DeepEval",
        "[GH] RobustBench",
        "[GH] What-If Tool",
        "[GH] Audit-AI",
        "[GH] Aequitas",
        "[GH] Adversarial Robustness Toolbox",
        "[GH] AIF360",
        "[GH] Fairlearn",
        "[GH] model auditing",
        "[GH] fairness metrics",
        "[GH] bias detection",
        "[GH] ai safety evaluation",
        "[GH] robustness benchmark",
        "[GH] trustworthy ai",
        "[GH] responsible ai",
        "[GH] algorithmic fairness",
        "[GH] red teaming",
        "[GH] adversarial robustness",
        "[GH] model card",
        "[GH] safety alignment",
        "[GH] bias mitigation",
        "[WEB] ai model auditing tools github",
        "[WEB] machine learning fairness library github",
        "[WEB] ai safety evaluation framework github",
        "[WEB] model bias detection python github",
        "[WEB] adversarial robustness toolbox github",
        "[WEB] trustworthy ml benchmark github"
      ],
      "total_candidates": 1171,
      "tool_candidates": 734,
      "final_tools": 210
    }
  },
  "tools": [
    {
      "name": "rexmex",
      "one_line_profile": "A general purpose recommender metrics library for fair evaluation",
      "detailed_description": "A general purpose recommender metrics library designed for fair evaluation of recommender systems. It provides a comprehensive set of metrics to assess the performance and fairness of recommendation algorithms, supporting the development of trustworthy AI systems.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "fairness_evaluation",
        "metrics_calculation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/AstraZeneca/rexmex",
      "help_website": [],
      "license": null,
      "tags": [
        "recommender-systems",
        "fairness",
        "metrics",
        "evaluation"
      ],
      "id": 1
    },
    {
      "name": "advertorch",
      "one_line_profile": "A Toolbox for Adversarial Robustness Research",
      "detailed_description": "A Python toolbox for adversarial robustness research, specifically designed for PyTorch. It provides implementations of various adversarial attacks, defenses, and robust training methods to evaluate and improve the security of machine learning models.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "adversarial_attack",
        "robustness_evaluation",
        "defense"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/BorealisAI/advertorch",
      "help_website": [],
      "license": "LGPL-3.0",
      "tags": [
        "adversarial-machine-learning",
        "robustness",
        "pytorch",
        "security"
      ],
      "id": 2
    },
    {
      "name": "CartAI",
      "one_line_profile": "AI supervisor agent for intelligent E2E oversight and compliance in the AI lifecycle",
      "detailed_description": "CartAI acts as an open-source AI supervisor agent designed to provide intelligent end-to-end oversight and compliance monitoring for trustworthy AI systems. It facilitates the auditing and governance of AI models throughout their lifecycle.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "auditing",
        "compliance_monitoring"
      ],
      "application_level": "platform",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/ContrastoAI/cartai",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ai-governance",
        "compliance",
        "auditing",
        "trustworthy-ai"
      ],
      "id": 3
    },
    {
      "name": "RobustART",
      "one_line_profile": "Comprehensive robustness investigation benchmark for ImageNet models",
      "detailed_description": "RobustART is a benchmark toolkit designed to investigate the robustness of large-scale image classification models (specifically on ImageNet) regarding architecture design and training techniques against diverse noises and adversarial attacks.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/DIG-Beihang/RobustART",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "robustness",
        "benchmark",
        "computer-vision",
        "adversarial-defense"
      ],
      "id": 4
    },
    {
      "name": "EqualityML",
      "one_line_profile": "Evidence-based tools for algorithmic bias detection and mitigation",
      "detailed_description": "EqualityML provides a suite of tools and algorithms designed to detect and mitigate algorithmic bias in machine learning models, fostering community collaboration for fair data science practices.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "fairness_auditing",
        "bias_mitigation"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/EqualityAI/EqualityML",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "fairness",
        "bias-detection",
        "machine-learning",
        "auditing"
      ],
      "id": 5
    },
    {
      "name": "EuroEval",
      "one_line_profile": "Robust benchmark for European language models",
      "detailed_description": "EuroEval is a benchmarking framework designed to evaluate the performance and robustness of language models specifically for European languages, providing standardized metrics for model assessment.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/EuroEval/EuroEval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nlp",
        "benchmark",
        "european-languages",
        "model-evaluation"
      ],
      "id": 6
    },
    {
      "name": "FAIR Metrics",
      "one_line_profile": "Reference implementations of FAIR Maturity Indicators",
      "detailed_description": "This repository contains the reference implementations (Ruby scripts/YAML definitions) of the FAIR (Findable, Accessible, Interoperable, Reusable) Maturity Indicators, used to programmatically evaluate the FAIRness of digital resources.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "data_auditing",
        "fairness_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Ruby",
      "repo_url": "https://github.com/FAIRMetrics/Metrics",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fair-principles",
        "data-auditing",
        "metrics",
        "reproducibility"
      ],
      "id": 7
    },
    {
      "name": "Phare",
      "one_line_profile": "LLM benchmark for AI security and safety evaluation",
      "detailed_description": "Phare is a benchmark tool designed to evaluate Large Language Models (LLMs) across key AI security and safety dimensions, helping auditors and developers assess model trustworthiness.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "safety_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Giskard-AI/phare",
      "help_website": [],
      "license": null,
      "tags": [
        "llm",
        "safety",
        "security",
        "benchmark"
      ],
      "id": 8
    },
    {
      "name": "TrustLLM",
      "one_line_profile": "Comprehensive benchmark for trustworthiness in Large Language Models",
      "detailed_description": "TrustLLM is a comprehensive framework and benchmark for evaluating the trustworthiness of Large Language Models across multiple dimensions, including truthfulness, safety, fairness, and robustness.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "trustworthiness_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/HowieHwong/TrustLLM",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "trustworthiness",
        "benchmark",
        "safety"
      ],
      "id": 9
    },
    {
      "name": "HEART Library",
      "one_line_profile": "Hardened Extension of the Adversarial Robustness Toolbox (HEART)",
      "detailed_description": "HEART (Hardened Extension of the Adversarial Robustness Toolbox) is a library that extends the Adversarial Robustness Toolbox (ART) to support the assessment of adversarial AI vulnerabilities specifically in Test & Evaluation workflows.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_assessment",
        "vulnerability_testing"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/IBM/heart-library",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "adversarial-robustness",
        "security",
        "testing",
        "ai-safety"
      ],
      "id": 10
    },
    {
      "name": "inFairness",
      "one_line_profile": "PyTorch package for training and auditing ML models for Individual Fairness",
      "detailed_description": "inFairness is a PyTorch-based library designed to train, audit, and evaluate machine learning models with a focus on Individual Fairness, providing algorithms and metrics to ensure equitable model outcomes.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "fairness_auditing",
        "model_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/IBM/inFairness",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "individual-fairness",
        "pytorch",
        "auditing",
        "bias-mitigation"
      ],
      "id": 11
    },
    {
      "name": "qux360",
      "one_line_profile": "AI-assisted trustworthy qualitative data analysis toolkit",
      "detailed_description": "A toolkit designed to assist in qualitative data analysis with a focus on trustworthiness, providing features for analyzing and auditing qualitative datasets.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "qualitative_analysis",
        "auditing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/IBM/qux360",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "qualitative-analysis",
        "trustworthy-ai",
        "auditing"
      ],
      "id": 12
    },
    {
      "name": "Infosys Responsible AI Toolkit",
      "one_line_profile": "Toolkit for ensuring AI solutions are trustworthy and transparent",
      "detailed_description": "A comprehensive toolkit incorporating features for safety, security, explainability, fairness, bias, and hallucination detection to audit and evaluate AI models.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "auditing",
        "fairness_analysis",
        "safety_check"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Infosys/Infosys-Responsible-AI-Toolkit",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "responsible-ai",
        "fairness",
        "explainability",
        "security"
      ],
      "id": 13
    },
    {
      "name": "JailbreakBench",
      "one_line_profile": "Open robustness benchmark for jailbreaking language models",
      "detailed_description": "A benchmark suite designed to evaluate the robustness of Large Language Models (LLMs) against jailbreaking attacks, providing standardized metrics and datasets.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "security_auditing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/JailbreakBench/jailbreakbench",
      "help_website": [
        "https://jailbreakbench.github.io/"
      ],
      "license": "MIT",
      "tags": [
        "llm",
        "jailbreak",
        "robustness",
        "benchmark"
      ],
      "id": 14
    },
    {
      "name": "MM_Robustness",
      "one_line_profile": "Benchmark for robustness of multimodal image-text models",
      "detailed_description": "A benchmarking tool for evaluating the robustness of multimodal image-text models under various distribution shifts.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Jielin-Qiu/MM_Robustness",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "multimodal",
        "robustness",
        "benchmark"
      ],
      "id": 15
    },
    {
      "name": "OpenOOD",
      "one_line_profile": "Benchmarking framework for Generalized Out-of-Distribution Detection",
      "detailed_description": "A comprehensive codebase and benchmark for evaluating out-of-distribution (OOD) detection methods, crucial for AI safety and reliability.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "ood_detection",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Jingkang50/OpenOOD",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ood-detection",
        "benchmark",
        "ai-safety"
      ],
      "id": 16
    },
    {
      "name": "FairRankTune",
      "one_line_profile": "Toolkit for fairness-aware ranking and data generation",
      "detailed_description": "A Python toolkit providing fairness-aware data generation, fairness metrics, and fair ranking algorithms for evaluating and mitigating bias in ranking systems.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "fairness_analysis",
        "ranking_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/KCachel/fairranktune",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "fairness",
        "ranking",
        "metrics"
      ],
      "id": 17
    },
    {
      "name": "RAOS",
      "one_line_profile": "Robustness evaluation benchmark for abdominal organ segmentation",
      "detailed_description": "A benchmark for evaluating the robustness of abdominal organ segmentation models in clinical scenarios, focusing on challenging cases.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "medical_image_segmentation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/Luoxd1996/RAOS",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "medical-imaging",
        "segmentation",
        "robustness",
        "benchmark"
      ],
      "id": 18
    },
    {
      "name": "Lyzr Agent Framework",
      "one_line_profile": "Agent framework with integrated Safe AI and Responsible AI modules",
      "detailed_description": "An agent framework that natively integrates Safe AI and Responsible AI modules, allowing for the development of auditable and transparent AI agents.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "safety_auditing",
        "agent_development"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/LyzrCore/lyzr-framework",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "responsible-ai",
        "agents",
        "safety"
      ],
      "id": 19
    },
    {
      "name": "MME-CoT",
      "one_line_profile": "Benchmarking Chain-of-Thought in LMMs",
      "detailed_description": "A benchmark suite for evaluating the reasoning quality, robustness, and efficiency of Chain-of-Thought capabilities in Large Multimodal Models.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "reasoning_evaluation",
        "robustness_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/MME-Benchmarks/MME-CoT",
      "help_website": [],
      "license": null,
      "tags": [
        "multimodal",
        "chain-of-thought",
        "benchmark"
      ],
      "id": 20
    },
    {
      "name": "fair-test",
      "one_line_profile": "Library to build and deploy FAIR metrics tests",
      "detailed_description": "A library to build and deploy FAIR (Findable, Accessible, Interoperable, Reusable) metrics tests APIs for evaluating scientific data quality and stewardship.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "data_quality_control",
        "fair_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/MaastrichtU-IDS/fair-test",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fair-data",
        "metrics",
        "evaluation"
      ],
      "id": 21
    },
    {
      "name": "CIFAR10 Adversarial Challenge",
      "one_line_profile": "Benchmark harness for adversarial robustness on CIFAR10",
      "detailed_description": "A challenge framework and codebase for exploring and benchmarking adversarial robustness of neural networks on the CIFAR10 dataset.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_benchmarking",
        "adversarial_defense"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/MadryLab/cifar10_challenge",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "adversarial-robustness",
        "cifar10",
        "benchmark"
      ],
      "id": 22
    },
    {
      "name": "MNIST Adversarial Challenge",
      "one_line_profile": "Benchmark harness for adversarial robustness on MNIST",
      "detailed_description": "A challenge framework and codebase for exploring and benchmarking adversarial robustness of neural networks on the MNIST dataset.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_benchmarking",
        "adversarial_defense"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/MadryLab/mnist_challenge",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "adversarial-robustness",
        "mnist",
        "benchmark"
      ],
      "id": 23
    },
    {
      "name": "robustness",
      "one_line_profile": "Library for training and evaluating robust neural networks",
      "detailed_description": "A library for experimenting with, training, and evaluating neural networks with a specific focus on adversarial robustness.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "adversarial_training"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/MadryLab/robustness",
      "help_website": [
        "https://robustness.readthedocs.io/en/latest/"
      ],
      "license": "MIT",
      "tags": [
        "adversarial-robustness",
        "deep-learning",
        "evaluation"
      ],
      "id": 24
    },
    {
      "name": "explabox",
      "one_line_profile": "Toolbox for model exploration, examination, and explanation",
      "detailed_description": "A toolbox designed to help users explore, examine, explain, and expose machine learning models, facilitating transparency and auditing.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "explainability",
        "model_auditing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/MarcelRobeer/explabox",
      "help_website": [],
      "license": "LGPL-3.0",
      "tags": [
        "xai",
        "explainability",
        "auditing"
      ],
      "id": 25
    },
    {
      "name": "CIL-ReID",
      "one_line_profile": "Benchmark for Corruption Invariant Person Re-identification",
      "detailed_description": "A benchmark suite for evaluating the robustness of person re-identification models against various image corruptions.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_benchmarking",
        "computer_vision"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/MinghuiChen43/CIL-ReID",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "re-identification",
        "robustness",
        "benchmark"
      ],
      "id": 26
    },
    {
      "name": "auditor",
      "one_line_profile": "Model verification, validation, and error analysis tool for R",
      "detailed_description": "A tool for model verification, validation, and error analysis. It provides methods for assessing model performance, analyzing residuals, and detecting potential issues in predictive models.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "model_auditing",
        "error_analysis",
        "validation"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/ModelOriented/auditor",
      "help_website": [
        "https://modeloriented.github.io/auditor/"
      ],
      "license": "GPL-2.0",
      "tags": [
        "model-validation",
        "auditing",
        "error-analysis",
        "r-package"
      ],
      "id": 27
    },
    {
      "name": "fairmodels",
      "one_line_profile": "Flexible tool for bias detection, visualization, and mitigation in R",
      "detailed_description": "An R package that facilitates fairness analysis in machine learning models. It offers tools for detecting bias, visualizing fairness metrics, and applying mitigation techniques to ensure equitable model outcomes.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "bias_detection",
        "fairness_auditing",
        "bias_mitigation"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/ModelOriented/fairmodels",
      "help_website": [
        "https://fairmodels.drwhy.ai/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "fairness",
        "bias-detection",
        "machine-learning",
        "r-package"
      ],
      "id": 28
    },
    {
      "name": "farsight",
      "one_line_profile": "In situ interactive widgets for responsible AI auditing",
      "detailed_description": "A collection of interactive widgets designed to help researchers and practitioners audit AI models for responsibility and fairness directly within their development environment.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "auditing",
        "visualization",
        "responsible_ai"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/PAIR-code/farsight",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "visualization",
        "responsible-ai",
        "auditing"
      ],
      "id": 29
    },
    {
      "name": "Know Your Data",
      "one_line_profile": "Tool for dataset understanding and bias mitigation",
      "detailed_description": "A tool developed to assist researchers and product teams in understanding datasets, improving data quality, and identifying and mitigating fairness and bias issues.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "data_auditing",
        "bias_detection",
        "data_quality"
      ],
      "application_level": "platform",
      "primary_language": "CSS",
      "repo_url": "https://github.com/PAIR-code/knowyourdata",
      "help_website": [
        "https://knowyourdata.withgoogle.com/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "dataset-auditing",
        "fairness",
        "visualization"
      ],
      "id": 30
    },
    {
      "name": "What-If Tool",
      "one_line_profile": "Interactive visual interface for model understanding and fairness auditing",
      "detailed_description": "An interactive visual interface designed to help users probe, understand, and debug machine learning models. It enables fairness auditing, counterfactual analysis, and performance visualization.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "model_auditing",
        "fairness_analysis",
        "counterfactual_analysis"
      ],
      "application_level": "platform",
      "primary_language": "HTML",
      "repo_url": "https://github.com/PAIR-code/what-if-tool",
      "help_website": [
        "https://pair-code.github.io/what-if-tool/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "visualization",
        "fairness",
        "model-debugging"
      ],
      "id": 31
    },
    {
      "name": "BeaverTails",
      "one_line_profile": "Dataset for safety alignment in large language models",
      "detailed_description": "A collection of datasets specifically designed to facilitate research on safety alignment in large language models (LLMs), covering various safety-related scenarios.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "safety_alignment",
        "dataset"
      ],
      "application_level": "dataset",
      "primary_language": "Makefile",
      "repo_url": "https://github.com/PKU-Alignment/beavertails",
      "help_website": [
        "https://huggingface.co/datasets/PKU-Alignment/BeaverTails"
      ],
      "license": "Apache-2.0",
      "tags": [
        "safety-alignment",
        "llm",
        "dataset"
      ],
      "id": 32
    },
    {
      "name": "Safe-RLHF",
      "one_line_profile": "Library for constrained value alignment via safe RLHF",
      "detailed_description": "A library implementing Safe Reinforcement Learning from Human Feedback (Safe RLHF) for constrained value alignment of large language models, ensuring helpfulness and harmlessness.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "safety_alignment",
        "rlhf",
        "model_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/PKU-Alignment/safe-rlhf",
      "help_website": [
        "https://safe-rlhf.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "rlhf",
        "safety",
        "alignment",
        "llm"
      ],
      "id": 33
    },
    {
      "name": "SafeSora",
      "one_line_profile": "Human preference dataset for text-to-video safety alignment",
      "detailed_description": "A human preference dataset designed to support safety alignment research in the text-to-video generation field, aiming to enhance the helpfulness and harmlessness of Large Vision Models.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "safety_alignment",
        "dataset",
        "video_generation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/PKU-Alignment/safe-sora",
      "help_website": [],
      "license": null,
      "tags": [
        "text-to-video",
        "safety",
        "alignment",
        "dataset"
      ],
      "id": 34
    },
    {
      "name": "RLAIF-V",
      "one_line_profile": "Dataset and method for AI feedback-based safety alignment",
      "detailed_description": "A framework and dataset for Reinforcement Learning from AI Feedback (RLAIF) applied to Vision-Language Models, aiming to improve trustworthiness and safety alignment.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "safety_alignment",
        "rlaif",
        "dataset"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/RLHF-V/RLAIF-V",
      "help_website": [],
      "license": null,
      "tags": [
        "rlaif",
        "safety",
        "alignment",
        "vlm"
      ],
      "id": 35
    },
    {
      "name": "responsibly",
      "one_line_profile": "Toolkit for auditing and mitigating bias and fairness in ML systems",
      "detailed_description": "A toolkit designed for auditing and mitigating bias and fairness issues in machine learning systems, providing practical tools for researchers and practitioners.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "bias_auditing",
        "fairness_mitigation",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ResponsiblyAI/responsibly",
      "help_website": [
        "https://docs.responsibly.ai/"
      ],
      "license": "MIT",
      "tags": [
        "fairness",
        "bias",
        "auditing",
        "toolkit"
      ],
      "id": 36
    },
    {
      "name": "RobustBench",
      "one_line_profile": "Standardized adversarial robustness benchmark",
      "detailed_description": "A standardized benchmark for evaluating the adversarial robustness of image classification models. It provides a leaderboard and a library to easily load robust models and evaluate them.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_benchmarking",
        "adversarial_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/RobustBench/robustbench",
      "help_website": [
        "https://robustbench.github.io/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "robustness",
        "benchmark",
        "adversarial-attacks"
      ],
      "id": 37
    },
    {
      "name": "CipherChat",
      "one_line_profile": "Framework to evaluate safety alignment generalization in LLMs",
      "detailed_description": "A framework designed to evaluate the generalization capability of safety alignment in Large Language Models (LLMs), particularly focusing on cipher-based scenarios to test robustness.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "safety_evaluation",
        "alignment_generalization",
        "robustness_testing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/RobustNLP/CipherChat",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "safety-alignment",
        "llm",
        "evaluation"
      ],
      "id": 38
    },
    {
      "name": "WORKBank",
      "one_line_profile": "Database of worker desire and capability for AI agents",
      "detailed_description": "A database derived from a large-scale audit of worker desire and technological capability of AI agents, serving as a resource for researching the social impact and alignment of AI in the workforce.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "social_impact_audit",
        "dataset",
        "ai_alignment"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/SALT-NLP/workbank",
      "help_website": [],
      "license": null,
      "tags": [
        "dataset",
        "ai-ethics",
        "workforce-alignment"
      ],
      "id": 39
    },
    {
      "name": "JailBreakV-28K",
      "one_line_profile": "Benchmark for evaluating jailbreak transferability to MLLMs",
      "detailed_description": "A comprehensive benchmark designed to evaluate the transferability of LLM jailbreak attacks to Multimodal Large Language Models (MLLMs) and assess their robustness and safety.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "safety_benchmarking",
        "jailbreak_evaluation",
        "robustness_testing"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/SaFo-Lab/JailBreakV_28K",
      "help_website": [
        "https://jailbreakv.github.io/"
      ],
      "license": null,
      "tags": [
        "benchmark",
        "jailbreak",
        "mllm",
        "safety"
      ],
      "id": 40
    },
    {
      "name": "Robust Gymnasium",
      "one_line_profile": "Unified modular benchmark for robust reinforcement learning",
      "detailed_description": "A unified modular benchmark designed for evaluating Robust Reinforcement Learning algorithms, providing a standardized environment for testing agent robustness.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_benchmarking",
        "reinforcement_learning",
        "evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/SafeRL-Lab/Robust-Gymnasium",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "reinforcement-learning",
        "robustness",
        "benchmark"
      ],
      "id": 41
    },
    {
      "name": "Face-Robustness-Benchmark",
      "one_line_profile": "Adversarial robustness evaluation library for face recognition models",
      "detailed_description": "A library designed to evaluate the adversarial robustness of face recognition systems, providing benchmarks and tools for testing against various adversarial attacks.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "adversarial_attack"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ShawnXYang/Face-Robustness-Benchmark",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "face-recognition",
        "adversarial-robustness",
        "benchmark"
      ],
      "id": 42
    },
    {
      "name": "VINE",
      "one_line_profile": "Robust watermarking method using generative priors against image editing",
      "detailed_description": "Official implementation of a robust watermarking technique that leverages generative priors to protect images against editing, serving as a solver for image provenance and integrity.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "watermarking",
        "image_protection"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Shilin-LU/VINE",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "watermarking",
        "generative-priors",
        "robustness"
      ],
      "id": 43
    },
    {
      "name": "SpeechIO Leaderboard",
      "one_line_profile": "Comprehensive benchmarking platform for Automatic Speech Recognition (ASR)",
      "detailed_description": "A robust and comprehensive leaderboard and benchmarking platform designed to evaluate the performance of Automatic Speech Recognition systems across various metrics.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/SpeechColab/Leaderboard",
      "help_website": [],
      "license": null,
      "tags": [
        "asr",
        "leaderboard",
        "benchmarking"
      ],
      "id": 44
    },
    {
      "name": "Graph Robustness Benchmark (GRB)",
      "one_line_profile": "Scalable benchmark for evaluating adversarial robustness of Graph Machine Learning",
      "detailed_description": "A unified, modular, and reproducible benchmark framework for assessing the adversarial robustness of Graph Machine Learning (GML) models against various attacks.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "graph_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/THUDM/grb",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "graph-neural-networks",
        "adversarial-robustness",
        "benchmark"
      ],
      "id": 45
    },
    {
      "name": "MMLU-Pro",
      "one_line_profile": "Robust and challenging multi-task language understanding benchmark",
      "detailed_description": "An enhanced benchmark dataset and evaluation suite for Large Language Models, designed to be more robust and challenging than the standard MMLU benchmark.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/TIGER-AI-Lab/MMLU-Pro",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "benchmark",
        "mmlu"
      ],
      "id": 46
    },
    {
      "name": "AI-Infra-Guard",
      "one_line_profile": "Comprehensive AI Red Teaming and safety evaluation platform",
      "detailed_description": "A platform developed by Tencent Zhuque Lab for AI Red Teaming, offering intelligent and easy-to-use tools for assessing AI system security and safety.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "red_teaming",
        "safety_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/Tencent/AI-Infra-Guard",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "red-teaming",
        "ai-safety",
        "security"
      ],
      "id": 47
    },
    {
      "name": "trust-safety-evals",
      "one_line_profile": "Reference stack for AI model and system evaluation",
      "detailed_description": "A project by The AI Alliance providing a reference stack, benchmarks, and leaderboards for evaluating the trust and safety of AI models and systems.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "safety_evaluation",
        "benchmarking"
      ],
      "application_level": "workflow",
      "primary_language": "Makefile",
      "repo_url": "https://github.com/The-AI-Alliance/trust-safety-evals",
      "help_website": [],
      "license": null,
      "tags": [
        "ai-safety",
        "evaluation",
        "benchmarks"
      ],
      "id": 48
    },
    {
      "name": "AIF360",
      "one_line_profile": "Comprehensive toolkit for fairness metrics and bias mitigation",
      "detailed_description": "The AI Fairness 360 toolkit is an open-source library to help detect and remove bias in machine learning models. It translates algorithmic research from the lab into practice.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "fairness_evaluation",
        "bias_mitigation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Trusted-AI/AIF360",
      "help_website": [
        "https://aif360.mybluemix.net/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "fairness",
        "bias-mitigation",
        "machine-learning"
      ],
      "id": 49
    },
    {
      "name": "Adversarial Robustness Toolbox (ART)",
      "one_line_profile": "Python library for machine learning security and robustness",
      "detailed_description": "A library dedicated to machine learning security, providing tools for evasion, poisoning, extraction, and inference attacks, as well as defenses for red and blue teams.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "adversarial_defense"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox",
      "help_website": [
        "https://adversarial-robustness-toolbox.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "adversarial-ml",
        "security",
        "robustness"
      ],
      "id": 50
    },
    {
      "name": "STAR-1",
      "one_line_profile": "Method for safer alignment of reasoning LLMs",
      "detailed_description": "Implementation of the STAR-1 approach for aligning reasoning Large Language Models (LLMs) to improve safety, serving as a solver for alignment tasks.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "safety_alignment",
        "model_training"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/UCSC-VLAA/STAR-1",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-alignment",
        "safety",
        "reasoning"
      ],
      "id": 51
    },
    {
      "name": "vllm-safety-benchmark",
      "one_line_profile": "Safety evaluation benchmark for Vision LLMs",
      "detailed_description": "A benchmark suite designed to evaluate the safety of Vision Large Language Models (VLLMs), specifically assessing risks like hallucinations or misinterpretations in visual contexts.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "safety_evaluation",
        "benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/UCSC-VLAA/vllm-safety-benchmark",
      "help_website": [],
      "license": null,
      "tags": [
        "vision-llm",
        "safety",
        "benchmark"
      ],
      "id": 52
    },
    {
      "name": "Inspect",
      "one_line_profile": "Framework for large language model evaluations",
      "detailed_description": "A framework developed by the UK AI Safety Institute for evaluating large language models, facilitating the creation and execution of safety and capability assessments.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "model_evaluation",
        "safety_assessment"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/UKGovernmentBEIS/inspect_ai",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm-evaluation",
        "ai-safety",
        "framework"
      ],
      "id": 53
    },
    {
      "name": "fair-sense-ai",
      "one_line_profile": "AI-powered tool for bias detection and risk management",
      "detailed_description": "A tool developed by Vector Institute for detecting bias and managing risks in AI systems, promoting sustainable and trustworthy AI.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "bias_detection",
        "risk_management"
      ],
      "application_level": "tool",
      "primary_language": "Python",
      "repo_url": "https://github.com/VectorInstitute/fair-sense-ai",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "bias-detection",
        "risk-management",
        "trustworthy-ai"
      ],
      "id": 54
    },
    {
      "name": "MarsFL",
      "one_line_profile": "Benchmark for Federated Learning generalization, robustness, and fairness",
      "detailed_description": "A benchmark framework for evaluating Federated Learning algorithms with a focus on generalization, robustness, and fairness.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "benchmarking",
        "federated_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/WenkeHuang/MarsFL",
      "help_website": [],
      "license": null,
      "tags": [
        "federated-learning",
        "robustness",
        "fairness"
      ],
      "id": 55
    },
    {
      "name": "SLAM-under-Perturbation",
      "one_line_profile": "Scalable benchmarking for robust SLAM and 3D reconstruction",
      "detailed_description": "A benchmarking tool for evaluating the robustness of SLAM (Simultaneous Localization and Mapping) and 3D reconstruction algorithms under noisy conditions.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "slam"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/Xiaohao-Xu/SLAM-under-Perturbation",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "slam",
        "robustness",
        "benchmarking"
      ],
      "id": 56
    },
    {
      "name": "FairDiverse",
      "one_line_profile": "Toolkit for fairness and diversity in Information Retrieval",
      "detailed_description": "A toolkit designed to implement and evaluate fairness-aware and diversity-aware algorithms within the context of Information Retrieval systems.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "fairness_evaluation",
        "information_retrieval"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/XuChen0427/FairDiverse",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fairness",
        "diversity",
        "information-retrieval"
      ],
      "id": 57
    },
    {
      "name": "Veridical Flow",
      "one_line_profile": "Framework for building stable and trustworthy data-science pipelines based on PCS",
      "detailed_description": "A Python framework designed to facilitate the construction of stable and trustworthy data science pipelines using the PCS (Predictability, Computability, and Stability) framework. It aids in managing the data science lifecycle with a focus on reliability.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "pipeline_management",
        "trustworthiness_assurance"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/Yu-Group/veridical-flow",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "data-science",
        "pipeline",
        "trustworthiness",
        "PCS-framework"
      ],
      "id": 58
    },
    {
      "name": "AdvBox",
      "one_line_profile": "Toolbox for generating adversarial examples and benchmarking model robustness",
      "detailed_description": "A comprehensive toolbox to generate adversarial examples that can fool neural networks across multiple frameworks (PaddlePaddle, PyTorch, Caffe2, MxNet, Keras, TensorFlow). It also provides benchmarking capabilities to evaluate the robustness of machine learning models against adversarial attacks.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "adversarial_attack",
        "robustness_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/advboxes/AdvBox",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "adversarial-attacks",
        "robustness",
        "security",
        "benchmark"
      ],
      "id": 59
    },
    {
      "name": "Perceptron Benchmark",
      "one_line_profile": "Robustness benchmark for Deep Neural Network models",
      "detailed_description": "A benchmarking tool designed to evaluate the robustness of Deep Neural Network (DNN) models. It provides a standardized environment and metrics for assessing model performance under various adversarial conditions.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_benchmarking",
        "model_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/advboxes/perceptron-benchmark",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "benchmark",
        "robustness",
        "dnn",
        "evaluation"
      ],
      "id": 60
    },
    {
      "name": "PromptInject",
      "one_line_profile": "Framework for analyzing LLM robustness to adversarial prompt attacks",
      "detailed_description": "A modular framework designed to assemble prompts for quantitatively analyzing the robustness of Large Language Models (LLMs) against adversarial prompt injection attacks. It helps in evaluating safety and security risks in LLM applications.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "safety_evaluation",
        "adversarial_testing"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/agencyenterprise/PromptInject",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "prompt-injection",
        "security",
        "robustness"
      ],
      "id": 61
    },
    {
      "name": "EMERALD",
      "one_line_profile": "Tool for calculating safety-windows in suboptimal alignment space",
      "detailed_description": "A C++ tool that calculates safety-windows by exploring the suboptimal alignment space, primarily used in bioinformatics for sequence alignment analysis. It helps in understanding the reliability and robustness of alignment results.",
      "domains": [
        "Bioinformatics"
      ],
      "subtask_category": [
        "sequence_alignment",
        "reliability_analysis"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/algbio/emerald",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "bioinformatics",
        "alignment",
        "safety-window"
      ],
      "id": 62
    },
    {
      "name": "BlackBoxAuditing",
      "one_line_profile": "Tool for auditing and exploring black box machine learning models",
      "detailed_description": "Research code and library for auditing black-box machine learning models to detect bias and ensure fairness. It provides methods to explore model behavior and identify potential discrimination without access to model internals.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "model_auditing",
        "fairness_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/algofairness/BlackBoxAuditing",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "auditing",
        "fairness",
        "black-box",
        "machine-learning"
      ],
      "id": 63
    },
    {
      "name": "GRIT Benchmark",
      "one_line_profile": "Benchmark for General Robust Image Task (GRIT)",
      "detailed_description": "The official repository for the General Robust Image Task (GRIT) Benchmark, designed to evaluate the robustness and generalization capabilities of computer vision models across various distortions and distribution shifts.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_benchmarking",
        "image_analysis"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/allenai/grit_official",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "benchmark",
        "computer-vision",
        "robustness",
        "generalization"
      ],
      "id": 64
    },
    {
      "name": "safety-eval",
      "one_line_profile": "Evaluation tool for generative language models and safety classifiers",
      "detailed_description": "A simple yet effective evaluation tool for assessing the safety of generative language models and the performance of safety classifiers. It helps in identifying potential safety issues in generated text.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "safety_evaluation",
        "model_auditing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/allenai/safety-eval",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "safety",
        "evaluation",
        "llm",
        "generative-ai"
      ],
      "id": 65
    },
    {
      "name": "Fairness.jl",
      "one_line_profile": "Julia toolkit for fairness metrics and bias mitigation",
      "detailed_description": "A Julia toolkit providing a collection of fairness metrics and bias mitigation algorithms for machine learning models. It enables researchers and practitioners to evaluate and improve the fairness of their models within the Julia ecosystem.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "fairness_evaluation",
        "bias_mitigation"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/ashryaagr/Fairness.jl",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "julia",
        "fairness",
        "bias-mitigation",
        "metrics"
      ],
      "id": 66
    },
    {
      "name": "MLIP Arena",
      "one_line_profile": "Benchmark for machine learning interatomic potentials",
      "detailed_description": "A fair and transparent benchmark platform for evaluating machine learning interatomic potentials (MLIPs). It goes beyond basic error metrics to assess the performance of MLIPs in materials science applications.",
      "domains": [
        "X1",
        "X1-05",
        "Materials Science"
      ],
      "subtask_category": [
        "benchmarking",
        "model_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/atomind-ai/mlip-arena",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "materials-science",
        "interatomic-potentials",
        "benchmark",
        "mlip"
      ],
      "id": 67
    },
    {
      "name": "Geocomplexity",
      "one_line_profile": "Tool for mitigating spatial bias through geographical complexity",
      "detailed_description": "A C++ tool designed to mitigate spatial bias in geographical data analysis by leveraging the concept of geographical complexity. It aids in producing more accurate and unbiased spatial models.",
      "domains": [
        "X1",
        "X1-05",
        "Earth Science"
      ],
      "subtask_category": [
        "spatial_analysis",
        "bias_mitigation"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/ausgis/geocomplexity",
      "help_website": [],
      "license": null,
      "tags": [
        "spatial-bias",
        "geography",
        "complexity",
        "gis"
      ],
      "id": 68
    },
    {
      "name": "Amazon SageMaker Clarify",
      "one_line_profile": "Fairness aware machine learning library for bias detection and mitigation",
      "detailed_description": "An open-source library (part of the broader SageMaker ecosystem) for detecting and mitigating bias in machine learning datasets and models. It provides tools for fairness-aware machine learning and model explainability.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "bias_detection",
        "fairness_mitigation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aws/amazon-sagemaker-clarify",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "fairness",
        "bias",
        "machine-learning",
        "explainability"
      ],
      "id": 69
    },
    {
      "name": "AI Compliance Auditor",
      "one_line_profile": "Compliance auditing tool for E-Commerce AI systems",
      "detailed_description": "A Python-based tool designed to audit AI systems in the e-commerce sector for compliance with regulations and standards. It helps in assessing the trustworthiness and adherence to safety guidelines of deployed AI models.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "compliance_auditing",
        "model_evaluation"
      ],
      "application_level": "tool",
      "primary_language": "Python",
      "repo_url": "https://github.com/awsdataarchitect/ai-compliance-auditor",
      "help_website": [],
      "license": null,
      "tags": [
        "compliance",
        "auditing",
        "e-commerce",
        "ai-safety"
      ],
      "id": 70
    },
    {
      "name": "BenchENAS",
      "one_line_profile": "Benchmarking platform for Evolutionary Neural Architecture Search (ENAS) algorithms",
      "detailed_description": "A platform designed to facilitate fair comparisons and reproducible research for Evolutionary Neural Architecture Search (ENAS) algorithms, providing standard benchmarks and evaluation protocols.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking",
        "neural_architecture_search"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/benchenas/BenchENAS",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "enas",
        "benchmarking",
        "evolutionary-algorithms",
        "automl"
      ],
      "id": 71
    },
    {
      "name": "model-vs-human",
      "one_line_profile": "Benchmark comparing computer vision models against human perception",
      "detailed_description": "A benchmarking toolkit and dataset for evaluating the robustness and out-of-distribution generalization of computer vision models by comparing their performance directly with human visual perception data.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "benchmarking",
        "human_alignment"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/bethgelab/model-vs-human",
      "help_website": [],
      "license": null,
      "tags": [
        "computer-vision",
        "robustness",
        "psychophysics",
        "benchmarking"
      ],
      "id": 72
    },
    {
      "name": "robust-detection-benchmark",
      "one_line_profile": "Benchmark for object detection robustness in autonomous driving",
      "detailed_description": "A benchmark suite containing code and data to evaluate the robustness of object detection models under varying environmental conditions (e.g., weather, corruption), specifically focused on autonomous driving scenarios.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "object_detection",
        "benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/bethgelab/robust-detection-benchmark",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "object-detection",
        "robustness",
        "autonomous-driving",
        "benchmark"
      ],
      "id": 73
    },
    {
      "name": "biological-alignment-gridworlds-benchmarks",
      "one_line_profile": "Gridworld environments for AI safety and biological alignment",
      "detailed_description": "A set of gridworld-based benchmark environments designed to test AI agents' ability to learn and act safely in biologically and economically relevant scenarios, focusing on alignment and safety challenges.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "safety_evaluation",
        "reinforcement_learning",
        "alignment"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/biological-alignment-benchmarks/biological-alignment-gridworlds-benchmarks",
      "help_website": [],
      "license": "MPL-2.0",
      "tags": [
        "ai-safety",
        "alignment",
        "gridworld",
        "benchmark"
      ],
      "id": 74
    },
    {
      "name": "EvalWise",
      "one_line_profile": "Platform for LLM evaluation and red teaming",
      "detailed_description": "A developer-friendly platform designed for evaluating Large Language Models (LLMs) and conducting red teaming exercises to test for safety, compliance, and performance issues.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "model_evaluation",
        "red_teaming",
        "safety_auditing"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/bluewave-labs/evalwise",
      "help_website": [],
      "license": null,
      "tags": [
        "llm",
        "evaluation",
        "red-teaming",
        "safety"
      ],
      "id": 75
    },
    {
      "name": "ConvolutionalNeuralOperator",
      "one_line_profile": "Convolutional Neural Operators for PDE solving",
      "detailed_description": "An implementation of Convolutional Neural Operators (CNO), a deep learning architecture designed for robust and accurate learning and solving of Partial Differential Equations (PDEs).",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "scientific_modeling",
        "pde_solving",
        "neural_operators"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/camlab-ethz/ConvolutionalNeuralOperator",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pde",
        "neural-operators",
        "scientific-machine-learning",
        "deep-learning"
      ],
      "id": 76
    },
    {
      "name": "nn_robust_attacks",
      "one_line_profile": "Library for adversarial attacks to evaluate neural network robustness",
      "detailed_description": "A library providing robust evasion attack algorithms (e.g., Carlini & Wagner attacks) to generate adversarial examples, used for evaluating the robustness and security of neural networks.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "adversarial_attack",
        "security_auditing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/carlini/nn_robust_attacks",
      "help_website": [],
      "license": "BSD-2-Clause",
      "tags": [
        "adversarial-attacks",
        "robustness",
        "neural-networks",
        "security"
      ],
      "id": 77
    },
    {
      "name": "ImageNet-D",
      "one_line_profile": "Benchmark dataset for evaluating object detection robustness",
      "detailed_description": "A benchmark dataset (ImageNet-D) designed to evaluate the robustness of object detection models against various domain shifts and corruptions.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "benchmarking",
        "dataset"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/chenshuang-zhang/imagenet_d",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "object-detection",
        "robustness",
        "benchmark",
        "dataset"
      ],
      "id": 78
    },
    {
      "name": "ResponsibleAI",
      "one_line_profile": "Library for responsible AI development and evaluation",
      "detailed_description": "A Python library designed to assist AI developers in various aspects of responsible AI, including fairness, robustness, and explainability evaluation.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "fairness_evaluation",
        "robustness_evaluation",
        "responsible_ai"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/cisco-open/ResponsibleAI",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "responsible-ai",
        "fairness",
        "robustness",
        "audit"
      ],
      "id": 79
    },
    {
      "name": "TedEval",
      "one_line_profile": "Fair evaluation metric for scene text detectors",
      "detailed_description": "An implementation of a fair evaluation metric for scene text detectors that addresses issues with traditional IoU-based metrics, handling one-to-many and many-to-one matches.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "model_evaluation",
        "metric_calculation",
        "scene_text_detection"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/clovaai/TedEval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "evaluation-metric",
        "scene-text",
        "ocr",
        "fairness"
      ],
      "id": 80
    },
    {
      "name": "DeepEval",
      "one_line_profile": "Framework for LLM evaluation and unit testing",
      "detailed_description": "An open-source evaluation framework for Large Language Models (LLMs) that allows developers to unit test their LLM applications for metrics such as hallucination, bias, toxicity, and relevance.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "model_evaluation",
        "unit_testing",
        "llm_auditing"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/confident-ai/deepeval",
      "help_website": [
        "https://docs.confident-ai.com"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "evaluation",
        "testing",
        "metrics"
      ],
      "id": 81
    },
    {
      "name": "DeepTeam",
      "one_line_profile": "Framework for automated red teaming of LLMs",
      "detailed_description": "A framework designed to automate the red teaming process for Large Language Models (LLMs) and LLM systems, helping to identify vulnerabilities, safety issues, and biases.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "red_teaming",
        "safety_auditing",
        "vulnerability_scanning"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/confident-ai/deepteam",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "red-teaming",
        "llm",
        "security",
        "safety"
      ],
      "id": 82
    },
    {
      "name": "themis-ml",
      "one_line_profile": "Library for fairness-aware machine learning",
      "detailed_description": "A Python library that implements various fairness-aware machine learning algorithms and metrics, enabling researchers to measure and mitigate bias in ML models.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "fairness_evaluation",
        "bias_mitigation",
        "model_training"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/cosmicBboy/themis-ml",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fairness",
        "machine-learning",
        "bias-mitigation",
        "ethics"
      ],
      "id": 83
    },
    {
      "name": "VerifyML",
      "one_line_profile": "Toolkit for responsible AI workflows and reporting",
      "detailed_description": "An open-source toolkit designed to help organizations implement responsible AI workflows, facilitating the documentation, verification, and reporting of model fairness and performance.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "workflow_management",
        "fairness_auditing",
        "reporting"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/cylynx/verifyml",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "responsible-ai",
        "workflow",
        "documentation",
        "audit"
      ],
      "id": 84
    },
    {
      "name": "WEFE",
      "one_line_profile": "Word Embeddings Fairness Evaluation Framework",
      "detailed_description": "A framework for measuring and mitigating bias in word embedding models, providing a standardized way to evaluate fairness in natural language processing tasks.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "fairness_evaluation",
        "bias_measurement",
        "nlp"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/dccuchile/wefe",
      "help_website": [
        "https://wefe.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "word-embeddings",
        "fairness",
        "bias",
        "nlp"
      ],
      "id": 85
    },
    {
      "name": "DebiAI",
      "one_line_profile": "Bias detection and contextual evaluation tool for AI models",
      "detailed_description": "A tool for detecting bias and performing contextual evaluation of AI models, allowing users to visualize model performance across different data subsets and identify potential fairness issues.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "bias_detection",
        "model_evaluation",
        "visualization"
      ],
      "application_level": "platform",
      "primary_language": "Vue",
      "repo_url": "https://github.com/debiai/DebiAI",
      "help_website": [
        "https://debiai.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "bias-detection",
        "evaluation",
        "visualization",
        "fairness"
      ],
      "id": 86
    },
    {
      "name": "Coupled-Biased-Random-Walks",
      "one_line_profile": "Outlier detection library for categorical data using coupled biased random walks",
      "detailed_description": "A Python implementation of the Coupled Biased Random Walks (CBRW) algorithm for outlier detection in categorical data. It models feature value interactions to identify anomalies in complex datasets.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "anomaly_detection",
        "data_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/dkaslovsky/Coupled-Biased-Random-Walks",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "outlier-detection",
        "categorical-data",
        "anomaly-detection"
      ],
      "id": 87
    },
    {
      "name": "Aequitas",
      "one_line_profile": "Open-source bias auditing and fair machine learning toolkit",
      "detailed_description": "A toolkit for auditing bias and fairness in machine learning models. It enables users to define and measure various fairness metrics across different population subgroups and visualize the results to inform mitigation strategies.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "bias_auditing",
        "fairness_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/dssg/aequitas",
      "help_website": [
        "http://aequitas.dssg.io"
      ],
      "license": "MIT",
      "tags": [
        "fairness",
        "bias-audit",
        "machine-learning"
      ],
      "id": 88
    },
    {
      "name": "acceptance-bench",
      "one_line_profile": "Robust LLM evaluation framework for measuring acceptance vs refusal",
      "detailed_description": "A framework for evaluating Large Language Models (LLMs) by measuring their acceptance or refusal of prompts across varying difficulty levels. It supports multi-prompt variations, temperature sweeping, and LLM-as-judge evaluation methods, focusing on safety and alignment.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "safety_evaluation",
        "alignment_testing",
        "llm_benchmarking"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/ellydee/acceptance-bench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm-evaluation",
        "safety",
        "red-teaming"
      ],
      "id": 89
    },
    {
      "name": "fairpy",
      "one_line_profile": "Library of fair division algorithms",
      "detailed_description": "An open-source Python library implementing various algorithms for fair division and resource allocation, supporting research in algorithmic game theory and economics.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "fair_division",
        "resource_allocation",
        "mathematical_modeling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/erelsgl/fairpy",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "fair-division",
        "game-theory",
        "algorithms"
      ],
      "id": 90
    },
    {
      "name": "grafana-panel-what-if",
      "one_line_profile": "Grafana panel for What-If predictive analysis with AI models",
      "detailed_description": "A Grafana plugin designed for conducting What-If analysis on Artificial Intelligence models. It allows users to interactively manipulate input variables and observe predicted outcomes, facilitating model explainability and decision support within monitoring dashboards.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "model_analysis",
        "explainability",
        "visualization"
      ],
      "application_level": "service",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/ertis-research/grafana-panel-what-if",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "grafana",
        "what-if-analysis",
        "explainable-ai"
      ],
      "id": 91
    },
    {
      "name": "DiffAI",
      "one_line_profile": "System for training provably robust neural networks",
      "detailed_description": "A library for training neural networks that are provably robust against adversarial attacks. It uses abstract interpretation and differentiable logic to certify the robustness of deep learning models during training.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robust_training",
        "adversarial_defense",
        "formal_verification"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/eth-sri/diffai",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "adversarial-robustness",
        "verification",
        "neural-networks"
      ],
      "id": 92
    },
    {
      "name": "Detectron",
      "one_line_profile": "Research platform for object detection and segmentation",
      "detailed_description": "FAIR's research platform for object detection research, implementing popular algorithms like Mask R-CNN and RetinaNet. It serves as a foundation for developing and evaluating computer vision models for scientific image analysis.",
      "domains": [
        "Cross-domain"
      ],
      "subtask_category": [
        "object_detection",
        "image_segmentation",
        "image_analysis"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/Detectron",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "computer-vision",
        "object-detection",
        "deep-learning"
      ],
      "id": 93
    },
    {
      "name": "disentangling-correlated-factors",
      "one_line_profile": "Benchmarking suite for disentanglement algorithms",
      "detailed_description": "A benchmarking suite designed to evaluate disentanglement algorithms, specifically focusing on robustness to correlated factors in data. It provides tools for generating datasets and measuring disentanglement metrics.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "disentanglement_evaluation",
        "robustness_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/disentangling-correlated-factors",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "disentanglement",
        "benchmarking",
        "representation-learning"
      ],
      "id": 94
    },
    {
      "name": "Polymath",
      "one_line_profile": "AI agent framework for symbolic reasoning and mathematics",
      "detailed_description": "An AI agent framework that leverages symbolic reasoning and auxiliary tools to solve complex problems in mathematics and logic. It integrates neural networks with symbolic solvers to enhance reasoning capabilities.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "symbolic_reasoning",
        "mathematical_solving",
        "inference"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/polymath",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "neuro-symbolic",
        "reasoning",
        "mathematics"
      ],
      "id": 95
    },
    {
      "name": "UniBench",
      "one_line_profile": "Library for evaluating VLM robustness across benchmarks",
      "detailed_description": "A Python library for evaluating the robustness of Vision-Language Models (VLMs) across a diverse set of benchmarks. It provides a unified interface for testing models against various distribution shifts and adversarial conditions.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "vlm_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/facebookresearch/unibench",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "vlm",
        "robustness",
        "evaluation"
      ],
      "id": 96
    },
    {
      "name": "Fairlearn",
      "one_line_profile": "Toolkit to assess and improve fairness of machine learning models",
      "detailed_description": "A Python package that empowers developers of artificial intelligence systems to assess their systems' fairness and mitigate any observed unfairness issues. It contains mitigation algorithms and metrics for model evaluation.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "fairness_assessment",
        "bias_mitigation",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/fairlearn/fairlearn",
      "help_website": [
        "https://fairlearn.org"
      ],
      "license": "MIT",
      "tags": [
        "fairness",
        "machine-learning",
        "responsible-ai"
      ],
      "id": 97
    },
    {
      "name": "fairCORELS",
      "one_line_profile": "Algorithm for learning fair and interpretable rule lists",
      "detailed_description": "An implementation of an algorithm for learning fair rule lists, balancing predictive accuracy, interpretability, and fairness. It extends the CORELS (Certifiably Optimal RulE ListS) algorithm to include fairness constraints.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "interpretable_ml",
        "fair_modeling",
        "rule_learning"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/ferryjul/fairCORELS",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "fairness",
        "interpretability",
        "rule-lists"
      ],
      "id": 98
    },
    {
      "name": "Jurity",
      "one_line_profile": "Fairness and evaluation library for ML models",
      "detailed_description": "A Python library for evaluating the fairness and performance of machine learning models. It provides a set of metrics and tools to detect bias and assess model quality in a trustworthy AI context.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "fairness_evaluation",
        "model_auditing",
        "metrics_calculation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/fidelity/jurity",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "fairness",
        "evaluation",
        "metrics"
      ],
      "id": 99
    },
    {
      "name": "AutoAttack",
      "one_line_profile": "Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks",
      "detailed_description": "A widely used library for evaluating the adversarial robustness of machine learning models. It implements an ensemble of parameter-free attacks (APGD-CE, APGD-DLR, FAB, Square Attack) to provide a reliable estimation of robustness accuracy, overcoming the pitfalls of weak or tuned attacks.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "adversarial_attack"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/fra31/auto-attack",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "adversarial-robustness",
        "evaluation",
        "pytorch",
        "security"
      ],
      "id": 100
    },
    {
      "name": "RETVec",
      "one_line_profile": "Resilient and Efficient Text Vectorizer for adversarial robustness",
      "detailed_description": "An efficient, multilingual, and adversarially-robust text vectorizer designed to protect models against character-level adversarial attacks (e.g., typos, homoglyphs). It serves as a robust preprocessing layer for NLP models.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "defense",
        "preprocessing",
        "robustness"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/google-research/retvec",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "nlp",
        "robustness",
        "vectorizer",
        "security"
      ],
      "id": 101
    },
    {
      "name": "infembed",
      "one_line_profile": "Influence functions and embedding analysis for model debugging",
      "detailed_description": "A library for analyzing and debugging generative models by identifying test samples where the model fails. It likely uses influence functions or embedding analysis to trace errors back to training data or model characteristics.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "debugging",
        "interpretability",
        "error_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/guidelabs/infembed",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "generative-ai",
        "debugging",
        "influence-functions"
      ],
      "id": 102
    },
    {
      "name": "Gyro Diagnostics",
      "one_line_profile": "AI Safety Diagnostics and Alignment Evaluation Lab",
      "detailed_description": "A toolkit for evaluating the safety and alignment of AI models. It provides diagnostic tools to assess potential risks and alignment issues in model behavior.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "safety_evaluation",
        "alignment"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/gyrogovernance/diagnostics",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ai-safety",
        "alignment",
        "evaluation"
      ],
      "id": 103
    },
    {
      "name": "EvalView",
      "one_line_profile": "Test harness for AI agents focusing on safety and cost",
      "detailed_description": "A pytest-style test harness designed for evaluating AI agents. It supports YAML-based scenarios, tool-call checks, and provides metrics for cost, latency, and safety evaluations, generating CI-friendly reports.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "safety_evaluation",
        "auditing",
        "agent_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/hidai25/eval-view",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ai-agents",
        "evaluation",
        "safety",
        "testing"
      ],
      "id": 104
    },
    {
      "name": "HolisticAI",
      "one_line_profile": "Library to assess and improve the trustworthiness of AI systems",
      "detailed_description": "An open-source library for auditing AI systems across multiple dimensions of trustworthiness, including bias, fairness, efficacy, and safety. It provides metrics and mitigation techniques.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "auditing",
        "fairness_evaluation",
        "safety_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/holistic-ai/holisticai",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "trustworthy-ai",
        "audit",
        "fairness",
        "risk-management"
      ],
      "id": 105
    },
    {
      "name": "Circular Bias Detection",
      "one_line_profile": "Framework for detecting circular reasoning bias in AI evaluation",
      "detailed_description": "A statistical framework designed to detect circular reasoning bias in the evaluation of AI algorithms. It helps ensure the validity and reliability of model performance assessments.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "bias_detection",
        "evaluation_audit"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/hongping-zh/circular-bias-detection",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bias",
        "evaluation",
        "statistics"
      ],
      "id": 106
    },
    {
      "name": "ScoreCardModel",
      "one_line_profile": "Library for developing credit scoring scorecards",
      "detailed_description": "A Python library for building scorecard models, commonly used in credit scoring. It implements statistical methods like Weight of Evidence (WoE) and Information Value (IV) for feature engineering and logistic regression for modeling, providing interpretable risk models.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "modeling",
        "interpretability",
        "risk_assessment"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/hsz1273327/ScoreCardModel",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "scorecard",
        "credit-scoring",
        "logistic-regression",
        "interpretability"
      ],
      "id": 107
    },
    {
      "name": "CROWN-IBP",
      "one_line_profile": "Certified defense and verification against adversarial examples",
      "detailed_description": "A library implementing certified defenses against adversarial examples using CROWN (Bounding Neural Network Robustness) and IBP (Interval Bound Propagation). It allows for the verification of neural network robustness guarantees.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "verification",
        "defense",
        "robustness"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/huanzhang12/CROWN-IBP",
      "help_website": [],
      "license": "BSD-2-Clause",
      "tags": [
        "verification",
        "adversarial-defense",
        "robustness",
        "neural-networks"
      ],
      "id": 108
    },
    {
      "name": "LLM Prompt Testing",
      "one_line_profile": "Framework for testing LLM prompts with Responsible AI metrics",
      "detailed_description": "A testing framework specifically for Large Language Models (LLMs) that computes NLP and Responsible AI metrics for model-generated answers. It helps in auditing and evaluating the safety and quality of LLM outputs.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "safety_evaluation",
        "auditing",
        "prompt_testing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/iamarunbrahma/llm-prompt-testing",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "responsible-ai",
        "testing",
        "metrics"
      ],
      "id": 109
    },
    {
      "name": "MultiCorrupt",
      "one_line_profile": "Benchmark for robust multi-modal 3D object detection",
      "detailed_description": "A benchmark toolkit for evaluating the robustness of LiDAR-Camera fusion models in autonomous driving. It simulates diverse corruption types (e.g., misalignment, weather) to assess model performance under challenging conditions.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/ika-rwth-aachen/MultiCorrupt",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "autonomous-driving",
        "robustness",
        "benchmark",
        "3d-object-detection"
      ],
      "id": 110
    },
    {
      "name": "ModelNet40-C",
      "one_line_profile": "Benchmark dataset and toolkit for 3D point cloud robustness against common corruptions",
      "detailed_description": "A benchmark suite designed to evaluate the robustness of 3D point cloud recognition models. It provides a set of common corruptions (e.g., noise, density changes) and tools to generate these corrupted datasets for rigorous model testing.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "data_corruption"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/jiachens/ModelNet40-C",
      "help_website": [
        "https://arxiv.org/abs/2201.12296"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "3d-point-cloud",
        "robustness",
        "benchmark",
        "corruptions"
      ],
      "id": 111
    },
    {
      "name": "ModelNet-C",
      "one_line_profile": "Benchmarking toolkit for analyzing point cloud classification under corruptions",
      "detailed_description": "A comprehensive benchmark for evaluating the robustness of point cloud classification models. It includes generated corrupted data variants and evaluation scripts to analyze model performance under various noise and distortion conditions.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "data_corruption"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/jiawei-ren/ModelNet-C",
      "help_website": [
        "https://arxiv.org/abs/2202.03377"
      ],
      "license": "Apache-2.0",
      "tags": [
        "point-cloud",
        "robustness",
        "classification",
        "benchmark"
      ],
      "id": 112
    },
    {
      "name": "MobileSafetyBench",
      "one_line_profile": "Benchmark for evaluating safety of autonomous agents in mobile device control",
      "detailed_description": "A safety evaluation benchmark specifically designed for autonomous agents operating in mobile environments. It provides scenarios and metrics to assess agent behavior and alignment with safety constraints.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "safety_evaluation",
        "agent_benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/jylee425/mobilesafetybench",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ai-safety",
        "autonomous-agents",
        "benchmark",
        "mobile-control"
      ],
      "id": 113
    },
    {
      "name": "LiDAR-Camera Robust Benchmark",
      "one_line_profile": "Toolkit to convert clean LiDAR-camera datasets into robustness benchmarks",
      "detailed_description": "A toolkit designed to generate robust benchmark datasets from clean LiDAR-camera data. It simulates various corruption scenarios to facilitate the evaluation of multi-modal perception models' robustness.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "data_generation",
        "robustness_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/kcyu2014/lidar-camera-robust-benchmark",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "lidar",
        "camera",
        "robustness",
        "data-generation"
      ],
      "id": 114
    },
    {
      "name": "fairness",
      "one_line_profile": "R package for computing and visualizing fair ML metrics",
      "detailed_description": "An R library dedicated to calculating, visualizing, and comparing various fairness metrics for machine learning models. It supports multiple definitions of fairness to aid in algorithmic auditing.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "fairness_evaluation",
        "auditing"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/kozodoi/fairness",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "r-package",
        "fairness-metrics",
        "visualization",
        "algorithmic-bias"
      ],
      "id": 115
    },
    {
      "name": "PointCloud-C",
      "one_line_profile": "Benchmarking toolkit for point cloud perception robustness",
      "detailed_description": "A toolkit and benchmark for analyzing the robustness of point cloud perception models against various corruptions. It includes code for generating corrupted data and evaluating model performance.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "data_corruption"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/ldkong1205/PointCloud-C",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "point-cloud",
        "robustness",
        "benchmark",
        "corruption-toolkit"
      ],
      "id": 116
    },
    {
      "name": "RobustMVD",
      "one_line_profile": "Benchmark for robust multi-view depth estimation",
      "detailed_description": "A benchmark suite for evaluating multi-view depth estimation models under robust conditions. It provides datasets and evaluation protocols to test model stability and accuracy.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "depth_estimation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/lmb-freiburg/robustmvd",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "multi-view-depth",
        "robustness",
        "benchmark",
        "computer-vision"
      ],
      "id": 117
    },
    {
      "name": "convex_adversarial",
      "one_line_profile": "Library for training neural networks with provable adversarial robustness",
      "detailed_description": "A Python library that implements methods for training neural networks that are provably robust to adversarial attacks, utilizing convex relaxation techniques.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robust_training",
        "verification"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/locuslab/convex_adversarial",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "adversarial-robustness",
        "provable-defense",
        "neural-networks",
        "convex-relaxation"
      ],
      "id": 118
    },
    {
      "name": "smoothing",
      "one_line_profile": "Code for randomized smoothing to achieve certified adversarial robustness",
      "detailed_description": "An implementation of randomized smoothing techniques to provide certified adversarial robustness for deep learning models at ImageNet scale.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_certification",
        "randomized_smoothing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/locuslab/smoothing",
      "help_website": [
        "https://arxiv.org/abs/1902.02918"
      ],
      "license": "NOASSERTION",
      "tags": [
        "certified-robustness",
        "randomized-smoothing",
        "imagenet",
        "adversarial-defense"
      ],
      "id": 119
    },
    {
      "name": "test-time-adaptation",
      "one_line_profile": "Benchmark and library for online test-time adaptation methods",
      "detailed_description": "A repository providing implementations and benchmarks for various online test-time adaptation (TTA) algorithms, allowing for the evaluation of model robustness and adaptation capabilities under distribution shifts.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "model_adaptation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/mariodoebler/test-time-adaptation",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "test-time-adaptation",
        "robustness",
        "benchmark"
      ],
      "id": 120
    },
    {
      "name": "Square Attack",
      "one_line_profile": "Query-efficient black-box adversarial attack implementation",
      "detailed_description": "An implementation of Square Attack, a score-based black-box adversarial attack that does not rely on local gradient information, used for evaluating the robustness of image classifiers.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "adversarial_attack",
        "robustness_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/max-andr/square-attack",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "adversarial-attacks",
        "robustness",
        "black-box-attack"
      ],
      "id": 121
    },
    {
      "name": "FairBench",
      "one_line_profile": "Comprehensive framework for AI fairness exploration and benchmarking",
      "detailed_description": "A framework designed to facilitate the exploration and evaluation of fairness in AI models, providing various metrics and tools to assess bias.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "fairness_evaluation",
        "bias_detection"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/mever-team/FairBench",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "fairness",
        "benchmark",
        "ai-ethics"
      ],
      "id": 122
    },
    {
      "name": "BIPIA",
      "one_line_profile": "Benchmark for indirect prompt injection attacks on LLMs",
      "detailed_description": "A benchmark dataset and evaluation framework for assessing the robustness of Large Language Models (LLMs) and their defenses against indirect prompt injection attacks.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "safety_testing"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/BIPIA",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "llm",
        "prompt-injection",
        "benchmark"
      ],
      "id": 123
    },
    {
      "name": "AI Agent Evals",
      "one_line_profile": "Evaluation tool for AI agent applications",
      "detailed_description": "A tool (often used as a GitHub Action) to evaluate AI agent applications using model-as-a-judge techniques, focusing on content safety and mathematical metrics.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "safety_evaluation",
        "agent_assessment"
      ],
      "application_level": "tool",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/ai-agent-evals",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ai-agents",
        "evaluation",
        "safety"
      ],
      "id": 124
    },
    {
      "name": "PromptBench",
      "one_line_profile": "Unified evaluation framework for large language models",
      "detailed_description": "A comprehensive framework for evaluating Large Language Models (LLMs) across various tasks, including robustness against adversarial prompts and other safety concerns.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "llm_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/promptbench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "benchmark",
        "robustness"
      ],
      "id": 125
    },
    {
      "name": "Responsible AI Toolbox",
      "one_line_profile": "Suite of tools for responsible AI assessment and debugging",
      "detailed_description": "A collection of interoperable tools and libraries for model and data exploration, enabling users to assess fairness, interpretability, error analysis, and counterfactuals.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "fairness_evaluation",
        "error_analysis",
        "interpretability"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/microsoft/responsible-ai-toolbox",
      "help_website": [
        "https://responsibleaitoolbox.ai/"
      ],
      "license": "MIT",
      "tags": [
        "responsible-ai",
        "fairness",
        "debugging"
      ],
      "id": 126
    },
    {
      "name": "GenBit",
      "one_line_profile": "Gender bias identification tool for text",
      "detailed_description": "A library within the Responsible AI Toolbox focused on measuring and identifying gender bias in text corpora and model outputs.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "bias_detection",
        "fairness_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/responsible-ai-toolbox-genbit",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "gender-bias",
        "nlp",
        "responsible-ai"
      ],
      "id": 127
    },
    {
      "name": "Responsible AI Mitigations",
      "one_line_profile": "Library for implementing Responsible AI mitigations",
      "detailed_description": "A Python library designed to help practitioners implement mitigation strategies for data and model issues identified during responsible AI assessment.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "bias_mitigation",
        "data_processing"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/microsoft/responsible-ai-toolbox-mitigations",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mitigation",
        "responsible-ai",
        "fairness"
      ],
      "id": 128
    },
    {
      "name": "Responsible AI Privacy",
      "one_line_profile": "Privacy estimation library using membership inference attacks",
      "detailed_description": "A library for statistically estimating the privacy risks of machine learning pipelines, specifically focusing on resistance to membership inference attacks.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "privacy_auditing",
        "membership_inference"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/responsible-ai-toolbox-privacy",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "privacy",
        "security",
        "auditing"
      ],
      "id": 129
    },
    {
      "name": "Responsible AI Tracker",
      "one_line_profile": "JupyterLab extension for tracking RAI experiments",
      "detailed_description": "A tool for tracking, managing, and comparing Responsible AI mitigations and experiments within JupyterLab, facilitating audit trails and reproducibility.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "experiment_tracking",
        "auditing"
      ],
      "application_level": "tool",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/microsoft/responsible-ai-toolbox-tracker",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "jupyterlab-extension",
        "tracking",
        "responsible-ai"
      ],
      "id": 130
    },
    {
      "name": "RobustLearn",
      "one_line_profile": "Library for robust machine learning",
      "detailed_description": "A library focused on robust machine learning techniques to improve model reliability and safety, supporting responsible AI development.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "model_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/robustlearn",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "robustness",
        "machine-learning",
        "responsible-ai"
      ],
      "id": 131
    },
    {
      "name": "AudioMarkBench",
      "one_line_profile": "Benchmark for audio watermarking robustness",
      "detailed_description": "A dataset and codebase for benchmarking the robustness of audio watermarking techniques against various attacks and perturbations.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "watermarking_analysis"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/mileskuo42/AudioMarkBench",
      "help_website": [],
      "license": "MPL-2.0",
      "tags": [
        "audio",
        "watermarking",
        "robustness"
      ],
      "id": 132
    },
    {
      "name": "FortisAVQA",
      "one_line_profile": "Robustness evaluation and bias mitigation for AVQA",
      "detailed_description": "A toolkit for evaluating robustness and mitigating bias in Audio-Visual Question Answering (AVQA) models.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "bias_mitigation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/mira-ai-lab/fortisavqa",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "avqa",
        "robustness",
        "bias"
      ],
      "id": 133
    },
    {
      "name": "MAITE",
      "one_line_profile": "Modular AI Trustworthy Engineering library",
      "detailed_description": "A library of common types, protocols, and utilities designed to support AI test and evaluation workflows, developed by MIT Lincoln Laboratory.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "test_and_evaluation",
        "trustworthy_engineering"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/mit-ll-ai-technology/maite",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "testing",
        "evaluation",
        "protocols"
      ],
      "id": 134
    },
    {
      "name": "MIT-LL Responsible AI Toolbox",
      "one_line_profile": "Library for evaluating AI robustness",
      "detailed_description": "A PyTorch-centric library developed by MIT Lincoln Laboratory for evaluating and enhancing the robustness of AI technologies.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "model_enhancement"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/mit-ll-responsible-ai/responsible-ai-toolbox",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "robustness",
        "pytorch",
        "evaluation"
      ],
      "id": 135
    },
    {
      "name": "Agentic Security",
      "one_line_profile": "Agentic LLM vulnerability scanner and red teaming kit",
      "detailed_description": "A tool designed to scan for vulnerabilities in LLM agents and perform red teaming operations to assess security and safety risks.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "red_teaming",
        "vulnerability_scanning"
      ],
      "application_level": "tool",
      "primary_language": "Python",
      "repo_url": "https://github.com/msoedov/agentic_security",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "security",
        "red-teaming"
      ],
      "id": 136
    },
    {
      "name": "NIC-RobustBench",
      "one_line_profile": "Toolkit for Neural Image Compression robustness analysis",
      "detailed_description": "A comprehensive open-source toolkit for evaluating the robustness of Neural Image Compression (NIC) methods against various perturbations.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "image_compression"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/msu-video-group/NIC-RobustBench",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "compression",
        "robustness",
        "benchmark"
      ],
      "id": 137
    },
    {
      "name": "fairensics",
      "one_line_profile": "Python library for discovering and mitigating biases in machine learning models",
      "detailed_description": "A Python library designed to help researchers and practitioners discover and mitigate biases in machine learning models and datasets. It provides implementations of various fairness metrics and mitigation algorithms.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "bias_detection",
        "fairness_mitigation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/nikikilbertus/fairensics",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fairness",
        "bias-mitigation",
        "machine-learning"
      ],
      "id": 138
    },
    {
      "name": "GenAIEval",
      "one_line_profile": "Evaluation framework for Generative AI performance and safety",
      "detailed_description": "A comprehensive evaluation framework, benchmark, and scorecard for Generative AI models. It targets performance metrics like throughput and latency, as well as accuracy, safety, and hallucination detection.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "model_evaluation",
        "safety_benchmarking",
        "hallucination_detection"
      ],
      "application_level": "platform",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/opea-project/GenAIEval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "genai",
        "evaluation",
        "benchmark",
        "safety"
      ],
      "id": 139
    },
    {
      "name": "huggingface_ding",
      "one_line_profile": "Utility for integrating DI-engine RL models with Hugging Face",
      "detailed_description": "Auxiliary tools for pulling and loading reinforcement learning models based on DI-engine from the Hugging Face Hub, and pushing them with auto-created model cards. Facilitates the management and sharing of scientific RL models.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "model_management",
        "reinforcement_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/opendilab/huggingface_ding",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "reinforcement-learning",
        "huggingface",
        "model-loading"
      ],
      "id": 140
    },
    {
      "name": "ethosai",
      "one_line_profile": "Platform for automating AI testing and ensuring model trustworthiness",
      "detailed_description": "A tool designed to automate the testing of AI models to ensure their trustworthiness. It likely provides interfaces for evaluating model performance, fairness, and robustness.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "model_testing",
        "trustworthiness_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "Vue",
      "repo_url": "https://github.com/osnHQ/ethosai",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "ai-testing",
        "trustworthiness",
        "automation"
      ],
      "id": 141
    },
    {
      "name": "openpilot_in_carla",
      "one_line_profile": "Integration of Openpilot with Carla simulator for safety evaluation",
      "detailed_description": "Provides configuration and tools to integrate the Openpilot autonomous driving stack with the Carla simulator. This setup is specifically designed for safety evaluation using almost-safe set based methods and includes radar input enhancements.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "safety_evaluation",
        "simulation",
        "autonomous_driving"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/pgchui/openpilot_in_carla",
      "help_website": [],
      "license": null,
      "tags": [
        "carla",
        "openpilot",
        "safety-evaluation",
        "simulation"
      ],
      "id": 142
    },
    {
      "name": "visual-auditor",
      "one_line_profile": "Interactive tool for auditing model biases and vulnerabilities",
      "detailed_description": "An interactive tool for scalable auditing of machine learning model biases and vulnerabilities. It provides interpretable mitigation strategies and visualization to help researchers understand model behavior.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "visual_auditing",
        "bias_detection",
        "interpretability"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/poloclub/visual-auditor",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "visualization",
        "auditing",
        "bias"
      ],
      "id": 143
    },
    {
      "name": "REVISE",
      "one_line_profile": "Tool for measuring and mitigating bias in visual datasets",
      "detailed_description": "REVISE (REvealing VIsual biaSEs) is a tool designed to measure and mitigate biases in visual datasets. It helps researchers identify potential issues in data distribution and representation before model training.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "dataset_auditing",
        "bias_mitigation",
        "computer_vision"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/princetonvisualai/revise-tool",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "dataset-bias",
        "computer-vision",
        "auditing"
      ],
      "id": 144
    },
    {
      "name": "promptfoo",
      "one_line_profile": "CLI tool for evaluating and red-teaming LLMs",
      "detailed_description": "A command-line tool for testing, evaluating, and red-teaming Large Language Models (LLMs). It allows users to compare performance across different models (GPT, Claude, Llama, etc.), scan for vulnerabilities, and ensure safety and accuracy in LLM applications.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "llm_evaluation",
        "red_teaming",
        "vulnerability_scanning"
      ],
      "application_level": "solver",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/promptfoo/promptfoo",
      "help_website": [
        "https://www.promptfoo.dev/"
      ],
      "license": "MIT",
      "tags": [
        "llm",
        "evaluation",
        "red-teaming",
        "security"
      ],
      "id": 145
    },
    {
      "name": "audit-ai",
      "one_line_profile": "Library for detecting demographic bias in ML models",
      "detailed_description": "A Python library designed to detect demographic differences and biases in the output of machine learning models or other assessments. It provides statistical tests to evaluate fairness.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "bias_detection",
        "fairness_auditing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pymetrics/audit-ai",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bias-detection",
        "fairness",
        "auditing"
      ],
      "id": 146
    },
    {
      "name": "MAGICAL",
      "one_line_profile": "Benchmark suite for robust imitation learning",
      "detailed_description": "The MAGICAL benchmark suite is designed to evaluate the robustness of imitation learning algorithms. It provides a set of tasks and environments to test how well agents generalize to variations in the environment.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "imitation_learning",
        "robustness_benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/qxcv/magical",
      "help_website": [],
      "license": "ISC",
      "tags": [
        "imitation-learning",
        "benchmark",
        "robustness"
      ],
      "id": 147
    },
    {
      "name": "AuditNLG",
      "one_line_profile": "Library for auditing and evaluating the trustworthiness of generative AI language models",
      "detailed_description": "AuditNLG is a Python library developed by Salesforce for auditing Generative AI language models. It provides tools to measure and evaluate trustworthiness, safety, and fairness in text generation, helping researchers identify biases and risks in NLG systems.",
      "domains": [
        "X1",
        "X1-05",
        "Computer Science"
      ],
      "subtask_category": [
        "auditing",
        "evaluation",
        "trustworthiness"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/salesforce/AuditNLG",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "nlp",
        "auditing",
        "trustworthiness",
        "generative-ai"
      ],
      "id": 148
    },
    {
      "name": "Fair-PCA",
      "one_line_profile": "Implementation of Fair PCA algorithm for dimensionality reduction with fairness constraints",
      "detailed_description": "This repository contains the MATLAB implementation of the Fair PCA algorithm. It provides a method for performing Principal Component Analysis while satisfying fairness constraints, ensuring that the dimensionality reduction process does not disproportionately disadvantage specific subgroups.",
      "domains": [
        "X1",
        "X1-05",
        "Mathematics"
      ],
      "subtask_category": [
        "dimensionality_reduction",
        "fairness"
      ],
      "application_level": "solver",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/samirasamadi/Fair-PCA",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pca",
        "fairness",
        "algorithm",
        "dimensionality-reduction"
      ],
      "id": 149
    },
    {
      "name": "OpenAgentSafety",
      "one_line_profile": "Framework for evaluating the safety of AI agents in realistic environments",
      "detailed_description": "OpenAgentSafety is a framework designed to evaluate the safety of AI agents. It provides realistic environments and scenarios to test agents for potential safety risks, ensuring they operate reliably and ethically in complex settings.",
      "domains": [
        "X1",
        "X1-05",
        "Computer Science"
      ],
      "subtask_category": [
        "safety_evaluation",
        "agent_auditing"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/sani903/OpenAgentSafety",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ai-safety",
        "agents",
        "evaluation",
        "benchmark"
      ],
      "id": 150
    },
    {
      "name": "scabench",
      "one_line_profile": "Benchmark framework for evaluating AI audit agents using real-world data",
      "detailed_description": "SCABench is a framework for evaluating AI audit agents. It utilizes recent real-world data to benchmark the performance of agents designed to audit other AI systems, focusing on their ability to detect vulnerabilities or compliance issues.",
      "domains": [
        "X1",
        "X1-05",
        "Computer Science"
      ],
      "subtask_category": [
        "benchmarking",
        "auditing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/scabench-org/scabench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "benchmark",
        "auditing",
        "ai-agents",
        "security"
      ],
      "id": 151
    },
    {
      "name": "frai",
      "one_line_profile": "Toolkit for responsible AI auditing, reporting, and risk assessment",
      "detailed_description": "frai is an open-source toolkit for responsible AI that includes a CLI and SDK. It helps developers scan code, collect evidence, and generate model cards and risk files, facilitating the auditing and documentation process for AI models.",
      "domains": [
        "X1",
        "X1-05",
        "Computer Science"
      ],
      "subtask_category": [
        "auditing",
        "documentation",
        "risk_assessment"
      ],
      "application_level": "workflow",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/sebuzdugan/frai",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "responsible-ai",
        "model-cards",
        "auditing",
        "cli"
      ],
      "id": 152
    },
    {
      "name": "ChemBench",
      "one_line_profile": "Benchmark datasets and tools for molecular machine learning",
      "detailed_description": "ChemBench provides benchmark datasets (MoleculeNet) and tools (MolMapNet) for evaluating machine learning models in chemistry. It serves as a standard resource for testing molecular property prediction and structure analysis algorithms.",
      "domains": [
        "Chemistry",
        "X1-05"
      ],
      "subtask_category": [
        "benchmarking",
        "molecular_property_prediction"
      ],
      "application_level": "dataset",
      "primary_language": "HTML",
      "repo_url": "https://github.com/shenwanxiang/ChemBench",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "chemistry",
        "benchmark",
        "moleculenet",
        "drug-discovery"
      ],
      "id": 153
    },
    {
      "name": "SIUO",
      "one_line_profile": "Cross-modality safety alignment framework for multimodal models",
      "detailed_description": "SIUO is a framework for Cross-Modality Safety Alignment, designed to improve the safety and robustness of multimodal AI models. It addresses challenges in aligning vision and language modalities to prevent harmful outputs.",
      "domains": [
        "X1",
        "X1-05",
        "Computer Science"
      ],
      "subtask_category": [
        "safety_alignment",
        "multimodal_learning"
      ],
      "application_level": "library",
      "primary_language": "HTML",
      "repo_url": "https://github.com/sinwang20/SIUO",
      "help_website": [],
      "license": null,
      "tags": [
        "safety",
        "alignment",
        "multimodal",
        "llm"
      ],
      "id": 154
    },
    {
      "name": "ASTRA-RL",
      "one_line_profile": "Adaptive Stress Testing for Robust AI toolbox",
      "detailed_description": "ASTRA-RL (Adaptive Stress Testing for Robust AI) is a toolbox for stress testing AI systems. It uses reinforcement learning to find failure modes and improve the robustness of AI models through adversarial training and adaptive testing scenarios.",
      "domains": [
        "X1",
        "X1-05",
        "Computer Science"
      ],
      "subtask_category": [
        "stress_testing",
        "robustness",
        "adversarial_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/sisl/astra-rl",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "robustness",
        "stress-testing",
        "reinforcement-learning",
        "safety"
      ],
      "id": 155
    },
    {
      "name": "NoW Evaluation",
      "one_line_profile": "Evaluation toolkit for 3D face reconstruction on the NoW benchmark",
      "detailed_description": "This repository contains the official evaluation scripts for the NoW Benchmark Dataset. It provides metrics to measure the accuracy and robustness of 3D face reconstruction methods from single images under varying conditions.",
      "domains": [
        "Computer Vision",
        "X1-05"
      ],
      "subtask_category": [
        "evaluation",
        "3d_reconstruction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/soubhiksanyal/now_evaluation",
      "help_website": [],
      "license": null,
      "tags": [
        "3d-face",
        "benchmark",
        "evaluation",
        "computer-vision"
      ],
      "id": 156
    },
    {
      "name": "VERITE",
      "one_line_profile": "Benchmark for multimodal misinformation detection accounting for unimodal bias",
      "detailed_description": "VERITE is a robust benchmark designed for evaluating multimodal misinformation detection models. It specifically accounts for unimodal bias to ensure that models are truly leveraging multimodal information for verification.",
      "domains": [
        "X1",
        "X1-05",
        "Computer Science"
      ],
      "subtask_category": [
        "misinformation_detection",
        "benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/stevejpapad/image-text-verification",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "misinformation",
        "multimodal",
        "benchmark",
        "verification"
      ],
      "id": 157
    },
    {
      "name": "LLaMA-MiLe-Loss",
      "one_line_profile": "Loss function implementation for mitigating learning difficulty bias in LLMs",
      "detailed_description": "This repository implements the MiLe (Mitigating Learning difficulty) Loss, a method designed to reduce bias in generative language models by accounting for the difficulty of learning different samples. It serves as a tool for improving model fairness and alignment.",
      "domains": [
        "X1",
        "X1-05",
        "Computer Science"
      ],
      "subtask_category": [
        "bias_mitigation",
        "model_training"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/suu990901/LLaMA-MiLe-Loss",
      "help_website": [],
      "license": null,
      "tags": [
        "llm",
        "bias-mitigation",
        "loss-function",
        "fairness"
      ],
      "id": 158
    },
    {
      "name": "NVIDIA Quantum Hybrid",
      "one_line_profile": "Prototype hybrid classical-quantum workflow with safety policy gates",
      "detailed_description": "This project provides a prototype workflow for hybrid classical-quantum computing. It includes policy gates for AI safety and security, supports Qiskit backends, and emits audit logs, serving as a template for secure quantum-classical integrations.",
      "domains": [
        "Quantum Computing",
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "workflow_orchestration",
        "safety_policy"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/sylvesterkaczmarek/nvidia-quantum-hybrid",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "quantum-computing",
        "hybrid-workflow",
        "safety",
        "audit"
      ],
      "id": 159
    },
    {
      "name": "PhiSat-2 Trustworthy AI",
      "one_line_profile": "Trustworthy onboard AI framework for satellite earth observation",
      "detailed_description": "This repository contains code for trustworthy onboard AI for the PhiSat-2 satellite mission. It includes tools for model calibration, telemetry, and execution in constrained environments (ONNX/INT8), specifically for Earth Observation tasks.",
      "domains": [
        "Aerospace",
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "onboard_inference",
        "calibration",
        "earth_observation"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/sylvesterkaczmarek/phisat2-trustworthy-onboard-ai",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "satellite",
        "edge-ai",
        "trustworthy-ai",
        "earth-observation"
      ],
      "id": 160
    },
    {
      "name": "LIBERO-plus",
      "one_line_profile": "Benchmark for robustness analysis of vision-language-action models",
      "detailed_description": "LIBERO-plus is a generalized benchmark designed for in-depth robustness analysis of vision-language-action models in robotics. It provides environments and metrics to evaluate how well agents generalize and handle perturbations.",
      "domains": [
        "Robotics",
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_analysis",
        "benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/sylvestf/LIBERO-plus",
      "help_website": [],
      "license": null,
      "tags": [
        "robotics",
        "benchmark",
        "robustness",
        "vla-models"
      ],
      "id": 161
    },
    {
      "name": "FairLens",
      "one_line_profile": "Library for identifying bias and measuring fairness in data",
      "detailed_description": "FairLens is an open-source Python library used to identify bias and measure fairness in datasets. It provides statistical metrics and visualization tools to help data scientists understand and mitigate biases before model training.",
      "domains": [
        "X1",
        "X1-05",
        "Data Science"
      ],
      "subtask_category": [
        "fairness_analysis",
        "bias_detection"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/synthesized-io/fairlens",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "fairness",
        "bias",
        "data-analysis",
        "visualization"
      ],
      "id": 162
    },
    {
      "name": "Re-Align",
      "one_line_profile": "Alignment framework to mitigate hallucinations in Vision Language Models",
      "detailed_description": "Re-Align is a framework that leverages image retrieval to mitigate hallucinations in Vision Language Models (VLMs). It aligns model outputs with retrieved visual evidence to improve factual accuracy and trustworthiness.",
      "domains": [
        "X1",
        "X1-05",
        "Computer Vision"
      ],
      "subtask_category": [
        "alignment",
        "hallucination_mitigation"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/taco-group/Re-Align",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "vlm",
        "alignment",
        "hallucination",
        "trustworthiness"
      ],
      "id": 163
    },
    {
      "name": "rGAN",
      "one_line_profile": "Label-Noise Robust Generative Adversarial Networks implementation",
      "detailed_description": "A PyTorch implementation of rGAN (Label-Noise Robust Generative Adversarial Networks), designed to learn clean data distributions from noisy training data in generative modeling tasks.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "generative_modeling",
        "robustness"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/takuhirok/rGAN",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gan",
        "label-noise",
        "robustness"
      ],
      "id": 164
    },
    {
      "name": "Fairness Indicators",
      "one_line_profile": "Fairness evaluation and visualization toolkit for TensorFlow models",
      "detailed_description": "A library that enables easy computation of commonly-identified fairness metrics for binary and multiclass classifiers, allowing researchers to evaluate model performance across different user slices.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "fairness_evaluation",
        "auditing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/tensorflow/fairness-indicators",
      "help_website": [
        "https://www.tensorflow.org/tfx/guide/fairness_indicators"
      ],
      "license": "Apache-2.0",
      "tags": [
        "fairness",
        "evaluation",
        "visualization"
      ],
      "id": 165
    },
    {
      "name": "Model Card Toolkit",
      "one_line_profile": "Toolkit for automating model card generation",
      "detailed_description": "A toolkit that streamlines and automates the generation of Model Cards, which are documents that provide context and transparency into a machine learning model's development and performance.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "auditing",
        "documentation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/tensorflow/model-card-toolkit",
      "help_website": [
        "https://www.tensorflow.org/responsible_ai/model_card_toolkit/guide"
      ],
      "license": "Apache-2.0",
      "tags": [
        "model-cards",
        "transparency",
        "auditing"
      ],
      "id": 166
    },
    {
      "name": "AISafetyLab",
      "one_line_profile": "Comprehensive framework for AI safety attack, defense, and evaluation",
      "detailed_description": "A framework covering safety attack, defense, and evaluation for AI models, providing a collection of methods to assess and improve model robustness.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "safety_evaluation",
        "adversarial_attack",
        "defense"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/thu-coai/AISafetyLab",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ai-safety",
        "adversarial-attacks",
        "defense"
      ],
      "id": 167
    },
    {
      "name": "cotk",
      "one_line_profile": "Toolkit for fast development and fair evaluation of text generation",
      "detailed_description": "Conversational Toolkit (cotk) is an open-source toolkit designed for fast development and fair evaluation of text generation models, providing standard metrics and datasets.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "evaluation",
        "text_generation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/thu-coai/cotk",
      "help_website": [
        "https://cotk.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "nlp",
        "evaluation",
        "text-generation"
      ],
      "id": 168
    },
    {
      "name": "3D_Corruptions_AD",
      "one_line_profile": "Benchmark for 3D object detection robustness in autonomous driving",
      "detailed_description": "A benchmark suite for evaluating the robustness of 3D object detection models against common corruptions in autonomous driving scenarios.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_benchmarking",
        "object_detection"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/thu-ml/3D_Corruptions_AD",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "robustness",
        "autonomous-driving",
        "3d-detection"
      ],
      "id": 169
    },
    {
      "name": "MMTrustEval",
      "one_line_profile": "Toolbox for benchmarking trustworthiness of multimodal LLMs",
      "detailed_description": "A toolbox designed for benchmarking the trustworthiness of multimodal large language models, covering various safety and reliability dimensions.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "trustworthiness_benchmarking",
        "multimodal_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/thu-ml/MMTrustEval",
      "help_website": [],
      "license": "CC-BY-SA-4.0",
      "tags": [
        "multimodal",
        "trustworthiness",
        "llm"
      ],
      "id": 170
    },
    {
      "name": "STAIR",
      "one_line_profile": "Safety alignment with introspective reasoning",
      "detailed_description": "Codebase for 'STAIR: Improving Safety Alignment with Introspective Reasoning', providing methods to align language models for safety using introspective reasoning techniques.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "safety_alignment",
        "reasoning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/thu-ml/STAIR",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "safety-alignment",
        "llm",
        "reasoning"
      ],
      "id": 171
    },
    {
      "name": "ARES",
      "one_line_profile": "Python library for adversarial machine learning and robustness benchmarking",
      "detailed_description": "A Python library focused on adversarial machine learning, specifically for benchmarking adversarial robustness of models against various attacks.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "adversarial_robustness",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/thu-ml/ares",
      "help_website": [
        "https://ares-ml.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "adversarial-ml",
        "robustness",
        "benchmarking"
      ],
      "id": 172
    },
    {
      "name": "Tiger",
      "one_line_profile": "Toolkit for building trustworthy LLM applications",
      "detailed_description": "An open-source LLM toolkit that includes modules for AI safety (TigerArmor), RAG (TigerRAG), and fine-tuning (TigerTune) to build trustworthy applications.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "safety_alignment",
        "rag",
        "fine_tuning"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/tigerlab-ai/tiger",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "trustworthiness",
        "safety"
      ],
      "id": 173
    },
    {
      "name": "Fast Adversarial Training",
      "one_line_profile": "Implementation of fast adversarial training methods",
      "detailed_description": "Code for 'Understanding and Improving Fast Adversarial Training', providing implementations for efficient adversarial training to improve model robustness.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "adversarial_training",
        "robustness"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/tml-epfl/understanding-fast-adv-training",
      "help_website": [],
      "license": null,
      "tags": [
        "adversarial-training",
        "robustness",
        "optimization"
      ],
      "id": 174
    },
    {
      "name": "Image Crop Analysis",
      "one_line_profile": "Code for analyzing fairness metrics in image cropping",
      "detailed_description": "A repository containing code and metrics for analyzing fairness and representation in image cropping algorithms, specifically focusing on Twitter's saliency algorithm.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "fairness_analysis",
        "auditing"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/twitter-research/image-crop-analysis",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "fairness",
        "image-cropping",
        "bias-analysis"
      ],
      "id": 175
    },
    {
      "name": "Armory",
      "one_line_profile": "Adversarial robustness evaluation test bed",
      "detailed_description": "A container-based test bed for evaluating the adversarial robustness of machine learning models against various attacks and scenarios.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "adversarial_defense"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/twosixlabs/armory",
      "help_website": [
        "https://armory.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "adversarial-robustness",
        "evaluation",
        "testbed"
      ],
      "id": 176
    },
    {
      "name": "Metric-Fairness",
      "one_line_profile": "Analysis of social bias in language model-based metrics",
      "detailed_description": "Code for analyzing social bias in language model-based metrics (like BERTScore) for text generation, providing tools to assess metric fairness.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "fairness_analysis",
        "metric_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/txsun1997/Metric-Fairness",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fairness",
        "nlp",
        "metrics"
      ],
      "id": 177
    },
    {
      "name": "CarDreamer",
      "one_line_profile": "World model based autonomous driving platform",
      "detailed_description": "A world model-based autonomous driving platform implemented in CARLA, enabling simulation and training of autonomous driving agents.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "autonomous_driving_simulation",
        "world_modeling"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/ucd-dare/CarDreamer",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "autonomous-driving",
        "world-model",
        "simulation"
      ],
      "id": 178
    },
    {
      "name": "WAVES",
      "one_line_profile": "Benchmark for image watermark robustness",
      "detailed_description": "Code for benchmarking the robustness of image watermarks against various attacks and distortions.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "watermark_robustness",
        "benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/umd-huang-lab/WAVES",
      "help_website": [],
      "license": null,
      "tags": [
        "watermarking",
        "robustness",
        "benchmarking"
      ],
      "id": 179
    },
    {
      "name": "Quantus",
      "one_line_profile": "Explainable AI toolkit for evaluation of neural network explanations",
      "detailed_description": "A comprehensive toolkit for the responsible evaluation of neural network explanations (XAI), providing a wide range of metrics to assess explanation quality.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "xai_evaluation",
        "interpretability"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/understandable-machine-intelligence-lab/Quantus",
      "help_website": [
        "https://quantus.readthedocs.io/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "xai",
        "evaluation",
        "interpretability"
      ],
      "id": 180
    },
    {
      "name": "Balanced-Datasets-Are-Not-Enough",
      "one_line_profile": "Gender bias estimation and mitigation in deep image representations",
      "detailed_description": "Implementation of methods to estimate and mitigate gender bias in deep image representations, going beyond simple dataset balancing.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "bias_mitigation",
        "fairness_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/uvavision/Balanced-Datasets-Are-Not-Enough",
      "help_website": [],
      "license": null,
      "tags": [
        "bias-mitigation",
        "computer-vision",
        "fairness"
      ],
      "id": 181
    },
    {
      "name": "Double-Hard-Debias",
      "one_line_profile": "Word embedding gender bias mitigation tool",
      "detailed_description": "Implementation of Double-Hard Debias, a method for tailoring word embeddings to mitigate gender bias.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "bias_mitigation",
        "nlp"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/uvavision/Double-Hard-Debias",
      "help_website": [],
      "license": null,
      "tags": [
        "debiasing",
        "word-embeddings",
        "fairness"
      ],
      "id": 182
    },
    {
      "name": "MagnetLoss-PyTorch",
      "one_line_profile": "PyTorch implementation of Magnet Loss for deep metric learning",
      "detailed_description": "A PyTorch implementation of Magnet Loss, a deep metric learning technique used for learning embeddings where local structure is preserved.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "metric_learning",
        "loss_function"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/vithursant/MagnetLoss-PyTorch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "metric-learning",
        "loss-function",
        "pytorch"
      ],
      "id": 183
    },
    {
      "name": "SDN",
      "one_line_profile": "Scene bias mitigation in action recognition",
      "detailed_description": "Implementation of a method to mitigate scene bias in action recognition models, improving generalization to unseen environments.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "bias_mitigation",
        "action_recognition"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/vt-vl-lab/SDN",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bias-mitigation",
        "computer-vision",
        "action-recognition"
      ],
      "id": 184
    },
    {
      "name": "Trojan-Activation-Attack",
      "one_line_profile": "Trojan activation attack for LLM safety alignment",
      "detailed_description": "Implementation of Trojan Activation Attack, a method to attack Large Language Models using activation steering to test safety alignment.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "adversarial_attack",
        "safety_alignment"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/wang2226/Trojan-Activation-Attack",
      "help_website": [],
      "license": null,
      "tags": [
        "llm",
        "adversarial-attack",
        "safety"
      ],
      "id": 185
    },
    {
      "name": "RoboBEV",
      "one_line_profile": "Benchmark for BEV perception robustness in autonomous driving",
      "detailed_description": "A benchmark for evaluating and improving the robustness of Bird's Eye View (BEV) perception models in autonomous driving scenarios.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_benchmarking",
        "autonomous_driving"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/worldbench/RoboBEV",
      "help_website": [],
      "license": null,
      "tags": [
        "robustness",
        "bev",
        "autonomous-driving"
      ],
      "id": 186
    },
    {
      "name": "Adversarial_Long-Tail",
      "one_line_profile": "Adversarial robustness under long-tailed distribution",
      "detailed_description": "PyTorch implementation of methods for improving adversarial robustness in models trained on long-tailed distributions.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness",
        "adversarial_defense"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/wutong16/Adversarial_Long-Tail",
      "help_website": [],
      "license": null,
      "tags": [
        "robustness",
        "long-tail",
        "adversarial-learning"
      ],
      "id": 187
    },
    {
      "name": "Structural Crack Detection",
      "one_line_profile": "Deep learning tool for structural crack detection and classification",
      "detailed_description": "A deep learning-based tool for automated visual inspection in aircraft maintenance, specifically for detecting and classifying structural cracks.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "defect_detection",
        "image_classification"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/xaviergoby/Deep-Learning-and-Computer-Vision-for-Structural-Crack-Detection-And-Classification",
      "help_website": [],
      "license": null,
      "tags": [
        "crack-detection",
        "computer-vision",
        "maintenance"
      ],
      "id": 188
    },
    {
      "name": "BETA",
      "one_line_profile": "Method for mitigating confirmation bias in domain adaptation of black-box predictors",
      "detailed_description": "A Python implementation of the 'Divide to Adapt' approach to mitigate confirmation bias during the domain adaptation process for black-box predictors, enhancing model reliability.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "bias_mitigation",
        "domain_adaptation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/xyupeng/BETA",
      "help_website": [],
      "license": null,
      "tags": [
        "domain-adaptation",
        "bias-mitigation",
        "black-box-optimization"
      ],
      "id": 189
    },
    {
      "name": "semisup-adv",
      "one_line_profile": "Semi-supervised learning framework for improving adversarial robustness",
      "detailed_description": "Code implementation for using semi-supervised learning techniques to enhance the adversarial robustness of machine learning models, as presented in NeurIPS 2019.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "adversarial_robustness",
        "semi_supervised_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/yaircarmon/semisup-adv",
      "help_website": [
        "https://arxiv.org/pdf/1905.13736.pdf"
      ],
      "license": "MIT",
      "tags": [
        "adversarial-robustness",
        "semi-supervised-learning",
        "deep-learning"
      ],
      "id": 190
    },
    {
      "name": "MLLM_safety_study",
      "one_line_profile": "Safety alignment framework for multi-modal large language models",
      "detailed_description": "Official code for a CVPR 2025 paper investigating the necessity of curated malicious data for safety alignment in multi-modal large language models (MLLMs).",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "safety_alignment",
        "model_auditing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ybwang119/MLLM_safety_study",
      "help_website": [],
      "license": null,
      "tags": [
        "mllm",
        "safety-alignment",
        "multimodal"
      ],
      "id": 191
    },
    {
      "name": "Robustness-Aware-Pruning-ADMM",
      "one_line_profile": "Model pruning framework balancing adversarial robustness and compression",
      "detailed_description": "Implementation of a robustness-aware pruning method using ADMM (Alternating Direction Method of Multipliers) to achieve both model compression and adversarial robustness.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "model_compression",
        "adversarial_robustness"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/yeshaokai/Robustness-Aware-Pruning-ADMM",
      "help_website": [],
      "license": null,
      "tags": [
        "pruning",
        "admm",
        "adversarial-robustness"
      ],
      "id": 192
    },
    {
      "name": "pytorch-adversarial-training",
      "one_line_profile": "PyTorch implementation of adversarial training for robust classification",
      "detailed_description": "A PyTorch implementation for adversarial training on datasets like MNIST and CIFAR-10, including visualization tools for robustness classifiers.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "adversarial_training",
        "robustness_visualization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ylsung/pytorch-adversarial-training",
      "help_website": [],
      "license": null,
      "tags": [
        "pytorch",
        "adversarial-training",
        "robustness"
      ],
      "id": 193
    },
    {
      "name": "bursting-burden",
      "one_line_profile": "Assessment tool for counterfactual-based fairness metrics",
      "detailed_description": "Code accompanying a paper that assesses the 'Burden' metric, a counterfactual-based fairness metric, providing tools to evaluate algorithmic fairness.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "fairness_evaluation",
        "metric_assessment"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/yochem/bursting-burden",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fairness",
        "counterfactuals",
        "metrics"
      ],
      "id": 194
    },
    {
      "name": "fairml-farm",
      "one_line_profile": "Collection of fair machine learning algorithm implementations",
      "detailed_description": "A library providing implementations of various fair machine learning algorithms to facilitate research and application of fairness in AI.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "fairness_algorithms",
        "bias_mitigation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/yoshavit/fairml-farm",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fair-ml",
        "fairness",
        "algorithms"
      ],
      "id": 195
    },
    {
      "name": "VLGuard",
      "one_line_profile": "Safety fine-tuning baseline for Vision Large Language Models",
      "detailed_description": "Implementation of VLGuard, a method for safety fine-tuning of Vision Large Language Models (VLLMs) with minimal cost, as presented at ICML 2024.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "safety_finetuning",
        "vllm_alignment"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ys-zong/VLGuard",
      "help_website": [],
      "license": null,
      "tags": [
        "vllm",
        "safety",
        "fine-tuning"
      ],
      "id": 196
    },
    {
      "name": "MT-Consistency",
      "one_line_profile": "Framework for evaluating and mitigating acquiescence bias in LLMs",
      "detailed_description": "A repository investigating LLMs' tendency to exhibit acquiescence bias in sequential QA interactions, including evaluation methods, datasets, and mitigation code.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "bias_evaluation",
        "consistency_check"
      ],
      "application_level": "benchmark",
      "primary_language": "Python",
      "repo_url": "https://github.com/yubol-bobo/MT-Consistency",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "bias",
        "consistency"
      ],
      "id": 197
    },
    {
      "name": "RoDLA",
      "one_line_profile": "Benchmark for robustness of document layout analysis models",
      "detailed_description": "A benchmarking tool designed to evaluate the robustness of Document Layout Analysis (DLA) models against various perturbations.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_benchmarking",
        "document_analysis"
      ],
      "application_level": "benchmark",
      "primary_language": "Python",
      "repo_url": "https://github.com/yufanchen96/RoDLA",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "robustness",
        "benchmark",
        "document-layout-analysis"
      ],
      "id": 198
    },
    {
      "name": "MTSA",
      "one_line_profile": "Multi-turn safety alignment for LLMs via multi-round red-teaming",
      "detailed_description": "Official implementation of MTSA, a method for aligning Large Language Models for safety through multi-round red-teaming interactions.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "safety_alignment",
        "red_teaming"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/yuki-younai/MTSA",
      "help_website": [],
      "license": null,
      "tags": [
        "llm",
        "safety",
        "red-teaming"
      ],
      "id": 199
    },
    {
      "name": "CPP",
      "one_line_profile": "Depth prediction improvement by mitigating pose distribution bias",
      "detailed_description": "Code for the CVPR 2021 paper 'Camera Pose Matters', providing methods to improve depth prediction by mitigating pose distribution bias.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "bias_mitigation",
        "depth_prediction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/yunhan-zhao/CPP",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "computer-vision",
        "bias",
        "depth-prediction"
      ],
      "id": 200
    },
    {
      "name": "EDITS",
      "one_line_profile": "Modeling and mitigating data bias for Graph Neural Networks",
      "detailed_description": "Open source code for the paper 'EDITS', focusing on modeling and mitigating data bias in Graph Neural Networks (GNNs).",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "bias_mitigation",
        "graph_neural_networks"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/yushundong/EDITS",
      "help_website": [],
      "license": null,
      "tags": [
        "gnn",
        "bias",
        "graph-mining"
      ],
      "id": 201
    },
    {
      "name": "PyGDebias",
      "one_line_profile": "Library for fairness-aware graph mining algorithms",
      "detailed_description": "An open-source library providing graph datasets and implementations of fairness-aware graph mining algorithms (PyGDebias).",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "fairness_algorithms",
        "graph_mining"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/yushundong/PyGDebias",
      "help_website": [],
      "license": "BSD-2-Clause",
      "tags": [
        "graph-mining",
        "fairness",
        "debiasing"
      ],
      "id": 202
    },
    {
      "name": "What-If-Explainability",
      "one_line_profile": "Explainability tool for LightGBM using FastTreeShap and What-If Tool",
      "detailed_description": "A notebook-based tool demonstrating how to explain LightGBM tree models using FastTreeShap (Shapley values) and Google's What-If Tool.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "explainability",
        "model_auditing"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/zabir-nabil/What-If-Explainability",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "explainability",
        "shap",
        "what-if-tool"
      ],
      "id": 203
    },
    {
      "name": "dps-benchmark",
      "one_line_profile": "Benchmark for diffusion posterior sampling in Bayesian inverse problems",
      "detailed_description": "A framework for fair and objective benchmarking of diffusion posterior sampling algorithms applied to Bayesian inverse problems.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "benchmarking",
        "bayesian_inference"
      ],
      "application_level": "benchmark",
      "primary_language": "Python",
      "repo_url": "https://github.com/zacmar/dps-benchmark",
      "help_website": [],
      "license": null,
      "tags": [
        "diffusion-models",
        "bayesian",
        "inverse-problems"
      ],
      "id": 204
    },
    {
      "name": "AIM-Fair",
      "one_line_profile": "Algorithmic fairness via selective fine-tuning with synthetic data",
      "detailed_description": "Implementation of AIM-Fair, a method to advance algorithmic fairness by selectively fine-tuning biased models using contextual synthetic data.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "fairness_optimization",
        "fine_tuning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/zengqunzhao/AIM-Fair",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fairness",
        "synthetic-data",
        "fine-tuning"
      ],
      "id": 205
    },
    {
      "name": "Debiased-Chat",
      "one_line_profile": "Gender bias mitigation for neural dialogue generation",
      "detailed_description": "PyTorch implementation of adversarial learning techniques to mitigate gender bias in neural dialogue generation models.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "bias_mitigation",
        "dialogue_generation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/zgahhblhc/Debiased-Chat",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gender-bias",
        "dialogue-systems",
        "adversarial-learning"
      ],
      "id": 206
    },
    {
      "name": "DebiAN",
      "one_line_profile": "Unknown bias discovery and mitigation network",
      "detailed_description": "Official code for 'Discover and Mitigate Unknown Biases with Debiasing Alternate Networks', providing a framework to handle unknown biases in models.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "bias_discovery",
        "bias_mitigation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/zhihengli-UR/DebiAN",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "bias-mitigation",
        "deep-learning",
        "fairness"
      ],
      "id": 207
    },
    {
      "name": "PE-RLHF",
      "one_line_profile": "Safe autonomous driving via RLHF and physics knowledge",
      "detailed_description": "Code for 'Trustworthy Human-AI Collaboration', integrating Reinforcement Learning with Human Feedback (RLHF) and physics knowledge for safe autonomous driving.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "safety_alignment",
        "rlhf"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/zilin-huang/PE-RLHF",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "autonomous-driving",
        "rlhf",
        "safety"
      ],
      "id": 208
    },
    {
      "name": "terrain_benchmark",
      "one_line_profile": "Robustness benchmark for legged locomotion on terrains",
      "detailed_description": "A benchmark suite for evaluating the robustness of legged locomotion control policies across various terrain types.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "robustness_benchmarking",
        "robotics_control"
      ],
      "application_level": "benchmark",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/zita-ch/terrain_benchmark",
      "help_website": [],
      "license": null,
      "tags": [
        "robotics",
        "robustness",
        "locomotion"
      ],
      "id": 209
    },
    {
      "name": "AdvTrajectoryPrediction",
      "one_line_profile": "Adversarial robustness evaluation for trajectory prediction",
      "detailed_description": "Implementation of methods to evaluate and improve the adversarial robustness of trajectory prediction models for autonomous vehicles.",
      "domains": [
        "X1",
        "X1-05"
      ],
      "subtask_category": [
        "adversarial_robustness",
        "trajectory_prediction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/zqzqz/AdvTrajectoryPrediction",
      "help_website": [
        "https://arxiv.org/abs/2201.05057"
      ],
      "license": null,
      "tags": [
        "autonomous-vehicles",
        "adversarial-robustness",
        "trajectory-prediction"
      ],
      "id": 210
    }
  ]
}
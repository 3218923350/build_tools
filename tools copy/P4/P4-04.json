{
  "generated_at": "2025-12-16T16:30:03.296345+08:00",
  "metadata": {
    "leaf_cluster": {
      "leaf_cluster_id": "P4",
      "leaf_cluster_name": "物理-控制/反问题/最优化生态",
      "domain": "Physics/Math",
      "typical_objects": "signals/fields",
      "task_chain": "反演→优化→UQ→控制→评测",
      "tool_form": "优化/推断库 + 可微分框架"
    },
    "unit": {
      "unit_id": "P4-04",
      "unit_name": "RL 控制与策略优化",
      "target_scale": "200–450",
      "coverage_tools": "RL toolkits、control libs"
    },
    "search": {
      "target_candidates": 450,
      "queries": [
        "[GH] MushroomRL",
        "[GH] Garage",
        "[GH] Acme",
        "[GH] DeepMind Control Suite",
        "[GH] Brax",
        "[GH] Gymnasium",
        "[GH] CleanRL",
        "[GH] Tianshou",
        "[GH] Ray Rllib",
        "[GH] Stable Baselines3",
        "[GH] reinforcement learning control",
        "[GH] optimal control library",
        "[GH] policy optimization",
        "[GH] model predictive control",
        "[GH] deep reinforcement learning physics",
        "[GH] differentiable control",
        "[GH] continuous control",
        "[GH] physics based reinforcement learning",
        "[GH] gym environment physics",
        "[GH] soft actor critic",
        "[GH] proximal policy optimization",
        "[GH] pde control rl",
        "[GH] fluid control rl",
        "[GH] quantum control rl",
        "[WEB] reinforcement learning for physical systems github",
        "[WEB] deep reinforcement learning control library github",
        "[WEB] physics informed reinforcement learning github",
        "[WEB] optimal control python library github",
        "[WEB] differentiable physics control github",
        "[WEB] model predictive control rl github"
      ],
      "total_candidates": 1082,
      "tool_candidates": 754,
      "final_tools": 248
    }
  },
  "tools": [
    {
      "name": "raylab",
      "one_line_profile": "Reinforcement learning algorithms extension for Ray RLlib",
      "detailed_description": "A collection of reinforcement learning algorithms implemented on top of Ray RLlib, providing additional solver capabilities for control and optimization tasks.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "control_policy"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/0xangelo/raylab",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rllib",
        "reinforcement-learning",
        "ray"
      ],
      "id": 1
    },
    {
      "name": "MPPI-Generic",
      "one_line_profile": "Templated C++/CUDA implementation of Model Predictive Path Integral Control",
      "detailed_description": "A generic, high-performance C++/CUDA implementation of the Model Predictive Path Integral (MPPI) control algorithm, designed for path planning and control in robotics and physics simulations.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "optimal_control",
        "path_planning"
      ],
      "application_level": "solver",
      "primary_language": "Cuda",
      "repo_url": "https://github.com/ACDSLab/MPPI-Generic",
      "help_website": [],
      "license": "BSD-2-Clause",
      "tags": [
        "mppi",
        "model-predictive-control",
        "cuda"
      ],
      "id": 2
    },
    {
      "name": "Matlab_PPO",
      "one_line_profile": "MATLAB-based reinforcement learning framework for control systems",
      "detailed_description": "A MATLAB framework implementing Proximal Policy Optimization (PPO) and Multi-Agent PPO (MAPPO) algorithms, supporting GPU acceleration for research and engineering in control systems.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "control_systems"
      ],
      "application_level": "library",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/AIResearcherHZ/Matlab_PPO",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "matlab",
        "ppo",
        "control-systems"
      ],
      "id": 3
    },
    {
      "name": "Fancy Gym",
      "one_line_profile": "Unified interface for RL benchmarks with Black Box optimization support",
      "detailed_description": "A library that unifies interfaces for various Reinforcement Learning benchmarks and extends OpenAI Gym to support Black Box approaches and motion primitives.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "benchmark"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ALRhub/fancy_gym",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gym",
        "reinforcement-learning",
        "benchmarking"
      ],
      "id": 4
    },
    {
      "name": "RF-MPC",
      "one_line_profile": "Representation-Free Model Predictive Control implementation",
      "detailed_description": "A MATLAB implementation of Representation-Free Model Predictive Control (RF-MPC) designed for dynamic quadruped robots, applicable to control theory research.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "model_predictive_control",
        "robotics_control"
      ],
      "application_level": "solver",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/ARCaD-Lab-UM/RF-MPC",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mpc",
        "control-theory",
        "quadruped"
      ],
      "id": 5
    },
    {
      "name": "ppo-pytorch",
      "one_line_profile": "Clean PyTorch implementation of Proximal Policy Optimization",
      "detailed_description": "A modular and readable implementation of the Proximal Policy Optimization (PPO) algorithm in PyTorch, serving as a baseline solver for RL research.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "policy_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ASzot/ppo-pytorch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ppo",
        "pytorch",
        "reinforcement-learning"
      ],
      "id": 6
    },
    {
      "name": "Admyral",
      "one_line_profile": "Continuous control monitoring platform",
      "detailed_description": "A tool enabling continuous monitoring for custom control systems, facilitating the management and observability of control loops in research and production.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "control_monitoring",
        "system_observability"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/Admyral-Technologies/admyral",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "control-systems",
        "monitoring",
        "automation"
      ],
      "id": 7
    },
    {
      "name": "Traffic-Signal-Control-Agent",
      "one_line_profile": "Deep Q-Learning framework for traffic signal control",
      "detailed_description": "A simulation framework where a Deep Q-Learning agent optimizes traffic light phases to maximize efficiency, serving as a research tool for traffic control systems.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "traffic_control",
        "reinforcement_learning"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/AndreaVidali/Deep-QLearning-Agent-for-Traffic-Signal-Control",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "traffic-control",
        "deep-q-learning",
        "simulation"
      ],
      "id": 8
    },
    {
      "name": "SimRLFab",
      "one_line_profile": "Simulation and RL framework for manufacturing control",
      "detailed_description": "A framework combining simulation and reinforcement learning for production planning and control in complex job shop manufacturing systems.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "production_control",
        "simulation"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/AndreasKuhnle/SimRLFab",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "manufacturing",
        "reinforcement-learning",
        "simulation"
      ],
      "id": 9
    },
    {
      "name": "rscope",
      "one_line_profile": "RL training visualizer for Mujoco and Brax",
      "detailed_description": "A visualization tool designed for monitoring Reinforcement Learning training processes in Mujoco Playground and Brax environments.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "visualization",
        "training_monitoring"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Andrew-Luo1/rscope",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "visualization",
        "mujoco",
        "brax"
      ],
      "id": 10
    },
    {
      "name": "spark-sched-sim",
      "one_line_profile": "Gymnasium environment for Spark job scheduling simulation",
      "detailed_description": "A Gymnasium-compliant environment for simulating and researching job scheduling algorithms in Apache Spark clusters using reinforcement learning.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "scheduling_simulation",
        "reinforcement_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ArchieGertsman/spark-sched-sim",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "spark",
        "scheduling",
        "gymnasium"
      ],
      "id": 11
    },
    {
      "name": "rosnav-rl",
      "one_line_profile": "Modular DRL framework for ROS 2 navigation",
      "detailed_description": "A modular Deep Reinforcement Learning framework for ROS 2, supporting pluggable RL backends like Stable-Baselines3 for robotic navigation research.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "robotics_navigation",
        "reinforcement_learning"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/Arena-Rosnav/rosnav-rl",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ros2",
        "navigation",
        "robotics"
      ],
      "id": 12
    },
    {
      "name": "SAC_discrete",
      "one_line_profile": "PyTorch implementation of discrete Soft-Actor-Critic",
      "detailed_description": "A PyTorch implementation of the Soft-Actor-Critic (SAC) algorithm adapted for discrete action spaces, serving as a solver for RL problems.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "algorithm_implementation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/BY571/SAC_discrete",
      "help_website": [],
      "license": null,
      "tags": [
        "sac",
        "discrete-control",
        "pytorch"
      ],
      "id": 13
    },
    {
      "name": "Soft-Actor-Critic-and-Extensions",
      "one_line_profile": "PyTorch library for SAC and its extensions",
      "detailed_description": "A comprehensive PyTorch implementation of Soft-Actor-Critic (SAC) along with extensions like PER, ERE, Munchausen RL, and D2RL for advanced RL research.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "algorithm_implementation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/BY571/Soft-Actor-Critic-and-Extensions",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "sac",
        "reinforcement-learning",
        "pytorch"
      ],
      "id": 14
    },
    {
      "name": "Advanced-Soft-Actor-Critic",
      "one_line_profile": "Soft Actor-Critic implementation with advanced features",
      "detailed_description": "A PyTorch implementation of the Soft Actor-Critic algorithm incorporating advanced features for robust reinforcement learning control.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "algorithm_implementation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/BlueFisher/Advanced-Soft-Actor-Critic",
      "help_website": [],
      "license": null,
      "tags": [
        "sac",
        "reinforcement-learning",
        "control"
      ],
      "id": 15
    },
    {
      "name": "DeepCoMP",
      "one_line_profile": "Deep RL framework for cooperative multipoint selection",
      "detailed_description": "A simulation framework using Multi-Agent Deep Reinforcement Learning for dynamic multi-cell selection in cooperative multipoint (CoMP) telecommunication systems.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "telecom_control",
        "multi_agent_rl"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/CN-UPB/DeepCoMP",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "telecommunications",
        "comp",
        "marl"
      ],
      "id": 16
    },
    {
      "name": "policy-distillation-baselines",
      "one_line_profile": "Policy Distillation baselines for control",
      "detailed_description": "A PyTorch implementation of Policy Distillation algorithms for control tasks, providing baselines for compressing teacher policies into student networks.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "policy_distillation",
        "model_compression"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/CUN-bjy/policy-distillation-baselines",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "policy-distillation",
        "control",
        "pytorch"
      ],
      "id": 17
    },
    {
      "name": "mujoco-benchmark",
      "one_line_profile": "Reinforcement learning benchmark for MuJoCo environments",
      "detailed_description": "A comprehensive benchmark suite for reinforcement learning algorithms (DDPG, SAC, TD3, etc.) on MuJoCo physics simulation environments.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "benchmarking",
        "physics_simulation"
      ],
      "application_level": "library",
      "primary_language": null,
      "repo_url": "https://github.com/ChenDRAG/mujoco-benchmark",
      "help_website": [],
      "license": null,
      "tags": [
        "mujoco",
        "benchmark",
        "reinforcement-learning"
      ],
      "id": 18
    },
    {
      "name": "gym-continuousDoubleAuction",
      "one_line_profile": "MARL environment for continuous double auction markets",
      "detailed_description": "A custom Multi-Agent Reinforcement Learning (MARL) environment simulating a zero-sum continuous double auction market for trading research.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "market_simulation",
        "multi_agent_rl"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/ChuaCheowHuan/gym-continuousDoubleAuction",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "auction-simulation",
        "marl",
        "trading"
      ],
      "id": 19
    },
    {
      "name": "Gym-Trading-Env",
      "one_line_profile": "Gymnasium environment for trading simulation",
      "detailed_description": "A customizable and easy-to-use Gymnasium environment designed for simulating financial trading and training reinforcement learning agents.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "trading_simulation",
        "reinforcement_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ClementPerroud/Gym-Trading-Env",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "trading",
        "gymnasium",
        "simulation"
      ],
      "id": 20
    },
    {
      "name": "arcle",
      "one_line_profile": "Gymnasium environment for Abstraction and Reasoning Corpus",
      "detailed_description": "A Gymnasium-based environment for the Abstraction and Reasoning Corpus (ARC), enabling RL research on abstract reasoning tasks.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reasoning_simulation",
        "reinforcement_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ConfeitoHS/arcle",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "arc",
        "reasoning",
        "gymnasium"
      ],
      "id": 21
    },
    {
      "name": "rl-baselines3-zoo",
      "one_line_profile": "Training framework for Stable Baselines3",
      "detailed_description": "A training framework built on Stable Baselines3 that includes hyperparameter optimization, training scripts, and a collection of pre-trained agents.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "hyperparameter_optimization"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/DLR-RM/rl-baselines3-zoo",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "stable-baselines3",
        "training-framework",
        "rl"
      ],
      "id": 22
    },
    {
      "name": "rl-trained-agents",
      "one_line_profile": "Pre-trained RL agents for Stable Baselines3",
      "detailed_description": "A repository containing a large collection of pre-trained reinforcement learning agents using Stable Baselines3, serving as a model zoo for research and benchmarking.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "model_zoo",
        "benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/DLR-RM/rl-trained-agents",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pretrained-models",
        "stable-baselines3",
        "rl"
      ],
      "id": 23
    },
    {
      "name": "stable-baselines3",
      "one_line_profile": "Reliable Reinforcement Learning algorithms in PyTorch",
      "detailed_description": "A set of reliable, high-quality implementations of reinforcement learning algorithms in PyTorch, widely used as a standard library for RL research and application.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "algorithm_library"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/DLR-RM/stable-baselines3",
      "help_website": [
        "https://stable-baselines3.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "reinforcement-learning",
        "pytorch",
        "algorithms"
      ],
      "id": 24
    },
    {
      "name": "gym4ReaL",
      "one_line_profile": "Gymnasium-based benchmarking suite for real-world RL scenarios",
      "detailed_description": "A benchmarking suite designed to test Reinforcement Learning algorithms on real-world inspired scenarios, built upon the Gymnasium API to ensure compatibility and standardization.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Daveonwave/gym4ReaL",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "reinforcement-learning",
        "benchmarking",
        "gymnasium",
        "real-world-rl"
      ],
      "id": 25
    },
    {
      "name": "machina",
      "one_line_profile": "Deep Reinforcement Learning framework for control tasks",
      "detailed_description": "A Deep Reinforcement Learning framework designed for control tasks, providing modular implementations of various DRL algorithms to facilitate research and development in continuous control.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "control_policy"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/DeepX-inc/machina",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "deep-reinforcement-learning",
        "control",
        "pytorch",
        "framework"
      ],
      "id": 26
    },
    {
      "name": "evogp",
      "one_line_profile": "GPU-accelerated library for Tree-based Genetic Programming",
      "detailed_description": "A GPU-accelerated library for Tree-based Genetic Programming leveraging PyTorch and custom CUDA kernels. It supports symbolic regression and policy optimization, enabling high-performance evolutionary computation for scientific modeling and control.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "evolutionary_computation",
        "symbolic_regression",
        "policy_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/EMI-Group/evogp",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "genetic-programming",
        "gpu-acceleration",
        "symbolic-regression",
        "pytorch"
      ],
      "id": 27
    },
    {
      "name": "evox",
      "one_line_profile": "Distributed GPU-Accelerated Framework for Evolutionary Computation",
      "detailed_description": "A comprehensive distributed and GPU-accelerated framework for evolutionary computation, offering a wide range of evolutionary algorithms and benchmark problems for optimization tasks including policy search.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "evolutionary_computation",
        "optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/EMI-Group/evox",
      "help_website": [
        "https://evox.readthedocs.io/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "evolutionary-algorithms",
        "distributed-computing",
        "jax",
        "optimization"
      ],
      "id": 28
    },
    {
      "name": "metade",
      "one_line_profile": "GPU-accelerated evolutionary framework for Differential Evolution",
      "detailed_description": "A GPU-accelerated evolutionary framework optimizing Differential Evolution (DE) strategies via meta-level evolution. It supports JAX and PyTorch for efficient large-scale black-box optimization.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "evolutionary_computation",
        "optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/EMI-Group/metade",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "differential-evolution",
        "gpu-acceleration",
        "black-box-optimization"
      ],
      "id": 29
    },
    {
      "name": "tensorneat",
      "one_line_profile": "GPU-accelerated NeuroEvolution of Augmenting Topologies (NEAT)",
      "detailed_description": "A library for NeuroEvolution of Augmenting Topologies (NEAT) that utilizes GPU acceleration to speed up the evolution of neural network structures and weights, applicable to policy optimization tasks.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "evolutionary_computation",
        "neuroevolution",
        "policy_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/EMI-Group/tensorneat",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "neat",
        "neuroevolution",
        "gpu",
        "jax"
      ],
      "id": 30
    },
    {
      "name": "tensorrvea",
      "one_line_profile": "GPU-accelerated Evolutionary Multiobjective Optimization",
      "detailed_description": "A GPU-accelerated library for Evolutionary Multiobjective Optimization using Tensorized RVEA, designed to solve complex multi-objective optimization problems efficiently.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "evolutionary_computation",
        "multi_objective_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/EMI-Group/tensorrvea",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "multi-objective-optimization",
        "evolutionary-algorithm",
        "gpu"
      ],
      "id": 31
    },
    {
      "name": "Gymnasium",
      "one_line_profile": "Standard API for reinforcement learning environments",
      "detailed_description": "The standard API for single-agent reinforcement learning environments, providing a unified interface for agents to interact with various simulation environments. It includes reference environments and utilities, serving as the successor to OpenAI Gym.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "simulation_environment"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/Farama-Foundation/Gymnasium",
      "help_website": [
        "https://gymnasium.farama.org/"
      ],
      "license": "MIT",
      "tags": [
        "reinforcement-learning",
        "api-standard",
        "simulation"
      ],
      "id": 32
    },
    {
      "name": "Gymnasium-Robotics",
      "one_line_profile": "Robotics simulation environments for reinforcement learning",
      "detailed_description": "A collection of robotics simulation environments compatible with the Gymnasium API, designed for training and benchmarking reinforcement learning agents in robotic control tasks.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "robotics_control",
        "simulation_environment"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Farama-Foundation/Gymnasium-Robotics",
      "help_website": [
        "https://robotics.farama.org/"
      ],
      "license": "MIT",
      "tags": [
        "robotics",
        "simulation",
        "reinforcement-learning",
        "mujoco"
      ],
      "id": 33
    },
    {
      "name": "MO-Gymnasium",
      "one_line_profile": "Multi-objective Gymnasium environments for RL",
      "detailed_description": "A library of multi-objective reinforcement learning environments based on the Gymnasium API, enabling the study and development of agents that optimize multiple conflicting objectives.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "multi_objective_optimization",
        "simulation_environment"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Farama-Foundation/MO-Gymnasium",
      "help_website": [
        "https://mo-gymnasium.farama.org/"
      ],
      "license": "MIT",
      "tags": [
        "multi-objective-rl",
        "gymnasium",
        "simulation"
      ],
      "id": 34
    },
    {
      "name": "Minari",
      "one_line_profile": "Standard format and tools for offline RL datasets",
      "detailed_description": "A library providing a standard format for offline reinforcement learning datasets, along with utilities for data loading, processing, and management, facilitating offline RL research.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "offline_rl",
        "data_processing"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/Farama-Foundation/Minari",
      "help_website": [
        "https://minari.farama.org/"
      ],
      "license": "MIT",
      "tags": [
        "offline-rl",
        "datasets",
        "standardization"
      ],
      "id": 35
    },
    {
      "name": "PettingZoo",
      "one_line_profile": "Standard API for multi-agent reinforcement learning environments",
      "detailed_description": "A standard API library for multi-agent reinforcement learning (MARL) environments, analogous to Gymnasium but for multi-agent settings. It includes a diverse set of reference environments.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "multi_agent_system",
        "simulation_environment"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/Farama-Foundation/PettingZoo",
      "help_website": [
        "https://pettingzoo.farama.org/"
      ],
      "license": "MIT",
      "tags": [
        "multi-agent-rl",
        "marl",
        "api-standard",
        "simulation"
      ],
      "id": 36
    },
    {
      "name": "Shimmy",
      "one_line_profile": "API conversion tool for RL environments",
      "detailed_description": "A utility library that provides API conversion tools to make popular external reinforcement learning environments compatible with Gymnasium and PettingZoo standards.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "interoperability"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Farama-Foundation/Shimmy",
      "help_website": [
        "https://shimmy.farama.org/"
      ],
      "license": "MIT",
      "tags": [
        "reinforcement-learning",
        "api-conversion",
        "compatibility"
      ],
      "id": 37
    },
    {
      "name": "SuperSuit",
      "one_line_profile": "Wrappers for Gymnasium and PettingZoo environments",
      "detailed_description": "A collection of wrappers for Gymnasium and PettingZoo environments to simplify preprocessing, vectorization, and other common RL environment modifications.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "data_preprocessing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Farama-Foundation/SuperSuit",
      "help_website": [
        "https://supersuit.farama.org/"
      ],
      "license": "MIT",
      "tags": [
        "reinforcement-learning",
        "wrappers",
        "preprocessing"
      ],
      "id": 38
    },
    {
      "name": "ViZDoom",
      "one_line_profile": "Doom-based Reinforcement Learning environments",
      "detailed_description": "A reinforcement learning research platform based on the classic game Doom, providing a fast and customizable 3D environment for visual reinforcement learning tasks.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "visual_control",
        "simulation_environment"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/Farama-Foundation/ViZDoom",
      "help_website": [
        "https://vizdoom.farama.org/"
      ],
      "license": "MIT",
      "tags": [
        "reinforcement-learning",
        "doom",
        "visual-control",
        "simulation"
      ],
      "id": 39
    },
    {
      "name": "stable-retro",
      "one_line_profile": "Retro game environments for Reinforcement Learning",
      "detailed_description": "A library that turns classic retro video games into reinforcement learning environments, enabling the study of generalization and transfer learning in RL agents.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "simulation_environment"
      ],
      "application_level": "library",
      "primary_language": "C",
      "repo_url": "https://github.com/Farama-Foundation/stable-retro",
      "help_website": [
        "https://stable-retro.farama.org/"
      ],
      "license": "MIT",
      "tags": [
        "reinforcement-learning",
        "retro-games",
        "emulation"
      ],
      "id": 40
    },
    {
      "name": "mpc_ros",
      "one_line_profile": "Nonlinear Model Predictive Control for ROS",
      "detailed_description": "A ROS package implementing Nonlinear Model Predictive Control (NMPC) for differential wheeled mobile robots, enabling precise trajectory tracking and control.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "control_policy",
        "robotics_control",
        "model_predictive_control"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/Geonhee-LEE/mpc_ros",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ros",
        "mpc",
        "robotics",
        "control"
      ],
      "id": 41
    },
    {
      "name": "fly-craft",
      "one_line_profile": "Goal-conditioned RL environment for fixed-wing UAVs",
      "detailed_description": "An efficient, goal-conditioned reinforcement learning environment for fixed-wing UAV velocity vector control, built on Gymnasium, facilitating research in aerial vehicle control.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "flight_control",
        "simulation_environment"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/GongXudong/fly-craft",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "uav",
        "flight-control",
        "reinforcement-learning",
        "gymnasium"
      ],
      "id": 42
    },
    {
      "name": "gym-jsbsim",
      "one_line_profile": "RL environment for aircraft control using JSBSim",
      "detailed_description": "A reinforcement learning environment for aircraft control that interfaces with the JSBSim flight dynamics model, providing a realistic physics simulation for training flight control agents.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "flight_control",
        "simulation_environment"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Gor-Ren/gym-jsbsim",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "jsbsim",
        "flight-dynamics",
        "reinforcement-learning",
        "aircraft"
      ],
      "id": 43
    },
    {
      "name": "dc-rl",
      "one_line_profile": "Data Center simulation and control environments",
      "detailed_description": "A set of Python environments (SustainDC) for Data Center simulation and control using Heterogeneous Multi-Agent Reinforcement Learning, including workload scheduling and cooling optimization.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "control_policy",
        "simulation_environment"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/HewlettPackard/dc-rl",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "data-center",
        "reinforcement-learning",
        "sustainability",
        "control"
      ],
      "id": 44
    },
    {
      "name": "sustain-cluster",
      "one_line_profile": "Gymnasium environment for sustainable workload scheduling",
      "detailed_description": "A high-fidelity Gymnasium environment for benchmarking multi-objective, sustainable workload scheduling across geo-distributed data centers, integrating real-world traces and physics models.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "scheduling",
        "simulation_environment"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/HewlettPackard/sustain-cluster",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "data-center",
        "sustainability",
        "gymnasium",
        "scheduling"
      ],
      "id": 45
    },
    {
      "name": "imitation",
      "one_line_profile": "Imitation and reward learning algorithms library",
      "detailed_description": "A library providing clean, tested PyTorch implementations of imitation learning and reward learning algorithms, designed to work seamlessly with the Gymnasium ecosystem.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "imitation_learning",
        "reward_learning",
        "reinforcement_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/HumanCompatibleAI/imitation",
      "help_website": [
        "https://imitation.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "imitation-learning",
        "inverse-reinforcement-learning",
        "pytorch",
        "gymnasium"
      ],
      "id": 46
    },
    {
      "name": "GOPS",
      "one_line_profile": "General Optimal control Problem Solver for industrial control",
      "detailed_description": "A PyTorch-based reinforcement learning solver package designed for industrial control problems, offering a general optimal control problem solver framework.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "optimal_control",
        "reinforcement_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Intelligent-Driving-Laboratory/GOPS",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "optimal-control",
        "reinforcement-learning",
        "industrial-control"
      ],
      "id": 47
    },
    {
      "name": "Aerial Autonomy Stack",
      "one_line_profile": "ROS2 control stack for multi-drone autonomy",
      "detailed_description": "A comprehensive stack for controlling quadcopters and VTOLs using PX4/ArduPilot and ROS2, integrating perception, simulation, and control algorithms.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "robotics_control",
        "autonomous_navigation"
      ],
      "application_level": "workflow",
      "primary_language": "C++",
      "repo_url": "https://github.com/JacopoPan/aerial-autonomy-stack",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ros2",
        "uav",
        "control-stack"
      ],
      "id": 48
    },
    {
      "name": "jaxrenderer",
      "one_line_profile": "Differentiable rasterizer implemented in JAX",
      "detailed_description": "A differentiable rendering library implemented in JAX, enabling gradient-based optimization for visual tasks in physics simulations and control environments.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "rendering",
        "simulation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/JoeyTeng/jaxrenderer",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "jax",
        "differentiable-rendering",
        "simulation"
      ],
      "id": 49
    },
    {
      "name": "QuOptimalControl.jl",
      "one_line_profile": "Quantum optimal control library in Julia",
      "detailed_description": "A Julia library for solving quantum optimal control problems, supporting algorithms like GRAPE and dCRAB for quantum system dynamics.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "quantum_control",
        "optimization"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/JuliaQuantumControl/QuOptimalControl.jl",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "quantum-control",
        "julia",
        "optimization"
      ],
      "id": 50
    },
    {
      "name": "MORL-Baselines",
      "one_line_profile": "Multi-Objective Reinforcement Learning algorithms library",
      "detailed_description": "A library implementing various Multi-Objective Reinforcement Learning (MORL) algorithms, serving as a baseline for research and application in complex control tasks.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "multi_objective_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/LucasAlegre/morl-baselines",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "morl",
        "reinforcement-learning",
        "multi-objective"
      ],
      "id": 51
    },
    {
      "name": "SUMO-RL",
      "one_line_profile": "RL environments for traffic signal control",
      "detailed_description": "A library providing Gymnasium-compatible environments for traffic signal control using the SUMO traffic simulator, facilitating RL research in traffic engineering.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "traffic_control",
        "simulation_interface"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/LucasAlegre/sumo-rl",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "traffic-control",
        "sumo",
        "reinforcement-learning"
      ],
      "id": 52
    },
    {
      "name": "Racing-LMPC-ROS2",
      "one_line_profile": "Learning Model Predictive Control for autonomous racing",
      "detailed_description": "ROS2 packages implementing Learning Model Predictive Control (LMPC) for high-performance autonomous racing, developed by MPC-Berkeley.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "model_predictive_control",
        "autonomous_racing"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/MPC-Berkeley/Racing-LMPC-ROS2",
      "help_website": [],
      "license": "LGPL-2.1",
      "tags": [
        "mpc",
        "ros2",
        "autonomous-racing"
      ],
      "id": 53
    },
    {
      "name": "ALPypeRL",
      "one_line_profile": "Connector for AnyLogic models and RL frameworks",
      "detailed_description": "A library facilitating the connection between AnyLogic simulation models and Reinforcement Learning frameworks via the OpenAI Gymnasium API.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "simulation_interface",
        "reinforcement_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/MarcEscandell/ALPypeRL",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "anylogic",
        "reinforcement-learning",
        "simulation"
      ],
      "id": 54
    },
    {
      "name": "deluca",
      "one_line_profile": "Differentiable reinforcement learning library",
      "detailed_description": "A library for performant and differentiable reinforcement learning and control, enabling gradient-based policy optimization.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "differentiable_control",
        "reinforcement_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/MinRegret/deluca",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "differentiable-programming",
        "control",
        "reinforcement-learning"
      ],
      "id": 55
    },
    {
      "name": "MyoSuite",
      "one_line_profile": "Musculoskeletal environments for RL",
      "detailed_description": "A collection of musculoskeletal environments and tasks simulated with MuJoCo, designed for investigating physiological motor control and biomechanics.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "biomechanics",
        "motor_control"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/MyoHub/myosuite",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "biomechanics",
        "mujoco",
        "reinforcement-learning"
      ],
      "id": 56
    },
    {
      "name": "easysim",
      "one_line_profile": "Unified API library for creating Gym environments across various physics simulators",
      "detailed_description": "A library for creating Gym environments with a unified API to various physics simulators, facilitating reinforcement learning research in physics-based simulation.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "simulation",
        "environment_creation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NVlabs/easysim",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "reinforcement-learning",
        "physics-simulation",
        "gym-environment"
      ],
      "id": 57
    },
    {
      "name": "gbrl_sb3",
      "one_line_profile": "GBRL-based Actor-Critic algorithms implemented in Stable-Baselines3",
      "detailed_description": "A library implementing GBRL-based Actor-Critic algorithms within the Stable-Baselines3 framework, providing tools for reinforcement learning research.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "algorithm_implementation",
        "reinforcement_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NVlabs/gbrl_sb3",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "reinforcement-learning",
        "stable-baselines3",
        "actor-critic"
      ],
      "id": 58
    },
    {
      "name": "robopal",
      "one_line_profile": "Modular robot simulation framework based on MuJoCo for RL and control",
      "detailed_description": "A multi-platform, modular robot simulation framework based on MuJoCo, designed for reinforcement learning and control algorithm implementation of robotic arms.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "simulation",
        "robot_control"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/NoneJou072/robopal",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "mujoco",
        "robotics",
        "reinforcement-learning",
        "simulation"
      ],
      "id": 59
    },
    {
      "name": "OpenOCL",
      "one_line_profile": "Optimal Control Library for Matlab with Trajectory Optimization and MPC",
      "detailed_description": "Open Optimal Control Library for Matlab, serving as a toolbox for Trajectory Optimization and non-linear Model Predictive Control (MPC).",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "optimal_control",
        "trajectory_optimization",
        "mpc"
      ],
      "application_level": "library",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/OpenOCL/OpenOCL",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "optimal-control",
        "matlab",
        "trajectory-optimization",
        "mpc"
      ],
      "id": 60
    },
    {
      "name": "openrl",
      "one_line_profile": "Unified Reinforcement Learning Framework",
      "detailed_description": "A unified reinforcement learning framework designed to facilitate the development and deployment of RL algorithms.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "framework"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenRL-Lab/openrl",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "reinforcement-learning",
        "framework",
        "ai"
      ],
      "id": 61
    },
    {
      "name": "Safe-Policy-Optimization",
      "one_line_profile": "Benchmark repository for safe reinforcement learning algorithms",
      "detailed_description": "A benchmark repository for safe reinforcement learning algorithms, providing standardized environments and baselines for evaluating safety in RL.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "benchmarking",
        "safe_rl"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/PKU-Alignment/Safe-Policy-Optimization",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "safe-rl",
        "benchmark",
        "reinforcement-learning"
      ],
      "id": 62
    },
    {
      "name": "safety-gymnasium",
      "one_line_profile": "Unified Safe Reinforcement Learning Benchmark Environments",
      "detailed_description": "A unified safe reinforcement learning benchmark providing a suite of environments to evaluate the safety and performance of RL agents.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "benchmarking",
        "simulation",
        "safe_rl"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/PKU-Alignment/safety-gymnasium",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "safe-rl",
        "gymnasium",
        "benchmark"
      ],
      "id": 63
    },
    {
      "name": "polympc",
      "one_line_profile": "Light-weight C++ library for embedded nonlinear optimization and optimal control",
      "detailed_description": "A light-weight C++ library designed for fast embedded nonlinear optimization and optimal control applications.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "optimization",
        "optimal_control"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/PREDICT-EPFL/polympc",
      "help_website": [],
      "license": "MPL-2.0",
      "tags": [
        "optimization",
        "optimal-control",
        "cpp"
      ],
      "id": 64
    },
    {
      "name": "MuJoCo_RL_UR5",
      "one_line_profile": "MuJoCo/Gym environment for UR5 robot control using RL",
      "detailed_description": "A MuJoCo/Gym environment specifically designed for UR5 robot control tasks using Reinforcement Learning, focusing on grasp success prediction.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "simulation",
        "robot_control",
        "environment"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/PaulDanielML/MuJoCo_RL_UR5",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mujoco",
        "gym",
        "robotics",
        "ur5"
      ],
      "id": 65
    },
    {
      "name": "eagle-mpc",
      "one_line_profile": "MPC & optimal control library for unmanned aerial manipulators",
      "detailed_description": "A model predictive control (MPC) and optimal control library tailored for unmanned aerial manipulators (UAMs).",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "mpc",
        "optimal_control",
        "aerial_manipulation"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/PepMS/eagle-mpc",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "mpc",
        "uam",
        "control"
      ],
      "id": 66
    },
    {
      "name": "RESCO",
      "one_line_profile": "Reinforcement Learning Benchmarks for Traffic Signal Control",
      "detailed_description": "A benchmark suite for evaluating Reinforcement Learning algorithms in the context of Traffic Signal Control.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "benchmarking",
        "traffic_control"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/Pi-Star-Lab/RESCO",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "traffic-control",
        "benchmark",
        "reinforcement-learning"
      ],
      "id": 67
    },
    {
      "name": "ProteoBench",
      "one_line_profile": "Community-curated benchmarks for proteomics data analysis pipelines",
      "detailed_description": "An open and collaborative platform for community-curated benchmarks for proteomics data analysis pipelines, allowing comparison of analysis workflows.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "benchmarking",
        "proteomics",
        "data_analysis"
      ],
      "application_level": "platform",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/Proteobench/ProteoBench",
      "help_website": [
        "https://proteobench.cubimed.rub.de/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "proteomics",
        "benchmark",
        "bioinformatics"
      ],
      "id": 68
    },
    {
      "name": "RLGC",
      "one_line_profile": "Platform for applying Reinforcement Learning for Grid Control",
      "detailed_description": "An open-source platform designed for applying Reinforcement Learning techniques to Power Grid Control problems.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "grid_control",
        "reinforcement_learning"
      ],
      "application_level": "platform",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/RLGC-Project/RLGC",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "grid-control",
        "power-systems",
        "reinforcement-learning"
      ],
      "id": 69
    },
    {
      "name": "MARLlib",
      "one_line_profile": "Comprehensive Multi-agent Reinforcement Learning (MARL) library",
      "detailed_description": "A comprehensive library for Multi-agent Reinforcement Learning (MARL), providing a unified interface for various MARL algorithms and environments.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "marl",
        "reinforcement_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Replicable-MARL/MARLlib",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "marl",
        "multi-agent",
        "reinforcement-learning"
      ],
      "id": 70
    },
    {
      "name": "ReLUQP-py",
      "one_line_profile": "GPU-Accelerated Quadratic Programming Solver for Model-Predictive Control",
      "detailed_description": "A GPU-accelerated Quadratic Programming (QP) solver specifically designed for Model-Predictive Control (MPC) applications.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "optimization",
        "mpc",
        "quadratic_programming"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/RoboticExplorationLab/ReLUQP-py",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "optimization",
        "mpc",
        "gpu",
        "solver"
      ],
      "id": 71
    },
    {
      "name": "Robust-Gymnasium",
      "one_line_profile": "Unified Modular Benchmark for Robust Reinforcement Learning",
      "detailed_description": "A unified modular benchmark for Robust Reinforcement Learning, facilitating the evaluation of RL agents under various perturbations and uncertainties.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "benchmarking",
        "robust_rl"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/SafeRL-Lab/Robust-Gymnasium",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "robust-rl",
        "benchmark",
        "gymnasium"
      ],
      "id": 72
    },
    {
      "name": "Simple Simulator",
      "one_line_profile": "Physics simulator designed for robotics and reinforcement learning",
      "detailed_description": "A C++ physics simulator aimed at simplifying the simulation process for robotics and reinforcement learning research, providing a lightweight alternative to complex engines.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "simulation",
        "robotics_control"
      ],
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/Simple-Robotics/Simple",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "simulator",
        "robotics",
        "physics-engine"
      ],
      "id": 73
    },
    {
      "name": "Stable-Baselines3 Contrib",
      "one_line_profile": "Community-contributed reinforcement learning algorithms for Stable-Baselines3",
      "detailed_description": "An extension package for Stable-Baselines3 containing experimental or community-contributed reinforcement learning algorithms (e.g., TQC, TRPO, ARS) that are not part of the core library.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "policy_optimization",
        "reinforcement_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Stable-Baselines-Team/stable-baselines3-contrib",
      "help_website": [
        "https://sb3-contrib.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "reinforcement-learning",
        "stable-baselines3",
        "algorithms"
      ],
      "id": 74
    },
    {
      "name": "BlueSky Gym",
      "one_line_profile": "Gymnasium environment for Air Traffic Management reinforcement learning research",
      "detailed_description": "A Gymnasium-style wrapper for the BlueSky Air Traffic Simulator, enabling standardized reinforcement learning research in air traffic management scenarios.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "simulation",
        "environment_creation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/TUDelft-CNS-ATM/bluesky-gym",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "air-traffic-management",
        "reinforcement-learning",
        "gym-environment"
      ],
      "id": 75
    },
    {
      "name": "TinyMPC",
      "one_line_profile": "High-speed model-predictive control solver optimized for microcontrollers",
      "detailed_description": "A lightweight, high-speed Model Predictive Control (MPC) solver designed specifically for embedded microcontrollers, enabling real-time optimization on resource-constrained hardware.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "model_predictive_control",
        "optimization"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/TinyMPC/TinyMPC",
      "help_website": [
        "https://tinympc.org/"
      ],
      "license": "MIT",
      "tags": [
        "mpc",
        "embedded-systems",
        "control-theory"
      ],
      "id": 76
    },
    {
      "name": "skrl",
      "one_line_profile": "Modular reinforcement learning library supporting PyTorch, JAX, and NVIDIA Isaac Lab",
      "detailed_description": "A modular and flexible Reinforcement Learning library designed to support PyTorch, JAX, and NVIDIA Warp, with integration for environments like Gymnasium, Isaac Lab, and Brax.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "policy_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Toni-SM/skrl",
      "help_website": [
        "https://skrl.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "reinforcement-learning",
        "pytorch",
        "jax",
        "isaac-gym"
      ],
      "id": 77
    },
    {
      "name": "diffmpc",
      "one_line_profile": "Differentiable Model Predictive Control library for GPU-accelerated optimization",
      "detailed_description": "A library for Differentiable Model Predictive Control (MPC) that leverages GPU acceleration to solve control problems, enabling end-to-end learning and optimization.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "model_predictive_control",
        "differentiable_physics"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/ToyotaResearchInstitute/diffmpc",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "differentiable-programming",
        "mpc",
        "control"
      ],
      "id": 78
    },
    {
      "name": "IDTO",
      "one_line_profile": "Inverse Dynamics Trajectory Optimization solver for contact-implicit MPC",
      "detailed_description": "A solver for Inverse Dynamics Trajectory Optimization, specifically designed for Contact-Implicit Model Predictive Control, useful in robotics and dynamic system control.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "trajectory_optimization",
        "model_predictive_control"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/ToyotaResearchInstitute/idto",
      "help_website": [],
      "license": null,
      "tags": [
        "trajectory-optimization",
        "robotics",
        "mpc"
      ],
      "id": 79
    },
    {
      "name": "sb3-extra-buffers",
      "one_line_profile": "Memory-efficient replay buffer extensions for Stable-Baselines3",
      "detailed_description": "A utility library providing additional, memory-optimized replay buffer classes for the Stable-Baselines3 reinforcement learning framework.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "data_management",
        "reinforcement_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Trenza1ore/sb3-extra-buffers",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "stable-baselines3",
        "replay-buffer",
        "optimization"
      ],
      "id": 80
    },
    {
      "name": "Marathon Envs",
      "one_line_profile": "High-dimensional continuous control environments for Unity ML-Agents",
      "detailed_description": "A collection of high-dimensional continuous control environments designed for use with the Unity ML-Agents Toolkit, serving as benchmarks for reinforcement learning.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "simulation",
        "benchmark_environment"
      ],
      "application_level": "library",
      "primary_language": "C#",
      "repo_url": "https://github.com/Unity-Technologies/marathon-envs",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "unity",
        "ml-agents",
        "continuous-control"
      ],
      "id": 81
    },
    {
      "name": "Dynax",
      "one_line_profile": "Differentiable simulator for dynamical systems supporting parameter inference and control",
      "detailed_description": "A JAX-based differentiable simulator designed for dynamical systems, supporting forward simulation, parameter inference, optimal control, and end-to-end learning.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "simulation",
        "system_identification",
        "optimal_control"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/YangyangFu/dynax",
      "help_website": [],
      "license": null,
      "tags": [
        "differentiable-simulation",
        "jax",
        "dynamical-systems"
      ],
      "id": 82
    },
    {
      "name": "ACADO Toolkit",
      "one_line_profile": "Software environment and algorithm collection for automatic control and dynamic optimization",
      "detailed_description": "A software environment and algorithm collection for automatic control and dynamic optimization, providing tools for direct optimal control, model predictive control, and parameter estimation.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "optimal_control",
        "model_predictive_control",
        "parameter_estimation"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/acado/acado",
      "help_website": [
        "http://acado.github.io/"
      ],
      "license": "LGPL-3.0",
      "tags": [
        "optimal-control",
        "mpc",
        "optimization"
      ],
      "id": 83
    },
    {
      "name": "acados",
      "one_line_profile": "Fast solvers for nonlinear optimal control and model predictive control on embedded systems",
      "detailed_description": "A software package providing fast and embedded solvers for nonlinear optimal control and nonlinear model predictive control (NMPC), focusing on performance and portability.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "optimal_control",
        "model_predictive_control"
      ],
      "application_level": "solver",
      "primary_language": "C",
      "repo_url": "https://github.com/acados/acados",
      "help_website": [
        "https://docs.acados.org/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "mpc",
        "embedded-optimization",
        "control-solvers"
      ],
      "id": 84
    },
    {
      "name": "Dreamer (PyTorch)",
      "one_line_profile": "PyTorch implementation of Dreamer v1 and v2 model-based reinforcement learning algorithms",
      "detailed_description": "A reproduction of the Dreamer v1 and v2 algorithms in PyTorch, designed for the DeepMind Control Suite. It serves as a solver for model-based reinforcement learning tasks involving continuous control.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "policy_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/adityabingi/Dreamer",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "dreamer",
        "model-based-rl",
        "pytorch",
        "deepmind-control-suite"
      ],
      "id": 85
    },
    {
      "name": "XuanCe",
      "one_line_profile": "Comprehensive and unified deep reinforcement learning library",
      "detailed_description": "XuanCe is an open-source ensemble of Deep Reinforcement Learning (DRL) algorithms. It supports multiple frameworks (PyTorch, TensorFlow, MindSpore) and provides a unified interface for training agents in various environments, including single-agent and multi-agent settings.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "policy_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/agi-brain/xuance",
      "help_website": [
        "https://xuance.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "deep-reinforcement-learning",
        "multi-agent-rl",
        "drl-library"
      ],
      "id": 86
    },
    {
      "name": "rllib-energyplus",
      "one_line_profile": "EnergyPlus environments for control optimization using reinforcement learning",
      "detailed_description": "A library connecting the EnergyPlus building energy simulation engine with Ray RLlib. It enables researchers to train and test reinforcement learning agents for building control optimization tasks.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "control_simulation",
        "reinforcement_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/airboxlab/rllib-energyplus",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "energyplus",
        "rllib",
        "building-control",
        "simulation"
      ],
      "id": 87
    },
    {
      "name": "rllib-fast-serve",
      "one_line_profile": "Tools to export Ray RLlib policies for lightweight inference",
      "detailed_description": "A utility toolkit for exporting reinforcement learning policies trained with Ray RLlib into lightweight formats for fast inference, facilitating the deployment of control policies in production or constrained environments.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "inference",
        "deployment"
      ],
      "application_level": "utility",
      "primary_language": "Python",
      "repo_url": "https://github.com/airboxlab/rllib-fast-serve",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rllib",
        "inference",
        "model-deployment"
      ],
      "id": 88
    },
    {
      "name": "wildfire-env",
      "one_line_profile": "Physics-informed wildfire simulation environment for RL",
      "detailed_description": "A Gymnasium-compatible wildfire simulation environment incorporating physics-informed fire spread dynamics. It is designed for reinforcement learning research focused on wildfire management and suppression strategies.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "control_simulation",
        "reinforcement_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aisystems-lab/wildfire-env",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "wildfire-simulation",
        "gymnasium",
        "physics-informed"
      ],
      "id": 89
    },
    {
      "name": "ACC_Vehicle_MPC",
      "one_line_profile": "Model Predictive Control solver for Adaptive Cruise Control vehicles",
      "detailed_description": "A MATLAB-based implementation of Model Predictive Control (MPC) for Adaptive Cruise Control (ACC) systems. It serves as a simulation and control solver for vehicle dynamics research.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "model_predictive_control",
        "vehicle_dynamics"
      ],
      "application_level": "solver",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/ajinkya-khade/ACC_Vehicle_MPC",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "mpc",
        "adaptive-cruise-control",
        "matlab",
        "vehicle-control"
      ],
      "id": 90
    },
    {
      "name": "fpo",
      "one_line_profile": "Implementation of Flow Policy Optimization (FPO) algorithm",
      "detailed_description": "A Python implementation of the Flow Policy Optimization (FPO) algorithm for reinforcement learning. It provides a solver for policy optimization tasks.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "policy_optimization",
        "reinforcement_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/akanazawa/fpo",
      "help_website": [],
      "license": null,
      "tags": [
        "flow-policy-optimization",
        "rl-algorithm"
      ],
      "id": 91
    },
    {
      "name": "Pytorch-DPPO",
      "one_line_profile": "PyTorch implementation of Distributed Proximal Policy Optimization",
      "detailed_description": "A PyTorch implementation of the Distributed Proximal Policy Optimization (DPPO) algorithm. It serves as a solver for training reinforcement learning agents in distributed settings.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "policy_optimization",
        "reinforcement_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/alexis-jacq/Pytorch-DPPO",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "dppo",
        "distributed-rl",
        "pytorch"
      ],
      "id": 92
    },
    {
      "name": "Continuous-PPO",
      "one_line_profile": "Continuous Proximal Policy Optimization implementation in PyTorch",
      "detailed_description": "A standalone PyTorch implementation of the Proximal Policy Optimization (PPO) algorithm specifically for continuous action spaces, serving as a solver for continuous control problems.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "policy_optimization",
        "continuous_control"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/alirezakazemipour/Continuous-PPO",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ppo",
        "continuous-control",
        "pytorch"
      ],
      "id": 93
    },
    {
      "name": "Discrete-SAC-PyTorch",
      "one_line_profile": "Discrete Soft Actor-Critic implementation in PyTorch",
      "detailed_description": "A PyTorch implementation of the Soft Actor-Critic (SAC) algorithm adapted for discrete action spaces, enabling entropy-regularized reinforcement learning for discrete control tasks.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "policy_optimization",
        "reinforcement_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/alirezakazemipour/Discrete-SAC-PyTorch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "sac",
        "discrete-control",
        "pytorch"
      ],
      "id": 94
    },
    {
      "name": "SAC",
      "one_line_profile": "Soft Actor-Critic implementation in PyTorch",
      "detailed_description": "A standard implementation of the Soft Actor-Critic (SAC) algorithm, an off-policy maximum entropy deep reinforcement learning method, for continuous control tasks.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "policy_optimization",
        "continuous_control"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/alirezakazemipour/SAC",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "sac",
        "soft-actor-critic",
        "pytorch"
      ],
      "id": 95
    },
    {
      "name": "SoulsGym",
      "one_line_profile": "Gymnasium environment wrapper for Souls-like games",
      "detailed_description": "A Gymnasium extension that wraps Dark Souls III, Elden Ring, and other Souls games into reinforcement learning environments. It allows researchers to test RL algorithms on complex, high-fidelity commercial game physics and logic.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "simulation_environment"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/amacati/SoulsGym",
      "help_website": [
        "https://soulsgym.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "gymnasium",
        "game-environment",
        "reinforcement-learning"
      ],
      "id": 96
    },
    {
      "name": "jaxsim",
      "one_line_profile": "Differentiable physics engine and multibody dynamics library in JAX",
      "detailed_description": "A differentiable physics engine and multibody dynamics library written in JAX. It is designed for control and robot learning research, enabling gradient-based optimization through physical simulations.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "physics_simulation",
        "robot_learning",
        "differentiable_physics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ami-iit/jaxsim",
      "help_website": [
        "https://jaxsim.readthedocs.io/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "jax",
        "differentiable-physics",
        "robotics",
        "multibody-dynamics"
      ],
      "id": 97
    },
    {
      "name": "sim2real-ur-gym-gazebo",
      "one_line_profile": "Universal Robot environment for Gymnasium and ROS Gazebo",
      "detailed_description": "A simulation environment interface connecting OpenAI Gymnasium with ROS Gazebo for Universal Robots. It facilitates Sim2Real reinforcement learning research for robotic manipulation.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "robot_simulation",
        "reinforcement_learning"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/ammar-n-abbas/sim2real-ur-gym-gazebo",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ros",
        "gazebo",
        "gymnasium",
        "robotics"
      ],
      "id": 98
    },
    {
      "name": "SAC-Lagrangian",
      "one_line_profile": "Constrained Reinforcement Learning implementation using SAC and Lagrangian methods",
      "detailed_description": "A PyTorch implementation of Constrained Reinforcement Learning using the Soft Actor-Critic (SAC) algorithm combined with Lagrangian relaxation. It solves control problems with safety constraints.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "constrained_rl",
        "policy_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ammarhydr/SAC-Lagrangian",
      "help_website": [],
      "license": null,
      "tags": [
        "constrained-rl",
        "sac",
        "lagrangian-relaxation"
      ],
      "id": 99
    },
    {
      "name": "B-ACE",
      "one_line_profile": "Godot-based environments for air combat and multi-UAV tasks",
      "detailed_description": "A set of simulation environments built with the Godot engine for air combat and multi-UAV task allocation. It integrates with PettingZoo and Tianshou for multi-agent reinforcement learning research.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "multi_agent_rl",
        "simulation_environment"
      ],
      "application_level": "library",
      "primary_language": "GDScript",
      "repo_url": "https://github.com/andrekuros/B-ACE",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "godot",
        "multi-agent",
        "uav",
        "pettingzoo"
      ],
      "id": 100
    },
    {
      "name": "CARLA-GymDrive",
      "one_line_profile": "Gym environment wrapper for CARLA autonomous driving simulator",
      "detailed_description": "A framework that wraps the CARLA simulator into a Gym environment, facilitating the generation of driving episodes and the training of reinforcement learning agents for autonomous driving.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "autonomous_driving",
        "simulation_environment"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/angelomorgado/CARLA-GymDrive",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "carla",
        "autonomous-driving",
        "gym-environment"
      ],
      "id": 101
    },
    {
      "name": "XY_universe",
      "one_line_profile": "2D particle survival environment for Deep RL",
      "detailed_description": "A lightweight 2D physics environment simulating particle survival scenarios, designed for testing and benchmarking deep reinforcement learning algorithms.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "simulation_environment",
        "reinforcement_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ankonzoid/XY_universe",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rl-environment",
        "particle-simulation"
      ],
      "id": 102
    },
    {
      "name": "KoopmanMPC_for_flowcontrol",
      "one_line_profile": "Koopman Model Predictive Control framework for nonlinear flows",
      "detailed_description": "A data-driven control framework that utilizes Koopman operator theory and Model Predictive Control (MPC) for the control of nonlinear fluid flows. It provides tools for modeling and controlling complex physical dynamics.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "flow_control",
        "model_predictive_control",
        "koopman_operator"
      ],
      "application_level": "library",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/arbabiha/KoopmanMPC_for_flowcontrol",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "koopman-operator",
        "mpc",
        "fluid-dynamics",
        "control-theory"
      ],
      "id": 103
    },
    {
      "name": "ocpy",
      "one_line_profile": "Python optimal control solver using SymPy and Numba",
      "detailed_description": "An optimal control solver implemented in Python that leverages SymPy for symbolic differentiation and Numba for JIT compilation, providing a fast and flexible tool for solving control problems.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "optimal_control",
        "numerical_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/arcuma/ocpy",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "optimal-control",
        "sympy",
        "numba",
        "solver"
      ],
      "id": 104
    },
    {
      "name": "carla_garage",
      "one_line_profile": "Research platform for end-to-end driving models in CARLA",
      "detailed_description": "A research toolkit and starter kit for the CARLA Leaderboard 2.0, focusing on analyzing hidden biases in end-to-end driving models and providing baselines for autonomous driving research.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "autonomous_driving",
        "model_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/autonomousvision/carla_garage",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "carla",
        "autonomous-driving",
        "benchmarking"
      ],
      "id": 105
    },
    {
      "name": "tuplan_garage",
      "one_line_profile": "Vehicle motion planning research platform",
      "detailed_description": "A comprehensive framework for learning-based vehicle motion planning, providing implementations and benchmarks for various planning algorithms in autonomous driving contexts.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "motion_planning",
        "autonomous_driving"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/autonomousvision/tuplan_garage",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "motion-planning",
        "autonomous-driving",
        "simulation"
      ],
      "id": 106
    },
    {
      "name": "racecar_gym",
      "one_line_profile": "PyBullet-based miniature racecar gym environment",
      "detailed_description": "A reinforcement learning environment simulating miniature racecars using the PyBullet physics engine. It is designed for testing control algorithms on vehicle dynamics.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "vehicle_simulation",
        "reinforcement_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/axelbr/racecar_gym",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pybullet",
        "gym-environment",
        "racecar"
      ],
      "id": 107
    },
    {
      "name": "MachinaScript For Robots",
      "one_line_profile": "Framework for LLM-powered robot control",
      "detailed_description": "A framework that enables the control of robots using Large Language Models (LLMs), translating natural language instructions into robotic control commands.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "robot_control",
        "llm_agent"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/babycommando/machinascript-for-robots",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "robotics",
        "llm",
        "control-framework"
      ],
      "id": 108
    },
    {
      "name": "bullet-gym",
      "one_line_profile": "OpenAI Gym environments implemented with Bullet Physics",
      "detailed_description": "A collection of OpenAI Gym environments ported to use the Bullet Physics engine, providing open-source alternatives to MuJoCo-based environments for reinforcement learning research.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "simulation_environment",
        "reinforcement_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/benelot/bullet-gym",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bullet-physics",
        "gym-environment",
        "simulation"
      ],
      "id": 109
    },
    {
      "name": "ppo_jax",
      "one_line_profile": "JAX implementation of PPO tuned for Procgen",
      "detailed_description": "A high-performance implementation of Proximal Policy Optimization (PPO) using JAX, specifically tuned and benchmarked for the Procgen environment suite.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "policy_optimization",
        "reinforcement_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/bmazoure/ppo_jax",
      "help_website": [],
      "license": null,
      "tags": [
        "jax",
        "ppo",
        "procgen"
      ],
      "id": 110
    },
    {
      "name": "sb3_to_coral",
      "one_line_profile": "Converter for deploying Stable-Baselines3 RL models to TFLite and Coral Edge TPU",
      "detailed_description": "A utility tool that facilitates the deployment of reinforcement learning models trained with Stable-Baselines3 onto edge hardware like Google Coral, enabling scientific experimentation in real-world robotics and control systems.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "model_deployment",
        "inference_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/chunky/sb3_to_coral",
      "help_website": [],
      "license": null,
      "tags": [
        "reinforcement-learning",
        "edge-computing",
        "stable-baselines3",
        "tflite",
        "coral-tpu"
      ],
      "id": 111
    },
    {
      "name": "OpTaS",
      "one_line_profile": "Optimization-based task specification library for trajectory optimization and MPC",
      "detailed_description": "A library designed for defining and solving trajectory optimization and model predictive control (MPC) problems in robotics, providing a flexible interface for specifying constraints and objectives.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "trajectory_optimization",
        "model_predictive_control"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/cmower/optas",
      "help_website": [],
      "license": null,
      "tags": [
        "trajectory-optimization",
        "mpc",
        "robotics",
        "control-theory"
      ],
      "id": 112
    },
    {
      "name": "RLOR",
      "one_line_profile": "Reinforcement learning library for operations research problems",
      "detailed_description": "A library that provides environments and implementations for solving operations research (OR) problems using reinforcement learning, leveraging OpenAI Gym and CleanRL.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "operations_research",
        "optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/cpwan/RLOR",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "operations-research",
        "reinforcement-learning",
        "optimization",
        "gym-environment"
      ],
      "id": 113
    },
    {
      "name": "deeprl_network",
      "one_line_profile": "Multi-agent deep reinforcement learning library for networked system control",
      "detailed_description": "A collection of deep reinforcement learning implementations specifically tailored for networked system control and traffic management, serving as a benchmark and solver for multi-agent control tasks.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "multi_agent_control",
        "network_control"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/cts198859/deeprl_network",
      "help_website": [],
      "license": null,
      "tags": [
        "multi-agent-rl",
        "traffic-control",
        "network-systems",
        "deep-learning"
      ],
      "id": 114
    },
    {
      "name": "deeprl_signal_control",
      "one_line_profile": "Multi-agent deep reinforcement learning for large-scale traffic signal control",
      "detailed_description": "A specialized reinforcement learning framework for simulating and controlling large-scale traffic signal networks, providing environments and solver implementations for intelligent transportation research.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "traffic_signal_control",
        "simulation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/cts198859/deeprl_signal_control",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "traffic-signal-control",
        "reinforcement-learning",
        "multi-agent-systems",
        "simulation"
      ],
      "id": 115
    },
    {
      "name": "gym-simplegrid",
      "one_line_profile": "Simple Grid Environment for Gymnasium",
      "detailed_description": "A lightweight Gymnasium-compatible environment library for grid-world reinforcement learning tasks, useful for testing and prototyping RL algorithms.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "environment_simulation",
        "rl_prototyping"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/damat-le/gym-simplegrid",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "gymnasium",
        "grid-world",
        "reinforcement-learning",
        "environment"
      ],
      "id": 116
    },
    {
      "name": "Edge-Computing-RL-Allocation",
      "one_line_profile": "Simulation and resource allocation tool for edge computing using DDPG",
      "detailed_description": "A simulation platform with a graphical interface for modeling edge computing environments and optimizing resource allocation (offloading, migration) using Deep Deterministic Policy Gradient (DDPG).",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "resource_allocation",
        "simulation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/davidtw0320/Resources-Allocation-in-The-Edge-Computing-Environment-Using-Reinforcement-Learning",
      "help_website": [],
      "license": null,
      "tags": [
        "edge-computing",
        "resource-allocation",
        "ddpg",
        "simulation",
        "reinforcement-learning"
      ],
      "id": 117
    },
    {
      "name": "ParNMPC",
      "one_line_profile": "Parallel Optimization Toolkit for Nonlinear Model Predictive Control",
      "detailed_description": "A MATLAB-based toolkit for solving nonlinear model predictive control (NMPC) problems using parallel optimization techniques, suitable for real-time control applications.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "nonlinear_control",
        "optimization"
      ],
      "application_level": "solver",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/deng-haoyang/ParNMPC",
      "help_website": [],
      "license": "BSD-2-Clause",
      "tags": [
        "nmpc",
        "optimal-control",
        "parallel-computing",
        "matlab"
      ],
      "id": 118
    },
    {
      "name": "dmc2gym",
      "one_line_profile": "OpenAI Gym wrapper for the DeepMind Control Suite",
      "detailed_description": "A wrapper library that allows DeepMind Control Suite environments to be used with the OpenAI Gym interface, facilitating the application of standard RL algorithms to physics-based control tasks.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "environment_wrapper",
        "interoperability"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/denisyarats/dmc2gym",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "deepmind-control-suite",
        "openai-gym",
        "reinforcement-learning",
        "wrapper"
      ],
      "id": 119
    },
    {
      "name": "pytorch_sac",
      "one_line_profile": "High-quality PyTorch implementation of Soft Actor-Critic (SAC)",
      "detailed_description": "A widely used, clean, and robust implementation of the Soft Actor-Critic (SAC) algorithm in PyTorch, serving as a standard solver and baseline for continuous control research.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "continuous_control",
        "policy_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/denisyarats/pytorch_sac",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "soft-actor-critic",
        "pytorch",
        "reinforcement-learning",
        "continuous-control"
      ],
      "id": 120
    },
    {
      "name": "pytorch_sac_ae",
      "one_line_profile": "PyTorch implementation of SAC with Autoencoder for pixel-based control",
      "detailed_description": "An implementation of Soft Actor-Critic combined with an Autoencoder (SAC+AE) for learning control policies directly from high-dimensional image observations.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "visual_control",
        "representation_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/denisyarats/pytorch_sac_ae",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "sac",
        "autoencoder",
        "pixel-based-control",
        "reinforcement-learning"
      ],
      "id": 121
    },
    {
      "name": "deformable_gym",
      "one_line_profile": "RL environments for grasping 3D deformable objects",
      "detailed_description": "A collection of Gymnasium environments specifically designed for the task of learning to grasp and manipulate 3D deformable objects using reinforcement learning.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "robotic_manipulation",
        "deformable_object_simulation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/dfki-ric/deformable_gym",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "deformable-objects",
        "robotics",
        "gym-environment",
        "grasping"
      ],
      "id": 122
    },
    {
      "name": "language-table",
      "one_line_profile": "Multi-task continuous control benchmark and dataset for visuolinguomotor learning",
      "detailed_description": "A suite of human-collected datasets and a multi-task continuous control benchmark designed for open vocabulary visuolinguomotor learning, providing a simulation environment for interactive tasks.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "benchmark",
        "robotics_simulation"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/google-research/language-table",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "benchmark",
        "robotics",
        "reinforcement-learning",
        "visuomotor-control"
      ],
      "id": 123
    },
    {
      "name": "Brax",
      "one_line_profile": "Differentiable physics engine for massive parallel rigid body simulation",
      "detailed_description": "A differentiable physics engine designed for massively parallel rigid body simulation on accelerator hardware (JAX-based), suitable for reinforcement learning and robotic control tasks.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "physics_simulation",
        "reinforcement_learning",
        "differentiable_physics"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/google/brax",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "jax",
        "physics-engine",
        "reinforcement-learning",
        "simulation"
      ],
      "id": 124
    },
    {
      "name": "gradsim",
      "one_line_profile": "Differentiable simulation framework for system identification and control",
      "detailed_description": "A differentiable simulation library designed for system identification and visuomotor control, allowing gradients to be propagated through physical dynamics.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "system_identification",
        "control",
        "differentiable_physics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/gradsim/gradsim",
      "help_website": [],
      "license": null,
      "tags": [
        "differentiable-simulation",
        "control",
        "system-identification"
      ],
      "id": 125
    },
    {
      "name": "gymprecice",
      "one_line_profile": "RL environment adapter for fluid dynamics control using preCICE",
      "detailed_description": "A framework to design and develop reinforcement learning environments for active flow control by coupling OpenAI Gym with physics solvers via the preCICE coupling library.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "flow_control",
        "reinforcement_learning",
        "multiphysics_simulation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/gymprecice/gymprecice",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "flow-control",
        "cfd",
        "reinforcement-learning",
        "precice"
      ],
      "id": 126
    },
    {
      "name": "SAC",
      "one_line_profile": "Reference implementation of Soft Actor-Critic algorithm",
      "detailed_description": "The original reference implementation of the Soft Actor-Critic (SAC) reinforcement learning algorithm, widely used as a baseline and solver for continuous control tasks.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "control"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/haarnoja/sac",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "soft-actor-critic",
        "reinforcement-learning",
        "continuous-control"
      ],
      "id": 127
    },
    {
      "name": "ir-sim",
      "one_line_profile": "Lightweight robot simulator for navigation and RL",
      "detailed_description": "A Python-based lightweight robot simulator designed for developing and testing navigation, control, and reinforcement learning algorithms.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "robotics_simulation",
        "navigation",
        "reinforcement_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/hanruihua/ir-sim",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "robotics",
        "simulator",
        "reinforcement-learning"
      ],
      "id": 128
    },
    {
      "name": "huggingface_sb3",
      "one_line_profile": "Integration utilities for Stable-Baselines3 and Hugging Face Hub",
      "detailed_description": "A utility library that enables loading and uploading Reinforcement Learning models trained with Stable-Baselines3 to/from the Hugging Face Hub.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "model_management",
        "reinforcement_learning"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/huggingface/huggingface_sb3",
      "help_website": [],
      "license": null,
      "tags": [
        "stable-baselines3",
        "huggingface",
        "reinforcement-learning"
      ],
      "id": 129
    },
    {
      "name": "Manipulator-Mujoco",
      "one_line_profile": "Mujoco Gymnasium environments for robot arm control",
      "detailed_description": "A library providing Mujoco Gymnasium environments for controlling robot arms with operational space control, facilitating RL research in robotics.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "robotics_simulation",
        "reinforcement_learning",
        "control"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ian-chuang/Manipulator-Mujoco",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mujoco",
        "gymnasium",
        "robotics",
        "reinforcement-learning"
      ],
      "id": 130
    },
    {
      "name": "homestri-ur5e-rl",
      "one_line_profile": "Simulation environment for UR5e robot manipulation",
      "detailed_description": "A Mujoco Gymnasium environment for manipulating flexible objects with a UR5e robot arm, supporting various control modes for RL experiments.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "robotics_simulation",
        "reinforcement_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ian-chuang/homestri-ur5e-rl",
      "help_website": [],
      "license": null,
      "tags": [
        "mujoco",
        "ur5e",
        "reinforcement-learning",
        "robotics"
      ],
      "id": 131
    },
    {
      "name": "mpx",
      "one_line_profile": "JAX-based Model Predictive Control library",
      "detailed_description": "A library for Model Predictive Control (MPC) implemented in JAX, enabling differentiable and accelerated control algorithms.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "model_predictive_control",
        "optimal_control"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/iit-DLSLab/mpx",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "mpc",
        "jax",
        "control-theory"
      ],
      "id": 132
    },
    {
      "name": "pytorch-a2c-ppo-acktr-gail",
      "one_line_profile": "PyTorch implementations of standard RL algorithms",
      "detailed_description": "A widely used library providing PyTorch implementations of Advantage Actor Critic (A2C), Proximal Policy Optimization (PPO), ACKTR, and GAIL for deep reinforcement learning.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "control"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ppo",
        "a2c",
        "reinforcement-learning",
        "pytorch"
      ],
      "id": 133
    },
    {
      "name": "pytorch-ddpg-naf",
      "one_line_profile": "PyTorch implementations of DDPG and NAF for continuous control",
      "detailed_description": "A library implementing Deep Deterministic Policy Gradient (DDPG) and Normalized Advantage Functions (NAF) for continuous control tasks in reinforcement learning.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "continuous_control"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ikostrikov/pytorch-ddpg-naf",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ddpg",
        "continuous-control",
        "reinforcement-learning"
      ],
      "id": 134
    },
    {
      "name": "pytorch-trpo",
      "one_line_profile": "PyTorch implementation of Trust Region Policy Optimization",
      "detailed_description": "A library providing a PyTorch implementation of the Trust Region Policy Optimization (TRPO) algorithm for reinforcement learning.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "control"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ikostrikov/pytorch-trpo",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "trpo",
        "reinforcement-learning",
        "pytorch"
      ],
      "id": 135
    },
    {
      "name": "ddp",
      "one_line_profile": "Differential Dynamic Programming with symbolic differentiation",
      "detailed_description": "A Python library implementing Differential Dynamic Programming (DDP) for optimal control, featuring automatic symbolic differentiation.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "optimal_control",
        "trajectory_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/imgeorgiev/ddp",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ddp",
        "optimal-control",
        "symbolic-differentiation"
      ],
      "id": 136
    },
    {
      "name": "dppo",
      "one_line_profile": "Implementation of Diffusion Policy Policy Optimization",
      "detailed_description": "The official implementation of Diffusion Policy Policy Optimization (DPPO), a reinforcement learning algorithm leveraging diffusion models for policy representation.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "policy_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/irom-princeton/dppo",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "diffusion-models",
        "reinforcement-learning",
        "policy-optimization"
      ],
      "id": 137
    },
    {
      "name": "IsaacGymEnvs",
      "one_line_profile": "Reinforcement learning environments for NVIDIA Isaac Gym",
      "detailed_description": "A collection of high-performance reinforcement learning environments designed to run on the NVIDIA Isaac Gym physics simulator.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "physics_simulation",
        "reinforcement_learning",
        "robotics_simulation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/isaac-sim/IsaacGymEnvs",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "isaac-gym",
        "reinforcement-learning",
        "simulation"
      ],
      "id": 138
    },
    {
      "name": "cpo",
      "one_line_profile": "Implementation of Constrained Policy Optimization",
      "detailed_description": "A library implementing Constrained Policy Optimization (CPO), an algorithm for safe reinforcement learning with constraint satisfaction.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "safe_rl"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/jachiam/cpo",
      "help_website": [],
      "license": null,
      "tags": [
        "constrained-policy-optimization",
        "safe-rl",
        "reinforcement-learning"
      ],
      "id": 139
    },
    {
      "name": "Titan",
      "one_line_profile": "CUDA-based physics simulation sandbox for soft robotics",
      "detailed_description": "A high-performance CUDA-based physics simulation sandbox designed for soft robotics and reinforcement learning research.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "physics_simulation",
        "soft_robotics",
        "reinforcement_learning"
      ],
      "application_level": "library",
      "primary_language": "Cuda",
      "repo_url": "https://github.com/jacobaustin123/Titan",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "cuda",
        "physics-simulation",
        "soft-robotics"
      ],
      "id": 140
    },
    {
      "name": "masac",
      "one_line_profile": "Multi-Agent Soft Actor-Critic implementation",
      "detailed_description": "A Python implementation of the Soft Actor-Critic algorithm adapted for multi-agent reinforcement learning environments.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "multi_agent_rl",
        "reinforcement_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/jaimeluengo/masac",
      "help_website": [],
      "license": null,
      "tags": [
        "multi-agent",
        "soft-actor-critic",
        "reinforcement-learning"
      ],
      "id": 141
    },
    {
      "name": "deep_control",
      "one_line_profile": "Deep Reinforcement Learning library for continuous control",
      "detailed_description": "A modular Deep Reinforcement Learning library implemented in PyTorch, specifically focused on continuous control tasks.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "continuous_control"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/jakegrigsby/deep_control",
      "help_website": [],
      "license": null,
      "tags": [
        "deep-reinforcement-learning",
        "continuous-control",
        "pytorch"
      ],
      "id": 142
    },
    {
      "name": "dmc_remastered",
      "one_line_profile": "Visual generalization benchmark for DeepMind Control Suite",
      "detailed_description": "A version of the DeepMind Control Suite featuring randomly generated graphics to benchmark visual generalization in continuous control RL agents.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "benchmark",
        "reinforcement_learning",
        "continuous_control"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/jakegrigsby/dmc_remastered",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "benchmark",
        "deepmind-control-suite",
        "generalization"
      ],
      "id": 143
    },
    {
      "name": "super_sac",
      "one_line_profile": "Extended Soft Actor-Critic implementation for offline learning",
      "detailed_description": "A general model-free off-policy actor-critic implementation supporting continuous and discrete Soft Actor-Critic, multimodal observations, and offline learning.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "offline_rl"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/jakegrigsby/super_sac",
      "help_website": [],
      "license": null,
      "tags": [
        "soft-actor-critic",
        "offline-rl",
        "reinforcement-learning"
      ],
      "id": 144
    },
    {
      "name": "mbpo",
      "one_line_profile": "Implementation of Model-Based Policy Optimization",
      "detailed_description": "The official code for 'When to Trust Your Model: Model-Based Policy Optimization', serving as a reference implementation for model-based reinforcement learning.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "model_based_rl",
        "reinforcement_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/jannerm/mbpo",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "model-based-rl",
        "policy-optimization",
        "reinforcement-learning"
      ],
      "id": 145
    },
    {
      "name": "Cylinder2DFlowControlDRL",
      "one_line_profile": "Deep Reinforcement Learning framework for active flow control of 2D Karman vortex street",
      "detailed_description": "A research code repository implementing Deep Reinforcement Learning (DRL) for active flow control in fluid dynamics simulations. It specifically targets the reduction of drag in a 2D cylinder flow (Karman vortex street) using proximal policy optimization (PPO) and interacts with numerical fluid solvers.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "flow_control",
        "fluid_dynamics_simulation",
        "policy_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/jerabaul29/Cylinder2DFlowControlDRL",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fluid-dynamics",
        "active-flow-control",
        "deep-reinforcement-learning",
        "cfd"
      ],
      "id": 146
    },
    {
      "name": "Interpolated-Policy-Gradient-with-PPO-for-Robotics-Control",
      "one_line_profile": "Robotics continuous control library extending PPO with Interpolated Policy Gradient",
      "detailed_description": "A reinforcement learning library for robotics continuous control tasks. It implements Proximal Policy Optimization (PPO) extended with Interpolated Policy Gradient (IPG) and Hindsight Experience Replay (HER) to improve sample efficiency and performance in robotic manipulation environments.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "robotics_control",
        "policy_optimization",
        "continuous_control"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/jianing-sun/Interpolated-Policy-Gradient-with-PPO-for-Robotics-Control-",
      "help_website": [],
      "license": null,
      "tags": [
        "robotics",
        "ppo",
        "hindsight-experience-replay",
        "continuous-control"
      ],
      "id": 147
    },
    {
      "name": "RL-MPC-LaneMerging",
      "one_line_profile": "Hybrid control framework combining RL and MPC for autonomous lane merging",
      "detailed_description": "A control framework that integrates Reinforcement Learning (RL) with Model Predictive Control (MPC) to solve the on-ramp lane merging problem for autonomous vehicles. It leverages the high-level decision-making of RL and the safety constraints of MPC.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "autonomous_driving",
        "motion_planning",
        "hybrid_control"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/jlubars/RL-MPC-LaneMerging",
      "help_website": [],
      "license": null,
      "tags": [
        "model-predictive-control",
        "reinforcement-learning",
        "autonomous-vehicles",
        "lane-merging"
      ],
      "id": 148
    },
    {
      "name": "nanoGRPO",
      "one_line_profile": "Lightweight implementation of Group Relative Policy Optimization (GRPO)",
      "detailed_description": "A minimalist and lightweight implementation of the Group Relative Policy Optimization (GRPO) algorithm. It serves as a portable library for applying this specific policy optimization technique to various reinforcement learning tasks.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "policy_optimization",
        "reinforcement_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/joey00072/nanoGRPO",
      "help_website": [],
      "license": null,
      "tags": [
        "grpo",
        "policy-optimization",
        "rl-algorithm"
      ],
      "id": 149
    },
    {
      "name": "geppo",
      "one_line_profile": "Generalized Proximal Policy Optimization with Sample Reuse (GePPO) implementation",
      "detailed_description": "An implementation of Generalized Proximal Policy Optimization (GePPO), a reinforcement learning algorithm designed to improve sample efficiency through sample reuse. It provides a reusable library for applying GePPO to continuous control problems.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "policy_optimization",
        "sample_efficiency",
        "continuous_control"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/jqueeney/geppo",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "ppo",
        "reinforcement-learning",
        "sample-reuse"
      ],
      "id": 150
    },
    {
      "name": "RTWholeBodyMPPI",
      "one_line_profile": "Real-time whole-body Model Predictive Path Integral control for quadrupeds",
      "detailed_description": "A control system deploying Model-Predictive Path Integral (MPPI) control for real-time, whole-body locomotion and manipulation on quadruped robots. It enables contact-rich behaviors using a single control policy.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "robotics_control",
        "motion_planning",
        "optimal_control"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/jrapudg/RTWholeBodyMPPI",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mppi",
        "quadruped-robot",
        "whole-body-control",
        "mpc"
      ],
      "id": 151
    },
    {
      "name": "MultiAgent-PPO",
      "one_line_profile": "Multi-agent Proximal Policy Optimization with Beta distribution",
      "detailed_description": "An implementation of Proximal Policy Optimization (PPO) tailored for multi-agent environments, specifically using Beta distributions for action sampling. It is demonstrated on the Unity ML Tennis environment but serves as a reference implementation for Multi-Agent PPO (MAPPO).",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "multi_agent_rl",
        "policy_optimization"
      ],
      "application_level": "library",
      "primary_language": "ASP",
      "repo_url": "https://github.com/jsztompka/MultiAgent-PPO",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mappo",
        "multi-agent",
        "unity-ml-agents"
      ],
      "id": 152
    },
    {
      "name": "PPO-Tensorflow-2.0",
      "one_line_profile": "TensorFlow 2.0 implementation of Proximal Policy Optimization",
      "detailed_description": "A clean and reusable implementation of the Proximal Policy Optimization (PPO) algorithm using TensorFlow 2.0. It serves as a baseline library for reinforcement learning research and application development.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "policy_optimization",
        "reinforcement_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/jw1401/PPO-Tensorflow-2.0",
      "help_website": [],
      "license": null,
      "tags": [
        "ppo",
        "tensorflow-2",
        "deep-rl"
      ],
      "id": 153
    },
    {
      "name": "UA-MPC",
      "one_line_profile": "Uncertainty-Aware Model Predictive Control for Motorized LiDAR Odometry",
      "detailed_description": "A control framework implementing Uncertainty-Aware Model Predictive Control (UA-MPC) specifically for motorized LiDAR odometry systems. It addresses state estimation uncertainty in the control loop to improve robotic mapping and navigation accuracy.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "robotics_control",
        "state_estimation",
        "optimal_control"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/kafeiyin00/UA-MPC",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "mpc",
        "lidar-odometry",
        "robotics",
        "uncertainty-aware"
      ],
      "id": 154
    },
    {
      "name": "multiagent-sac",
      "one_line_profile": "Centralized training, centralized execution Soft Actor-Critic for multi-agent systems",
      "detailed_description": "An implementation of the Soft Actor-Critic (SAC) algorithm adapted for multi-agent settings using a centralized training and centralized execution paradigm. It provides a codebase for experimenting with maximum entropy RL in multi-agent environments.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "multi_agent_rl",
        "policy_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/kantologist/multiagent-sac",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "sac",
        "multi-agent",
        "reinforcement-learning"
      ],
      "id": 155
    },
    {
      "name": "esac",
      "one_line_profile": "Evolution-based Soft Actor-Critic (ESAC) implementation",
      "detailed_description": "An implementation of Evolution-based Soft Actor-Critic (ESAC), a hybrid reinforcement learning algorithm combining evolutionary strategies with soft actor-critic for improved exploration and stability in continuous control tasks.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "policy_optimization",
        "evolutionary_strategies",
        "continuous_control"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/karush17/esac",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "sac",
        "evolutionary-algorithms",
        "reinforcement-learning"
      ],
      "id": 156
    },
    {
      "name": "arp",
      "one_line_profile": "Autoregressive policies for continuous control reinforcement learning",
      "detailed_description": "A library implementing autoregressive policies for continuous control in reinforcement learning. It enables the modeling of complex dependencies in action spaces, improving performance in high-dimensional control tasks.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "continuous_control",
        "policy_modeling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/kindredresearch/arp",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "autoregressive-models",
        "continuous-control",
        "reinforcement-learning"
      ],
      "id": 157
    },
    {
      "name": "mppi_playground",
      "one_line_profile": "PyTorch implementation of Model Predictive Path Integral Control (MPPI)",
      "detailed_description": "A PyTorch-based implementation of Model Predictive Path Integral (MPPI) control. It serves as a flexible library and testing ground for sampling-based model predictive control algorithms, leveraging GPU acceleration for parallel trajectory sampling.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "optimal_control",
        "motion_planning",
        "trajectory_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/kohonda/mppi_playground",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mppi",
        "pytorch",
        "optimal-control",
        "mpc"
      ],
      "id": 158
    },
    {
      "name": "proj-svg_mppi",
      "one_line_profile": "Stein Variational Guided Model Predictive Path Integral Control",
      "detailed_description": "An implementation of Stein Variational Guided Model Predictive Path Integral (SVG-MPPI) control. This advanced control algorithm improves upon standard MPPI by using Stein Variational Gradient Descent to guide the sampling process, suitable for fast maneuvering vehicles.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "optimal_control",
        "motion_planning",
        "vehicle_dynamics"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/kohonda/proj-svg_mppi",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mppi",
        "stein-variational-gradient-descent",
        "autonomous-racing",
        "control-theory"
      ],
      "id": 159
    },
    {
      "name": "parallel-trpo",
      "one_line_profile": "Parallel implementation of Trust Region Policy Optimization (TRPO)",
      "detailed_description": "A high-performance, parallelized implementation of the Trust Region Policy Optimization (TRPO) algorithm. It is designed to accelerate RL training by distributing experience collection and gradient computation.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "policy_optimization",
        "reinforcement_learning",
        "distributed_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/kvfrans/parallel-trpo",
      "help_website": [],
      "license": null,
      "tags": [
        "trpo",
        "parallel-computing",
        "reinforcement-learning"
      ],
      "id": 160
    },
    {
      "name": "grail",
      "one_line_profile": "Gas Reliability Analysis Integrated Library for pipeline optimization and control",
      "detailed_description": "A library developed by Los Alamos National Laboratory for the optimization, optimal control, and simulation of natural gas pipelines. It provides algorithms for analyzing gas reliability and managing pipeline network dynamics.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "pipeline_optimization",
        "optimal_control",
        "network_simulation"
      ],
      "application_level": "library",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/lanl-ansi/grail",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "gas-pipeline",
        "optimization",
        "optimal-control",
        "scientific-computing"
      ],
      "id": 161
    },
    {
      "name": "Carla_Ray_Rlib",
      "one_line_profile": "Integration interface for Carla Simulator with Ray/RLlib",
      "detailed_description": "A middleware tool that connects the Carla autonomous driving simulator with the Ray/RLlib reinforcement learning library. It facilitates the training of autonomous driving agents by providing a compatible environment interface.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "autonomous_driving",
        "simulation_interface",
        "reinforcement_learning"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/layssi/Carla_Ray_Rlib",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "carla-simulator",
        "ray-rllib",
        "autonomous-driving",
        "reinforcement-learning"
      ],
      "id": 162
    },
    {
      "name": "MPCPy",
      "one_line_profile": "Platform for Model Predictive Control in building energy systems",
      "detailed_description": "An open-source platform developed by LBL for implementing and testing Model Predictive Control (MPC) in building energy management. It focuses on optimizing energy consumption and comfort in building systems through advanced control strategies.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "building_control",
        "energy_optimization",
        "optimal_control"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/lbl-srg/MPCPy",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "mpc",
        "building-energy",
        "smart-buildings",
        "control-systems"
      ],
      "id": 163
    },
    {
      "name": "ur5_reinforcement_learning_grasp_object",
      "one_line_profile": "Custom PyBullet environment for UR5 robotic manipulation",
      "detailed_description": "A custom reinforcement learning environment based on PyBullet for the UR5 robot arm. It is designed for training and benchmarking RL algorithms on robotic pick-and-place and grasping tasks.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "robotics_manipulation",
        "simulation_environment",
        "reinforcement_learning"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/leesweqq/ur5_reinforcement_learning_grasp_object",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ur5",
        "pybullet",
        "robotics",
        "reinforcement-learning"
      ],
      "id": 164
    },
    {
      "name": "perceptive_mpc",
      "one_line_profile": "Perceptive Model Predictive Control for continuous mobile manipulation",
      "detailed_description": "A C++ implementation of Perceptive Model Predictive Control (MPC) designed for continuous mobile manipulation tasks, integrating perception with control loops.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "control_policy",
        "trajectory_optimization"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/leggedrobotics/perceptive_mpc",
      "help_website": [],
      "license": null,
      "tags": [
        "mpc",
        "robotics",
        "mobile-manipulation",
        "control"
      ],
      "id": 165
    },
    {
      "name": "Robotic World Model",
      "one_line_profile": "Neural network simulator for robust policy optimization in robotics",
      "detailed_description": "A neural network-based simulator designed to facilitate robust policy optimization in robotics by modeling complex world dynamics for reinforcement learning agents.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "simulation",
        "policy_optimization"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/leggedrobotics/robotic_world_model",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "robotics",
        "simulation",
        "world-model",
        "reinforcement-learning"
      ],
      "id": 166
    },
    {
      "name": "PADQOC",
      "one_line_profile": "Parallel Automatic Differentiation Quantum Optimal Control solver",
      "detailed_description": "An open-source, Python-based quantum optimal control solver built with TensorFlow 2. It supports GPU computing, Hamiltonian distributions, and arbitrary parameterization of the control basis for general quantum systems.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "optimal_control",
        "quantum_control"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/lifeishard/PADQOC",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "quantum-control",
        "optimal-control",
        "tensorflow",
        "automatic-differentiation"
      ],
      "id": 167
    },
    {
      "name": "CVPO-Safe-RL",
      "one_line_profile": "Constrained Variational Policy Optimization for Safe Reinforcement Learning",
      "detailed_description": "Implementation of the Constrained Variational Policy Optimization (CVPO) algorithm, providing a solver for safe reinforcement learning problems where constraints must be satisfied during policy optimization.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "policy_optimization",
        "safe_rl"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/liuzuxin/cvpo-safe-rl",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "safe-rl",
        "reinforcement-learning",
        "policy-optimization",
        "constrained-optimization"
      ],
      "id": 168
    },
    {
      "name": "Crocoddyl",
      "one_line_profile": "Optimal control library for robot control under contact sequence",
      "detailed_description": "An optimal control library for robot control under contact sequence, featuring solvers based on efficient Differential Dynamic Programming (DDP)-like algorithms.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "optimal_control",
        "trajectory_optimization"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/loco-3d/crocoddyl",
      "help_website": [
        "https://gepettoweb.laas.fr/doc/loco-3d/crocoddyl/master/doxygen-html/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "optimal-control",
        "ddp",
        "robotics",
        "trajectory-optimization"
      ],
      "id": 169
    },
    {
      "name": "mpc.pytorch",
      "one_line_profile": "Fast and differentiable model predictive control (MPC) solver for PyTorch",
      "detailed_description": "A fast and differentiable model predictive control (MPC) solver integrated with PyTorch, allowing for end-to-end learning of control policies and dynamics models.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "optimal_control",
        "differentiable_programming"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/locuslab/mpc.pytorch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mpc",
        "pytorch",
        "differentiable-control",
        "optimization"
      ],
      "id": 170
    },
    {
      "name": "SAC-pytorch",
      "one_line_profile": "PyTorch implementation of Soft Actor Critic (SAC)",
      "detailed_description": "A reusable and clean PyTorch implementation of the Soft Actor Critic (SAC) algorithm, widely used as a baseline or component in reinforcement learning research.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "policy_optimization",
        "reinforcement_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/lucidrains/SAC-pytorch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "sac",
        "reinforcement-learning",
        "pytorch",
        "soft-actor-critic"
      ],
      "id": 171
    },
    {
      "name": "evolutionary-policy-optimization",
      "one_line_profile": "PyTorch implementation of Evolutionary Policy Optimization",
      "detailed_description": "A PyTorch implementation of the Evolutionary Policy Optimization algorithm, providing a tool for gradient-free policy search and optimization.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "policy_optimization",
        "evolutionary_algorithms"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/lucidrains/evolutionary-policy-optimization",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "evolutionary-strategies",
        "policy-optimization",
        "reinforcement-learning",
        "pytorch"
      ],
      "id": 172
    },
    {
      "name": "CPPO",
      "one_line_profile": "Accelerated Group Relative Policy Optimization for Reasoning Models",
      "detailed_description": "A framework implementing CPPO (Coherent Proximal Policy Optimization) to accelerate the training of reasoning models using Group Relative Policy Optimization, applicable in RLHF and complex policy search tasks.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "policy_optimization",
        "reinforcement_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/lzhxmu/CPPO",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ppo",
        "reinforcement-learning",
        "policy-optimization",
        "reasoning-models"
      ],
      "id": 173
    },
    {
      "name": "dm_control2gym",
      "one_line_profile": "OpenAI Gym Wrapper for DeepMind Control Suite",
      "detailed_description": "A utility library that wraps the DeepMind Control Suite environments to make them compatible with the OpenAI Gym interface, facilitating RL research and benchmarking.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "simulation",
        "environment_wrapper"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/martinseilair/dm_control2gym",
      "help_website": [],
      "license": null,
      "tags": [
        "openai-gym",
        "deepmind-control-suite",
        "reinforcement-learning",
        "wrapper"
      ],
      "id": 174
    },
    {
      "name": "benchnav",
      "one_line_profile": "Off-road navigation simulator for benchmarking planning algorithms",
      "detailed_description": "A simulation environment designed for benchmarking navigation and planning algorithms in off-road scenarios, supporting research in autonomous mobile robotics.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "simulation",
        "benchmarking"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/masafumiendo/benchnav",
      "help_website": [],
      "license": null,
      "tags": [
        "navigation",
        "simulation",
        "benchmarking",
        "robotics"
      ],
      "id": 175
    },
    {
      "name": "LeanRL",
      "one_line_profile": "Performance-optimized fork of CleanRL",
      "detailed_description": "A high-performance reinforcement learning library forked from CleanRL, optimized using PyTorch compile and CUDA graphs for faster training and experimentation.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "policy_optimization",
        "reinforcement_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/meta-pytorch/LeanRL",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "reinforcement-learning",
        "pytorch",
        "optimization",
        "cleanrl"
      ],
      "id": 176
    },
    {
      "name": "ppo_libtorch",
      "one_line_profile": "C++ implementation of Proximal Policy Optimization",
      "detailed_description": "A C++ implementation of the Proximal Policy Optimization (PPO) algorithm using LibTorch, enabling high-performance RL training and deployment in C++ environments.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "policy_optimization",
        "reinforcement_learning"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/mhubii/ppo_libtorch",
      "help_website": [],
      "license": null,
      "tags": [
        "ppo",
        "libtorch",
        "c++",
        "reinforcement-learning"
      ],
      "id": 177
    },
    {
      "name": "Craftium",
      "one_line_profile": "Framework for creating 3D Minecraft-like AI environments",
      "detailed_description": "A framework for creating rich, 3D, Minecraft-like single and multi-agent environments for AI research, supporting complex task simulation and agent training.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "simulation",
        "environment_generation"
      ],
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/mikelma/craftium",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "simulation",
        "multi-agent",
        "reinforcement-learning",
        "minecraft-like"
      ],
      "id": 178
    },
    {
      "name": "mppi_numba",
      "one_line_profile": "GPU implementation of Model Predictive Path Integral (MPPI) control",
      "detailed_description": "A GPU-accelerated implementation of Model Predictive Path Integral (MPPI) control using Numba, featuring a probabilistic traversability model for risk-aware trajectory planning.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "optimal_control",
        "trajectory_planning"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/mit-acl/mppi_numba",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mppi",
        "control",
        "gpu",
        "numba"
      ],
      "id": 179
    },
    {
      "name": "PGMORL",
      "one_line_profile": "Prediction-Guided Multi-Objective Reinforcement Learning",
      "detailed_description": "Implementation of Prediction-Guided Multi-Objective Reinforcement Learning (PGMORL) for continuous robot control, enabling the optimization of multiple conflicting objectives in robotic tasks.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "policy_optimization",
        "multi_objective_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/mit-gfx/PGMORL",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "multi-objective-rl",
        "robotics",
        "control",
        "reinforcement-learning"
      ],
      "id": 180
    },
    {
      "name": "reinforcement-learning-sumo",
      "one_line_profile": "Integration of Reinforcement Learning with SUMO traffic simulation",
      "detailed_description": "A tool that integrates Reinforcement Learning with traffic microsimulation (SUMO) using Ray RLLIB and OpenAI Gym, facilitating the development of traffic control policies.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "simulation",
        "traffic_control"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/mschrader15/reinforcement-learning-sumo",
      "help_website": [],
      "license": null,
      "tags": [
        "sumo",
        "traffic-simulation",
        "reinforcement-learning",
        "rllib"
      ],
      "id": 181
    },
    {
      "name": "PARC",
      "one_line_profile": "Physics-based Augmentation with Reinforcement Learning for Character Controllers",
      "detailed_description": "A framework for Physics-based Augmentation with Reinforcement Learning for Character Controllers (PARC), enabling the creation of physically realistic character movements.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "simulation",
        "character_control"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/mshoe/PARC",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "character-control",
        "physics-simulation",
        "reinforcement-learning",
        "animation"
      ],
      "id": 182
    },
    {
      "name": "Multi-Commander",
      "one_line_profile": "Multi & Single Agent Reinforcement Learning for Traffic Signal Control",
      "detailed_description": "A modular framework designed for simulating and solving Traffic Signal Control problems using both Multi-Agent and Single-Agent Reinforcement Learning approaches.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "simulation",
        "traffic_control"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/multi-commander/Multi-Commander",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "traffic-control",
        "multi-agent-rl",
        "reinforcement-learning",
        "simulation"
      ],
      "id": 183
    },
    {
      "name": "UniROS",
      "one_line_profile": "ROS-based reinforcement learning framework for robotics simulation and real-world training",
      "detailed_description": "A comprehensive framework for reinforcement learning in robotics that bridges ROS (Robot Operating System) with RL agents, enabling training in both simulated environments and on physical robots.",
      "domains": [
        "P4-04",
        "Robotics"
      ],
      "subtask_category": [
        "robotics_control",
        "simulation"
      ],
      "application_level": "framework",
      "primary_language": "CMake",
      "repo_url": "https://github.com/ncbdrck/UniROS",
      "help_website": [],
      "license": null,
      "tags": [
        "ros",
        "robotics",
        "reinforcement-learning",
        "simulation"
      ],
      "id": 184
    },
    {
      "name": "TD-MPC",
      "one_line_profile": "Implementation of Temporal Difference Learning for Model Predictive Control",
      "detailed_description": "A codebase providing the implementation of the TD-MPC algorithm, which combines model-based reinforcement learning with model predictive control for continuous control tasks.",
      "domains": [
        "P4-04",
        "P4"
      ],
      "subtask_category": [
        "model_predictive_control",
        "policy_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/nicklashansen/tdmpc",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mpc",
        "reinforcement-learning",
        "continuous-control"
      ],
      "id": 185
    },
    {
      "name": "TD-MPC2",
      "one_line_profile": "Scalable and robust world models for continuous control",
      "detailed_description": "The second version of TD-MPC, providing scalable and robust world models for continuous control tasks, serving as a state-of-the-art baseline for model-based reinforcement learning.",
      "domains": [
        "P4-04",
        "P4"
      ],
      "subtask_category": [
        "model_predictive_control",
        "world_models"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/nicklashansen/tdmpc2",
      "help_website": [
        "https://tdmpc2.github.io/"
      ],
      "license": "MIT",
      "tags": [
        "mpc",
        "world-models",
        "reinforcement-learning"
      ],
      "id": 186
    },
    {
      "name": "PPO-PyTorch",
      "one_line_profile": "Minimal PyTorch implementation of Proximal Policy Optimization",
      "detailed_description": "A widely used, minimal, and clean implementation of the Proximal Policy Optimization (PPO) algorithm in PyTorch, serving as a reference solver for reinforcement learning research.",
      "domains": [
        "P4-04"
      ],
      "subtask_category": [
        "policy_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/nikhilbarhate99/PPO-PyTorch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ppo",
        "reinforcement-learning",
        "pytorch"
      ],
      "id": 187
    },
    {
      "name": "min-decision-transformer",
      "one_line_profile": "Minimal implementation of Decision Transformer for RL",
      "detailed_description": "A minimal PyTorch implementation of the Decision Transformer architecture, treating reinforcement learning as a sequence modeling problem, specifically tailored for MuJoCo control tasks.",
      "domains": [
        "P4-04"
      ],
      "subtask_category": [
        "sequence_modeling",
        "policy_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/nikhilbarhate99/min-decision-transformer",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "decision-transformer",
        "offline-rl",
        "mujoco"
      ],
      "id": 188
    },
    {
      "name": "pgpelib",
      "one_line_profile": "Library for Policy Gradients with Parameter-based Exploration",
      "detailed_description": "A library implementing Policy Gradients with Parameter-based Exploration (PGPE) and the ClipUp optimizer, designed for neuroevolution and reinforcement learning tasks.",
      "domains": [
        "P4-04"
      ],
      "subtask_category": [
        "policy_gradient",
        "parameter_exploration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/nnaisense/pgpelib",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "evolutionary-strategies",
        "policy-gradient",
        "reinforcement-learning"
      ],
      "id": 189
    },
    {
      "name": "SINDy-RL",
      "one_line_profile": "Interpretable and efficient model-based reinforcement learning using SINDy",
      "detailed_description": "A framework combining Sparse Identification of Nonlinear Dynamics (SINDy) with reinforcement learning to create interpretable, data-efficient, and robust models for control systems.",
      "domains": [
        "P4-04",
        "P4-02"
      ],
      "subtask_category": [
        "system_identification",
        "model_based_rl"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/nzolman/sindy-rl",
      "help_website": [],
      "license": null,
      "tags": [
        "sindy",
        "interpretable-ml",
        "dynamics-modeling"
      ],
      "id": 190
    },
    {
      "name": "AutoGenU Jupyter",
      "one_line_profile": "Automatic code generator for nonlinear model predictive control (NMPC)",
      "detailed_description": "A tool that automatically generates C++ code for nonlinear model predictive control (NMPC) solvers based on the Continuation/GMRES method, accessible via Jupyter Notebook.",
      "domains": [
        "P4-04",
        "P4"
      ],
      "subtask_category": [
        "code_generation",
        "nmpc",
        "optimal_control"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/ohtsukalab/autogenu-jupyter",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nmpc",
        "code-generation",
        "control-theory"
      ],
      "id": 191
    },
    {
      "name": "QueST",
      "one_line_profile": "Self-Supervised Skill Abstractions for Continuous Control",
      "detailed_description": "Official implementation of QueST, a method for learning self-supervised skill abstractions to facilitate continuous control in reinforcement learning environments.",
      "domains": [
        "P4-04"
      ],
      "subtask_category": [
        "skill_discovery",
        "continuous_control"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/pairlab/QueST",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "hierarchical-rl",
        "skill-learning",
        "continuous-control"
      ],
      "id": 192
    },
    {
      "name": "Brax Viewer",
      "one_line_profile": "Real-time web viewer for monitoring Brax RL policies",
      "detailed_description": "An interactive web-based visualization tool for monitoring and analyzing reinforcement learning policies trained with the Brax physics engine.",
      "domains": [
        "P4-04",
        "P4"
      ],
      "subtask_category": [
        "visualization",
        "simulation_monitoring"
      ],
      "application_level": "application",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/pal-robotics/brax_training_viewer",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "brax",
        "visualization",
        "reinforcement-learning"
      ],
      "id": 193
    },
    {
      "name": "libsia",
      "one_line_profile": "Library for model-based stochastic estimation and optimal control",
      "detailed_description": "A C++/Python library designed for model-based stochastic estimation and optimal control, providing tools for handling dynamic systems with uncertainty.",
      "domains": [
        "P4-04",
        "P4"
      ],
      "subtask_category": [
        "stochastic_control",
        "estimation"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/parkerowan/libsia",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "optimal-control",
        "estimation",
        "stochastic-systems"
      ],
      "id": 194
    },
    {
      "name": "TRPO",
      "one_line_profile": "Implementation of Trust Region Policy Optimization",
      "detailed_description": "A reference implementation of the Trust Region Policy Optimization (TRPO) algorithm using TensorFlow, often used as a baseline for policy gradient research.",
      "domains": [
        "P4-04"
      ],
      "subtask_category": [
        "policy_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/pat-coady/trpo",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "trpo",
        "reinforcement-learning",
        "tensorflow"
      ],
      "id": 195
    },
    {
      "name": "torchcde",
      "one_line_profile": "Differentiable controlled differential equation solvers for PyTorch",
      "detailed_description": "A library providing differentiable solvers for controlled differential equations (CDEs) in PyTorch, enabling the modeling of continuous-time dynamics driven by irregular data.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "differential_equations",
        "time_series_modeling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/patrick-kidger/torchcde",
      "help_website": [
        "https://github.com/patrick-kidger/torchcde"
      ],
      "license": "Apache-2.0",
      "tags": [
        "cde",
        "differential-equations",
        "pytorch"
      ],
      "id": 196
    },
    {
      "name": "xpag",
      "one_line_profile": "Modular reinforcement learning library with JAX agents",
      "detailed_description": "A modular reinforcement learning library built on JAX, designed for flexibility and efficiency in developing and testing RL agents.",
      "domains": [
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "agent_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/perrin-isir/xpag",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "jax",
        "reinforcement-learning",
        "modular-rl"
      ],
      "id": 197
    },
    {
      "name": "SAC_PyTorch",
      "one_line_profile": "Minimal PyTorch Soft Actor Critic (SAC) implementation",
      "detailed_description": "A clean and minimal implementation of the Soft Actor-Critic (SAC) algorithm in PyTorch, suitable for research and educational purposes in continuous control.",
      "domains": [
        "P4-04"
      ],
      "subtask_category": [
        "policy_optimization",
        "continuous_control"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/philipjball/SAC_PyTorch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "sac",
        "reinforcement-learning",
        "pytorch"
      ],
      "id": 198
    },
    {
      "name": "pi-optimal",
      "one_line_profile": "Python library for RL-based control of dynamic systems",
      "detailed_description": "An open-source Python library for Reinforcement Learning designed to model, optimize, and control dynamic systems.",
      "domains": [
        "P4-04",
        "P4"
      ],
      "subtask_category": [
        "dynamic_control",
        "optimization"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/pi-optimal/pi-optimal",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "control-systems",
        "reinforcement-learning",
        "optimization"
      ],
      "id": 199
    },
    {
      "name": "Neuromancer",
      "one_line_profile": "Differentiable programming for constrained optimization and system identification",
      "detailed_description": "A PyTorch-based framework for solving parametric constrained optimization problems, physics-informed system identification, and parametric model predictive control using differentiable programming.",
      "domains": [
        "P4",
        "P4-04",
        "P4-02"
      ],
      "subtask_category": [
        "constrained_optimization",
        "system_identification",
        "mpc"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/pnnl/neuromancer",
      "help_website": [
        "https://pnnl.github.io/neuromancer/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "differentiable-programming",
        "optimization",
        "mpc",
        "physics-informed"
      ],
      "id": 200
    },
    {
      "name": "pytorch-soft-actor-critic",
      "one_line_profile": "PyTorch implementation of Soft Actor-Critic (SAC)",
      "detailed_description": "A highly popular and reusable PyTorch implementation of the Soft Actor-Critic (SAC) algorithm, widely used as a baseline in RL research.",
      "domains": [
        "P4-04"
      ],
      "subtask_category": [
        "policy_optimization",
        "continuous_control"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/pranz24/pytorch-soft-actor-critic",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "sac",
        "reinforcement-learning",
        "pytorch"
      ],
      "id": 201
    },
    {
      "name": "PyPose",
      "one_line_profile": "Library for differentiable robotics on manifolds",
      "detailed_description": "A PyTorch-based library for differentiable robotics that handles Lie groups and manifold operations, enabling gradient-based optimization for control and state estimation tasks.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "robotics_control",
        "state_estimation",
        "differentiable_physics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pypose/pypose",
      "help_website": [
        "https://pypose.org/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "robotics",
        "lie-groups",
        "differentiable-programming"
      ],
      "id": 202
    },
    {
      "name": "pyRDDLGym",
      "one_line_profile": "Toolkit for auto-generation of Gym environments from RDDL",
      "detailed_description": "A toolkit that automatically generates OpenAI Gym environments from Relational Dynamic Influence Diagram Language (RDDL) description files, facilitating planning and RL research.",
      "domains": [
        "P4-04",
        "P4"
      ],
      "subtask_category": [
        "environment_generation",
        "planning"
      ],
      "application_level": "toolkit",
      "primary_language": "Python",
      "repo_url": "https://github.com/pyrddlgym-project/pyRDDLGym",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rddl",
        "planning",
        "reinforcement-learning"
      ],
      "id": 203
    },
    {
      "name": "pyRDDLGym-jax",
      "one_line_profile": "JAX compilation and differentiable planner for RDDL",
      "detailed_description": "An extension of pyRDDLGym that compiles RDDL descriptions into JAX for accelerated simulation and differentiable planning.",
      "domains": [
        "P4-04",
        "P4"
      ],
      "subtask_category": [
        "differentiable_planning",
        "simulation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pyrddlgym-project/pyRDDLGym-jax",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "jax",
        "rddl",
        "planning"
      ],
      "id": 204
    },
    {
      "name": "TorchRL",
      "one_line_profile": "Modular, primitive-first PyTorch library for Reinforcement Learning",
      "detailed_description": "A library built on PyTorch that provides modular primitives for constructing reinforcement learning algorithms, designed for flexibility, efficiency, and ease of use.",
      "domains": [
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "agent_design"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pytorch/rl",
      "help_website": [
        "https://pytorch.org/rl/"
      ],
      "license": "MIT",
      "tags": [
        "pytorch",
        "reinforcement-learning",
        "modular-rl"
      ],
      "id": 205
    },
    {
      "name": "panda-gym",
      "one_line_profile": "Robotic environments based on PyBullet and Gymnasium",
      "detailed_description": "A set of robotic simulation environments for the Franka Emika Panda robot, built on the PyBullet physics engine and compatible with the Gymnasium API for RL training.",
      "domains": [
        "P4-04",
        "Robotics"
      ],
      "subtask_category": [
        "simulation",
        "robotics_control"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/qgallouedec/panda-gym",
      "help_website": [
        "https://panda-gym.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "pybullet",
        "robotics",
        "gymnasium"
      ],
      "id": 206
    },
    {
      "name": "gym-games",
      "one_line_profile": "Collection of Gymnasium compatible games for RL",
      "detailed_description": "A collection of game environments compatible with the Gymnasium API, serving as benchmarks for testing and developing reinforcement learning algorithms.",
      "domains": [
        "P4-04"
      ],
      "subtask_category": [
        "simulation",
        "benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/qlan3/gym-games",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gymnasium",
        "games",
        "reinforcement-learning"
      ],
      "id": 207
    },
    {
      "name": "Popular-RL-Algorithms",
      "one_line_profile": "Collection of PyTorch implementations for popular RL algorithms",
      "detailed_description": "A comprehensive collection of PyTorch implementations for popular reinforcement learning algorithms (SAC, TD3, PPO, etc.), used as a reference codebase for research.",
      "domains": [
        "P4-04"
      ],
      "subtask_category": [
        "policy_optimization",
        "algorithm_implementation"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/quantumiracle/Popular-RL-Algorithms",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rl-algorithms",
        "pytorch",
        "reference-implementation"
      ],
      "id": 208
    },
    {
      "name": "softlearning",
      "one_line_profile": "Reinforcement learning framework for maximum entropy policies",
      "detailed_description": "A reinforcement learning framework specialized for training maximum entropy policies in continuous domains, including the official implementation of the Soft Actor-Critic algorithm.",
      "domains": [
        "P4-04"
      ],
      "subtask_category": [
        "maximum_entropy_rl",
        "continuous_control"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/rail-berkeley/softlearning",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "soft-actor-critic",
        "maximum-entropy",
        "reinforcement-learning"
      ],
      "id": 209
    },
    {
      "name": "Ray",
      "one_line_profile": "Unified framework for scaling AI and Python applications",
      "detailed_description": "A high-performance distributed computing framework that includes libraries for reinforcement learning (RLlib) and hyperparameter tuning (Ray Tune), widely used for scaling scientific AI workloads.",
      "domains": [
        "P4-04",
        "P4"
      ],
      "subtask_category": [
        "distributed_computing",
        "reinforcement_learning",
        "hyperparameter_optimization"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/ray-project/ray",
      "help_website": [
        "https://docs.ray.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "distributed-computing",
        "rllib",
        "scaling"
      ],
      "id": 210
    },
    {
      "name": "Enflow",
      "one_line_profile": "Framework for modelling sequential decision problems in the energy sector",
      "detailed_description": "An open-source Python framework designed for modeling and solving sequential decision-making problems specifically within the energy sector using reinforcement learning and optimization.",
      "domains": [
        "P4-04",
        "Energy"
      ],
      "subtask_category": [
        "energy_optimization",
        "sequential_decision_making"
      ],
      "application_level": "framework",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/rebase-energy/enflow",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "energy",
        "optimization",
        "reinforcement-learning"
      ],
      "id": 211
    },
    {
      "name": "ros-gazebo-gym",
      "one_line_profile": "Integration of ROS and Gazebo with Gymnasium",
      "detailed_description": "A framework that integrates ROS (Robot Operating System) and Gazebo simulation with the Gymnasium API, streamlining the training of reinforcement learning algorithms for robotics.",
      "domains": [
        "P4-04",
        "Robotics"
      ],
      "subtask_category": [
        "robotics_simulation",
        "environment_integration"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/rickstaa/ros-gazebo-gym",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ros",
        "gazebo",
        "gymnasium",
        "robotics"
      ],
      "id": 212
    },
    {
      "name": "rl-tools",
      "one_line_profile": "High-performance Deep Reinforcement Learning Library in C++",
      "detailed_description": "A header-only, dependency-free C++ library for deep reinforcement learning, optimized for performance and embedded devices.",
      "domains": [
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "embedded_control"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/rl-tools/rl-tools",
      "help_website": [
        "https://rl.tools"
      ],
      "license": "MIT",
      "tags": [
        "c++",
        "high-performance",
        "embedded-rl"
      ],
      "id": 213
    },
    {
      "name": "garage",
      "one_line_profile": "A toolkit for reproducible reinforcement learning research",
      "detailed_description": "A toolkit for developing and evaluating reinforcement learning algorithms, focusing on reproducibility and modularity. It supports various RL algorithms and environments.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "policy_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/rlworkgroup/garage",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "reinforcement-learning",
        "reproducibility",
        "toolkit"
      ],
      "id": 214
    },
    {
      "name": "gym-anm",
      "one_line_profile": "Reinforcement Learning environments for Active Network Management in electricity networks",
      "detailed_description": "A framework designed to model Active Network Management (ANM) tasks in electricity distribution networks as Reinforcement Learning environments, facilitating research in grid control.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "simulation",
        "environment_design",
        "grid_control"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/robinhenry/gym-anm",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "reinforcement-learning",
        "smart-grid",
        "simulation"
      ],
      "id": 215
    },
    {
      "name": "control_box_rst",
      "one_line_profile": "C++ libraries for predictive and optimal control",
      "detailed_description": "Provides C++ libraries for predictive control, direct optimal control, optimization, and simulation, serving as a foundation for control theory implementation.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "optimal_control",
        "predictive_control",
        "optimization"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/rst-tu-dortmund/control_box_rst",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "control-theory",
        "mpc",
        "optimization"
      ],
      "id": 216
    },
    {
      "name": "mpc_local_planner",
      "one_line_profile": "Model Predictive Control (MPC) plugin for ROS navigation",
      "detailed_description": "Implements a generic and versatile Model Predictive Control (MPC) local planner plugin for the ROS navigation stack, supporting minimum-time and quadratic-form configurations.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "robotics_control",
        "trajectory_planning",
        "mpc"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/rst-tu-dortmund/mpc_local_planner",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "ros",
        "mpc",
        "robotics"
      ],
      "id": 217
    },
    {
      "name": "RLLib",
      "one_line_profile": "C++ Template Library for Reinforcement Learning",
      "detailed_description": "A C++ Template Library designed to predict, control, learn behaviors, and represent learnable knowledge using On/Off Policy Reinforcement Learning algorithms.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "control"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/samindaa/RLLib",
      "help_website": [],
      "license": null,
      "tags": [
        "reinforcement-learning",
        "cpp",
        "control"
      ],
      "id": 218
    },
    {
      "name": "multiagent_mujoco",
      "one_line_profile": "Benchmark for Continuous Multi-Agent Robotic Control",
      "detailed_description": "A benchmark environment for Continuous Multi-Agent Robotic Control, built upon OpenAI's MuJoCo Gym environments, facilitating research in multi-agent RL.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "simulation",
        "benchmarking",
        "multi_agent_control"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/schroederdewitt/multiagent_mujoco",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "multi-agent",
        "mujoco",
        "robotics"
      ],
      "id": 219
    },
    {
      "name": "SimionZoo",
      "one_line_profile": "Workbench for online model-free Reinforcement Learning",
      "detailed_description": "A workbench designed for online model-free Reinforcement Learning applied to continuous control problems, providing tools for experimentation and simulation.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "continuous_control",
        "simulation"
      ],
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/simionsoft/SimionZoo",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "reinforcement-learning",
        "workbench",
        "control"
      ],
      "id": 220
    },
    {
      "name": "jsbgym",
      "one_line_profile": "JSBSim environment for reinforcement learning",
      "detailed_description": "An environment wrapping the JSBSim flight dynamics model for use with Reinforcement Learning algorithms, enabling flight control research.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "flight_simulation",
        "reinforcement_learning",
        "environment_design"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/sryu1/jsbgym",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "flight-dynamics",
        "jsbsim",
        "reinforcement-learning"
      ],
      "id": 221
    },
    {
      "name": "osim-rl",
      "one_line_profile": "Reinforcement learning environments with musculoskeletal models",
      "detailed_description": "Provides Reinforcement Learning environments integrated with OpenSim musculoskeletal models, facilitating research in biomechanics and motor control.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "biomechanics_simulation",
        "reinforcement_learning",
        "motor_control"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/stanfordnmbl/osim-rl",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "opensim",
        "biomechanics",
        "reinforcement-learning"
      ],
      "id": 222
    },
    {
      "name": "mobile-env",
      "one_line_profile": "Gymnasium environment for autonomous coordination in wireless mobile networks",
      "detailed_description": "An open, minimalist Gymnasium environment designed for simulating and training reinforcement learning agents for autonomous coordination tasks in wireless mobile networks.",
      "domains": [
        "P4",
        "P4-04",
        "Telecommunications"
      ],
      "subtask_category": [
        "simulation",
        "environment"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/stefanbschneider/mobile-env",
      "help_website": [
        "https://mobile-env.readthedocs.io"
      ],
      "license": "MIT",
      "tags": [
        "reinforcement-learning",
        "wireless-networks",
        "gym-environment",
        "simulation"
      ],
      "id": 223
    },
    {
      "name": "qpmpc_layers",
      "one_line_profile": "Differentiable linear model predictive control layers",
      "detailed_description": "A Python library providing differentiable layers for linear Model Predictive Control (MPC), enabling the integration of MPC into differentiable programming and learning pipelines.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "control",
        "modeling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/stephane-caron/qpmpc_layers",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "mpc",
        "differentiable-programming",
        "control-theory",
        "optimization"
      ],
      "id": 224
    },
    {
      "name": "jumpstart-rl",
      "one_line_profile": "Implementation of Jump-Start Reinforcement Learning (JSRL) for Stable Baselines3",
      "detailed_description": "A library extension for Stable Baselines3 that implements Jump-Start Reinforcement Learning (JSRL), allowing agents to leverage a guide policy to improve exploration and learning efficiency.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "control",
        "policy_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/steventango/jumpstart-rl",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "reinforcement-learning",
        "stable-baselines3",
        "jsrl",
        "exploration"
      ],
      "id": 225
    },
    {
      "name": "d3rlpy",
      "one_line_profile": "Offline deep reinforcement learning library",
      "detailed_description": "A comprehensive offline deep reinforcement learning library for Python, providing implementations of various offline RL algorithms and easy-to-use interfaces for training and evaluation.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "control",
        "policy_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/takuseno/d3rlpy",
      "help_website": [
        "https://d3rlpy.readthedocs.io"
      ],
      "license": "MIT",
      "tags": [
        "offline-rl",
        "reinforcement-learning",
        "deep-learning",
        "control"
      ],
      "id": 226
    },
    {
      "name": "Tensorforce",
      "one_line_profile": "TensorFlow library for applied reinforcement learning",
      "detailed_description": "A modular reinforcement learning library built on TensorFlow, designed for applied RL with a focus on modularity and flexibility for various control and optimization tasks.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "control",
        "policy_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/tensorforce/tensorforce",
      "help_website": [
        "https://tensorforce.readthedocs.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "reinforcement-learning",
        "tensorflow",
        "control",
        "optimization"
      ],
      "id": 227
    },
    {
      "name": "Tianshou",
      "one_line_profile": "Elegant PyTorch deep reinforcement learning library",
      "detailed_description": "A highly modular and fast deep reinforcement learning library based on PyTorch, supporting a wide range of algorithms and environments for research and application.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "control",
        "policy_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/thu-ml/tianshou",
      "help_website": [
        "https://tianshou.readthedocs.io"
      ],
      "license": "MIT",
      "tags": [
        "reinforcement-learning",
        "pytorch",
        "deep-learning",
        "control"
      ],
      "id": 228
    },
    {
      "name": "building-energy-storage-simulation",
      "one_line_profile": "Energy storage environment for RL and MPC",
      "detailed_description": "An open-source simulation environment for building energy storage, designed to explore and benchmark reinforcement learning and model predictive control algorithms.",
      "domains": [
        "P4",
        "P4-04",
        "Energy"
      ],
      "subtask_category": [
        "simulation",
        "environment"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/tobirohrer/building-energy-storage-simulation",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "energy-storage",
        "reinforcement-learning",
        "mpc",
        "simulation"
      ],
      "id": 229
    },
    {
      "name": "mppi-isaac",
      "one_line_profile": "Model Predictive Path Integral Control for Isaac Gym",
      "detailed_description": "A GPU-accelerated implementation of Model Predictive Path Integral (MPPI) control designed to work with Isaac Gym for high-performance robotics simulation and control.",
      "domains": [
        "P4",
        "P4-04",
        "Robotics"
      ],
      "subtask_category": [
        "control",
        "simulation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/tud-amr/mppi-isaac",
      "help_website": [],
      "license": null,
      "tags": [
        "mppi",
        "isaac-gym",
        "robotics",
        "control",
        "gpu-accelerated"
      ],
      "id": 230
    },
    {
      "name": "PhiFlow",
      "one_line_profile": "Differentiable PDE solving framework for machine learning",
      "detailed_description": "A differentiable PDE solving framework built for machine learning, enabling the integration of physical simulations (fluids, etc.) into deep learning models for physics-informed learning.",
      "domains": [
        "P4",
        "Physics"
      ],
      "subtask_category": [
        "modeling",
        "simulation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/tum-pbs/PhiFlow",
      "help_website": [
        "https://tum-pbs.github.io/PhiFlow"
      ],
      "license": "MIT",
      "tags": [
        "pde-solver",
        "differentiable-physics",
        "fluid-simulation",
        "machine-learning"
      ],
      "id": 231
    },
    {
      "name": "Sinergym",
      "one_line_profile": "Gym environment for building simulation and control",
      "detailed_description": "A gymnasium-compatible environment for building energy simulation and control, wrapping EnergyPlus to facilitate reinforcement learning research in building automation.",
      "domains": [
        "P4",
        "P4-04",
        "Energy"
      ],
      "subtask_category": [
        "simulation",
        "environment"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/ugr-sail/sinergym",
      "help_website": [
        "https://ugr-sail.github.io/sinergym/"
      ],
      "license": "MIT",
      "tags": [
        "energyplus",
        "reinforcement-learning",
        "building-simulation",
        "gym-environment"
      ],
      "id": 232
    },
    {
      "name": "gym-pybullet-drones",
      "one_line_profile": "PyBullet Gymnasium environments for quadcopter control",
      "detailed_description": "A set of PyBullet-based Gymnasium environments for single and multi-agent reinforcement learning of quadcopter control, supporting physics-based simulation.",
      "domains": [
        "P4",
        "P4-04",
        "Robotics"
      ],
      "subtask_category": [
        "simulation",
        "environment"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/utiasDSL/gym-pybullet-drones",
      "help_website": [
        "https://gym-pybullet-drones.readthedocs.io"
      ],
      "license": "MIT",
      "tags": [
        "drones",
        "pybullet",
        "reinforcement-learning",
        "multi-agent",
        "simulation"
      ],
      "id": 233
    },
    {
      "name": "high_mpc",
      "one_line_profile": "Policy Search for Model Predictive Control",
      "detailed_description": "A library for high-speed Model Predictive Control (MPC) and policy search, specifically designed for agile drone flight and robotics control tasks.",
      "domains": [
        "P4",
        "P4-04",
        "Robotics"
      ],
      "subtask_category": [
        "control",
        "optimization"
      ],
      "application_level": "library",
      "primary_language": "C",
      "repo_url": "https://github.com/uzh-rpg/high_mpc",
      "help_website": [],
      "license": null,
      "tags": [
        "mpc",
        "robotics",
        "drone-control",
        "policy-search"
      ],
      "id": 234
    },
    {
      "name": "rpg_mpc",
      "one_line_profile": "Model Predictive Control for Quadrotors",
      "detailed_description": "A robust Model Predictive Control (MPC) library for quadrotors, supporting perception-aware MPC and trajectory tracking, widely used in robotics research.",
      "domains": [
        "P4",
        "P4-04",
        "Robotics"
      ],
      "subtask_category": [
        "control",
        "optimization"
      ],
      "application_level": "library",
      "primary_language": "C",
      "repo_url": "https://github.com/uzh-rpg/rpg_mpc",
      "help_website": [
        "https://github.com/uzh-rpg/rpg_mpc/wiki"
      ],
      "license": "GPL-3.0",
      "tags": [
        "mpc",
        "quadrotor",
        "robotics",
        "control"
      ],
      "id": 235
    },
    {
      "name": "hydrax",
      "one_line_profile": "Sampling-based model predictive control (MPC) library on GPU using JAX",
      "detailed_description": "A library for sampling-based Model Predictive Control (MPC) that leverages JAX for hardware acceleration and automatic differentiation. It is designed for high-performance control tasks in robotics and physics simulations.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "optimal_control",
        "model_predictive_control",
        "robotics_control"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/vincekurtz/hydrax",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mpc",
        "jax",
        "control-theory",
        "gpu-acceleration"
      ],
      "id": 236
    },
    {
      "name": "cleanba",
      "one_line_profile": "Distributed Deep Reinforcement Learning implementation based on Podracer Sebulba",
      "detailed_description": "A high-performance distributed Deep Reinforcement Learning (DRL) implementation following DeepMind's Podracer Sebulba architecture. It is part of the CleanRL ecosystem and is designed for scalable RL research.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "distributed_rl",
        "policy_optimization",
        "training_framework"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/vwxyzjn/cleanba",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "distributed-rl",
        "cleanrl",
        "sebulba",
        "reinforcement-learning"
      ],
      "id": 237
    },
    {
      "name": "CleanRL",
      "one_line_profile": "High-quality single-file implementations of Deep Reinforcement Learning algorithms",
      "detailed_description": "A popular library providing clean, single-file implementations of deep reinforcement learning algorithms (PPO, DQN, SAC, etc.). It is widely used as a baseline and reference for RL research due to its simplicity and reproducibility.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "algorithm_implementation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/vwxyzjn/cleanrl",
      "help_website": [
        "https://docs.cleanrl.dev"
      ],
      "license": "NOASSERTION",
      "tags": [
        "ppo",
        "dqn",
        "sac",
        "reinforcement-learning",
        "baselines"
      ],
      "id": 238
    },
    {
      "name": "Pontryagin-Differentiable-Programming",
      "one_line_profile": "End-to-end learning and control framework using Pontryagin's Maximum Principle",
      "detailed_description": "A unified framework for solving control system tasks by integrating learning with control theory. It allows for learning objective functions, dynamics, or control policies by differentiating through the Pontryagin's Maximum Principle conditions.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "optimal_control",
        "differentiable_programming",
        "system_identification"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/wanxinjin/Pontryagin-Differentiable-Programming",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "optimal-control",
        "pontryagin-maximum-principle",
        "differentiable-physics",
        "learning-for-control"
      ],
      "id": 239
    },
    {
      "name": "Safe-PDP",
      "one_line_profile": "Safe differentiable framework for safety-critical learning and control",
      "detailed_description": "An extension of Pontryagin Differentiable Programming (PDP) specifically designed for safety-critical control tasks. It provides a theoretical and algorithmic framework to ensure safety constraints are met during learning and control.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "safe_control",
        "constrained_optimization",
        "safety_critical_systems"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/wanxinjin/Safe-PDP",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "safe-rl",
        "optimal-control",
        "safety-constraints"
      ],
      "id": 240
    },
    {
      "name": "IntelliLight",
      "one_line_profile": "Reinforcement learning framework for intelligent traffic light control",
      "detailed_description": "A reinforcement learning-based system for optimizing traffic light control. It serves as a domain-specific solver for traffic flow optimization problems using RL agents.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "traffic_control",
        "multi_agent_rl",
        "optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/wingsweihua/IntelliLight",
      "help_website": [],
      "license": null,
      "tags": [
        "traffic-signal-control",
        "reinforcement-learning",
        "smart-city"
      ],
      "id": 241
    },
    {
      "name": "RL-EV-Charging-Control",
      "one_line_profile": "Reinforcement learning solver for pricing and scheduling in EV charging stations",
      "detailed_description": "A reinforcement learning implementation for real-time pricing and scheduling control in Electric Vehicle (EV) charging stations. It addresses a specific optimization and control problem in smart grids.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "smart_grid_control",
        "scheduling",
        "pricing_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/wsyCUHK/Reinforcement-Learning-for-Real-time-Pricing-and-Scheduling-Control-in-EV-Charging-Stations",
      "help_website": [],
      "license": null,
      "tags": [
        "ev-charging",
        "smart-grid",
        "reinforcement-learning",
        "scheduling"
      ],
      "id": 242
    },
    {
      "name": "rtgym",
      "one_line_profile": "Framework for implementing real-time Gymnasium environments",
      "detailed_description": "A library designed to facilitate the implementation of custom Gymnasium environments for real-time applications, such as robotics or real-time simulations, handling timing and synchronization issues.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "environment_creation",
        "real_time_control",
        "robotics_simulation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/yannbouteiller/rtgym",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gymnasium",
        "real-time",
        "robotics",
        "reinforcement-learning"
      ],
      "id": 243
    },
    {
      "name": "POMO",
      "one_line_profile": "Policy Optimization with Multiple Optima for Neural Combinatorial Optimization",
      "detailed_description": "An implementation of the POMO method for solving combinatorial optimization problems (like TSP, CVRP) using reinforcement learning. It serves as a solver and baseline for neural combinatorial optimization research.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "combinatorial_optimization",
        "neural_solver",
        "vehicle_routing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/yd-kwon/POMO",
      "help_website": [],
      "license": null,
      "tags": [
        "combinatorial-optimization",
        "tsp",
        "cvrp",
        "reinforcement-learning"
      ],
      "id": 244
    },
    {
      "name": "CraftGround",
      "one_line_profile": "High-performance Minecraft Reinforcement Learning environment",
      "detailed_description": "A fast reinforcement learning environment based on Minecraft, designed for training agents in complex, open-ended 3D worlds. It provides a high-throughput interface for RL research.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "rl_environment",
        "simulation",
        "agent_training"
      ],
      "application_level": "platform",
      "primary_language": "Kotlin",
      "repo_url": "https://github.com/yhs0602/CraftGround",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "minecraft",
        "reinforcement-learning",
        "environment",
        "simulation"
      ],
      "id": 245
    },
    {
      "name": "DRL-for-Path-Planning",
      "one_line_profile": "Deep Reinforcement Learning library for robotic path planning and obstacle avoidance",
      "detailed_description": "A comprehensive collection of Deep Reinforcement Learning algorithms (SAC, Adaptive-SAC) specifically tailored for robotic path planning and LiDAR-based obstacle avoidance tasks.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "path_planning",
        "obstacle_avoidance",
        "robotics_control"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/zhaohaojie1998/DRL-for-Path-Planning",
      "help_website": [],
      "license": null,
      "tags": [
        "path-planning",
        "lidar",
        "sac",
        "robotics"
      ],
      "id": 246
    },
    {
      "name": "DiffFR",
      "one_line_profile": "Differentiable SPH-based Fluid-Rigid Coupling for Control",
      "detailed_description": "A differentiable physics simulation framework for fluid-rigid body coupling based on Smoothed Particle Hydrodynamics (SPH). It enables gradient-based optimization and control for complex physical systems involving fluids and rigid bodies.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "differentiable_physics",
        "fluid_simulation",
        "rigid_body_control"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/zhehaoli1999/DiffFR",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "differentiable-physics",
        "sph",
        "fluid-simulation",
        "control"
      ],
      "id": 247
    },
    {
      "name": "dm2gym",
      "one_line_profile": "Bridge library converting DeepMind Control Suite environments to OpenAI Gym API",
      "detailed_description": "A lightweight utility library that wraps DeepMind Control Suite physics simulation environments to expose the standard OpenAI Gym interface. This enables researchers to apply standard Gym-compatible Reinforcement Learning algorithms and baselines to physics-based control tasks defined in the DM Control Suite.",
      "domains": [
        "P4",
        "P4-04"
      ],
      "subtask_category": [
        "environment_integration",
        "simulation_interface"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/zuoxingdong/dm2gym",
      "help_website": [
        "https://github.com/zuoxingdong/dm2gym"
      ],
      "license": "MIT",
      "tags": [
        "reinforcement-learning",
        "deepmind-control-suite",
        "openai-gym",
        "physics-control",
        "wrapper"
      ],
      "id": 248
    }
  ]
}
{
  "generated_at": "2025-12-16T02:13:43.532801+08:00",
  "metadata": {
    "leaf_cluster": {
      "leaf_cluster_id": "AI2",
      "leaf_cluster_name": "科研表示学习与 embedding 生态",
      "domain": "AI Toolchain",
      "typical_objects": "embeddings/index",
      "task_chain": "表示→索引→检索→探针评测→更新",
      "tool_form": "表示库 + 向量检索 + 评测"
    },
    "unit": {
      "unit_id": "AI2-02",
      "unit_name": "多模态对齐与统一表示",
      "target_scale": "200–450",
      "coverage_tools": "alignment toolkits"
    },
    "search": {
      "target_candidates": 450,
      "queries": [
        "[GH] Transformers",
        "[GH] TorchMultimodal",
        "[GH] CLAP",
        "[GH] BLIP",
        "[GH] MMF",
        "[GH] Uni-Mol",
        "[GH] BioMedCLIP",
        "[GH] ImageBind",
        "[GH] LAVIS",
        "[GH] OpenCLIP",
        "[GH] multimodal alignment",
        "[GH] contrastive learning framework",
        "[GH] joint embedding",
        "[GH] cross-modal retrieval",
        "[GH] vision-language pretraining",
        "[GH] CLIP fine-tuning",
        "[GH] unified representation learning",
        "[GH] image-text alignment",
        "[GH] molecule-text alignment",
        "[GH] multimodal fusion",
        "[GH] audio-text alignment",
        "[GH] multimodal embedding",
        "[GH] zero-shot classification",
        "[GH] modality gap",
        "[WEB] multimodal alignment toolkit github",
        "[WEB] contrastive learning library github",
        "[WEB] vision language pretraining framework github",
        "[WEB] biomedical multimodal representation learning github",
        "[WEB] cross-modal retrieval tools github",
        "[WEB] unified embedding space library github"
      ],
      "total_candidates": 1247,
      "tool_candidates": 852,
      "final_tools": 146
    }
  },
  "tools": [
    {
      "name": "RzenEmbed",
      "one_line_profile": "Multimodal embedding model optimized for RAG and visual document understanding",
      "detailed_description": "A multimodal embedding model designed to prioritize Multimodal Retrieval-Augmented Generation (RAG). It achieves high performance on benchmarks like MMEB and VisDoc, making it suitable for scientific document retrieval and analysis tasks involving mixed text and visual data.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "embedding",
        "multimodal_rag",
        "document_retrieval"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/360CVGroup/RzenEmbed",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "multimodal-embedding",
        "rag",
        "visual-document-understanding"
      ],
      "id": 1
    },
    {
      "name": "Ovis",
      "one_line_profile": "Multimodal Large Language Model architecture for structural visual-textual alignment",
      "detailed_description": "A novel Multimodal Large Language Model (MLLM) architecture designed to structurally align visual and textual embeddings. It serves as a foundational tool for tasks requiring deep integration and understanding of visual and textual data, applicable to scientific image analysis and description.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "multimodal_alignment",
        "representation_learning",
        "image_captioning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/AIDC-AI/Ovis",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "mllm",
        "visual-embedding",
        "alignment"
      ],
      "id": 2
    },
    {
      "name": "CEBRA",
      "one_line_profile": "Learnable latent embeddings for joint behavioral and neural analysis",
      "detailed_description": "A library for learning latent embeddings that jointly represent behavioral and neural data. It uses contrastive learning to uncover consistent latent structures in complex neuroscientific datasets, facilitating the analysis of neural dynamics and behavior.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "neural_analysis",
        "embedding",
        "behavioral_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/AdaptiveMotorControlLab/CEBRA",
      "help_website": [
        "https://cebra.ai"
      ],
      "license": "NOASSERTION",
      "tags": [
        "neuroscience",
        "contrastive-learning",
        "latent-embedding"
      ],
      "id": 3
    },
    {
      "name": "coupledAE-patchseq",
      "one_line_profile": "Multimodal data alignment and cell type analysis using coupled autoencoders",
      "detailed_description": "A tool for aligning multimodal data, specifically designed for Patch-seq data analysis. It utilizes coupled autoencoders to integrate electrophysiological, transcriptomic, and morphological data, enabling robust cell type classification and analysis in neuroscience.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "data_alignment",
        "cell_type_analysis",
        "multimodal_integration"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/AllenInstitute/coupledAE-patchseq",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "neuroscience",
        "patch-seq",
        "autoencoder"
      ],
      "id": 4
    },
    {
      "name": "GPT-RAG Ingestion",
      "one_line_profile": "Data ingestion service for multimodal RAG pipelines",
      "detailed_description": "A service automating the processing of diverse documents (PDFs, images, spreadsheets) for Azure AI Search. It performs smart chunking and generates text and image embeddings, facilitating the creation of multimodal retrieval-augmented generation (RAG) systems for scientific knowledge management.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "data_ingestion",
        "embedding_generation",
        "multimodal_rag"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/Azure/gpt-rag-ingestion",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "data-processing",
        "multimodal"
      ],
      "id": 5
    },
    {
      "name": "UME-Search",
      "one_line_profile": "Universal Multimodal Embedding framework",
      "detailed_description": "A framework aiming to provide universal multimodal embeddings. It supports the generation and utilization of embeddings that align different modalities, suitable for building cross-modal search and retrieval systems in scientific domains.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "embedding",
        "multimodal_retrieval",
        "alignment"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/BIGBALLON/UME-Search",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "multimodal-embedding",
        "search",
        "retrieval"
      ],
      "id": 6
    },
    {
      "name": "biomed-multi-view",
      "one_line_profile": "Multi-view Molecular Embedding with Late Fusion (MMELON)",
      "detailed_description": "An implementation of the MMELON architecture that combines molecular representations from images, graphs, and text. It learns a joint embedding to predict chemical and biological properties, serving as a specialized tool for drug discovery and molecular analysis.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "molecular_embedding",
        "property_prediction",
        "multimodal_fusion"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/BiomedSciAI/biomed-multi-view",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "biomedicine",
        "molecular-embedding",
        "multi-view-learning"
      ],
      "id": 7
    },
    {
      "name": "RWKV-LM",
      "one_line_profile": "RNN-based Large Language Model with efficient representation learning",
      "detailed_description": "RWKV is an RNN architecture that offers Transformer-level performance with linear complexity. It provides efficient sequence modeling and embedding capabilities, which can be applied to scientific sequence data (e.g., genomics, protein sequences) and large-scale text analysis.",
      "domains": [
        "AI2"
      ],
      "subtask_category": [
        "sequence_modeling",
        "representation_learning",
        "embedding"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/BlinkDL/RWKV-LM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rnn",
        "llm",
        "efficient-inference"
      ],
      "id": 8
    },
    {
      "name": "cca_layer",
      "one_line_profile": "Canonical Correlation Analysis (CCA) Layer for PyTorch",
      "detailed_description": "A PyTorch implementation of the Canonical Correlation Analysis (CCA) layer. It is used for cross-modality retrieval and alignment tasks by maximizing the correlation between two views of data, a fundamental technique in multimodal scientific data analysis.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "alignment",
        "dimensionality_reduction",
        "correlation_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/CPJKU/cca_layer",
      "help_website": [],
      "license": null,
      "tags": [
        "cca",
        "pytorch",
        "cross-modal"
      ],
      "id": 9
    },
    {
      "name": "block.bootstrap.pytorch",
      "one_line_profile": "Multimodal fusion library based on BLOCK bilinear fusion",
      "detailed_description": "A library implementing the BLOCK (Bilinear Superdiagonal Fusion) method for multimodal fusion. It provides tools for combining representations from different modalities, applicable to various multimodal learning tasks in science.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "multimodal_fusion",
        "representation_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Cadene/block.bootstrap.pytorch",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "multimodal-fusion",
        "bilinear-pooling",
        "pytorch"
      ],
      "id": 10
    },
    {
      "name": "Clapeyron.jl",
      "one_line_profile": "Thermodynamic modeling framework for fluid properties and equations of state",
      "detailed_description": "Clapeyron.jl is a Julia library designed for the development and application of fluid-thermodynamic models. It supports a wide range of equations of state (SAFT, cubic, multi-parameter) to estimate fluid properties and phase equilibria, serving as a computational tool for chemical engineering and physics simulations.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "thermodynamic_modeling",
        "fluid_property_prediction",
        "equation_of_state"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/ClapeyronThermo/Clapeyron.jl",
      "help_website": [
        "https://clapeyronthermo.github.io/Clapeyron.jl/stable/"
      ],
      "license": "MIT",
      "tags": [
        "thermodynamics",
        "saft",
        "fluid-dynamics",
        "physics-simulation"
      ],
      "id": 11
    },
    {
      "name": "Prot2Text-V2",
      "one_line_profile": "Protein function prediction via multimodal contrastive alignment",
      "detailed_description": "Prot2Text-V2 is a deep learning framework for predicting protein functions by aligning protein sequences with textual descriptions. It utilizes multimodal contrastive learning to generate unified representations that capture both structural and semantic functional information.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "protein_function_prediction",
        "multimodal_alignment",
        "sequence_embedding"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ColinFX/Prot2Text-V2",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "protein-function",
        "multimodal-learning",
        "bioinformatics",
        "contrastive-learning"
      ],
      "id": 12
    },
    {
      "name": "MuSe-GNN",
      "one_line_profile": "Unified gene representation learning from multimodal biological graphs",
      "detailed_description": "MuSe-GNN is a Graph Neural Network framework designed to learn unified gene representations by integrating multimodal biological graph data. It addresses the challenge of fusing diverse biological signals to improve gene functional analysis and related bioinformatics tasks.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "gene_representation",
        "biological_graph_learning",
        "multimodal_integration"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/HelloWorldLTY/MuSe-GNN",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "genomics",
        "graph-neural-networks",
        "representation-learning",
        "bioinformatics"
      ],
      "id": 13
    },
    {
      "name": "BioMedKG",
      "one_line_profile": "Multimodal contrastive representation learning for biomedical knowledge graphs",
      "detailed_description": "BioMedKG implements a multimodal contrastive learning approach for Augmented Biomedical Knowledge Graphs. It aims to improve the representation of biomedical entities by leveraging both structural graph information and associated multimodal data, facilitating better knowledge discovery in biomedicine.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "biomedical_knowledge_graph",
        "representation_learning",
        "contrastive_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/HySonLab/BioMedKG",
      "help_website": [],
      "license": null,
      "tags": [
        "knowledge-graph",
        "biomedicine",
        "multimodal-learning",
        "graph-embedding"
      ],
      "id": 14
    },
    {
      "name": "DBT-Reconstruction",
      "one_line_profile": "Open-source reconstruction toolbox for digital breast tomosynthesis (DBT)",
      "detailed_description": "A MATLAB-based toolbox for reconstruction in digital breast tomosynthesis, providing algorithms for medical imaging analysis and processing.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "medical_imaging",
        "reconstruction",
        "tomography"
      ],
      "application_level": "solver",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/LAVI-USP/DBT-Reconstruction",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "medical-imaging",
        "tomosynthesis",
        "reconstruction"
      ],
      "id": 15
    },
    {
      "name": "pyDBT",
      "one_line_profile": "Python interface for the LAVI-USP DBT reconstruction toolbox",
      "detailed_description": "A Python extension package for the DBT (Digital Breast Tomosynthesis) reconstruction toolbox, enabling integration with Python-based scientific workflows.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "medical_imaging",
        "reconstruction",
        "python_binding"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/LAVI-USP/pyDBT",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "medical-imaging",
        "python-wrapper",
        "tomosynthesis"
      ],
      "id": 16
    },
    {
      "name": "UnityMol",
      "one_line_profile": "Molecular visualization and analysis software",
      "detailed_description": "A molecular viewer and analysis tool based on the Unity3D game engine, designed for high-quality visualization of biological macromolecules and interactions.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "molecular_visualization",
        "structural_biology"
      ],
      "application_level": "platform",
      "primary_language": "C#",
      "repo_url": "https://github.com/LBT-CNRS/UnityMol-Releases",
      "help_website": [
        "http://unitymol.sourceforge.net"
      ],
      "license": "NOASSERTION",
      "tags": [
        "molecular-visualization",
        "structural-biology",
        "unity3d"
      ],
      "id": 17
    },
    {
      "name": "VoCo",
      "one_line_profile": "Volume Contrastive Learning Framework for 3D Medical Image Analysis",
      "detailed_description": "A framework for self-supervised learning on 3D medical images using volume contrastive learning, designed to improve representation learning for medical analysis tasks.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "medical_imaging",
        "contrastive_learning",
        "representation_learning"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/Luffy03/VoCo",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "medical-imaging",
        "3d-vision",
        "self-supervised-learning"
      ],
      "id": 18
    },
    {
      "name": "MedKLIP",
      "one_line_profile": "Medical knowledge-enhanced language-image pre-training model for radiology",
      "detailed_description": "A domain-specific vision-language model pre-trained on medical data to handle unseen diseases in zero-shot classification and grounding tasks, leveraging medical knowledge to enhance representation learning.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "medical_image_analysis",
        "zero_shot_classification",
        "report_generation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/MediaBrain-SJTU/MedKLIP",
      "help_website": [],
      "license": null,
      "tags": [
        "radiology",
        "medical-vlm",
        "pre-training",
        "multimodal-alignment"
      ],
      "id": 19
    },
    {
      "name": "EEG-To-Text",
      "one_line_profile": "Open vocabulary electroencephalography-to-text decoding model",
      "detailed_description": "A framework for decoding EEG signals into text and performing zero-shot sentiment classification, bridging the gap between brain signals and natural language representations.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "eeg_decoding",
        "brain_computer_interface",
        "signal_processing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/MikeWangWZHL/EEG-To-Text",
      "help_website": [],
      "license": null,
      "tags": [
        "neuroscience",
        "eeg",
        "bci",
        "multimodal-decoding"
      ],
      "id": 20
    },
    {
      "name": "PLIP",
      "one_line_profile": "Vision and language foundation model for pathology image analysis",
      "detailed_description": "A large-scale pre-trained model for Pathology AI that extracts visual and language features from pathology images and text descriptions, enabling zero-shot classification and retrieval.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "pathology_image_analysis",
        "image_retrieval",
        "zero_shot_classification"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/PathologyFoundation/plip",
      "help_website": [],
      "license": null,
      "tags": [
        "pathology",
        "histopathology",
        "foundation-model",
        "medical-ai"
      ],
      "id": 21
    },
    {
      "name": "RLHF-V",
      "one_line_profile": "Fine-grained correctional human feedback framework for aligning multimodal large language models",
      "detailed_description": "A research framework and toolkit for aligning Multimodal Large Language Models (MLLMs) using fine-grained segment-level human corrections. It addresses the hallucination problem in MLLMs by collecting high-quality human feedback and performing behavior alignment, enhancing model trustworthiness.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "model_alignment",
        "human_feedback",
        "hallucination_reduction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/RLHF-V/RLHF-V",
      "help_website": [],
      "license": null,
      "tags": [
        "rlhf",
        "multimodal-alignment",
        "mllm",
        "hallucination"
      ],
      "id": 22
    },
    {
      "name": "OpenOmni",
      "one_line_profile": "Open-source omnimodal large language model with progressive alignment",
      "detailed_description": "An implementation of an omnimodal large language model capable of processing and synthesizing speech, text, and other modalities. It features a progressive multimodal alignment strategy and supports real-time self-aware emotional speech synthesis.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "multimodal_alignment",
        "speech_synthesis",
        "omnimodal_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/RainBowLuoCS/OpenOmni",
      "help_website": [],
      "license": null,
      "tags": [
        "omnimodal",
        "llm",
        "alignment",
        "speech-synthesis"
      ],
      "id": 23
    },
    {
      "name": "PCL-CLIP",
      "one_line_profile": "Prototypical contrastive learning for CLIP-based object re-identification",
      "detailed_description": "A toolkit for fine-tuning CLIP models specifically for the task of object re-identification (ReID). It utilizes prototypical contrastive learning to align visual representations for better retrieval performance.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "object_reid",
        "visual_alignment",
        "contrastive_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/RikoLi/PCL-CLIP",
      "help_website": [],
      "license": null,
      "tags": [
        "re-identification",
        "clip",
        "contrastive-learning",
        "fine-tuning"
      ],
      "id": 24
    },
    {
      "name": "Radar-RGB-Attentive-Multimodal-Object-Detection",
      "one_line_profile": "Multimodal fusion framework for object detection using Radar and RGB sensors",
      "detailed_description": "A deep learning framework for robust object detection in autonomous vehicles by fusing data from Radar sensors and RGB cameras. It employs attention mechanisms to align and integrate features from different modalities.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "sensor_fusion",
        "object_detection",
        "multimodal_alignment"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/RituYadav92/Radar-RGB-Attentive-Multimodal-Object-Detection",
      "help_website": [
        "https://ieeexplore.ieee.org/document/9191046"
      ],
      "license": "MIT",
      "tags": [
        "radar",
        "rgb",
        "sensor-fusion",
        "autonomous-driving"
      ],
      "id": 25
    },
    {
      "name": "ZeroshotSemanticSegmentation",
      "one_line_profile": "Zero-shot semantic segmentation via joint visual-semantic embeddings",
      "detailed_description": "A library for performing zero-shot semantic segmentation by leveraging a joint embedding space that aligns visual features with semantic word vectors, allowing the model to segment unseen classes.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "semantic_segmentation",
        "zero_shot_learning",
        "embedding_alignment"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/RohanDoshi2018/ZeroshotSemanticSegmentation",
      "help_website": [],
      "license": null,
      "tags": [
        "semantic-segmentation",
        "zero-shot",
        "visual-semantic-embedding"
      ],
      "id": 26
    },
    {
      "name": "PepBCL",
      "one_line_profile": "BERT-based contrastive learning for protein-peptide binding prediction",
      "detailed_description": "A bioinformatics tool that uses a BERT-based architecture and contrastive learning to predict protein-peptide binding residues from protein sequences. It aligns sequence representations to identify functional binding sites.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "protein_binding_prediction",
        "sequence_embedding",
        "bioinformatics"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Ruheng-W/PepBCL",
      "help_website": [],
      "license": null,
      "tags": [
        "protein-peptide",
        "binding-prediction",
        "contrastive-learning",
        "bert"
      ],
      "id": 27
    },
    {
      "name": "IMG-Multimodal-Diffusion-Alignment",
      "one_line_profile": "Implicit multimodal guidance for calibrating diffusion models",
      "detailed_description": "A framework for aligning diffusion models using implicit multimodal guidance (IMG). It aims to improve the generation quality and alignment of diffusion models by integrating multimodal signals during the generation process.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "diffusion_model",
        "multimodal_alignment",
        "generative_modeling"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/SHI-Labs/IMG-Multimodal-Diffusion-Alignment",
      "help_website": [],
      "license": null,
      "tags": [
        "diffusion-models",
        "multimodal-guidance",
        "alignment"
      ],
      "id": 28
    },
    {
      "name": "VisPer-LM",
      "one_line_profile": "Visual embedding distillation for enhancing multimodal LLMs",
      "detailed_description": "A toolkit for elevating visual perception in Multimodal Large Language Models (MLLMs) through visual embedding distillation. It aligns visual features more effectively with language models to improve performance on visual tasks.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "visual_perception",
        "distillation",
        "multimodal_llm"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/SHI-Labs/VisPer-LM",
      "help_website": [],
      "license": null,
      "tags": [
        "distillation",
        "mllm",
        "visual-embedding"
      ],
      "id": 29
    },
    {
      "name": "H-CLIP",
      "one_line_profile": "Hyperspherical parameter-efficient fine-tuning for CLIP",
      "detailed_description": "A method and codebase for parameter-efficient fine-tuning of CLIP models in hyperspherical space, specifically designed for open-vocabulary semantic segmentation tasks.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "semantic_segmentation",
        "fine_tuning",
        "embedding_alignment"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/SJTU-DeepVisionLab/H-CLIP",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "clip",
        "hyperspherical-space",
        "semantic-segmentation"
      ],
      "id": 30
    },
    {
      "name": "HyperCLIP",
      "one_line_profile": "Hyperbolic space fine-tuning for CLIP-based segmentation",
      "detailed_description": "A research tool for understanding and implementing fine-tuning of CLIP models in hyperbolic space, targeting open-vocabulary semantic segmentation. It explores non-Euclidean geometries for better visual-semantic alignment.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "semantic_segmentation",
        "hyperbolic_embedding",
        "fine_tuning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/SJTU-DeepVisionLab/HyperCLIP",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "hyperbolic-space",
        "clip",
        "semantic-segmentation"
      ],
      "id": 31
    },
    {
      "name": "MMFAIR",
      "one_line_profile": "Mixed models for statistical analysis in agriculture",
      "detailed_description": "An R package providing mixed model methodologies for agricultural data analysis. It facilitates scientific statistical modeling and inference for agricultural experiments.",
      "domains": [
        "AI2"
      ],
      "subtask_category": [
        "statistical_analysis",
        "agricultural_modeling"
      ],
      "application_level": "library",
      "primary_language": "HTML",
      "repo_url": "https://github.com/SchmidtPaul/MMFAIR",
      "help_website": [],
      "license": null,
      "tags": [
        "agriculture",
        "mixed-models",
        "statistics",
        "r"
      ],
      "id": 32
    },
    {
      "name": "llano",
      "one_line_profile": "LLM-based data annotation and information extraction tool",
      "detailed_description": "A tool that leverages Large Language Models (like ChatGPT) to serve as data annotators and zero-shot/few-shot information extractors, facilitating the creation of labeled datasets for scientific research.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "data_annotation",
        "information_extraction",
        "dataset_creation"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/SeanLee97/llano",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "data-annotation",
        "llm",
        "information-extraction"
      ],
      "id": 33
    },
    {
      "name": "ICon",
      "one_line_profile": "Unified framework for representation learning",
      "detailed_description": "A framework implementation for 'I-Con', designed to unify various representation learning paradigms. It provides tools for training and evaluating models that learn aligned representations across different domains.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "representation_learning",
        "model_training"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/ShadeAlsha/ICon",
      "help_website": [],
      "license": null,
      "tags": [
        "representation-learning",
        "framework",
        "iclr"
      ],
      "id": 34
    },
    {
      "name": "FIANet",
      "one_line_profile": "Fine-grained image-text alignment for remote sensing segmentation",
      "detailed_description": "A deep learning network for referring remote sensing image segmentation. It focuses on fine-grained alignment between textual descriptions and remote sensing imagery to accurately segment referred objects.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "remote_sensing",
        "image_segmentation",
        "image_text_alignment"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Shaosifan/FIANet",
      "help_website": [],
      "license": null,
      "tags": [
        "remote-sensing",
        "segmentation",
        "multimodal-alignment"
      ],
      "id": 35
    },
    {
      "name": "JointEmbedding",
      "one_line_profile": "Joint embedding of 3D shapes and images",
      "detailed_description": "A tool for learning joint embeddings of 3D shapes and 2D images via CNN image purification. It enables cross-modal retrieval and alignment between 3D geometry and visual data.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "3d_shape_analysis",
        "cross_modal_retrieval",
        "embedding_alignment"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ShapeNet/JointEmbedding",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "3d-shapes",
        "shapenet",
        "joint-embedding"
      ],
      "id": 36
    },
    {
      "name": "TALE",
      "one_line_profile": "Transformer-based protein function annotation with joint embedding",
      "detailed_description": "A bioinformatics tool for protein function annotation that uses a Transformer architecture to learn joint embeddings of protein features and functional labels.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "protein_function_prediction",
        "bioinformatics",
        "joint_embedding"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Shen-Lab/TALE",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "protein-annotation",
        "transformer",
        "bioinformatics"
      ],
      "id": 37
    },
    {
      "name": "CVPR2022-AURL",
      "one_line_profile": "Alignment-uniformity aware representation learning for video classification",
      "detailed_description": "An implementation of Alignment-Uniformity aware Representation Learning (AURL) for zero-shot video classification. It optimizes the geometric properties of embeddings to improve video-text alignment.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "video_classification",
        "zero_shot_learning",
        "representation_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ShipuLoveMili/CVPR2022-AURL",
      "help_website": [],
      "license": null,
      "tags": [
        "video-classification",
        "zero-shot",
        "alignment-uniformity"
      ],
      "id": 38
    },
    {
      "name": "FusAtNet",
      "one_line_profile": "Dual attention-based multimodal fusion for hyperspectral and LiDAR classification",
      "detailed_description": "A network for fusing hyperspectral and LiDAR data for land cover classification. It utilizes dual attention mechanisms to align and integrate spectral and spatial features from different sensors.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "remote_sensing",
        "hyperspectral_analysis",
        "lidar",
        "multimodal_fusion"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ShivamP1993/FusAtNet-Dual-Attention-based-SpectroSpatial-Multimodal-Fusion-Network-for-Hyperspectral-and-LiDAR-",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "hyperspectral",
        "lidar",
        "fusion",
        "classification"
      ],
      "id": 39
    },
    {
      "name": "HACAN",
      "one_line_profile": "Hybrid attention-driven cross-layer alignment for image-text retrieval",
      "detailed_description": "A network architecture designed for image-text retrieval that employs hybrid attention mechanisms to perform cross-layer alignment between visual and textual features.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "image_text_retrieval",
        "multimodal_alignment",
        "attention_mechanism"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ShuaiLyu0110/HACAN",
      "help_website": [],
      "license": null,
      "tags": [
        "image-text-retrieval",
        "alignment",
        "attention"
      ],
      "id": 40
    },
    {
      "name": "DuMMF",
      "one_line_profile": "Stochastic multi-person 3D motion forecasting",
      "detailed_description": "A tool for forecasting 3D motion of multiple persons using stochastic methods. It models the interactions and dynamics of human motion for predictive analysis.",
      "domains": [
        "AI2"
      ],
      "subtask_category": [
        "motion_forecasting",
        "3d_modeling",
        "dynamics_prediction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Sirui-Xu/DuMMF",
      "help_website": [],
      "license": null,
      "tags": [
        "motion-forecasting",
        "3d-motion",
        "stochastic-modeling"
      ],
      "id": 41
    },
    {
      "name": "MAKE",
      "one_line_profile": "Knowledge-enhanced vision-language pretraining for dermatological assessment",
      "detailed_description": "A medical AI tool for zero-shot dermatological assessment. It uses multi-aspect knowledge-enhanced vision-language pretraining to align medical images with clinical knowledge.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "medical_imaging",
        "dermatology",
        "vision_language_pretraining"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/SiyuanYan1/MAKE",
      "help_website": [],
      "license": null,
      "tags": [
        "dermatology",
        "medical-ai",
        "vision-language"
      ],
      "id": 42
    },
    {
      "name": "SupWMA",
      "one_line_profile": "Superficial white matter analysis with supervised contrastive learning",
      "detailed_description": "A deep learning framework for superficial white matter analysis using point-cloud-based networks and supervised contrastive learning. It ensures consistent tractography parcellation across populations and dMRI acquisitions.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "medical_imaging",
        "tractography",
        "white_matter_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/SlicerDMRI/SupWMA",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "dmri",
        "white-matter",
        "contrastive-learning",
        "medical-imaging"
      ],
      "id": 43
    },
    {
      "name": "CRKD",
      "one_line_profile": "Cross-modality knowledge distillation framework",
      "detailed_description": "A framework for cross-modality knowledge distillation (KD) designed to bridge the performance gap between different detector types (LC and CR). It facilitates knowledge transfer between modalities.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "knowledge_distillation",
        "object_detection",
        "model_compression"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Song-Jingyu/CRKD",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "knowledge-distillation",
        "cross-modality",
        "detection"
      ],
      "id": 44
    },
    {
      "name": "Stem-JEPA",
      "one_line_profile": "Joint embedding predictive architecture for musical stem compatibility",
      "detailed_description": "A scientific tool for analyzing musical audio by estimating the compatibility of stems using a Joint Embedding Predictive Architecture (JEPA). It applies representation learning to music information retrieval.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "audio_analysis",
        "music_information_retrieval",
        "representation_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/SonyCSLParis/Stem-JEPA",
      "help_website": [],
      "license": "LGPL-3.0",
      "tags": [
        "audio-processing",
        "jepa",
        "music-analysis"
      ],
      "id": 45
    },
    {
      "name": "Content-Aware-Node2Vec",
      "one_line_profile": "Joint embedding of biomedical ontologies and network structure",
      "detailed_description": "A tool for embedding biomedical ontologies by jointly encoding network structure and textual node descriptors. It is useful for bioinformatics tasks requiring semantic representation of biological entities.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "bioinformatics",
        "ontology_embedding",
        "graph_representation_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/SotirisKot/Content-Aware-Node2Vec",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "bioinformatics",
        "node2vec",
        "ontology",
        "embedding"
      ],
      "id": 46
    },
    {
      "name": "SimCLR",
      "one_line_profile": "Simple framework for contrastive learning of visual representations",
      "detailed_description": "A widely used PyTorch implementation of SimCLR, a framework for contrastive learning of visual representations. It serves as a foundational tool for self-supervised learning and alignment tasks.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "contrastive_learning",
        "representation_learning",
        "self_supervised_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Spijkervet/SimCLR",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "simclr",
        "contrastive-learning",
        "pytorch"
      ],
      "id": 47
    },
    {
      "name": "MiniRWKV-4",
      "one_line_profile": "Multimodal dialogue model implementation using RWKV and QFormer",
      "detailed_description": "An implementation of a multimodal image-text dialogue model combining Blip2, RWKV, and QFormer. It aims to achieve high-level cognitive capabilities with efficient resource usage.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "multimodal_dialogue",
        "model_implementation",
        "vision_language"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/StarRing2022/MiniRWKV-4",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "rwkv",
        "multimodal",
        "blip2",
        "llm"
      ],
      "id": 48
    },
    {
      "name": "PROMU",
      "one_line_profile": "Prompt-based alignment for multimodal entity and relation extraction",
      "detailed_description": "A toolkit for multimodal entity and relation extraction that utilizes prompt-based learning to align visual and textual information, enhancing information extraction performance.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "information_extraction",
        "relation_extraction",
        "multimodal_alignment"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/THU-BPM/PROMU",
      "help_website": [],
      "license": null,
      "tags": [
        "entity-extraction",
        "relation-extraction",
        "multimodal",
        "prompt-learning"
      ],
      "id": 49
    },
    {
      "name": "KECG",
      "one_line_profile": "Semi-supervised entity alignment via joint knowledge embedding",
      "detailed_description": "A tool for semi-supervised entity alignment in knowledge graphs. It employs a joint knowledge embedding model and a cross-graph model to align entities across different graphs.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "entity_alignment",
        "knowledge_graph",
        "embedding_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/THU-KEG/KECG",
      "help_website": [],
      "license": null,
      "tags": [
        "knowledge-graph",
        "entity-alignment",
        "semi-supervised"
      ],
      "id": 50
    },
    {
      "name": "ABC",
      "one_line_profile": "Control framework for multimodal embeddings using VLMs",
      "detailed_description": "A framework for achieving better control over multimodal embeddings by leveraging Vision-Language Models (VLMs). It provides methods to regulate and refine embedding spaces for specific tasks.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "embedding_control",
        "multimodal_alignment",
        "vlm"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/TIGER-AI-Lab/ABC",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "multimodal-embedding",
        "control",
        "vlm"
      ],
      "id": 51
    },
    {
      "name": "VLM2Vec",
      "one_line_profile": "Training vision-language models for massive multimodal embedding tasks",
      "detailed_description": "A library and training framework for converting Vision-Language Models into powerful embedding models capable of handling massive multimodal embedding tasks.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "multimodal_embedding",
        "representation_learning",
        "model_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/TIGER-AI-Lab/VLM2Vec",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "vlm",
        "embedding",
        "multimodal"
      ],
      "id": 52
    },
    {
      "name": "COSA",
      "one_line_profile": "Concatenated sample pretrained vision-language foundation model",
      "detailed_description": "Code and models for COSA, a vision-language foundation model pretrained with concatenated samples. It serves as a base for various multimodal downstream tasks.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "vision_language_model",
        "pretraining",
        "foundation_model"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/TXH-mercury/COSA",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "foundation-model",
        "vision-language",
        "pretraining"
      ],
      "id": 53
    },
    {
      "name": "HunyuanVideo-Foley",
      "one_line_profile": "Multimodal diffusion with representation alignment for foley audio generation",
      "detailed_description": "A generative AI tool for creating high-fidelity foley audio from video. It uses multimodal diffusion models with representation alignment to ensure the generated audio matches the video content.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "audio_generation",
        "multimodal_alignment",
        "diffusion_model"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Tencent-Hunyuan/HunyuanVideo-Foley",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "audio-generation",
        "diffusion",
        "multimodal"
      ],
      "id": 54
    },
    {
      "name": "FLM",
      "one_line_profile": "Free language modeling for efficient vision-language pretraining",
      "detailed_description": "A framework for accelerating vision-language pretraining using a 'Free Language Modeling' approach. It optimizes the pretraining process for better efficiency and performance.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "vision_language_pretraining",
        "model_optimization",
        "language_modeling"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/TencentARC/FLM",
      "help_website": [],
      "license": null,
      "tags": [
        "pretraining",
        "vision-language",
        "efficiency"
      ],
      "id": 55
    },
    {
      "name": "alignment_macaque-human",
      "one_line_profile": "Cross-species joint embedding alignment between human and macaque",
      "detailed_description": "A scientific analysis tool for performing cross-species alignment, specifically creating joint embeddings between human and macaque data (likely neuroimaging or genomic), facilitating comparative biological studies.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "cross_species_alignment",
        "comparative_biology",
        "joint_embedding"
      ],
      "application_level": "solver",
      "primary_language": null,
      "repo_url": "https://github.com/TingsterX/alignment_macaque-human",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "neuroscience",
        "alignment",
        "cross-species"
      ],
      "id": 56
    },
    {
      "name": "ARGF_multimodal_fusion",
      "one_line_profile": "Adversarial representation learning and graph fusion network",
      "detailed_description": "A toolkit for multimodal fusion and modality-to-modality translation using adversarial representation learning and graph fusion networks.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "multimodal_fusion",
        "representation_learning",
        "modality_translation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/TmacMai/ARGF_multimodal_fusion",
      "help_website": [],
      "license": null,
      "tags": [
        "fusion",
        "adversarial-learning",
        "graph-network"
      ],
      "id": 57
    },
    {
      "name": "X-modaler",
      "one_line_profile": "Versatile and high-performance codebase for cross-modal analytics",
      "detailed_description": "A modular and extensible framework for cross-modal research, supporting tasks like image captioning, visual question answering, and cross-modal retrieval. It provides unified interfaces for training and inference of various multimodal models.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "image_captioning",
        "vqa",
        "cross_modal_retrieval"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/YehLi/xmodaler",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "multimodal",
        "captioning",
        "vqa",
        "deep-learning"
      ],
      "id": 58
    },
    {
      "name": "torchange",
      "one_line_profile": "Unified Change Representation Learning Benchmark Library",
      "detailed_description": "A PyTorch-based library designed for change detection and representation learning in remote sensing. It provides a unified interface for datasets, models, and evaluation metrics to facilitate research in earth observation.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "change_detection",
        "remote_sensing",
        "representation_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Z-Zheng/pytorch-change-models",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "remote-sensing",
        "change-detection",
        "pytorch",
        "benchmark"
      ],
      "id": 59
    },
    {
      "name": "JDE (Towards-Realtime-MOT)",
      "one_line_profile": "Joint Detection and Embedding for fast multi-object tracking",
      "detailed_description": "A widely used solver for Multi-Object Tracking (MOT) that learns a shared representation for detection and embedding (re-identification). It is applicable to scientific tracking tasks involving biological agents or particles.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "multi_object_tracking",
        "embedding_learning",
        "object_detection"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Zhongdao/Towards-Realtime-MOT",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "tracking",
        "mot",
        "embedding",
        "computer-vision"
      ],
      "id": 60
    },
    {
      "name": "Text2Mol",
      "one_line_profile": "Cross-modal retrieval tool aligning natural language descriptions with molecular structures",
      "detailed_description": "Text2Mol is a cross-modal retrieval framework that aligns natural language queries with molecular representations. It enables semantic search for molecules using text descriptions, facilitating drug discovery and chemical property exploration by bridging the gap between chemical structures and textual data.",
      "domains": [
        "Chemistry",
        "Drug Discovery",
        "AI2-02"
      ],
      "subtask_category": [
        "molecular_retrieval",
        "cross_modal_alignment"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/cnedwards/text2mol",
      "help_website": [],
      "license": null,
      "tags": [
        "molecule-retrieval",
        "cross-modal",
        "chemistry",
        "nlp"
      ],
      "id": 61
    },
    {
      "name": "SensorLLM",
      "one_line_profile": "Framework aligning LLMs with motion sensor data for human activity recognition",
      "detailed_description": "SensorLLM is a framework designed to align Large Language Models (LLMs) with time-series motion sensor data (e.g., IMU). It facilitates Human Activity Recognition (HAR) by enabling LLMs to interpret and reason about physical sensor signals, bridging the gap between raw signal processing and semantic understanding.",
      "domains": [
        "Signal Processing",
        "Healthcare",
        "AI2-02"
      ],
      "subtask_category": [
        "sensor_alignment",
        "human_activity_recognition",
        "time_series_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/cruiseresearchgroup/SensorLLM",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "sensor-data",
        "llm-alignment",
        "har",
        "wearables"
      ],
      "id": 62
    },
    {
      "name": "Many Models Forecasting",
      "one_line_profile": "Solution accelerator for large-scale time series forecasting",
      "detailed_description": "A framework for training and managing thousands of time series forecasting models in parallel. It is designed for large-scale industrial and scientific forecasting tasks, automating the process of fitting independent models to distinct time series data subsets.",
      "domains": [
        "Data Science",
        "Physics",
        "AI2"
      ],
      "subtask_category": [
        "forecasting",
        "time_series_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/databricks-industry-solutions/many-model-forecasting",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "time-series",
        "forecasting",
        "parallel-computing"
      ],
      "id": 63
    },
    {
      "name": "Uni-Mol",
      "one_line_profile": "Universal 3D molecular representation learning framework",
      "detailed_description": "Uni-Mol is a universal 3D molecular representation learning framework that leverages transformer-based architectures to process molecular conformations. It supports various downstream tasks in drug discovery and chemistry, including molecular property prediction, conformation generation, and pocket-ligand binding prediction.",
      "domains": [
        "Chemistry",
        "Drug Discovery",
        "AI2-02"
      ],
      "subtask_category": [
        "molecular_representation",
        "property_prediction",
        "conformation_generation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/deepmodeling/Uni-Mol",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "molecular-dynamics",
        "3d-conformation",
        "pretraining",
        "chemistry"
      ],
      "id": 64
    },
    {
      "name": "ST-Align",
      "one_line_profile": "Multimodal alignment model for spatial transcriptomics",
      "detailed_description": "ST-Align is a multimodal foundation model designed to align histology images with gene expression data in spatial transcriptomics. It facilitates the integration of visual tissue information with molecular profiles, enabling enhanced analysis of tissue architecture and gene function.",
      "domains": [
        "Biology",
        "Genomics",
        "AI2-02"
      ],
      "subtask_category": [
        "spatial_transcriptomics",
        "image_gene_alignment",
        "multimodal_integration"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/dumbgoos/ST-Align",
      "help_website": [],
      "license": null,
      "tags": [
        "spatial-transcriptomics",
        "histology",
        "gene-expression",
        "multimodal"
      ],
      "id": 65
    },
    {
      "name": "mmflood",
      "one_line_profile": "Flood delineation tool using Sentinel-1 SAR imagery",
      "detailed_description": "A tool and dataset for delineating flood events from Sentinel-1 Synthetic Aperture Radar (SAR) imagery. It provides multimodal data processing capabilities to identify water bodies and flood extent, supporting earth observation and disaster management tasks.",
      "domains": [
        "Earth Science",
        "Remote Sensing",
        "AI2-02"
      ],
      "subtask_category": [
        "flood_detection",
        "image_segmentation",
        "remote_sensing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/edornd/mmflood",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "sar",
        "flood-mapping",
        "remote-sensing",
        "earth-observation"
      ],
      "id": 66
    },
    {
      "name": "NICE-EEG",
      "one_line_profile": "Contrastive learning framework for EEG-Image alignment",
      "detailed_description": "NICE-EEG is a framework for decoding visual stimuli from EEG signals using contrastive learning. It aligns brain activity (EEG) with visual data to improve the biological plausibility and accuracy of neural decoding models, useful in neuroscience and brain-computer interface research.",
      "domains": [
        "Neuroscience",
        "AI2-02"
      ],
      "subtask_category": [
        "eeg_decoding",
        "signal_alignment",
        "brain_computer_interface"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/eeyhsong/NICE-EEG",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "eeg",
        "neuroscience",
        "contrastive-learning",
        "brain-decoding"
      ],
      "id": 67
    },
    {
      "name": "RS-TransCLIP",
      "one_line_profile": "Vision-Language model framework for remote sensing scene classification",
      "detailed_description": "RS-TransCLIP is a framework that adapts Vision-Language Models (like CLIP) for Remote Sensing tasks. It specifically targets zero-shot scene classification by aligning satellite imagery with textual descriptions, overcoming the domain gap between natural images and remote sensing data.",
      "domains": [
        "Earth Science",
        "Remote Sensing",
        "AI2-02"
      ],
      "subtask_category": [
        "scene_classification",
        "zero_shot_learning",
        "remote_sensing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/elkhouryk/RS-TransCLIP",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "remote-sensing",
        "clip",
        "scene-classification",
        "vision-language"
      ],
      "id": 68
    },
    {
      "name": "clip-image-search",
      "one_line_profile": "Medical image search tool using fine-tuned CLIP models",
      "detailed_description": "A toolkit for fine-tuning OpenAI's CLIP model specifically for medical imaging datasets (e.g., ROCO). It enables semantic search and retrieval of medical images using natural language queries, facilitating data exploration in healthcare research.",
      "domains": [
        "Medicine",
        "Medical Imaging",
        "AI2-02"
      ],
      "subtask_category": [
        "medical_image_retrieval",
        "semantic_search",
        "fine_tuning"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/elsevierlabs-os/clip-image-search",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "medical-imaging",
        "clip",
        "image-search",
        "healthcare"
      ],
      "id": 69
    },
    {
      "name": "text-to-image-eval",
      "one_line_profile": "Evaluation toolkit for text-to-image and zero-shot classification models",
      "detailed_description": "A library designed to evaluate custom and HuggingFace text-to-image or zero-shot image classification models (e.g., CLIP, SigLIP). It provides metrics such as zero-shot accuracy, linear probe performance, image retrieval quality, and KNN accuracy to assess model alignment and representation quality.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/encord-team/text-to-image-eval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "evaluation",
        "clip",
        "zero-shot",
        "metrics"
      ],
      "id": 70
    },
    {
      "name": "DiT",
      "one_line_profile": "Scalable Diffusion Models with Transformers implementation",
      "detailed_description": "The official PyTorch implementation of Diffusion Transformers (DiT), a class of diffusion models that replace the traditional U-Net backbone with a Transformer. It serves as a foundational solver for generating high-fidelity images and learning scalable visual representations.",
      "domains": [
        "AI2",
        "AI3"
      ],
      "subtask_category": [
        "image_generation",
        "generative_modeling"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/DiT",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "diffusion-models",
        "transformers",
        "generative-ai"
      ],
      "id": 71
    },
    {
      "name": "ImageBind",
      "one_line_profile": "Unified embedding space across six modalities",
      "detailed_description": "A library for learning a joint embedding space across six different modalities: images, text, audio, depth, thermal, and IMU data. It enables holistic understanding and cross-modal retrieval/generation without requiring all modalities to be present during training.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "multimodal_embedding",
        "alignment"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/ImageBind",
      "help_website": [
        "https://facebookresearch.github.io/ImageBind/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "multimodal",
        "embeddings",
        "joint-embedding",
        "audio-visual"
      ],
      "id": 72
    },
    {
      "name": "MultiModalExplorer",
      "one_line_profile": "Interactive visualization tool for multi-modal embedding spaces",
      "detailed_description": "A tool designed to visualize and explore high-dimensional multi-modal embedding spaces. It supports scrolling, zooming, and searching via text, image, or audio queries to understand the layout and alignment of embeddings.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "visualization",
        "embedding_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/MultiModalExplorer",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "visualization",
        "embeddings",
        "dimensionality-reduction"
      ],
      "id": 73
    },
    {
      "name": "SONAR",
      "one_line_profile": "Multilingual and multimodal fixed-size sentence embedding suite",
      "detailed_description": "A complete suite for generating fixed-size sentence embeddings across many languages and modalities (speech and text). It includes encoders and decoders for creating unified representations suitable for translation and cross-lingual retrieval tasks.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "embedding_generation",
        "speech_translation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/SONAR",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "multilingual",
        "speech-to-text",
        "sentence-embeddings"
      ],
      "id": 74
    },
    {
      "name": "DETR",
      "one_line_profile": "End-to-End Object Detection with Transformers",
      "detailed_description": "A library implementing the DEtection TRansformer (DETR), which streamlines the object detection pipeline by viewing it as a direct set prediction problem. It serves as a key solver for visual recognition tasks using transformer architectures.",
      "domains": [
        "AI2",
        "AI3"
      ],
      "subtask_category": [
        "object_detection",
        "visual_recognition"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/detr",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "object-detection",
        "transformers",
        "computer-vision"
      ],
      "id": 75
    },
    {
      "name": "DINO",
      "one_line_profile": "Self-supervised vision transformer training library",
      "detailed_description": "A PyTorch implementation of the DINO (Self-distillation with no labels) method for self-supervised learning of visual representations. It enables the training of Vision Transformers (ViT) that capture semantic information without explicit supervision.",
      "domains": [
        "AI2"
      ],
      "subtask_category": [
        "self_supervised_learning",
        "representation_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/dino",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "self-supervised",
        "vision-transformer",
        "ssl"
      ],
      "id": 76
    },
    {
      "name": "I-JEPA",
      "one_line_profile": "Image-based Joint-Embedding Predictive Architecture",
      "detailed_description": "The official codebase for I-JEPA, a non-generative self-supervised learning method that learns highly semantic image representations by predicting the representations of various target blocks from a single context block.",
      "domains": [
        "AI2"
      ],
      "subtask_category": [
        "self_supervised_learning",
        "representation_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/ijepa",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "jepa",
        "self-supervised",
        "computer-vision"
      ],
      "id": 77
    },
    {
      "name": "MMF",
      "one_line_profile": "Modular framework for vision and language multimodal research",
      "detailed_description": "A comprehensive framework designed for multimodal AI research, specifically combining vision and language. It provides a modular architecture for training, evaluating, and deploying state-of-the-art multimodal models like VisualBERT and ViLBERT.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "multimodal_learning",
        "vqa"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/mmf",
      "help_website": [
        "https://mmf.sh"
      ],
      "license": "NOASSERTION",
      "tags": [
        "vision-language",
        "multimodal",
        "framework"
      ],
      "id": 78
    },
    {
      "name": "TorchMultimodal",
      "one_line_profile": "PyTorch library for training multimodal multi-task models",
      "detailed_description": "A PyTorch domain library for building, training, and evaluating multimodal models. It offers composable building blocks (modules, transforms, loss functions) to accelerate research in generative AI and cross-modal understanding.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "multimodal_learning",
        "model_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/multimodal",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "pytorch",
        "multimodal",
        "generative-ai"
      ],
      "id": 79
    },
    {
      "name": "VGGT",
      "one_line_profile": "Visual Geometry Grounded Transformer model",
      "detailed_description": "Implementation of the Visual Geometry Grounded Transformer (VGGT), a model designed to integrate geometric information into visual transformers for improved spatial understanding and representation learning.",
      "domains": [
        "AI2",
        "AI3"
      ],
      "subtask_category": [
        "visual_reasoning",
        "geometry_grounding"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/vggt",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "transformer",
        "geometry",
        "computer-vision"
      ],
      "id": 80
    },
    {
      "name": "xFormers",
      "one_line_profile": "Optimized and composable Transformers building blocks",
      "detailed_description": "A library providing highly optimized and hackable building blocks for Transformers. It focuses on memory-efficient attention mechanisms and other components essential for training large-scale representation models in science and AI.",
      "domains": [
        "AI2",
        "AI3"
      ],
      "subtask_category": [
        "model_optimization",
        "infrastructure"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/xformers",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "transformers",
        "optimization",
        "attention"
      ],
      "id": 81
    },
    {
      "name": "fusilli",
      "one_line_profile": "Deep-learning multi-modal data fusion pipelines",
      "detailed_description": "A Python package providing a collection of deep-learning methods for multi-modal data fusion. It handles the end-to-end pipeline from data loading and training to evaluation, supporting various fusion strategies for scientific data analysis.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "data_fusion",
        "multimodal_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/florencejt/fusilli",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "data-fusion",
        "multimodal",
        "pipeline"
      ],
      "id": 82
    },
    {
      "name": "MMT",
      "one_line_profile": "Multi-Modal Transformer for Video Retrieval",
      "detailed_description": "Implementation of the Multi-Modal Transformer (MMT) for video retrieval tasks. It encodes and fuses different video modalities (visual, audio, text) to enable effective cross-modal search and retrieval.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "video_retrieval",
        "multimodal_fusion"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/gabeur/mmt",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "video-retrieval",
        "transformer",
        "multimodal"
      ],
      "id": 83
    },
    {
      "name": "T5",
      "one_line_profile": "Text-to-Text Transfer Transformer library",
      "detailed_description": "The official library for the Text-to-Text Transfer Transformer (T5), a unified framework that converts all text-based language problems into a text-to-text format. It is widely used for transfer learning in NLP and scientific text processing.",
      "domains": [
        "AI2"
      ],
      "subtask_category": [
        "sequence_modeling",
        "transfer_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/google-research/text-to-text-transfer-transformer",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "transformer",
        "nlp",
        "transfer-learning"
      ],
      "id": 84
    },
    {
      "name": "MMTM",
      "one_line_profile": "Multimodal Transfer Module for CNN Fusion",
      "detailed_description": "A plug-and-play module for fusing information from different modalities in Convolutional Neural Networks (CNNs). It allows for slow modality fusion by recalibrating channel-wise features based on multimodal context.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "multimodal_fusion",
        "feature_extraction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/haamoon/mmtm",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "cnn",
        "fusion",
        "multimodal"
      ],
      "id": 85
    },
    {
      "name": "LLaMA-Factory",
      "one_line_profile": "Unified efficient fine-tuning platform for LLMs and VLMs",
      "detailed_description": "A comprehensive platform for the efficient fine-tuning of over 100 Large Language Models (LLMs) and Vision-Language Models (VLMs). It supports various training techniques (LoRA, QLoRA, etc.) and provides a unified interface for adapting models to scientific and general domains.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "model_finetuning",
        "alignment"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/hiyouga/LLaMA-Factory",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "vlm",
        "fine-tuning",
        "lora"
      ],
      "id": 86
    },
    {
      "name": "PEFT",
      "one_line_profile": "Library for parameter-efficient fine-tuning of large pre-trained models",
      "detailed_description": "PEFT (Parameter-Efficient Fine-Tuning) is a library that enables efficient adaptation of pre-trained language models to various downstream applications without fine-tuning all the model's parameters. It supports methods like LoRA, Prefix Tuning, and P-Tuning.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "model_training",
        "fine_tuning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/huggingface/peft",
      "help_website": [
        "https://huggingface.co/docs/peft"
      ],
      "license": "Apache-2.0",
      "tags": [
        "fine-tuning",
        "llm",
        "lora",
        "parameter-efficient"
      ],
      "id": 87
    },
    {
      "name": "PyTorch Image Models (timm)",
      "one_line_profile": "Collection of state-of-the-art PyTorch image models, scripts, and weights",
      "detailed_description": "A comprehensive library containing a wide variety of image classification models (ResNet, EfficientNet, ViT, etc.), optimizers, schedulers, and training scripts. It serves as a fundamental toolkit for computer vision research and application.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "feature_extraction",
        "image_classification"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/huggingface/pytorch-image-models",
      "help_website": [
        "https://huggingface.co/docs/timm"
      ],
      "license": "Apache-2.0",
      "tags": [
        "computer-vision",
        "pytorch",
        "backbones",
        "pretrained-models"
      ],
      "id": 88
    },
    {
      "name": "Sentence Transformers",
      "one_line_profile": "Framework for state-of-the-art text and image embeddings",
      "detailed_description": "A Python framework for state-of-the-art sentence, text, and image embeddings. It allows for easy computation of dense vector representations for sentences, paragraphs, and images, enabling tasks like semantic search, clustering, and retrieval.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "embedding_generation",
        "semantic_search"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/huggingface/sentence-transformers",
      "help_website": [
        "https://www.sbert.net/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "embeddings",
        "nlp",
        "semantic-search",
        "bert"
      ],
      "id": 89
    },
    {
      "name": "Tokenizers",
      "one_line_profile": "Fast and customizable text tokenization library",
      "detailed_description": "A high-performance tokenization library optimized for research and production. It provides implementations of today's most used tokenizers (BPE, WordPiece, Unigram) and is designed to be extremely fast and versatile.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "data_preprocessing",
        "tokenization"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/huggingface/tokenizers",
      "help_website": [
        "https://huggingface.co/docs/tokenizers"
      ],
      "license": "Apache-2.0",
      "tags": [
        "nlp",
        "preprocessing",
        "rust",
        "python-bindings"
      ],
      "id": 90
    },
    {
      "name": "Transformers",
      "one_line_profile": "State-of-the-art machine learning for Pytorch, TensorFlow, and JAX",
      "detailed_description": "A library providing thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio. It is the de facto standard for accessing and using transformer-based models in research and industry.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "model_inference",
        "model_training",
        "multimodal_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/huggingface/transformers",
      "help_website": [
        "https://huggingface.co/docs/transformers"
      ],
      "license": "Apache-2.0",
      "tags": [
        "nlp",
        "computer-vision",
        "audio",
        "multimodal",
        "deep-learning"
      ],
      "id": 91
    },
    {
      "name": "Transformers.js",
      "one_line_profile": "Run Transformers directly in the browser",
      "detailed_description": "A library that enables running state-of-the-art machine learning models directly in the web browser using ONNX Runtime. It allows for client-side inference of text, vision, and audio models without server dependencies.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "model_inference",
        "web_deployment"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/huggingface/transformers.js",
      "help_website": [
        "https://huggingface.co/docs/transformers.js"
      ],
      "license": "Apache-2.0",
      "tags": [
        "javascript",
        "web-assembly",
        "inference",
        "onnx"
      ],
      "id": 92
    },
    {
      "name": "TRL",
      "one_line_profile": "Transformer Reinforcement Learning library",
      "detailed_description": "A library for training transformer language models with reinforcement learning, including PPO (Proximal Policy Optimization). It integrates with the Hugging Face ecosystem to support RLHF (Reinforcement Learning from Human Feedback).",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "model_alignment",
        "reinforcement_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/huggingface/trl",
      "help_website": [
        "https://huggingface.co/docs/trl"
      ],
      "license": "Apache-2.0",
      "tags": [
        "rlhf",
        "ppo",
        "alignment",
        "transformers"
      ],
      "id": 93
    },
    {
      "name": "EVF-SAM",
      "one_line_profile": "Early Vision-Language Fusion for Text-Prompted Segment Anything Model",
      "detailed_description": "An implementation of EVF-SAM, a model that incorporates early vision-language fusion into the Segment Anything Model (SAM) framework for improved text-prompted segmentation. It serves as a solver for multimodal segmentation tasks.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "image_segmentation",
        "multimodal_fusion"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/hustvl/EVF-SAM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "segmentation",
        "sam",
        "vision-language",
        "multimodal"
      ],
      "id": 94
    },
    {
      "name": "IPEX-LLM",
      "one_line_profile": "LLM inference and fine-tuning acceleration on Intel hardware",
      "detailed_description": "A library to accelerate local LLM inference and fine-tuning on Intel XPU (CPUs, GPUs, NPUs). It integrates with popular frameworks like llama.cpp, HuggingFace, and LangChain to provide optimized performance for large language models on Intel architecture.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "inference_acceleration",
        "model_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/intel/ipex-llm",
      "help_website": [
        "https://ipex-llm.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "intel",
        "acceleration",
        "inference",
        "llm"
      ],
      "id": 95
    },
    {
      "name": "SupCon-Framework",
      "one_line_profile": "Framework for Supervised Contrastive Learning",
      "detailed_description": "A PyTorch implementation and framework for Supervised Contrastive Learning (SupCon), incorporating best practices and tricks like AMP (Automatic Mixed Precision), EMA (Exponential Moving Average), and SWA (Stochastic Weight Averaging) for robust representation learning.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "representation_learning",
        "contrastive_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ivanpanshin/SupCon-Framework",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "contrastive-learning",
        "pytorch",
        "representation-learning"
      ],
      "id": 96
    },
    {
      "name": "PyTorch Grad-CAM",
      "one_line_profile": "Explainability library for PyTorch computer vision models",
      "detailed_description": "A library providing advanced AI explainability methods for computer vision, including Grad-CAM, Score-CAM, and others. It supports various architectures like CNNs and Vision Transformers, helping to visualize and understand model predictions.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "visualization",
        "explainability"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/jacobgil/pytorch-grad-cam",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "explainability",
        "grad-cam",
        "visualization",
        "computer-vision"
      ],
      "id": 97
    },
    {
      "name": "Attention Is All You Need (PyTorch)",
      "one_line_profile": "Reference implementation of the Transformer model",
      "detailed_description": "A widely used PyTorch implementation of the original Transformer model from the paper 'Attention Is All You Need'. It serves as a standard reference and educational tool for understanding and building Transformer architectures.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "model_implementation",
        "sequence_modeling"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/jadore801120/attention-is-all-you-need-pytorch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "transformer",
        "attention",
        "nlp",
        "reference-implementation"
      ],
      "id": 98
    },
    {
      "name": "BertViz",
      "one_line_profile": "Interactive visualization tool for Transformer attention",
      "detailed_description": "A tool for visualizing attention in Transformer language models like BERT, GPT-2, and BART. It provides interactive views to explore how models attend to different parts of the input, aiding in interpretability and analysis.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "visualization",
        "interpretability"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/jessevig/bertviz",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "visualization",
        "attention",
        "nlp",
        "bert"
      ],
      "id": 99
    },
    {
      "name": "AgentChain",
      "one_line_profile": "Orchestration framework for chaining LLMs and tools",
      "detailed_description": "A library to chain together Large Language Models (LLMs) for reasoning and orchestrating multiple models to accomplish complex tasks. It facilitates the creation of agentic workflows.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "workflow_orchestration",
        "agent_design"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/jina-ai/agentchain",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "agents",
        "llm",
        "orchestration",
        "chaining"
      ],
      "id": 100
    },
    {
      "name": "Finetuner",
      "one_line_profile": "Task-oriented embedding tuning tool",
      "detailed_description": "A tool for fine-tuning embedding models (like BERT, CLIP) for specific tasks. It simplifies the process of adapting pre-trained embeddings to domain-specific data to improve retrieval and clustering performance.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "embedding_tuning",
        "model_adaptation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/jina-ai/finetuner",
      "help_website": [
        "https://finetuner.jina.ai/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "embeddings",
        "fine-tuning",
        "neural-search",
        "clip"
      ],
      "id": 101
    },
    {
      "name": "mesh-transformer-jax",
      "one_line_profile": "High-performance JAX library for training large-scale model-parallel transformers",
      "detailed_description": "A library designed for training large transformer models (like GPT-J) using model parallelism in JAX. It serves as critical infrastructure for training large-scale scientific foundation models.",
      "domains": [
        "AI2"
      ],
      "subtask_category": [
        "model_training",
        "distributed_computing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/kingoflolz/mesh-transformer-jax",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "jax",
        "transformer",
        "distributed-training",
        "hpc"
      ],
      "id": 102
    },
    {
      "name": "E5-V",
      "one_line_profile": "Universal multimodal embeddings framework using Multimodal Large Language Models",
      "detailed_description": "A framework for generating universal multimodal embeddings (E5-V) that bridges the gap between different modalities using Multimodal Large Language Models (MLLMs), applicable to scientific multimodal alignment tasks.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "embedding_generation",
        "multimodal_alignment"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/kongds/E5-V",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "embeddings",
        "multimodal",
        "mllm",
        "alignment"
      ],
      "id": 103
    },
    {
      "name": "HealNet",
      "one_line_profile": "Multimodal fusion model for heterogeneous biomedical data",
      "detailed_description": "A deep learning model specifically designed for fusing heterogeneous biomedical data modalities (e.g., pathology images and genomic data) to learn unified representations for medical analysis.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "multimodal_fusion",
        "biomedical_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/konst-int-i/healnet",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "biomedical",
        "multimodal-fusion",
        "pathology",
        "genomics"
      ],
      "id": 104
    },
    {
      "name": "HoneyBee",
      "one_line_profile": "Multimodal oncology data embedding framework for precision medicine",
      "detailed_description": "A framework for learning embeddings from multimodal oncology data to support precision medicine tasks like prognosis. Developed by a scientific lab (Rasool Lab).",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "biomedical_embedding",
        "oncology_prognosis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/lab-rasool/HoneyBee",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "oncology",
        "multimodal",
        "precision-medicine",
        "embedding"
      ],
      "id": 105
    },
    {
      "name": "LanceDB",
      "one_line_profile": "Embedded vector database for multimodal AI retrieval",
      "detailed_description": "A developer-friendly, serverless vector database designed for managing and retrieving multimodal embeddings, serving as critical infrastructure for AI representation learning ecosystems.",
      "domains": [
        "AI2"
      ],
      "subtask_category": [
        "vector_retrieval",
        "data_management"
      ],
      "application_level": "platform",
      "primary_language": "Rust",
      "repo_url": "https://github.com/lancedb/lancedb",
      "help_website": [
        "https://lancedb.github.io/lancedb/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "vector-database",
        "retrieval",
        "multimodal",
        "embeddings"
      ],
      "id": 106
    },
    {
      "name": "Lightly",
      "one_line_profile": "Self-supervised learning library for computer vision",
      "detailed_description": "A computer vision library for self-supervised learning (SSL). It helps in curating datasets and learning robust image embeddings without labels, widely used in scientific data analysis.",
      "domains": [
        "AI2"
      ],
      "subtask_category": [
        "self_supervised_learning",
        "data_curation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/lightly-ai/lightly",
      "help_website": [
        "https://docs.lightly.ai/"
      ],
      "license": "MIT",
      "tags": [
        "self-supervised-learning",
        "computer-vision",
        "embeddings",
        "data-curation"
      ],
      "id": 107
    },
    {
      "name": "contrastive-learner",
      "one_line_profile": "PyTorch wrapper for contrastive self-supervised learning",
      "detailed_description": "A library that provides a simple-to-use wrapper for implementing contrastive self-supervised learning algorithms (like SimCLR, MoCo) on any neural network, facilitating representation learning research.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "representation_learning",
        "model_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/lucidrains/contrastive-learner",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "contrastive-learning",
        "pytorch",
        "self-supervised-learning"
      ],
      "id": 108
    },
    {
      "name": "vit-pytorch",
      "one_line_profile": "Implementation of Vision Transformer (ViT) in PyTorch",
      "detailed_description": "A widely used library implementing Vision Transformers, serving as a foundational backbone for various scientific image analysis tasks (e.g., microscopy, pathology).",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "image_analysis",
        "model_backbone"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/lucidrains/vit-pytorch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "vision-transformer",
        "pytorch",
        "deep-learning"
      ],
      "id": 109
    },
    {
      "name": "x-transformers",
      "one_line_profile": "Concise and complete full-attention transformer implementation",
      "detailed_description": "A flexible Transformer library incorporating experimental features, often used as a building block for developing specialized scientific foundation models (e.g., in biology or chemistry).",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "model_architecture",
        "sequence_modeling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/lucidrains/x-transformers",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "transformer",
        "attention-mechanism",
        "pytorch"
      ],
      "id": 110
    },
    {
      "name": "PathomicFusion",
      "one_line_profile": "Multimodal fusion of histology and genomics for survival prediction",
      "detailed_description": "A deep learning framework for fusing histology images and genomic data to predict patient survival outcomes, serving as a solver for multimodal biomedical analysis.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "multimodal_fusion",
        "survival_analysis",
        "bioinformatics"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/mahmoodlab/PathomicFusion",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "histology",
        "genomics",
        "multimodal-learning"
      ],
      "id": 111
    },
    {
      "name": "mysteryann",
      "one_line_profile": "High-performance cross-modal vector retrieval engine",
      "detailed_description": "A highly optimized Approximate Nearest Neighbor (ANN) search engine designed for large-scale vector retrieval, suitable for scientific embedding indexing and search.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "vector_search",
        "indexing"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/matchyc/mysteryann",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ann",
        "vector-search",
        "retrieval"
      ],
      "id": 112
    },
    {
      "name": "gpt-fast",
      "one_line_profile": "Simple and efficient PyTorch-native transformer text generation",
      "detailed_description": "A lightweight and highly optimized library for transformer inference, enabling efficient execution of large language models which can be applied to scientific text generation or reasoning tasks.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "inference_optimization",
        "text_generation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/meta-pytorch/gpt-fast",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "inference",
        "transformer",
        "optimization"
      ],
      "id": 113
    },
    {
      "name": "infinity",
      "one_line_profile": "High-throughput serving engine for embeddings and reranking",
      "detailed_description": "A deployment tool for serving text-embeddings and multimodal models (CLIP, CLAP) with high throughput, essential for building scientific semantic search or retrieval systems.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "model_serving",
        "embedding_inference"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/michaelfeil/infinity",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "embedding-server",
        "inference",
        "clip"
      ],
      "id": 114
    },
    {
      "name": "BiomedCLIP_data_pipeline",
      "one_line_profile": "Data pipeline for BiomedCLIP training",
      "detailed_description": "A data processing pipeline designed to handle biomedical image-text pairs for training BiomedCLIP, facilitating scientific multimodal model development.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "data_processing",
        "biomedical_imaging"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/microsoft/BiomedCLIP_data_pipeline",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "biomedical",
        "clip",
        "data-pipeline"
      ],
      "id": 115
    },
    {
      "name": "CLAP",
      "one_line_profile": "Contrastive Language-Audio Pretraining model",
      "detailed_description": "Microsoft's implementation of CLAP, a model for learning audio concepts from natural language, applicable to scientific acoustic analysis and classification.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "audio_analysis",
        "multimodal_alignment"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/CLAP",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "audio",
        "contrastive-learning",
        "multimodal"
      ],
      "id": 116
    },
    {
      "name": "Data-Discovery-Toolkit",
      "one_line_profile": "Data discovery and manipulation toolset for unstructured data",
      "detailed_description": "A toolkit for discovering, analyzing, and manipulating unstructured data, aiding in the curation and preparation of scientific datasets.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "data_curation",
        "data_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/microsoft/Data-Discovery-Toolkit",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "data-discovery",
        "unstructured-data",
        "toolkit"
      ],
      "id": 117
    },
    {
      "name": "Swin-Transformer",
      "one_line_profile": "Hierarchical Vision Transformer using Shifted Windows",
      "detailed_description": "Official implementation of Swin Transformer, a general-purpose backbone for computer vision that is widely used in scientific imaging tasks (e.g., medical image segmentation).",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "image_analysis",
        "model_backbone"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/Swin-Transformer",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "vision-transformer",
        "swin",
        "backbone"
      ],
      "id": 118
    },
    {
      "name": "UniVL",
      "one_line_profile": "Unified Video and Language Pre-Training Model",
      "detailed_description": "A unified model for video and language pre-training, enabling multimodal understanding and generation tasks which can be applied to scientific video data analysis.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "video_analysis",
        "multimodal_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/UniVL",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "video-language",
        "pretraining",
        "multimodal"
      ],
      "id": 119
    },
    {
      "name": "VITRA",
      "one_line_profile": "Vision-Language-Action Model Pretraining for Robotic Manipulation",
      "detailed_description": "A framework for pretraining vision-language-action models, specifically designed for robotic manipulation tasks, bridging multimodal AI with embodied scientific experimentation.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "robotics",
        "embodied_ai",
        "multimodal_pretraining"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/VITRA",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "robotics",
        "vla",
        "multimodal"
      ],
      "id": 120
    },
    {
      "name": "Presidio",
      "one_line_profile": "PII detection and anonymization framework",
      "detailed_description": "A framework for detecting and anonymizing sensitive data (PII) in text and images, critical for processing medical or social science datasets while maintaining privacy compliance.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "data_anonymization",
        "privacy_protection"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/presidio",
      "help_website": [
        "https://microsoft.github.io/presidio/"
      ],
      "license": "MIT",
      "tags": [
        "pii",
        "anonymization",
        "data-privacy"
      ],
      "id": 121
    },
    {
      "name": "UniTS",
      "one_line_profile": "Unified multi-task foundation model for time series analysis",
      "detailed_description": "A unified multi-task time series model capable of handling classification, forecasting, anomaly detection, and imputation within a single framework. It serves as a foundational tool for processing temporal scientific data.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "time_series_analysis",
        "forecasting",
        "anomaly_detection"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/mims-harvard/UniTS",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "time-series",
        "foundation-model",
        "multi-task-learning",
        "scientific-data"
      ],
      "id": 122
    },
    {
      "name": "ms-swift",
      "one_line_profile": "Comprehensive training and fine-tuning framework for LLMs and MLLMs",
      "detailed_description": "A scalable and efficient library for fine-tuning (PEFT/SFT/DPO) large language models and multimodal models. It supports a vast array of models and is essential for adapting foundation models to scientific domains.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "model_training",
        "fine_tuning",
        "multimodal_alignment"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/modelscope/ms-swift",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "peft",
        "fine-tuning",
        "multimodal",
        "modelscope"
      ],
      "id": 123
    },
    {
      "name": "ULTS",
      "one_line_profile": "Unified library for unsupervised representation learning on time series",
      "detailed_description": "A standardized library providing various unsupervised representation learning approaches for time series data, facilitating the extraction of features from temporal scientific datasets without labeled data.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "representation_learning",
        "time_series_analysis",
        "unsupervised_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/mqwfrog/ULTS",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "time-series",
        "representation-learning",
        "unsupervised-learning"
      ],
      "id": 124
    },
    {
      "name": "LocalAI",
      "one_line_profile": "Self-hosted, local-first inference platform for AI models",
      "detailed_description": "A drop-in replacement for OpenAI API that runs LLMs, image generation, and audio models locally on consumer-grade hardware. Critical for processing sensitive scientific data without relying on external cloud services.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "inference_serving",
        "model_deployment",
        "privacy_preserving_computation"
      ],
      "application_level": "platform",
      "primary_language": "Go",
      "repo_url": "https://github.com/mudler/LocalAI",
      "help_website": [
        "https://localai.io"
      ],
      "license": "MIT",
      "tags": [
        "inference",
        "local-ai",
        "llm",
        "privacy"
      ],
      "id": 125
    },
    {
      "name": "txtai",
      "one_line_profile": "All-in-one embeddings database and semantic search framework",
      "detailed_description": "A framework that builds semantic search applications and LLM workflows using embeddings. It integrates vector indexes, graph networks, and language models, enabling efficient retrieval and analysis of scientific literature and data.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "semantic_search",
        "embedding_generation",
        "rag"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/neuml/txtai",
      "help_website": [
        "https://neuml.github.io/txtai"
      ],
      "license": "Apache-2.0",
      "tags": [
        "embeddings",
        "semantic-search",
        "nlp",
        "rag"
      ],
      "id": 126
    },
    {
      "name": "OMML",
      "one_line_profile": "Multi-modal learning toolkit based on PaddlePaddle and PyTorch",
      "detailed_description": "A toolkit supporting multiple multi-modal applications such as classification, cross-modal retrieval, and image captioning. It provides a unified interface for developing and deploying multi-modal models in scientific contexts.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "multimodal_learning",
        "cross_modal_retrieval",
        "image_captioning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/njustkmg/OMML",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "multimodal",
        "toolkit",
        "paddlepaddle",
        "pytorch"
      ],
      "id": 127
    },
    {
      "name": "VL-CheckList",
      "one_line_profile": "Evaluation framework for Vision-Language Pretraining Models",
      "detailed_description": "A tool for evaluating the capabilities of Vision-Language Pretraining (VLP) models regarding objects, attributes, and relations. It aids in the quality control and benchmarking of multimodal models used in research.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking",
        "quality_control"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/om-ai-lab/VL-CheckList",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "evaluation",
        "vision-language",
        "benchmark"
      ],
      "id": 128
    },
    {
      "name": "mmfewshot",
      "one_line_profile": "OpenMMLab Few-Shot Learning Toolbox",
      "detailed_description": "A comprehensive toolbox for few-shot learning, supporting various methods for classification and detection. It is particularly useful in scientific domains where labeled data is scarce.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "few_shot_learning",
        "image_classification",
        "object_detection"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/open-mmlab/mmfewshot",
      "help_website": [
        "https://mmfewshot.readthedocs.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "few-shot-learning",
        "computer-vision",
        "openmmlab"
      ],
      "id": 129
    },
    {
      "name": "mmflow",
      "one_line_profile": "OpenMMLab Optical Flow Toolbox",
      "detailed_description": "A toolbox for optical flow estimation, which is a fundamental technique for analyzing motion in video data. It has applications in fluid dynamics, microscopy, and other physical sciences.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "optical_flow_estimation",
        "motion_analysis",
        "video_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/open-mmlab/mmflow",
      "help_website": [
        "https://mmflow.readthedocs.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "optical-flow",
        "computer-vision",
        "openmmlab",
        "motion-estimation"
      ],
      "id": 130
    },
    {
      "name": "OpenVINO",
      "one_line_profile": "Toolkit for optimizing and deploying AI inference",
      "detailed_description": "An open-source toolkit for optimizing and deploying deep learning models on Intel hardware. It accelerates AI inference, which is critical for high-throughput scientific data analysis and real-time applications.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "inference_optimization",
        "model_deployment",
        "acceleration"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/openvinotoolkit/openvino",
      "help_website": [
        "https://docs.openvino.ai"
      ],
      "license": "Apache-2.0",
      "tags": [
        "inference",
        "optimization",
        "deployment",
        "intel"
      ],
      "id": 131
    },
    {
      "name": "Magnitude",
      "one_line_profile": "Fast, efficient universal vector embedding utility package",
      "detailed_description": "A Python library and command-line utility for handling vector embeddings (Word2Vec, GloVe, FastText, etc.) efficiently. It supports lazy-loading, fast lookups, and format conversion, serving as a foundational tool for representation learning and NLP tasks in scientific workflows.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "embedding_management",
        "data_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/plasticityai/magnitude",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "embeddings",
        "nlp",
        "vector-search",
        "dimensionality-reduction"
      ],
      "id": 132
    },
    {
      "name": "FastEmbed",
      "one_line_profile": "Lightweight and fast Python library for embedding generation",
      "detailed_description": "A library built for speed and accuracy in generating vector embeddings from text and images. It supports quantized models and ONNX runtime, designed for efficient integration with vector databases and retrieval systems in scientific data pipelines.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "embedding_generation",
        "inference"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/qdrant/fastembed",
      "help_website": [
        "https://qdrant.github.io/fastembed/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "embeddings",
        "onnx",
        "quantization",
        "retrieval",
        "multimodal"
      ],
      "id": 133
    },
    {
      "name": "Segmentation Models PyTorch",
      "one_line_profile": "Comprehensive library for image segmentation models",
      "detailed_description": "A Python library providing a wide range of pretrained segmentation architectures (UNet, FPN, PSPNet, etc.) with various backbones. It is widely used in scientific imaging tasks such as medical image analysis, remote sensing, and microscopy.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "image_segmentation",
        "feature_extraction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/qubvel-org/segmentation_models.pytorch",
      "help_website": [
        "https://smp.readthedocs.io/en/latest/"
      ],
      "license": "MIT",
      "tags": [
        "segmentation",
        "computer-vision",
        "pytorch",
        "medical-imaging",
        "remote-sensing"
      ],
      "id": 134
    },
    {
      "name": "pMMF",
      "one_line_profile": "Multiresolution Matrix Factorization Library",
      "detailed_description": "A C++ library for performing Multiresolution Matrix Factorization (MMF), a mathematical technique for analyzing multi-scale structure in complex matrices. It is applicable to scientific data analysis in fields like computational physics and graph learning.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "matrix_factorization",
        "data_analysis"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/risi-kondor/pMMF",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "matrix-factorization",
        "multiresolution-analysis",
        "linear-algebra",
        "scientific-computing"
      ],
      "id": 135
    },
    {
      "name": "CLIP Retrieval",
      "one_line_profile": "Toolkit for computing CLIP embeddings and building retrieval systems",
      "detailed_description": "A set of tools to efficiently compute CLIP embeddings for large-scale image-text datasets and build low-latency retrieval systems. It is a key component in multimodal dataset curation and analysis workflows (e.g., LAION).",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "embedding_generation",
        "retrieval",
        "dataset_curation"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/rom1504/clip-retrieval",
      "help_website": [
        "https://rom1504.github.io/clip-retrieval/"
      ],
      "license": "MIT",
      "tags": [
        "clip",
        "retrieval",
        "multimodal",
        "embeddings",
        "dataset-processing"
      ],
      "id": 136
    },
    {
      "name": "BLIP",
      "one_line_profile": "Bootstrapping Language-Image Pre-training framework for unified vision-language understanding and generation",
      "detailed_description": "A PyTorch implementation of BLIP, a framework for pre-training multimodal models that can perform various vision-language tasks including image captioning, visual question answering, and image-text retrieval. It is widely used as a foundation model in scientific multimodal analysis.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "multimodal_alignment",
        "image_captioning",
        "vqa"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/salesforce/BLIP",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "vision-language",
        "pretraining",
        "multimodal",
        "transformer"
      ],
      "id": 137
    },
    {
      "name": "LAVIS",
      "one_line_profile": "A one-stop library for language-vision intelligence and multimodal representation learning",
      "detailed_description": "LAVIS is a comprehensive library for training and evaluating language-vision models. It supports a wide range of tasks including retrieval, captioning, VQA, and classification, and provides access to state-of-the-art models like BLIP, ALBEF, and CLIP, facilitating multimodal research in scientific domains.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "multimodal_alignment",
        "representation_learning",
        "inference"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/salesforce/LAVIS",
      "help_website": [
        "https://salesforce.github.io/LAVIS/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "multimodal",
        "vision-language",
        "deep-learning",
        "library"
      ],
      "id": 138
    },
    {
      "name": "PubMedCLIP",
      "one_line_profile": "CLIP model fine-tuned on medical image-caption pairs from PubMed",
      "detailed_description": "A specialized multimodal model adapted for the medical domain by fine-tuning CLIP on the ROCO dataset (Radiology Objects in COntext). It serves as a foundational tool for medical image-text alignment and retrieval tasks.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "medical_imaging",
        "multimodal_alignment",
        "domain_adaptation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/sarahESL/PubMedCLIP",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "medical-ai",
        "clip",
        "fine-tuning",
        "radiology"
      ],
      "id": 139
    },
    {
      "name": "Lbl2Vec",
      "one_line_profile": "Tool for learning jointly embedded label, document, and word vectors",
      "detailed_description": "Lbl2Vec is a library for unsupervised document classification and retrieval. It creates joint embeddings of labels, documents, and words, allowing for semantic alignment and topic modeling, which is useful for analyzing scientific literature and text data.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "text_embedding",
        "topic_modeling",
        "semantic_alignment"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/sebischair/Lbl2Vec",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "nlp",
        "embedding",
        "doc2vec",
        "classification"
      ],
      "id": 140
    },
    {
      "name": "SpeechBrain",
      "one_line_profile": "A PyTorch-based speech toolkit for multimodal audio processing",
      "detailed_description": "SpeechBrain is an open-source and all-in-one speech toolkit. It supports various tasks including speech recognition, speaker recognition, and multimodal processing (audio+text), serving as a robust platform for scientific research in acoustics and linguistics.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "audio_processing",
        "speech_recognition",
        "multimodal_alignment"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/speechbrain/speechbrain",
      "help_website": [
        "https://speechbrain.github.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "speech",
        "audio",
        "pytorch",
        "toolkit"
      ],
      "id": 141
    },
    {
      "name": "multi_token",
      "one_line_profile": "Library to embed arbitrary modalities into large language models",
      "detailed_description": "A library designed to facilitate the integration of various data modalities (images, audio, documents) into Large Language Models (LLMs) by converting them into token embeddings, enabling multimodal scientific analysis and agent construction.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "multimodal_embedding",
        "llm_integration",
        "representation_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/sshh12/multi_token",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "multimodal",
        "llm",
        "embedding",
        "tokens"
      ],
      "id": 142
    },
    {
      "name": "pyMMF",
      "one_line_profile": "Numerical solver for propagation mode profiles in multimode optical fibers",
      "detailed_description": "A Python module designed to calculate numerical propagation mode profiles and propagation constants for multimode fibers with arbitrary radial index profiles. It solves the scalar Helmholtz equation or the vectorial eigenvalue problem, supporting scientific simulation in optics and photonics.",
      "domains": [
        "AI2",
        "AI2-02"
      ],
      "subtask_category": [
        "simulation",
        "physics_modeling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/wavefrontshaping/pyMMF",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "optics",
        "fiber-modes",
        "physics-simulation",
        "wave-propagation"
      ],
      "id": 143
    },
    {
      "name": "BenchX",
      "one_line_profile": "Unified benchmark framework for medical vision-language pretraining on Chest X-Rays",
      "detailed_description": "A comprehensive benchmark framework designed to facilitate the development and evaluation of medical vision-language pretraining models, specifically focused on Chest X-Ray analysis. It provides unified data loading, evaluation metrics, and baseline implementations.",
      "domains": [
        "AI2",
        "AI2-02",
        "Medical Imaging"
      ],
      "subtask_category": [
        "benchmarking",
        "model_evaluation",
        "medical_image_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/yangzhou12/BenchX",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "medical-imaging",
        "vision-language",
        "chest-x-ray",
        "benchmark"
      ],
      "id": 144
    },
    {
      "name": "conST",
      "one_line_profile": "Interpretable multi-modal contrastive learning framework for spatial transcriptomics",
      "detailed_description": "A deep learning framework designed for spatial transcriptomics analysis. It utilizes multi-modal contrastive learning to integrate gene expression, spatial information, and histology images to learn effective representations for downstream tasks like clustering and visualization.",
      "domains": [
        "AI2",
        "AI2-02",
        "Bioinformatics"
      ],
      "subtask_category": [
        "spatial_transcriptomics",
        "representation_learning",
        "clustering"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ys-zong/conST",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "spatial-transcriptomics",
        "bioinformatics",
        "contrastive-learning",
        "multi-modal"
      ],
      "id": 145
    },
    {
      "name": "M3R-CR",
      "one_line_profile": "Multimodal data fusion baseline for cloud removal in satellite imagery",
      "detailed_description": "A deep learning baseline and benchmark for removing cloud cover from high-resolution satellite imagery using multimodal and multiresolution data fusion. It addresses a key preprocessing step in Earth observation and remote sensing analysis.",
      "domains": [
        "AI2",
        "AI2-02",
        "Earth Science"
      ],
      "subtask_category": [
        "image_restoration",
        "cloud_removal",
        "remote_sensing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/zhu-xlab/M3R-CR",
      "help_website": [],
      "license": null,
      "tags": [
        "remote-sensing",
        "cloud-removal",
        "satellite-imagery",
        "data-fusion"
      ],
      "id": 146
    }
  ]
}
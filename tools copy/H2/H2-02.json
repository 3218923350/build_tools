{
  "generated_at": "2025-12-16T09:19:03.149590+08:00",
  "metadata": {
    "leaf_cluster": {
      "leaf_cluster_id": "H2",
      "leaf_cluster_name": "数字健康-临床文本与多模态生态",
      "domain": "Digital Health",
      "typical_objects": "EHR/text+image",
      "task_chain": "抽取→标准化→检索→问答→评测",
      "tool_form": "NLP/检索 + 合规/脱敏"
    },
    "unit": {
      "unit_id": "H2-02",
      "unit_name": "医疗LLM/微调/对齐",
      "target_scale": "200–450",
      "coverage_tools": "medical LLM toolkits"
    },
    "search": {
      "target_candidates": 450,
      "queries": [
        "[GH] BenTsao",
        "[GH] DoctorGLM",
        "[GH] BioMistral",
        "[GH] Meditron",
        "[GH] ClinicalGPT",
        "[GH] BioMedLM",
        "[GH] MedAlpaca",
        "[GH] PMC-LLaMA",
        "[GH] ChatDoctor",
        "[GH] HuatuoGPT",
        "[GH] medical llm",
        "[GH] clinical llm",
        "[GH] biomedical llm",
        "[GH] medical fine-tuning",
        "[GH] clinical instruction tuning",
        "[GH] medical dialogue system",
        "[GH] ehr language model",
        "[GH] healthcare llm",
        "[GH] medical rlhf",
        "[GH] medical question answering",
        "[GH] chinese medical llm",
        "[GH] medical nlp toolkit",
        "[GH] medical benchmark",
        "[WEB] open source medical llm github",
        "[WEB] clinical large language model fine-tuning github",
        "[WEB] medical instruction tuning dataset github",
        "[WEB] best medical llm toolkit github",
        "[WEB] healthcare llm evaluation benchmark github"
      ],
      "total_candidates": 679,
      "tool_candidates": 363,
      "final_tools": 131
    }
  },
  "tools": [
    {
      "name": "HBAM (HHH)",
      "one_line_profile": "Hierarchical Bi-directional Word Attention Model for medical question answering",
      "detailed_description": "An implementation of the Hierarchical Bi-directional Word Attention Model (HBAM) designed for online medical question answering systems. It serves as a solver for processing medical queries and retrieving relevant answers.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "medical_qa",
        "natural_language_processing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/14H034160212/HHH-An-Online-Question-Answering-System-for-Medical-Questions",
      "help_website": [],
      "license": null,
      "tags": [
        "medical-qa",
        "attention-model",
        "nlp"
      ],
      "id": 1
    },
    {
      "name": "MedResearcher-R1",
      "one_line_profile": "Deep research agent for medical scenarios with knowledge-informed trajectory synthesis",
      "detailed_description": "A deep research agent designed for medical scenarios that utilizes a knowledge-informed trajectory synthesis framework to conduct comprehensive medical research and reasoning tasks.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "medical_research_agent",
        "clinical_reasoning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/AQ-MedAI/MedResearcher-R1",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "agent",
        "medical-research",
        "llm"
      ],
      "id": 2
    },
    {
      "name": "Med-VQA",
      "one_line_profile": "Medical Visual Question Answering via Conditional Reasoning",
      "detailed_description": "A PyTorch implementation of a Medical Visual Question Answering (VQA) model that utilizes conditional reasoning to answer questions based on medical images.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "visual_question_answering",
        "medical_imaging"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Awenbocc/med-vqa",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "vqa",
        "medical-imaging",
        "deep-learning"
      ],
      "id": 3
    },
    {
      "name": "Me-LLaMA",
      "one_line_profile": "Medical Large Language Model family (13/70B)",
      "detailed_description": "A family of medical large language models (13B and 70B parameters) fine-tuned for various medical tasks, providing state-of-the-art performance in medical reasoning and question answering.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "medical_llm",
        "clinical_reasoning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/BIDS-Xu-Lab/Me-LLaMA",
      "help_website": [],
      "license": null,
      "tags": [
        "llm",
        "medical-nlp",
        "fine-tuning"
      ],
      "id": 4
    },
    {
      "name": "QiZhenGPT",
      "one_line_profile": "Open Source Chinese Medical Large Language Model",
      "detailed_description": "An open-source Chinese medical Large Language Model designed to assist in medical dialogue, diagnosis support, and health consultation.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "medical_llm",
        "medical_dialogue"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/CMKRG/QiZhenGPT",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "chinese-medical-llm",
        "nlp",
        "healthcare"
      ],
      "id": 5
    },
    {
      "name": "OpenGPT",
      "one_line_profile": "Framework for training conversational domain expert LLMs",
      "detailed_description": "A framework for creating grounded instruction-based datasets and training conversational domain expert Large Language Models (LLMs), specifically tailored for healthcare applications.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "llm_training",
        "dataset_generation"
      ],
      "application_level": "framework",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/CogStack/OpenGPT",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "instruction-tuning",
        "healthcare"
      ],
      "id": 6
    },
    {
      "name": "AMEGA-benchmark",
      "one_line_profile": "Autonomous Medical Evaluation for Guideline Adherence of LLMs",
      "detailed_description": "A benchmarking tool for evaluating the adherence of Large Language Models to medical guidelines, providing an autonomous evaluation framework for medical AI systems.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "guideline_adherence"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/DATEXIS/AMEGA-benchmark",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "benchmark",
        "evaluation",
        "medical-guidelines"
      ],
      "id": 7
    },
    {
      "name": "Taiyi-LLM",
      "one_line_profile": "Bilingual (Chinese and English) Biomedical Large Language Model",
      "detailed_description": "Taiyi is a bilingual (Chinese and English) Large Language Model fine-tuned for diverse biomedical tasks, including information extraction and question answering in the medical domain.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "medical_llm",
        "biomedical_nlp"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/DUTIR-BioNLP/Taiyi-LLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "biomedical-llm",
        "bilingual",
        "nlp"
      ],
      "id": 8
    },
    {
      "name": "Medical_Image_Analysis",
      "one_line_profile": "Foundation models based medical image analysis toolkit",
      "detailed_description": "A collection of foundation model implementations and tools for medical image analysis, facilitating research in medical computer vision.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "medical_image_analysis",
        "computer_vision"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Event-AHU/Medical_Image_Analysis",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "medical-imaging",
        "foundation-models",
        "deep-learning"
      ],
      "id": 9
    },
    {
      "name": "Medical-Image-Segmentation-Benchmarks",
      "one_line_profile": "PyTorch implementation of medical image segmentation benchmarks",
      "detailed_description": "A PyTorch library providing implementations of various U-shape architectures and benchmarks for medical image segmentation tasks.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "image_segmentation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/FengheTan9/Medical-Image-Segmentation-Benchmarks",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "segmentation",
        "pytorch",
        "u-net"
      ],
      "id": 10
    },
    {
      "name": "Apollo",
      "one_line_profile": "Multilingual Medical LLM ecosystem (Model, Dataset, Benchmark)",
      "detailed_description": "A comprehensive project for multilingual medical LLMs, including model weights, training code, datasets, and benchmarks for evaluating medical language understanding across languages.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "medical_llm",
        "multilingual_nlp"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/FreedomIntelligence/Apollo",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "multilingual",
        "medical-llm",
        "benchmark"
      ],
      "id": 11
    },
    {
      "name": "ApolloMoE",
      "one_line_profile": "Mixture of Experts (MoE) Medical LLM for 50 languages",
      "detailed_description": "An efficient medical Large Language Model utilizing Mixture of Experts (MoE) architecture to support 50 languages, democratizing access to medical AI.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "medical_llm",
        "moe_architecture"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/FreedomIntelligence/ApolloMoE",
      "help_website": [],
      "license": null,
      "tags": [
        "moe",
        "medical-llm",
        "multilingual"
      ],
      "id": 12
    },
    {
      "name": "CMB",
      "one_line_profile": "Comprehensive Medical Benchmark in Chinese",
      "detailed_description": "A comprehensive benchmark suite for evaluating Chinese medical Large Language Models, including datasets and evaluation scripts for various medical competency tasks.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmark"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/FreedomIntelligence/CMB",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "benchmark",
        "chinese-medical",
        "evaluation"
      ],
      "id": 13
    },
    {
      "name": "Chain-of-Diagnosis",
      "one_line_profile": "Interpretable LLM framework for medical diagnosis",
      "detailed_description": "An interpretable Large Language Model framework designed to perform medical diagnosis through a chain-of-thought process, enhancing transparency in clinical decision support.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "medical_diagnosis",
        "clinical_reasoning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/FreedomIntelligence/Chain-of-Diagnosis",
      "help_website": [],
      "license": null,
      "tags": [
        "diagnosis",
        "interpretability",
        "llm"
      ],
      "id": 14
    },
    {
      "name": "HuatuoGPT",
      "one_line_profile": "Open Medical GPT for doctor-like consultation",
      "detailed_description": "An open-source medical Large Language Model trained to function as a doctor, providing medical consultation and answering health-related queries with high accuracy.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "medical_llm",
        "medical_consultation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/FreedomIntelligence/HuatuoGPT",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "medical-gpt",
        "consultation",
        "llm"
      ],
      "id": 15
    },
    {
      "name": "HuatuoGPT-II",
      "one_line_profile": "One-stage Training for Medical Adaption of LLMs",
      "detailed_description": "The second generation of HuatuoGPT, featuring a one-stage training approach for adapting Large Language Models to the medical domain.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "medical_llm",
        "model_training"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/FreedomIntelligence/HuatuoGPT-II",
      "help_website": [],
      "license": null,
      "tags": [
        "medical-llm",
        "training-strategy",
        "domain-adaptation"
      ],
      "id": 16
    },
    {
      "name": "HuatuoGPT-Vision",
      "one_line_profile": "Medical multimodal large language model for visual question answering and reasoning",
      "detailed_description": "A multimodal medical Large Language Model (LLM) designed to process and reason with both medical text and images, enabling tasks such as medical visual question answering (VQA) and report generation.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "medical_vqa",
        "multimodal_reasoning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/FreedomIntelligence/HuatuoGPT-Vision",
      "help_website": [],
      "license": null,
      "tags": [
        "medical-llm",
        "multimodal",
        "vqa"
      ],
      "id": 17
    },
    {
      "name": "HuatuoGPT-o1",
      "one_line_profile": "Medical LLM optimized for complex clinical reasoning",
      "detailed_description": "A specialized medical Large Language Model focusing on complex reasoning capabilities in clinical scenarios, aiming to simulate doctor-like diagnostic logic.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "clinical_reasoning",
        "medical_diagnosis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/FreedomIntelligence/HuatuoGPT-o1",
      "help_website": [],
      "license": null,
      "tags": [
        "medical-llm",
        "reasoning",
        "clinical-decision-support"
      ],
      "id": 18
    },
    {
      "name": "DISC-MedLLM",
      "one_line_profile": "Conversational medical LLM solution for healthcare services",
      "detailed_description": "A comprehensive medical Large Language Model solution designed to provide accurate and truthful medical responses in end-to-end conversational healthcare settings, trained on high-quality medical datasets.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "medical_dialogue",
        "health_consultation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/FudanDISC/DISC-MedLLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "medical-llm",
        "chatbot",
        "healthcare"
      ],
      "id": 19
    },
    {
      "name": "ActiveSegmentation",
      "one_line_profile": "Simulation framework for active learning in medical image segmentation",
      "detailed_description": "A simulation framework designed to benchmark and evaluate active learning strategies specifically for 3D medical image segmentation tasks.",
      "domains": [
        "H2"
      ],
      "subtask_category": [
        "image_segmentation",
        "active_learning",
        "simulation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/HealthML/active-segmentation",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "medical-imaging",
        "active-learning",
        "segmentation"
      ],
      "id": 20
    },
    {
      "name": "MedMax",
      "one_line_profile": "Mixed-modal instruction tuning framework for biomedical assistants",
      "detailed_description": "A framework for mixed-modal instruction tuning aimed at training biomedical assistants, enabling models to handle diverse medical tasks.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "instruction_tuning",
        "model_training"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/Hritikbansal/medmax",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "instruction-tuning",
        "biomedical",
        "llm"
      ],
      "id": 21
    },
    {
      "name": "AIDoctor",
      "one_line_profile": "Training pipeline for medical GPT models including SFT and RLHF",
      "detailed_description": "A complete pipeline implementation for training medical GPT models, covering Pretraining, Supervised Fine-tuning (SFT), Reward Modeling, Reinforcement Learning (RLHF), and DPO.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "model_training",
        "rlhf",
        "fine_tuning"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/Jerry-XDL/AIDoctor",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-training",
        "rlhf",
        "medical-gpt"
      ],
      "id": 22
    },
    {
      "name": "MCM",
      "one_line_profile": "Multimodal Chinese Medical Large Language Model",
      "detailed_description": "A multimodal Large Language Model specifically designed for Traditional Chinese Medicine (TCM) and general Chinese medical consultation.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "medical_consultation",
        "tcm_diagnosis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/JerryMazeyu/MCM",
      "help_website": [],
      "license": null,
      "tags": [
        "tcm",
        "medical-llm",
        "multimodal"
      ],
      "id": 23
    },
    {
      "name": "LLMAnonymizer",
      "one_line_profile": "Tool for anonymizing medical documents using LLMs",
      "detailed_description": "A utility leveraging Large Language Models to automatically anonymize and de-identify sensitive medical documents for privacy protection.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "de-identification",
        "data_privacy"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/KatherLab/LLMAnonymizer-Publication",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "anonymization",
        "privacy",
        "medical-records"
      ],
      "id": 24
    },
    {
      "name": "Medical_LLM",
      "one_line_profile": "Structured pipeline for LLM-based medical projects",
      "detailed_description": "A structured pipeline designed to facilitate the development and deployment of Large Language Model projects in the medical domain, provided by Kather Lab.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "pipeline_orchestration",
        "project_scaffolding"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/KatherLab/Medical_LLM",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pipeline",
        "medical-llm",
        "workflow"
      ],
      "id": 25
    },
    {
      "name": "FedFMS",
      "one_line_profile": "Federated learning framework for fine-tuning Segment Anything Model (SAM)",
      "detailed_description": "A framework enabling the fine-tuning of the Segment Anything Model (SAM) within a federated learning paradigm, specifically for medical image segmentation tasks.",
      "domains": [
        "H2"
      ],
      "subtask_category": [
        "federated_learning",
        "image_segmentation",
        "fine_tuning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/LIU-YUXI/FedFMS",
      "help_website": [],
      "license": null,
      "tags": [
        "federated-learning",
        "sam",
        "segmentation"
      ],
      "id": 26
    },
    {
      "name": "AI Hospital",
      "one_line_profile": "Interactive simulation environment for LLMs as clinical interns",
      "detailed_description": "A simulation platform that evaluates and enables collaboration among Large Language Models acting as intern doctors for clinical diagnosis in a virtual hospital setting.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "agent_simulation",
        "clinical_diagnosis",
        "evaluation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/LibertFan/AI_Hospital",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "simulation",
        "medical-agent",
        "evaluation"
      ],
      "id": 27
    },
    {
      "name": "Medical-AGI",
      "one_line_profile": "Multi-agent platform for coordinating healthcare layers",
      "detailed_description": "An LLM-powered multi-agent platform designed to coordinate health information and services from global to individual levels.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "agent_coordination",
        "healthcare_management"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/LifestyleCorp/Medical-AGI",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "multi-agent",
        "agi",
        "healthcare"
      ],
      "id": 28
    },
    {
      "name": "MedicalChatbot",
      "one_line_profile": "Dialogue system for disease identification",
      "detailed_description": "A medical-domain dialogue system designed to identify diseases through automated conversation and symptom analysis.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "disease_identification",
        "medical_dialogue"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/LiuQL2/MedicalChatbot",
      "help_website": [],
      "license": null,
      "tags": [
        "chatbot",
        "diagnosis",
        "nlp"
      ],
      "id": 29
    },
    {
      "name": "Large-Scale-Medical",
      "one_line_profile": "Pre-training framework for 3D medical images with geometric priors",
      "detailed_description": "A framework for large-scale pre-training of 3D medical image models, incorporating geometric context priors to improve representation learning.",
      "domains": [
        "H2"
      ],
      "subtask_category": [
        "model_pretraining",
        "image_representation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Luffy03/Large-Scale-Medical",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "pre-training",
        "3d-imaging",
        "geometric-priors"
      ],
      "id": 30
    },
    {
      "name": "DiagGym",
      "one_line_profile": "Virtual clinical environment for training diagnostic agents",
      "detailed_description": "A virtual gym environment designed for training and evaluating self-evolving Large Language Model agents in clinical diagnostic tasks.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "agent_training",
        "clinical_simulation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/MAGIC-AI4Med/DiagGym",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "reinforcement-learning",
        "medical-agent",
        "simulation"
      ],
      "id": 31
    },
    {
      "name": "MedMNIST",
      "one_line_profile": "Standardized datasets and tools for biomedical image classification",
      "detailed_description": "A large-scale collection of standardized biomedical image datasets for 2D and 3D classification, accompanied by a Python library for easy data loading and evaluation.",
      "domains": [
        "H2"
      ],
      "subtask_category": [
        "image_classification",
        "benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/MedMNIST/MedMNIST",
      "help_website": [
        "https://medmnist.com/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "dataset",
        "medical-imaging",
        "classification"
      ],
      "id": 32
    },
    {
      "name": "MING",
      "one_line_profile": "Chinese medical question answering large language model",
      "detailed_description": "A specialized Chinese medical Large Language Model (MING) designed for medical question answering and consultation tasks.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "medical_qa",
        "consultation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/MediaBrain-SJTU/MING",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "medical-llm",
        "chinese-nlp",
        "qa"
      ],
      "id": 33
    },
    {
      "name": "RAG-HPO",
      "one_line_profile": "Automated deep phenotype analysis of clinical information using LLMs and RAG",
      "detailed_description": "A Python-based tool for automated deep phenotype analysis of clinical information, leveraging Large Language Models (LLMs) and Retrieval Augmented Generation (RAG) to map clinical text to Human Phenotype Ontology (HPO) terms.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "phenotyping",
        "clinical_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/PoseyPod/RAG-HPO",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "hpo",
        "phenotype",
        "rag",
        "clinical-text"
      ],
      "id": 34
    },
    {
      "name": "Huatuo-Llama-Med-Chinese",
      "one_line_profile": "Chinese medical instruction-tuning LLM based on LLaMA",
      "detailed_description": "A medical large language model (LLM) fine-tuned on Chinese medical knowledge graphs and dialogue data. It provides the model weights and code for instruction tuning, serving as a foundational solver for Chinese medical question answering and dialogue tasks.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "medical_llm",
        "instruction_tuning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "medical-llm",
        "chinese-nlp",
        "instruction-tuning",
        "ben-tsao"
      ],
      "id": 35
    },
    {
      "name": "AgentClinic",
      "one_line_profile": "Agent benchmark environment for medical diagnosis",
      "detailed_description": "A benchmark environment designed to evaluate LLM agents in a simulated clinical setting. It allows for the assessment of agents' capabilities in medical diagnosis and patient interaction.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "benchmarking",
        "agent_simulation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/SamuelSchmidgall/AgentClinic",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "medical-agent",
        "benchmark",
        "diagnosis",
        "simulation"
      ],
      "id": 36
    },
    {
      "name": "CLUE",
      "one_line_profile": "Clinical Language Understanding Evaluation benchmark for LLMs",
      "detailed_description": "A benchmark suite designed to evaluate the performance of Large Language Models (LLMs) on clinical language understanding tasks.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "benchmarking",
        "evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/TIO-IKIM/CLUE",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "benchmark",
        "clinical-nlp",
        "evaluation"
      ],
      "id": 37
    },
    {
      "name": "Open-MAI-Dx-Orchestrator",
      "one_line_profile": "Orchestrator for sequential medical diagnosis with language models",
      "detailed_description": "An open-source implementation of a sequential diagnosis framework using Large Language Models (LLMs), designed to orchestrate the diagnostic process.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "diagnosis",
        "workflow_orchestration"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/The-Swarm-Corporation/Open-MAI-Dx-Orchestrator",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "diagnosis",
        "agent",
        "orchestrator"
      ],
      "id": 38
    },
    {
      "name": "MedXpertQA",
      "one_line_profile": "Benchmark for expert-level medical reasoning and understanding",
      "detailed_description": "A comprehensive benchmark designed to evaluate expert-level medical reasoning and understanding capabilities in Large Language Models.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "benchmarking",
        "medical_reasoning"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/TsinghuaC3I/MedXpertQA",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "benchmark",
        "qa",
        "medical-reasoning"
      ],
      "id": 39
    },
    {
      "name": "3DMed-RAG",
      "one_line_profile": "Multimodal RAG system for medical consultation",
      "detailed_description": "A multimodal Retrieval-Augmented Generation (RAG) system capable of processing text, image-text, and image queries for medical consultation and diagnosis. It features domain-aligned negative rejection and enhanced flowchart data processing.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "medical_rag",
        "multimodal_qa"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Tsiphen/Multimodal-RAG-for-Medical-Consultation",
      "help_website": [],
      "license": null,
      "tags": [
        "rag",
        "multimodal",
        "medical-consultation"
      ],
      "id": 40
    },
    {
      "name": "MedReason",
      "one_line_profile": "Framework for eliciting medical reasoning in LLMs via Knowledge Graphs",
      "detailed_description": "A framework and dataset designed to improve and evaluate the factual medical reasoning steps of Large Language Models by leveraging Knowledge Graphs.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "medical_reasoning",
        "knowledge_graph"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/UCSC-VLAA/MedReason",
      "help_website": [],
      "license": null,
      "tags": [
        "reasoning",
        "knowledge-graph",
        "llm"
      ],
      "id": 41
    },
    {
      "name": "m1",
      "one_line_profile": "Test-time scaling method for medical reasoning in LLMs",
      "detailed_description": "An implementation of test-time scaling techniques to enhance the medical reasoning capabilities of Large Language Models, as presented at ML4H 2025.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "medical_reasoning",
        "inference_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/UCSC-VLAA/m1",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "test-time-scaling",
        "reasoning",
        "medical-llm"
      ],
      "id": 42
    },
    {
      "name": "discharge-documentation-generator",
      "one_line_profile": "Tool for generating draft clinical discharge letters using LLMs",
      "detailed_description": "A utility tool designed to assist clinicians by generating draft versions of clinical discharge letters using Large Language Models, aiming to streamline clinical documentation workflows.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "clinical_documentation",
        "text_generation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/UMCU-Digital-Health/discharge-documentation-generator",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ehr",
        "discharge-letter",
        "clinical-nlp"
      ],
      "id": 43
    },
    {
      "name": "LingYi",
      "one_line_profile": "Multi-modal Medical Conversational QA System based on Knowledge Graph",
      "detailed_description": "A medical conversational question answering system that integrates multi-modal data and knowledge graphs to provide accurate medical information.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "medical_qa",
        "dialogue_system"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/WENGSYX/LingYi",
      "help_website": [],
      "license": null,
      "tags": [
        "qa",
        "knowledge-graph",
        "multimodal"
      ],
      "id": 44
    },
    {
      "name": "CareGPT",
      "one_line_profile": "Comprehensive open-source medical LLM toolkit",
      "detailed_description": "A comprehensive toolkit for Medical LLMs, including pre-trained models, fine-tuning datasets, training scripts, evaluation benchmarks, and deployment tools.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "medical_llm",
        "model_training",
        "deployment"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/WangRongsheng/CareGPT",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "medical-llm",
        "toolkit",
        "fine-tuning"
      ],
      "id": 45
    },
    {
      "name": "IvyGPT",
      "one_line_profile": "Interactive Chinese pathway language model in medical domain",
      "detailed_description": "An interactive medical Large Language Model designed for the Chinese medical domain, capable of handling medical dialogue and pathway reasoning.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "medical_llm",
        "dialogue_system"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/WangRongsheng/IvyGPT",
      "help_website": [],
      "license": null,
      "tags": [
        "medical-llm",
        "chinese-nlp",
        "pathway-model"
      ],
      "id": 46
    },
    {
      "name": "MedQA-ChatGLM",
      "one_line_profile": "Fine-tuning framework for ChatGLM on medical data",
      "detailed_description": "A framework for fine-tuning the ChatGLM model on real-world medical dialogue data using techniques like LoRA, P-Tuning V2, and RLHF.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "fine_tuning",
        "medical_llm"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/WangRongsheng/MedQA-ChatGLM",
      "help_website": [],
      "license": null,
      "tags": [
        "chatglm",
        "fine-tuning",
        "lora",
        "rlhf"
      ],
      "id": 47
    },
    {
      "name": "Sunsimiao",
      "one_line_profile": "Chinese Medical Large Language Model",
      "detailed_description": "A safe and reliable Chinese medical Large Language Model (Sunsimiao), providing model weights and inference code for medical consultation and knowledge retrieval.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "medical_llm",
        "consultation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/X-D-Lab/Sunsimiao",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "medical-llm",
        "chinese-nlp",
        "sunsimiao"
      ],
      "id": 48
    },
    {
      "name": "LLM-Pretrain-FineTune",
      "one_line_profile": "Workflow for pre-training and fine-tuning medical LLMs",
      "detailed_description": "A codebase providing workflows for pre-training and fine-tuning Large Language Models on medical dialogue data, utilizing DeepSpeed for efficiency.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "model_training",
        "fine_tuning"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/X-jun-0130/LLM-Pretrain-FineTune",
      "help_website": [],
      "license": null,
      "tags": [
        "deepspeed",
        "pretraining",
        "fine-tuning",
        "medical-llm"
      ],
      "id": 49
    },
    {
      "name": "medkit-learn",
      "one_line_profile": "Environment for medical decision modelling through simulation",
      "detailed_description": "A simulation environment for medical decision modeling, allowing for the development and evaluation of decision-making algorithms in a controlled medical setting.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "simulation",
        "decision_modeling"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/XanderJC/medkit-learn",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "simulation",
        "decision-making",
        "medical-env"
      ],
      "id": 50
    },
    {
      "name": "EHRStruct",
      "one_line_profile": "Benchmark framework for evaluating LLMs on structured EHR tasks",
      "detailed_description": "A comprehensive benchmark framework designed to evaluate the performance of Large Language Models on tasks involving structured Electronic Health Records (EHR).",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "benchmarking",
        "ehr_analysis"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/YXNTU/EHRStruct",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "ehr",
        "benchmark",
        "structured-data"
      ],
      "id": 51
    },
    {
      "name": "ECG-Expert-QA",
      "one_line_profile": "Benchmark for evaluating medical LLMs in heart disease diagnosis",
      "detailed_description": "A benchmark dataset and evaluation tool for assessing the capabilities of Medical Large Language Models specifically in the domain of heart disease diagnosis using ECG data.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "benchmarking",
        "diagnosis",
        "cardiology"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/Zaozzz/ECG-Expert-QA",
      "help_website": [],
      "license": null,
      "tags": [
        "ecg",
        "benchmark",
        "heart-disease"
      ],
      "id": 52
    },
    {
      "name": "Qwen3-Medical-SFT",
      "one_line_profile": "Fine-tuning scripts for Qwen3 on medical data",
      "detailed_description": "A repository containing scripts and tools for Supervised Fine-Tuning (SFT) of the Qwen3 model on medical datasets, aiming to create medical chat models.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "fine_tuning",
        "medical_llm"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/Zeyi-Lin/Qwen3-Medical-SFT",
      "help_website": [],
      "license": null,
      "tags": [
        "qwen",
        "fine-tuning",
        "sft"
      ],
      "id": 53
    },
    {
      "name": "MedEmbed",
      "one_line_profile": "Collection of embedding models fine-tuned for medical and clinical data",
      "detailed_description": "MedEmbed provides a suite of embedding models specifically fine-tuned on medical and clinical datasets, enabling improved performance in downstream tasks such as semantic search, clustering, and classification within the healthcare domain.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "scientific_data_processing",
        "embedding_generation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/abhinand5/MedEmbed",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "medical-embeddings",
        "nlp",
        "clinical-data"
      ],
      "id": 54
    },
    {
      "name": "AI-Agents-for-Medical-Diagnostics",
      "one_line_profile": "Framework for creating LLM-based AI agents for medical diagnostics",
      "detailed_description": "A Python-based framework designed to build and deploy specialized AI agents that leverage Large Language Models (LLMs) for analyzing complex medical cases. It facilitates the integration of insights from various medical perspectives to provide comprehensive assessments.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "scientific_data_analysis",
        "medical_diagnostics"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/ahmadvh/AI-Agents-for-Medical-Diagnostics",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "medical-agents",
        "llm",
        "diagnostics"
      ],
      "id": 55
    },
    {
      "name": "MedEvalKit",
      "one_line_profile": "Unified evaluation framework for medical Large Language Models",
      "detailed_description": "MedEvalKit is a comprehensive toolkit developed by Alibaba DAMO Academy for evaluating medical LLMs. It supports various medical tasks and datasets, providing a standardized way to benchmark model performance in the biomedical domain.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "scientific_data_analysis",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/alibaba-damo-academy/MedEvalKit",
      "help_website": [],
      "license": null,
      "tags": [
        "evaluation",
        "medical-llm",
        "benchmark"
      ],
      "id": 56
    },
    {
      "name": "DeepSeek-R1-Distill-Qwen-32B-Medical-Fine-tune",
      "one_line_profile": "Medical fine-tuned version of DeepSeek-R1-Distill-Qwen-32B",
      "detailed_description": "A specialized Large Language Model fine-tuned on 2 million medical data points based on the DeepSeek-R1-Distill-Qwen-32B architecture. It serves as a solver for medical reasoning and question-answering tasks.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "scientific_inference",
        "medical_reasoning"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/beita6969/DeepSeek-R1-Distill-Qwen-32B-Medical-Fine-tune",
      "help_website": [],
      "license": null,
      "tags": [
        "medical-llm",
        "fine-tuning",
        "deepseek"
      ],
      "id": 57
    },
    {
      "name": "PMC-LLaMA",
      "one_line_profile": "Open-source language model for medicine based on LLaMA",
      "detailed_description": "PMC-LLaMA is an open-source Large Language Model specifically adapted for the medical domain. It is fine-tuned on biomedical literature (PMC) and clinical notes, serving as a foundational model for various medical NLP tasks.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "scientific_inference",
        "medical_nlp"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/chaoyi-wu/PMC-LLaMA",
      "help_website": [],
      "license": null,
      "tags": [
        "medical-llm",
        "llama",
        "nlp"
      ],
      "id": 58
    },
    {
      "name": "MultiMedEval",
      "one_line_profile": "Evaluation toolkit for Medical Vision-Language Models",
      "detailed_description": "MultiMedEval is a Python library designed to evaluate the performance of Vision-Language Models (VLMs) in the medical domain. It provides a unified interface for benchmarking models across multiple medical datasets and tasks.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "scientific_data_analysis",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/corentin-ryr/MultiMedEval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "vlm",
        "evaluation",
        "medical-imaging"
      ],
      "id": 59
    },
    {
      "name": "ClinicalTrials.gov MCP Server",
      "one_line_profile": "MCP server for retrieving clinical trial data for LLMs",
      "detailed_description": "A Model Context Protocol (MCP) server that acts as a bridge between Large Language Models and the ClinicalTrials.gov API, enabling AI agents to search and retrieve clinical study details for scientific analysis.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "information_retrieval",
        "data_acquisition"
      ],
      "application_level": "service",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/cyanheads/clinicaltrialsgov-mcp-server",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "mcp",
        "clinical-trials",
        "llm-tool"
      ],
      "id": 60
    },
    {
      "name": "MedVidQACL",
      "one_line_profile": "Benchmark implementation for medical video classification and QA",
      "detailed_description": "Implementation of benchmark approaches for the MedVidCL (Medical Instructional Video Classification) and MedVidQA (Medical Video Question Answering) tasks, providing tools for analyzing medical instructional videos.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "video_classification",
        "visual_question_answering"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/deepaknlp/MedVidQACL",
      "help_website": [],
      "license": null,
      "tags": [
        "medical-video",
        "vqa",
        "classification"
      ],
      "id": 61
    },
    {
      "name": "pyomop",
      "one_line_profile": "Python toolkit for managing OHDSI clinical data models",
      "detailed_description": "A Python package for managing OHDSI clinical data models (CDM), supporting LLM-based plain text queries, MCP server integration, and FHIR data import for clinical informatics.",
      "domains": [
        "H2"
      ],
      "subtask_category": [
        "data_management",
        "clinical_informatics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/dermatologist/pyomop",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "ohdsi",
        "omop",
        "fhir",
        "clinical-data"
      ],
      "id": 62
    },
    {
      "name": "RAG2",
      "one_line_profile": "Rationale-guided RAG for medical question answering",
      "detailed_description": "Implementation of a Rationale-Guided Retrieval Augmented Generation framework for medical question answering, designed to improve the accuracy and explainability of LLM responses in healthcare contexts.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "question_answering",
        "rag"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/dmis-lab/RAG2",
      "help_website": [],
      "license": null,
      "tags": [
        "rag",
        "medical-qa",
        "llm"
      ],
      "id": 63
    },
    {
      "name": "HealthChain",
      "one_line_profile": "Middleware framework for building healthcare AI applications",
      "detailed_description": "A Python framework designed as a middleware layer to facilitate the development and deployment of healthcare AI applications, connecting clinical data sources with AI models.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "workflow_management",
        "application_development"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/dotimplement/HealthChain",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "healthcare-ai",
        "middleware",
        "pipeline"
      ],
      "id": 64
    },
    {
      "name": "gpt2-bert-medical-qa-chat",
      "one_line_profile": "Medical domain-focused GPT-2 fine-tuning implementation",
      "detailed_description": "A research repository providing code for fine-tuning, optimizing, and lightweighting GPT-2 models specifically for medical question-answering tasks.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "model_training",
        "question_answering"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/dsdanielpark/gpt2-bert-medical-qa-chat",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "gpt-2",
        "fine-tuning",
        "medical-qa"
      ],
      "id": 65
    },
    {
      "name": "Expert-CFG",
      "one_line_profile": "Expert-controlled guidance for medical VQA",
      "detailed_description": "Implementation of Expert-Controlled Classifier-Free Guidance (Expert-CFG) to improve reliability and accuracy in Medical Visual Question Answering tasks.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "visual_question_answering",
        "model_inference"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ecoxial2007/Expert-CFG",
      "help_website": [],
      "license": null,
      "tags": [
        "vqa",
        "medical-imaging",
        "classifier-free-guidance"
      ],
      "id": 66
    },
    {
      "name": "LDCT Benchmark",
      "one_line_profile": "Benchmark for low dose CT image denoising",
      "detailed_description": "A benchmarking tool and framework for evaluating deep learning-based methods for low dose CT (LDCT) image denoising.",
      "domains": [
        "H2"
      ],
      "subtask_category": [
        "image_denoising",
        "benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/eeulig/ldct-benchmark",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ct-imaging",
        "denoising",
        "benchmark"
      ],
      "id": 67
    },
    {
      "name": "CLIP Image Search",
      "one_line_profile": "Fine-tuned CLIP model for medical image search",
      "detailed_description": "A tool for fine-tuning OpenAI's CLIP model specifically for semantic search on medical images, enabling efficient retrieval of medical visual data.",
      "domains": [
        "H2"
      ],
      "subtask_category": [
        "image_retrieval",
        "model_training"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/elsevierlabs-os/clip-image-search",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "clip",
        "medical-image-search",
        "fine-tuning"
      ],
      "id": 68
    },
    {
      "name": "Meditron",
      "one_line_profile": "Suite of open-source medical Large Language Models",
      "detailed_description": "A comprehensive suite of open-source Large Language Models (LLMs) specifically pre-trained and fine-tuned for the medical domain, providing high-performance foundation models for healthcare applications.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "model_training",
        "inference"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/epfLLM/meditron",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "medical-llm",
        "foundation-model",
        "llama"
      ],
      "id": 69
    },
    {
      "name": "SMMILE",
      "one_line_profile": "Benchmark for multimodal medical in-context learning",
      "detailed_description": "An expert-driven benchmark designed to evaluate Multimodal Medical In-Context Learning capabilities of AI models, developed by ETH Medical AI Lab.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "benchmarking",
        "multimodal_learning"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/eth-medical-ai-lab/smmile",
      "help_website": [],
      "license": null,
      "tags": [
        "benchmark",
        "in-context-learning",
        "multimodal"
      ],
      "id": 70
    },
    {
      "name": "MedTsLLM",
      "one_line_profile": "LLM-based tool for medical time series analysis",
      "detailed_description": "A framework leveraging Large Language Models for the analysis of multimodal medical time series data, enabling advanced interpretation of temporal clinical data.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "time_series_analysis",
        "multimodal_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/flixpar/med-ts-llm",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "time-series",
        "medical-llm",
        "multimodal"
      ],
      "id": 71
    },
    {
      "name": "BioMCP",
      "one_line_profile": "Biomedical Model Context Protocol implementation",
      "detailed_description": "An implementation of the Model Context Protocol (MCP) specifically tailored for biomedical data, facilitating standardized interaction between LLMs and biomedical data sources.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "data_integration",
        "interoperability"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/genomoncology/biomcp",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mcp",
        "biomedical",
        "protocol"
      ],
      "id": 72
    },
    {
      "name": "MedAgents",
      "one_line_profile": "Framework for zero-shot medical reasoning using LLM agents",
      "detailed_description": "A framework that utilizes Large Language Models as collaborating agents to perform complex zero-shot medical reasoning tasks, enhancing diagnostic and decision-making capabilities.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "medical_reasoning",
        "agent_framework"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/gersteinlab/MedAgents",
      "help_website": [],
      "license": null,
      "tags": [
        "agents",
        "medical-reasoning",
        "zero-shot"
      ],
      "id": 73
    },
    {
      "name": "MedAgentsBench",
      "one_line_profile": "Benchmark for medical reasoning agents",
      "detailed_description": "A benchmarking suite for evaluating thinking models and agent frameworks on complex medical reasoning tasks.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "benchmarking",
        "medical_reasoning"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/gersteinlab/medagents-benchmark",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "benchmark",
        "agents",
        "reasoning"
      ],
      "id": 74
    },
    {
      "name": "CUFIT",
      "one_line_profile": "Curriculum fine-tuning for medical image classification",
      "detailed_description": "Implementation of a curriculum fine-tuning strategy for vision foundation models, specifically designed to handle label noise in medical image classification tasks.",
      "domains": [
        "H2"
      ],
      "subtask_category": [
        "image_classification",
        "model_training"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/gist-ailab/CUFIT",
      "help_website": [],
      "license": null,
      "tags": [
        "fine-tuning",
        "medical-imaging",
        "label-noise"
      ],
      "id": 75
    },
    {
      "name": "CMSA-MTPT-4-MedicalVQA",
      "one_line_profile": "Multi-task pre-training for medical VQA",
      "detailed_description": "A framework for Medical Visual Question Answering utilizing multi-task pre-training and cross-modal self-attention mechanisms to improve performance.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "visual_question_answering",
        "model_training"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/haifangong/CMSA-MTPT-4-MedicalVQA",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "vqa",
        "attention",
        "pre-training"
      ],
      "id": 76
    },
    {
      "name": "diseaseBERT",
      "one_line_profile": "BERT model infused with disease knowledge",
      "detailed_description": "A BERT-based model and toolkit that infuses disease knowledge for enhanced performance in health question answering, medical inference, and disease name recognition.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "question_answering",
        "named_entity_recognition"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/heyunh2015/diseaseBERT",
      "help_website": [],
      "license": null,
      "tags": [
        "bert",
        "knowledge-infusion",
        "ner"
      ],
      "id": 77
    },
    {
      "name": "viBioGPT",
      "one_line_profile": "Vietnamese medical LLM for question answering",
      "detailed_description": "A Vietnamese Large Language Model fine-tuned specifically for question answering tasks within the medical and healthcare domain.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "question_answering",
        "model_training"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/hungnlp/viBioGPT",
      "help_website": [],
      "license": null,
      "tags": [
        "vietnamese",
        "medical-llm",
        "qa"
      ],
      "id": 78
    },
    {
      "name": "Doctor-SAM",
      "one_line_profile": "Fine-tuned SAM for medical image segmentation",
      "detailed_description": "A fine-tuned version of the Segment Anything Model (SAM) optimized for medical image segmentation tasks.",
      "domains": [
        "H2"
      ],
      "subtask_category": [
        "image_segmentation",
        "model_training"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/huoxiangzuo/Doctor-SAM",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "sam",
        "segmentation",
        "medical-imaging"
      ],
      "id": 79
    },
    {
      "name": "LLM Meta-Analysis",
      "one_line_profile": "Tool for automating clinical trial meta-analysis",
      "detailed_description": "A Python tool designed to automate the process of performing meta-analysis on clinical trials (RCTs) using Large Language Models.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "meta_analysis",
        "data_synthesis"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/hyesunyun/llm-meta-analysis",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "meta-analysis",
        "clinical-trials",
        "automation"
      ],
      "id": 80
    },
    {
      "name": "EHRNoteQA",
      "one_line_profile": "Benchmark for clinical practice using discharge summaries",
      "detailed_description": "A benchmark suite for evaluating Large Language Models on real-world clinical practice tasks using Electronic Health Record (EHR) discharge summaries.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "benchmarking",
        "question_answering"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/ji-youn-kim/EHRNoteQA",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ehr",
        "benchmark",
        "discharge-summary"
      ],
      "id": 81
    },
    {
      "name": "PeFoMed",
      "one_line_profile": "Parameter efficient fine-tuning for medical VQA",
      "detailed_description": "Implementation of PeFoM-Med, a parameter-efficient fine-tuning method for Multi-modal Large Language Models applied to Medical Visual Question Answering.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "visual_question_answering",
        "model_training"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/jinlHe/PeFoMed",
      "help_website": [],
      "license": null,
      "tags": [
        "peft",
        "vqa",
        "multimodal"
      ],
      "id": 82
    },
    {
      "name": "medAlpaca",
      "one_line_profile": "LLM fine-tuned for medical question answering",
      "detailed_description": "An open-source Large Language Model fine-tuned on medical datasets to provide accurate answers to medical questions, serving as a specialized tool for healthcare NLP tasks.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "question_answering",
        "model_training"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/kbressem/medAlpaca",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "alpaca",
        "medical-qa",
        "llm"
      ],
      "id": 83
    },
    {
      "name": "IMCS21 Benchmark",
      "one_line_profile": "Benchmark for automatic medical consultation systems",
      "detailed_description": "A benchmark toolkit containing datasets and frameworks for evaluating automatic medical consultation systems, focusing on dialogue understanding and generation in healthcare.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "benchmarking",
        "dialogue_system"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/lemuria-wchen/imcs21",
      "help_website": [],
      "license": null,
      "tags": [
        "medical-dialogue",
        "benchmark",
        "consultation"
      ],
      "id": 84
    },
    {
      "name": "PreProcPipe",
      "one_line_profile": "LLM-automated pipeline for medical image preprocessing",
      "detailed_description": "A pipeline leveraging Large Language Models to automate the preprocessing of medical images, aiming to simplify and clarify the workflow for medical image analysis.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "medical_image_preprocessing",
        "workflow_automation"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/lgy112112/PreProcPipe",
      "help_website": [],
      "license": null,
      "tags": [
        "medical-imaging",
        "preprocessing",
        "llm-automation"
      ],
      "id": 85
    },
    {
      "name": "doctorwithbloom",
      "one_line_profile": "Fine-tuning scripts for Bloomz on medical dialogue datasets",
      "detailed_description": "Provides implementation for fine-tuning the Bloomz-7b1-mt model using LoRA on the ChatDoctor dataset to create a medical consultation assistant.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "model_finetuning",
        "medical_dialogue"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/linhduongtuan/doctorwithbloom",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "bloomz",
        "lora",
        "medical-llm",
        "finetuning"
      ],
      "id": 86
    },
    {
      "name": "MedChain",
      "one_line_profile": "Framework bridging LLM agents with clinical decision making",
      "detailed_description": "Implementation of MedChain, a system designed to integrate Large Language Model agents into real-world clinical decision-making processes, enhancing reasoning and reliability.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "clinical_decision_support",
        "agent_framework"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ljwztc/MedChain",
      "help_website": [],
      "license": null,
      "tags": [
        "clinical-decision-making",
        "llm-agents",
        "medical-reasoning"
      ],
      "id": 87
    },
    {
      "name": "clinical-calculator-tooluse",
      "one_line_profile": "Training LLMs to use clinical calculators from patient history",
      "detailed_description": "Explores and implements methods to train open-source Large Language Models to extract patient history and correctly utilize clinical calculators (e.g., Wells' Criteria).",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "tool_use",
        "clinical_calculation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/lucidrains/clinical-calculator-tooluse",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "clinical-calculators",
        "tool-learning",
        "medical-llm"
      ],
      "id": 88
    },
    {
      "name": "openmed",
      "one_line_profile": "Open-source healthcare AI library",
      "detailed_description": "A library providing open-source tools and models for healthcare AI applications, facilitating the development and deployment of medical AI solutions.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "healthcare_ai",
        "model_deployment"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/maziyarpanahi/openmed",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "healthcare-ai",
        "open-source",
        "medical-models"
      ],
      "id": 89
    },
    {
      "name": "finetune-SAM",
      "one_line_profile": "Fine-tuning Segment Anything Model (SAM) for medical images",
      "detailed_description": "Official repository for fine-tuning the Segment Anything Model (SAM) specifically for customized medical image segmentation tasks.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "medical_image_segmentation",
        "model_finetuning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/mazurowski-lab/finetune-SAM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "sam",
        "segmentation",
        "medical-imaging",
        "fine-tuning"
      ],
      "id": 90
    },
    {
      "name": "failure_detection_benchmark",
      "one_line_profile": "Benchmark for failure detection in medical image classification",
      "detailed_description": "Code and benchmarking framework for evaluating failure detection methods in medical image classification, as presented in the TMLR 2022 paper.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "failure_detection",
        "benchmarking",
        "image_classification"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/melanibe/failure_detection_benchmark",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "failure-detection",
        "medical-imaging",
        "benchmarking"
      ],
      "id": 91
    },
    {
      "name": "clinical-self-verification",
      "one_line_profile": "Self-verification mechanisms for clinical LLMs",
      "detailed_description": "Implementation of self-verification techniques to improve the accuracy and reliability of Large Language Models in clinical contexts.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "verification",
        "hallucination_reduction"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/microsoft/clinical-self-verification",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "self-verification",
        "clinical-llm",
        "reliability"
      ],
      "id": 92
    },
    {
      "name": "Madrigal",
      "one_line_profile": "Multimodal AI for predicting drug combination outcomes",
      "detailed_description": "A multimodal AI framework (Madrigal) that predicts clinical outcomes of drug combinations by leveraging preclinical data.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "drug_response_prediction",
        "multimodal_learning"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/mims-harvard/Madrigal",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "drug-combinations",
        "clinical-outcomes",
        "multimodal-ai"
      ],
      "id": 93
    },
    {
      "name": "MDAgents",
      "one_line_profile": "Adaptive collaboration of LLMs for medical decision-making",
      "detailed_description": "Implementation of MDAgents, a framework enabling adaptive collaboration among multiple Large Language Models to enhance medical decision-making capabilities.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "medical_decision_making",
        "multi_agent_system"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/mitmedialab/MDAgents",
      "help_website": [],
      "license": null,
      "tags": [
        "multi-agent",
        "medical-decision-making",
        "collaboration"
      ],
      "id": 94
    },
    {
      "name": "MedPerf",
      "one_line_profile": "Open benchmarking platform for medical AI",
      "detailed_description": "An open benchmarking platform designed for medical artificial intelligence, utilizing federated evaluation to assess model performance across distributed datasets.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "benchmarking",
        "federated_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/mlcommons/medperf",
      "help_website": [
        "https://medperf.org"
      ],
      "license": "Apache-2.0",
      "tags": [
        "benchmarking",
        "federated-learning",
        "medical-ai"
      ],
      "id": 95
    },
    {
      "name": "FFA-IR",
      "one_line_profile": "Benchmark for explainable medical report generation",
      "detailed_description": "Official code for the FFA-IR benchmark, focusing on evaluating the explainability and reliability of medical report generation models.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "report_generation",
        "benchmarking",
        "explainability"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/mlii0117/FFA-IR",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "medical-report-generation",
        "benchmark",
        "explainable-ai"
      ],
      "id": 96
    },
    {
      "name": "MedCalc-Bench",
      "one_line_profile": "Benchmark for evaluating LLMs on medical calculations",
      "detailed_description": "A benchmark suite designed to evaluate the performance of Large Language Models on medical calculation tasks, ensuring accuracy in clinical computations.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "medical_calculation",
        "benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ncbi-nlp/MedCalc-Bench",
      "help_website": [],
      "license": null,
      "tags": [
        "medical-calculations",
        "llm-evaluation",
        "benchmark"
      ],
      "id": 97
    },
    {
      "name": "annotateai",
      "one_line_profile": "Automated paper annotation using LLMs",
      "detailed_description": "A tool to automatically annotate scientific papers using Large Language Models, facilitating literature review and knowledge extraction.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "literature_annotation",
        "knowledge_extraction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/neuml/annotateai",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "annotation",
        "literature-review",
        "llm"
      ],
      "id": 98
    },
    {
      "name": "INSIGHT",
      "one_line_profile": "Autonomous AI agent for medical research",
      "detailed_description": "An autonomous AI system designed to conduct medical research tasks, acting as a research assistant or agent.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "medical_research_agent",
        "autonomous_research"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/oneil512/INSIGHT",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "autonomous-agent",
        "medical-research",
        "ai-scientist"
      ],
      "id": 99
    },
    {
      "name": "MIMIC-Clinical-Decision-Making-Analysis",
      "one_line_profile": "Analysis code for LLM performance on MIMIC CDM dataset",
      "detailed_description": "A repository containing code to analyze the performance and results of selected Large Language Models using the MIMIC Clinical Decision Making (CDM) dataset.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "performance_analysis",
        "clinical_decision_making"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/paulhager/MIMIC-Clinical-Decision-Making-Analysis",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mimic",
        "clinical-decision-making",
        "llm-analysis"
      ],
      "id": 100
    },
    {
      "name": "RuMedBench",
      "one_line_profile": "Benchmark for Russian medical language understanding",
      "detailed_description": "A benchmark suite for evaluating Russian language models on medical tasks, as described in the associated arXiv paper.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "benchmarking",
        "multilingual_medical_nlp"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/pavel-blinov/RuMedBench",
      "help_website": [],
      "license": null,
      "tags": [
        "russian-nlp",
        "medical-benchmark",
        "evaluation"
      ],
      "id": 101
    },
    {
      "name": "M2I2",
      "one_line_profile": "Self-supervised pretraining for medical VQA",
      "detailed_description": "Implementation of M2I2, a self-supervised vision-language pretraining framework specifically designed for Medical Visual Question Answering tasks.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "medical_vqa",
        "vision_language_pretraining"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/pengfeiliHEU/M2I2",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "vqa",
        "medical-imaging",
        "pretraining"
      ],
      "id": 102
    },
    {
      "name": "MUMC",
      "one_line_profile": "Masked vision and language pre-training for Medical VQA",
      "detailed_description": "Implementation of MUMC, a framework using masked vision and language pre-training with unimodal and multimodal contrastive losses for Medical Visual Question Answering.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "medical_vqa",
        "contrastive_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/pengfeiliHEU/MUMC",
      "help_website": [],
      "license": null,
      "tags": [
        "vqa",
        "multimodal",
        "pretraining"
      ],
      "id": 103
    },
    {
      "name": "CARES",
      "one_line_profile": "Benchmark of trustworthiness in medical vision language models",
      "detailed_description": "A comprehensive benchmark suite for evaluating the trustworthiness (e.g., safety, fairness, reliability) of Medical Vision Language Models.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "trustworthiness_evaluation",
        "medical_vlm"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/richard-peng-xia/CARES",
      "help_website": [],
      "license": "CC-BY-4.0",
      "tags": [
        "trustworthiness",
        "benchmark",
        "medical-vlm"
      ],
      "id": 104
    },
    {
      "name": "BMRetriever",
      "one_line_profile": "Tuning LLMs as biomedical text retrievers",
      "detailed_description": "Implementation of BMRetriever, a method for tuning Large Language Models to function as effective retrievers for biomedical text data.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "information_retrieval",
        "biomedical_text_mining"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ritaranx/BMRetriever",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "retrieval",
        "biomedical-nlp",
        "llm-tuning"
      ],
      "id": 105
    },
    {
      "name": "ClinGen",
      "one_line_profile": "Knowledge-infused prompting for clinical text generation",
      "detailed_description": "Code for ClinGen, a framework that uses knowledge-infused prompting to assess and advance the generation of clinical text data using Large Language Models.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "clinical_text_generation",
        "prompt_engineering"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ritaranx/ClinGen",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "text-generation",
        "clinical-nlp",
        "knowledge-infusion"
      ],
      "id": 106
    },
    {
      "name": "RAM-EHR",
      "one_line_profile": "Retrieval augmentation for clinical predictions on EHR",
      "detailed_description": "Implementation of RAM-EHR, a method combining retrieval augmentation with clinical prediction models for Electronic Health Records.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "clinical_prediction",
        "retrieval_augmented_generation",
        "ehr_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ritaranx/RAM-EHR",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ehr",
        "rag",
        "clinical-prediction"
      ],
      "id": 107
    },
    {
      "name": "LLM-From-Scratch",
      "one_line_profile": "Medical Language Model fine-tuning and alignment pipeline",
      "detailed_description": "A pipeline for creating medical language models, covering pretraining, instruction tuning, and Direct Preference Optimization (DPO) for improved alignment.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "model_training",
        "alignment",
        "dpo"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/samadon1/LLM-From-Scratch",
      "help_website": [],
      "license": null,
      "tags": [
        "medical-llm",
        "dpo",
        "instruction-tuning"
      ],
      "id": 108
    },
    {
      "name": "MedicalGPT",
      "one_line_profile": "Comprehensive toolkit for training and fine-tuning medical Large Language Models",
      "detailed_description": "A complete pipeline for training medical LLMs, supporting incremental pre-training, supervised fine-tuning (SFT), RLHF, DPO, and other alignment techniques specifically adapted for the medical domain.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "model_training",
        "fine_tuning",
        "alignment"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/shibing624/MedicalGPT",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "medical-llm",
        "rlhf",
        "sft",
        "dpo"
      ],
      "id": 109
    },
    {
      "name": "JMED-LLM",
      "one_line_profile": "Japanese Medical Evaluation Dataset for Large Language Models",
      "detailed_description": "A benchmark dataset designed to evaluate the performance of Large Language Models on Japanese medical tasks, facilitating the assessment of cross-lingual medical AI capabilities.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmark"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/sociocom/JMED-LLM",
      "help_website": [],
      "license": null,
      "tags": [
        "benchmark",
        "japanese-medical",
        "llm-evaluation"
      ],
      "id": 110
    },
    {
      "name": "factehr",
      "one_line_profile": "Fact verification tool for clinical notes using LLMs",
      "detailed_description": "A tool designed to verify factual correctness in clinical notes generated or processed by Large Language Models, addressing hallucination issues in medical text generation.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "fact_verification",
        "clinical_nlp"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/som-shahlab/factehr",
      "help_website": [],
      "license": null,
      "tags": [
        "fact-checking",
        "clinical-notes",
        "hallucination-detection"
      ],
      "id": 111
    },
    {
      "name": "MedAgentBench",
      "one_line_profile": "Realistic Virtual EHR Environment to Benchmark Medical LLM Agents",
      "detailed_description": "A benchmarking environment that simulates Electronic Health Records (EHR) interactions to evaluate the capability of medical LLM agents in performing clinical tasks.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "agent_evaluation",
        "benchmark",
        "ehr_simulation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/stanfordmlgroup/MedAgentBench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "medical-agent",
        "ehr",
        "benchmark"
      ],
      "id": 112
    },
    {
      "name": "Asclepius",
      "one_line_profile": "Clinical Large Language Model built on synthetic clinical notes",
      "detailed_description": "Provides a clinical LLM trained on synthetic data to overcome privacy barriers, serving as a resource for clinical NLP research and model development.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "model_generation",
        "synthetic_data"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/starmpcc/Asclepius",
      "help_website": [],
      "license": null,
      "tags": [
        "synthetic-data",
        "clinical-llm",
        "privacy-preserving"
      ],
      "id": 113
    },
    {
      "name": "CAMEL",
      "one_line_profile": "Clinically Adapted Model Enhanced from LLaMA",
      "detailed_description": "A medical LLM adapted from LLaMA, designed to provide enhanced clinical reasoning and knowledge capabilities for medical NLP tasks.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "medical_llm",
        "inference"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/starmpcc/CAMEL",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llama",
        "clinical-adaptation",
        "medical-llm"
      ],
      "id": 114
    },
    {
      "name": "ChiMed-GPT",
      "one_line_profile": "Chinese Medical Large Language Model with full training pipeline",
      "detailed_description": "A Chinese medical LLM developed through continual pre-training, SFT, and RLHF on Chinese medical data, providing a resource for Chinese clinical NLP.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "medical_llm",
        "model_training"
      ],
      "application_level": "solver",
      "primary_language": null,
      "repo_url": "https://github.com/synlp/ChiMed-GPT",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "chinese-medical-llm",
        "rlhf",
        "sft"
      ],
      "id": 115
    },
    {
      "name": "GMAI-MMBench",
      "one_line_profile": "Comprehensive Multimodal Evaluation Benchmark for General Medical AI",
      "detailed_description": "A benchmark suite for evaluating multimodal medical AI models across various tasks and modalities, ensuring robust assessment of general medical AI capabilities.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "multimodal_evaluation",
        "benchmark"
      ],
      "application_level": "dataset",
      "primary_language": null,
      "repo_url": "https://github.com/uni-medical/GMAI-MMBench",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "multimodal",
        "benchmark",
        "medical-ai"
      ],
      "id": 116
    },
    {
      "name": "IMIS-Bench",
      "one_line_profile": "Benchmark Dataset and Baseline for Interactive Medical Image Segmentation",
      "detailed_description": "Provides a benchmark dataset and baseline models for the task of interactive medical image segmentation, facilitating research in human-in-the-loop medical imaging.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "image_segmentation",
        "benchmark"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/uni-medical/IMIS-Bench",
      "help_website": [],
      "license": null,
      "tags": [
        "segmentation",
        "interactive-segmentation",
        "benchmark"
      ],
      "id": 117
    },
    {
      "name": "TREQS",
      "one_line_profile": "Text-to-SQL Generation for Question Answering on Electronic Medical Records",
      "detailed_description": "A specialized tool for converting natural language questions into SQL queries specifically for Electronic Medical Records (EMR), enabling data extraction and analysis.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "text_to_sql",
        "emr_analysis",
        "question_answering"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/wangpinggl/TREQS",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "text-to-sql",
        "emr",
        "clinical-qa"
      ],
      "id": 118
    },
    {
      "name": "CMExam",
      "one_line_profile": "Chinese National Medical Licensing Examination dataset and benchmarks",
      "detailed_description": "A dataset and benchmark suite derived from the Chinese National Medical Licensing Examination to evaluate the medical knowledge and reasoning of LLMs.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmark",
        "medical_knowledge"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/williamliujl/CMExam",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "benchmark",
        "medical-exam",
        "chinese-medical"
      ],
      "id": 119
    },
    {
      "name": "EhrAgent",
      "one_line_profile": "Agent framework for complex tabular reasoning on Electronic Health Records",
      "detailed_description": "An agent-based framework that empowers LLMs to perform complex tabular reasoning tasks on Electronic Health Records (EHR) by generating and executing code.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "tabular_reasoning",
        "ehr_analysis",
        "agent"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/wshi83/EhrAgent",
      "help_website": [],
      "license": null,
      "tags": [
        "ehr",
        "agent",
        "tabular-reasoning"
      ],
      "id": 120
    },
    {
      "name": "MedAdapter",
      "one_line_profile": "Efficient Test-Time Adaptation of LLMs for Medical Reasoning",
      "detailed_description": "A lightweight adapter module designed for efficient test-time adaptation of Large Language Models to improve their performance on medical reasoning tasks.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "model_adaptation",
        "medical_reasoning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/wshi83/MedAdapter",
      "help_website": [],
      "license": null,
      "tags": [
        "adapter",
        "test-time-adaptation",
        "medical-reasoning"
      ],
      "id": 121
    },
    {
      "name": "MedAgentGym",
      "one_line_profile": "Training environment for LLM Agents in Code-Based Medical Reasoning",
      "detailed_description": "A training environment (Gym) designed to scale the training of LLM agents for code-based medical reasoning tasks, facilitating the development of autonomous medical agents.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "agent_training",
        "reinforcement_learning",
        "medical_reasoning"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/wshi83/MedAgentGym",
      "help_website": [],
      "license": null,
      "tags": [
        "agent-gym",
        "medical-reasoning",
        "training-environment"
      ],
      "id": 122
    },
    {
      "name": "PMC-VQA",
      "one_line_profile": "Large-scale medical visual question-answering dataset",
      "detailed_description": "A large-scale dataset for medical Visual Question Answering (VQA) sourced from PubMed Central, covering various modalities and diseases to train and evaluate multimodal models.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "visual_question_answering",
        "dataset"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/xiaoman-zhang/PMC-VQA",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "vqa",
        "medical-imaging",
        "multimodal"
      ],
      "id": 123
    },
    {
      "name": "DoctorGLM",
      "one_line_profile": "Chinese medical consultation model based on ChatGLM-6B",
      "detailed_description": "A specialized Chinese medical LLM fine-tuned on ChatGLM-6B for medical consultation and dialogue tasks, providing a deployable solution for automated patient interaction.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "medical_consultation",
        "dialogue_system"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/xionghonglin/DoctorGLM",
      "help_website": [],
      "license": null,
      "tags": [
        "chatglm",
        "medical-dialogue",
        "chinese-medical"
      ],
      "id": 124
    },
    {
      "name": "BenchX",
      "one_line_profile": "Unified Benchmark Framework for Medical Vision-Language Pretraining on Chest X-Rays",
      "detailed_description": "A benchmarking framework designed to evaluate medical vision-language pretraining models specifically on Chest X-Ray data, standardizing the assessment of multimodal medical AI.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "vision_language_pretraining",
        "benchmark",
        "chest_xray"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/yangzhou12/BenchX",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "chest-xray",
        "vision-language",
        "benchmark"
      ],
      "id": 125
    },
    {
      "name": "HealthFlow",
      "one_line_profile": "Self-evolving AI agent framework for autonomous healthcare research",
      "detailed_description": "HealthFlow is a self-evolving AI agent equipped with meta-planning capabilities designed to automate healthcare research tasks. It can perform data analysis, hypothesis generation, and workflow planning for medical research.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "scientific_modeling",
        "autonomous_research",
        "workflow_planning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/yhzhu99/HealthFlow",
      "help_website": [],
      "license": null,
      "tags": [
        "ai-agent",
        "healthcare",
        "autonomous-research",
        "llm"
      ],
      "id": 126
    },
    {
      "name": "MedAgentBoard",
      "one_line_profile": "Benchmark suite for evaluating multi-agent collaboration in medical tasks",
      "detailed_description": "MedAgentBoard is a benchmarking framework designed to evaluate the performance of multi-agent systems in diverse medical scenarios, comparing them against conventional methods.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "benchmarking",
        "model_evaluation",
        "multi_agent_system"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/yhzhu99/MedAgentBoard",
      "help_website": [],
      "license": null,
      "tags": [
        "benchmark",
        "multi-agent",
        "medical-ai",
        "evaluation"
      ],
      "id": 127
    },
    {
      "name": "llm4healthcare",
      "one_line_profile": "Framework for zero-shot clinical prediction using LLMs on EHR data",
      "detailed_description": "A library for prompting Large Language Models to perform zero-shot clinical prediction tasks using structured longitudinal Electronic Health Record (EHR) data.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "clinical_prediction",
        "ehr_analysis",
        "zero_shot_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/yhzhu99/llm4healthcare",
      "help_website": [],
      "license": null,
      "tags": [
        "ehr",
        "clinical-prediction",
        "llm",
        "zero-shot"
      ],
      "id": 128
    },
    {
      "name": "MEDFAIR",
      "one_line_profile": "Benchmarking framework for fairness in medical imaging models",
      "detailed_description": "MEDFAIR is a codebase and benchmark suite for evaluating the fairness of machine learning models in medical imaging, providing standardized datasets and evaluation metrics.",
      "domains": [
        "H2"
      ],
      "subtask_category": [
        "benchmarking",
        "fairness_evaluation",
        "medical_imaging"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ys-zong/MEDFAIR",
      "help_website": [],
      "license": null,
      "tags": [
        "medical-imaging",
        "fairness",
        "benchmark",
        "deep-learning"
      ],
      "id": 129
    },
    {
      "name": "STELLA",
      "one_line_profile": "Self-evolving LLM agent for biomedical research and coding",
      "detailed_description": "STELLA is a self-evolving Large Language Model agent designed to assist in biomedical research tasks, including literature mining and code generation for scientific discovery.",
      "domains": [
        "H2",
        "H2-02"
      ],
      "subtask_category": [
        "scientific_discovery",
        "literature_mining",
        "code_generation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/zaixizhang/STELLA",
      "help_website": [],
      "license": null,
      "tags": [
        "llm-agent",
        "biomedical-research",
        "self-evolving",
        "scientific-discovery"
      ],
      "id": 130
    },
    {
      "name": "MedSegBench",
      "one_line_profile": "Standardized library for medical image segmentation datasets and benchmarking",
      "detailed_description": "MedSegBench is a Python library that provides easy access to standardized medical segmentation datasets across different modalities and tools for benchmarking segmentation models.",
      "domains": [
        "H2"
      ],
      "subtask_category": [
        "image_segmentation",
        "data_loading",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/zekikus/MedSegBench",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "medical-segmentation",
        "datasets",
        "benchmark",
        "computer-vision"
      ],
      "id": 131
    }
  ]
}
{
  "generated_at": "2025-12-16T10:36:44.165074+08:00",
  "metadata": {
    "leaf_cluster": {
      "leaf_cluster_id": "H2",
      "leaf_cluster_name": "数字健康-临床文本与多模态生态",
      "domain": "Digital Health",
      "typical_objects": "EHR/text+image",
      "task_chain": "抽取→标准化→检索→问答→评测",
      "tool_form": "NLP/检索 + 合规/脱敏"
    },
    "unit": {
      "unit_id": "H2-05",
      "unit_name": "多模态融合（影像+文本）",
      "target_scale": "150–350",
      "coverage_tools": "multimodal models"
    },
    "search": {
      "target_candidates": 350,
      "queries": [
        "[GH] PLIP",
        "[GH] RadFM",
        "[GH] BioViL",
        "[GH] GLoRIA",
        "[GH] ConVIRT",
        "[GH] Med-Flamingo",
        "[GH] LLaVA-Med",
        "[GH] MedCLIP",
        "[GH] medical vision language pretraining",
        "[GH] multimodal medical imaging",
        "[GH] radiology report generation",
        "[GH] medical visual question answering",
        "[GH] Med-CLIP",
        "[GH] pathology image text",
        "[GH] multimodal ehr fusion",
        "[GH] medical image captioning",
        "[GH] clinical multimodal learning",
        "[GH] chest x-ray report generation",
        "[GH] medical cross-modal retrieval",
        "[GH] multimodal foundation models medical",
        "[WEB] medical vision language models github",
        "[WEB] radiology report generation tools github",
        "[WEB] multimodal medical image analysis github",
        "[WEB] MedCLIP medical contrastive learning github",
        "[WEB] medical visual question answering datasets models github"
      ],
      "total_candidates": 731,
      "tool_candidates": 258,
      "final_tools": 100
    }
  },
  "tools": [
    {
      "name": "Med-VQA",
      "one_line_profile": "Medical Visual Question Answering model via conditional reasoning",
      "detailed_description": "A PyTorch implementation of a medical visual question answering (VQA) system that utilizes conditional reasoning to interpret medical images and answer clinical questions.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "medical_vqa",
        "clinical_reasoning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Awenbocc/med-vqa",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "vqa",
        "medical-imaging",
        "deep-learning"
      ],
      "id": 1
    },
    {
      "name": "M3D",
      "one_line_profile": "Multi-modal large language model for 3D medical image analysis",
      "detailed_description": "A comprehensive suite for 3D medical image analysis, featuring a multi-modal large language model (M3D-LaMed), a massive dataset (M3D-Data), and a benchmark (M3D-Bench) for tasks like report generation and VQA.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "medical_vqa",
        "report_generation",
        "3d_image_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/BAAI-DCAI/M3D",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "3d-medical-imaging",
        "multimodal-llm",
        "foundation-model"
      ],
      "id": 2
    },
    {
      "name": "Chinese-LLaVA-Med",
      "one_line_profile": "Large Chinese language-and-vision assistant for biomedicine",
      "detailed_description": "A multimodal large language model specifically designed for the Chinese biomedical domain, capable of processing image-text inputs for tasks such as modality classification and report generation.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "multimodal_chat",
        "report_generation",
        "biomedical_vqa"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/BUAADreamer/Chinese-LLaVA-Med",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "multimodal",
        "chinese-medical-nlp"
      ],
      "id": 3
    },
    {
      "name": "Med-CLIP",
      "one_line_profile": "Contrastive language-image pretraining for medical data",
      "detailed_description": "An implementation of the CLIP framework adapted for medical imaging and text, enabling zero-shot classification and cross-modal retrieval in the medical domain.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "image_text_alignment",
        "zero_shot_classification"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/CHB-learner/Med-CLIP",
      "help_website": [],
      "license": null,
      "tags": [
        "clip",
        "medical-imaging",
        "contrastive-learning"
      ],
      "id": 4
    },
    {
      "name": "RaDialog",
      "one_line_profile": "Large vision-language model for radiology report generation",
      "detailed_description": "A specialized vision-language model designed for generating radiology reports and engaging in conversational assistance regarding radiological images.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "report_generation",
        "radiology_assistant"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ChantalMP/RaDialog",
      "help_website": [],
      "license": null,
      "tags": [
        "radiology",
        "report-generation",
        "vlm"
      ],
      "id": 5
    },
    {
      "name": "HealthGPT",
      "one_line_profile": "Medical large vision-language model for comprehension and generation",
      "detailed_description": "A medical multimodal foundation model that unifies comprehension and generation tasks via heterogeneous knowledge adaptation, suitable for various medical vision-language tasks.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "medical_vqa",
        "report_generation",
        "medical_reasoning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/DCDmllm/HealthGPT",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "foundation-model",
        "multimodal",
        "health-ai"
      ],
      "id": 6
    },
    {
      "name": "Med-MAT",
      "one_line_profile": "Compositional generalization framework for medical multimodal LLMs",
      "detailed_description": "A framework exploring and enhancing the compositional generalization capabilities of multimodal large language models specifically for medical imaging tasks.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "medical_vqa",
        "model_generalization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/FreedomIntelligence/Med-MAT",
      "help_website": [],
      "license": null,
      "tags": [
        "multimodal-llm",
        "medical-imaging",
        "generalization"
      ],
      "id": 7
    },
    {
      "name": "COMG_model",
      "one_line_profile": "Complex organ mask guided radiology report generation",
      "detailed_description": "A model for generating radiology reports that utilizes complex organ masks to guide the generation process, improving anatomical accuracy.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "report_generation",
        "image_segmentation_guidance"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/GaryGuTC/COMG_model",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "radiology",
        "report-generation",
        "mask-guided"
      ],
      "id": 8
    },
    {
      "name": "LaPA",
      "one_line_profile": "Latent prompt assist model for medical visual question answering",
      "detailed_description": "A medical VQA model that employs latent prompt assistance to improve question answering performance on medical images.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "medical_vqa",
        "prompt_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/GaryGuTC/LaPA_model",
      "help_website": [],
      "license": null,
      "tags": [
        "vqa",
        "medical-imaging",
        "prompt-tuning"
      ],
      "id": 9
    },
    {
      "name": "HERGen",
      "one_line_profile": "Radiology report generation with longitudinal data",
      "detailed_description": "A framework for generating radiology reports that incorporates longitudinal patient data (historical records) to improve report quality and context.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "report_generation",
        "longitudinal_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/HKU-MedAI/HERGen",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "radiology",
        "longitudinal-data",
        "report-generation"
      ],
      "id": 10
    },
    {
      "name": "Federated-Retina-Screening",
      "one_line_profile": "Federated self-supervised multimodal retina screening framework",
      "detailed_description": "A research framework for federated learning in retinal disease screening, addressing challenges like label noise and differential privacy in a multimodal setting.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "federated_learning",
        "disease_screening",
        "retinal_imaging"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Hazrat-Ali9/Federated-Self-Supervised-Multimodal-Retina-Screening-under-Label-Noise-and-Differential-Privacy",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "federated-learning",
        "retina",
        "privacy"
      ],
      "id": 11
    },
    {
      "name": "MedCLIP-SAM",
      "one_line_profile": "Medical image segmentation using CLIP and SAM",
      "detailed_description": "A framework bridging MedCLIP and the Segment Anything Model (SAM) for text-guided medical image segmentation.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "image_segmentation",
        "text_guided_segmentation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/HealthX-Lab/MedCLIP-SAM",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "segmentation",
        "sam",
        "clip",
        "medical-imaging"
      ],
      "id": 12
    },
    {
      "name": "MedCLIP-SAMv2",
      "one_line_profile": "Advanced text-guided medical image segmentation model",
      "detailed_description": "The second version of MedCLIP-SAM, offering improved performance for text-prompted segmentation in medical imaging.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "image_segmentation",
        "text_guided_segmentation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/HealthX-Lab/MedCLIP-SAMv2",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "segmentation",
        "sam",
        "clip"
      ],
      "id": 13
    },
    {
      "name": "EKAID",
      "one_line_profile": "Expert knowledge-aware difference-aware medical VQA",
      "detailed_description": "A medical VQA model focusing on difference-aware reasoning and expert knowledge integration for analyzing changes in medical images.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "medical_vqa",
        "difference_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Holipori/EKAID",
      "help_website": [],
      "license": null,
      "tags": [
        "vqa",
        "medical-imaging",
        "knowledge-graph"
      ],
      "id": 14
    },
    {
      "name": "TV-SAM",
      "one_line_profile": "Zero-shot segmentation for multimodal medical images",
      "detailed_description": "A zero-shot segmentation algorithm leveraging the Segment Anything Model (SAM) adapted for multimodal medical imaging tasks.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "image_segmentation",
        "zero_shot_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/JZK00/TV-SAM",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "segmentation",
        "zero-shot",
        "multimodal"
      ],
      "id": 15
    },
    {
      "name": "MedCLIP",
      "one_line_profile": "Multi-modal CLIP model trained on the medical dataset ROCO",
      "detailed_description": "A vision-language pre-training model based on CLIP, fine-tuned on the ROCO (Radiology Objects in COntext) dataset to align medical images and textual descriptions for tasks like retrieval and classification.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "medical_vision_language_pretraining",
        "image_text_retrieval"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/Kaushalya/medclip",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "medclip",
        "clip",
        "medical-imaging",
        "roco"
      ],
      "id": 16
    },
    {
      "name": "Self-Guided-Framework",
      "one_line_profile": "Self-guided framework for radiology report generation",
      "detailed_description": "Implementation of a self-guided framework for generating radiology reports from medical images, as presented at MICCAI 2022. It focuses on improving the accuracy and clinical relevance of generated text.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "radiology_report_generation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/LijunRio/A-Self-Guided-Framework",
      "help_website": [],
      "license": null,
      "tags": [
        "radiology-report-generation",
        "miccai",
        "medical-imaging"
      ],
      "id": 17
    },
    {
      "name": "RaTEScore",
      "one_line_profile": "Metric for evaluating radiology report generation",
      "detailed_description": "A specialized metric designed to evaluate the quality of generated radiology reports. It assesses the alignment between generated reports and ground truth in terms of medical entities and relations.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "evaluation_metric",
        "radiology_report_generation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/MAGIC-AI4Med/RaTEScore",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "evaluation-metric",
        "nlp",
        "radiology"
      ],
      "id": 18
    },
    {
      "name": "MediConfusion",
      "one_line_profile": "Dataset and evaluation code for probing reliability of multimodal medical models",
      "detailed_description": "A dataset and evaluation framework designed to test the reliability and hallucination tendencies of multimodal medical foundation models, specifically questioning whether AI radiologists can be trusted.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "model_evaluation",
        "hallucination_detection"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/MShahabSepehri/MediConfusion",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "reliability",
        "hallucination",
        "multimodal-medical-ai"
      ],
      "id": 19
    },
    {
      "name": "MSD-U-Net-GDC",
      "one_line_profile": "Generic U-Net implementation for Medical Segmentation Decathlon",
      "detailed_description": "A generic U-Net CNN architecture implementation using Generalized Dice Coefficient, designed for the Medical Segmentation Decathlon (MSD) challenge. It targets segmentation of organs like liver and spleen from 3D medical data.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "medical_image_segmentation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Maor-Oz/Medical-Segmentation-Decathlon-U-net-CNN-with-Generalized-Dice-Coefficient",
      "help_website": [],
      "license": null,
      "tags": [
        "u-net",
        "segmentation",
        "medical-segmentation-decathlon"
      ],
      "id": 20
    },
    {
      "name": "XProNet",
      "one_line_profile": "Cross-modal Prototype Driven Network for Radiology Report Generation",
      "detailed_description": "Official implementation of XProNet (ECCV 2022), a deep learning model that utilizes cross-modal prototypes to enhance radiology report generation from medical images.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "radiology_report_generation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Markin-Wang/XProNet",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "radiology-report-generation",
        "cross-modal",
        "deep-learning"
      ],
      "id": 21
    },
    {
      "name": "MedCLIP-Captioning",
      "one_line_profile": "Medical image captioning using OpenAI's CLIP",
      "detailed_description": "A project leveraging OpenAI's CLIP model adapted for medical image captioning tasks, enabling the generation of textual descriptions for medical imagery.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "medical_image_captioning"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/Mauville/MedCLIP",
      "help_website": [],
      "license": null,
      "tags": [
        "clip",
        "image-captioning",
        "medical-imaging"
      ],
      "id": 22
    },
    {
      "name": "MIU-VL",
      "one_line_profile": "Medical Image Understanding with Pretrained Vision Language Models",
      "detailed_description": "Code for the ICLR 2023 paper 'Medical Image Understanding with Pretrained Vision Language Models', providing a comprehensive study and models for applying VLMs to medical image analysis.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "medical_image_understanding",
        "vision_language_models"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/MembrAI/MIU-VL",
      "help_website": [],
      "license": null,
      "tags": [
        "vlm",
        "medical-imaging",
        "iclr2023"
      ],
      "id": 23
    },
    {
      "name": "PLIP",
      "one_line_profile": "Pathology Language and Image Pre-Training foundation model",
      "detailed_description": "A large-scale vision and language foundation model specifically pre-trained for pathology. It enables extraction of visual and language features from pathology images and text descriptions, fine-tuned from CLIP.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "pathology_image_analysis",
        "vision_language_pretraining"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/PathologyFoundation/plip",
      "help_website": [],
      "license": null,
      "tags": [
        "pathology",
        "foundation-model",
        "clip",
        "computational-pathology"
      ],
      "id": 24
    },
    {
      "name": "TransBTS",
      "one_line_profile": "Multimodal Brain Tumor Segmentation Using Transformer",
      "detailed_description": "Implementation of TransBTS and TransBTSV2, utilizing transformers for 3D multimodal brain tumor segmentation from MRI scans. It addresses the limitations of CNNs in modeling long-range dependencies.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "medical_image_segmentation",
        "brain_tumor_segmentation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Rubics-Xuan/TransBTS",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "transformer",
        "segmentation",
        "brain-tumor",
        "mri"
      ],
      "id": 25
    },
    {
      "name": "MedCLIP-Contrastive",
      "one_line_profile": "Contrastive Learning from Unpaired Medical Images and Texts",
      "detailed_description": "Implementation of MedCLIP (EMNLP 2022), a contrastive learning framework that decouples image-text pairing to leverage unpaired medical data for pre-training effective medical vision-language models.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "medical_vision_language_pretraining",
        "contrastive_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/RyanWangZf/MedCLIP",
      "help_website": [],
      "license": null,
      "tags": [
        "medclip",
        "contrastive-learning",
        "unpaired-data"
      ],
      "id": 26
    },
    {
      "name": "LABODOCK",
      "one_line_profile": "Colab-Based Molecular Docking Tools",
      "detailed_description": "A suite of Jupyter Notebooks designed to run molecular docking simulations (using tools like AutoDock Vina) directly in Google Colab, facilitating accessible drug discovery workflows.",
      "domains": [
        "H2"
      ],
      "subtask_category": [
        "molecular_docking",
        "drug_discovery"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/RyanZR/labodock",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "molecular-docking",
        "colab",
        "drug-discovery"
      ],
      "id": 27
    },
    {
      "name": "u2Tokenizer",
      "one_line_profile": "Multiscale multimodal large language models for radiology report generation",
      "detailed_description": "A framework utilizing multiscale multimodal large language models to generate radiology reports. It focuses on tokenizing visual information effectively to drive language generation.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "radiology_report_generation",
        "multimodal_llm"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Siyou-Li/u2Tokenizer",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mllm",
        "radiology",
        "report-generation"
      ],
      "id": 28
    },
    {
      "name": "GREEN",
      "one_line_profile": "Radiology report generation metric using language models",
      "detailed_description": "A metric for evaluating radiology report generation that leverages the natural language understanding capabilities of large language models to identify and explain clinically significant errors.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "evaluation_metric",
        "radiology_report_generation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Stanford-AIMI/GREEN",
      "help_website": [],
      "license": null,
      "tags": [
        "evaluation",
        "llm",
        "radiology"
      ],
      "id": 29
    },
    {
      "name": "HC-LLM",
      "one_line_profile": "Framework for Longitudinal Radiology Report Generation",
      "detailed_description": "A framework designed for generating longitudinal radiology reports, taking into account the patient's history and changes over time, rather than just a single time-point image.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "longitudinal_report_generation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/TengfeiLiu966/HC-LLM",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "longitudinal-analysis",
        "radiology",
        "report-generation"
      ],
      "id": 30
    },
    {
      "name": "Medical-Image-Fusion",
      "one_line_profile": "Laplacian Re-Decomposition for Multimodal Medical Image Fusion",
      "detailed_description": "MATLAB implementation of a Laplacian Re-Decomposition method for fusing multimodal medical images (e.g., CT and MRI) to combine complementary information.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "medical_image_fusion"
      ],
      "application_level": "solver",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/WHU-lab/Medical-Image-Fusion",
      "help_website": [],
      "license": null,
      "tags": [
        "image-fusion",
        "matlab",
        "medical-imaging"
      ],
      "id": 31
    },
    {
      "name": "CMCRL",
      "one_line_profile": "Cross-Modal Causal Representation Learning for Radiology Report Generation",
      "detailed_description": "Implementation of a cross-modal causal representation learning approach for radiology report generation, aiming to improve the logical consistency and causal reasoning in generated reports.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "radiology_report_generation",
        "causal_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/WissingChen/CMCRL",
      "help_website": [],
      "license": null,
      "tags": [
        "causal-inference",
        "report-generation",
        "cross-modal"
      ],
      "id": 32
    },
    {
      "name": "Libra",
      "one_line_profile": "Temporally-aware MLLM toolkit for Biomedical Radiology Analysis",
      "detailed_description": "A flexible toolkit featuring a Temporally-aware Multimodal Large Language Model (MLLM) for biomedical radiology analysis and report generation. It supports real-time validation and smart model saving.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "radiology_analysis",
        "report_generation",
        "mllm"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/X-iZhang/Libra",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "mllm",
        "toolkit",
        "radiology"
      ],
      "id": 33
    },
    {
      "name": "VQA-Med-2019",
      "one_line_profile": "Dataset and resources for Medical Visual Question Answering 2019",
      "detailed_description": "The official repository for the VQA-Med 2019 challenge, providing datasets and evaluation resources for visual question answering tasks in the medical domain.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "medical_vqa",
        "dataset"
      ],
      "application_level": "dataset",
      "primary_language": null,
      "repo_url": "https://github.com/abachaa/VQA-Med-2019",
      "help_website": [],
      "license": null,
      "tags": [
        "vqa",
        "medical-dataset",
        "challenge"
      ],
      "id": 34
    },
    {
      "name": "cvt2distilgpt2",
      "one_line_profile": "Chest X-Ray Report Generation leveraging Warm-Starting",
      "detailed_description": "A model implementation for improving chest X-ray report generation by leveraging warm-starting techniques with CvT (Convolutional Vision Transformer) and DistilGPT2.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "radiology_report_generation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/aehrc/cvt2distilgpt2",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "chest-x-ray",
        "report-generation",
        "transformer"
      ],
      "id": 35
    },
    {
      "name": "CXRMate",
      "one_line_profile": "Chest X-Ray report generation tool with longitudinal data support and semantic rewards",
      "detailed_description": "A framework for generating chest X-ray reports that incorporates longitudinal patient data and optimizes for semantic similarity, developed by the Australian e-Health Research Centre.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "report_generation",
        "medical_nlp"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/aehrc/cxrmate",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "chest-x-ray",
        "report-generation",
        "longitudinal-data"
      ],
      "id": 36
    },
    {
      "name": "MMedPO",
      "one_line_profile": "Preference optimization framework for aligning medical vision-language models",
      "detailed_description": "A tool implementing Clinical-Aware Multimodal Preference Optimization to align medical vision-language models with clinical preferences, enhancing reliability and accuracy.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "model_alignment",
        "vlm_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/aiming-lab/MMedPO",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "preference-optimization",
        "medical-vlm",
        "alignment"
      ],
      "id": 37
    },
    {
      "name": "MedVQA (MICCAI19)",
      "one_line_profile": "Medical Visual Question Answering framework overcoming data limitations",
      "detailed_description": "Implementation of a Medical Visual Question Answering (MedVQA) system using Meta-Learning (MAML) and Denoising Auto-Encoders to handle limited medical data.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "visual_question_answering",
        "meta_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/aioz-ai/MICCAI19-MedVQA",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "medvqa",
        "meta-learning",
        "medical-imaging"
      ],
      "id": 38
    },
    {
      "name": "MMQ (MICCAI21)",
      "one_line_profile": "Meta-model quantifying method for Medical Visual Question Answering",
      "detailed_description": "A framework for Medical VQA that utilizes multiple meta-models to quantify uncertainty and improve answer reliability.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "visual_question_answering",
        "uncertainty_quantification"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/aioz-ai/MICCAI21_MMQ",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "medvqa",
        "meta-learning",
        "uncertainty"
      ],
      "id": 39
    },
    {
      "name": "MedICaT",
      "one_line_profile": "Large-scale dataset and toolkit for medical image captioning and text references",
      "detailed_description": "A dataset and associated tools for processing medical images, captions, subfigure annotations, and inline textual references to support multimodal medical AI research.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "dataset_access",
        "image_captioning"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/allenai/medicat",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "medical-dataset",
        "image-captioning",
        "multimodal"
      ],
      "id": 40
    },
    {
      "name": "APA2Seg-Net",
      "one_line_profile": "Anatomy-guided multimodal registration and segmentation network",
      "detailed_description": "A deep learning framework for joint segmentation and registration of multimodal medical images (e.g., CBCT/MR) without ground truth segmentation, using anatomy-guided priors.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "image_registration",
        "image_segmentation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/bbbbbbzhou/APA2Seg-Net",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "registration",
        "segmentation",
        "multimodal-imaging"
      ],
      "id": 41
    },
    {
      "name": "MedGround-R1",
      "one_line_profile": "Medical image grounding tool via spatial-semantic rewarded policy optimization",
      "detailed_description": "A framework for medical image grounding that uses Group Relative Policy Optimization with spatial and semantic rewards to improve localization accuracy.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "image_grounding",
        "multimodal_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/bio-mlhui/MedGround-R1",
      "help_website": [],
      "license": null,
      "tags": [
        "grounding",
        "reinforcement-learning",
        "medical-imaging"
      ],
      "id": 42
    },
    {
      "name": "RadFM",
      "one_line_profile": "Generalist foundation model for 2D and 3D radiology data",
      "detailed_description": "A multimodal foundation model capable of handling both 2D and 3D medical imaging data for various radiology tasks, leveraging web-scale data.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "foundation_model",
        "radiology_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/chaoyi-wu/RadFM",
      "help_website": [
        "https://chaoyi-wu.github.io/RadFM/"
      ],
      "license": "MIT",
      "tags": [
        "foundation-model",
        "radiology",
        "3d-imaging"
      ],
      "id": 43
    },
    {
      "name": "FORTE",
      "one_line_profile": "Multimodal LLM framework for 3D brain CT report generation",
      "detailed_description": "A holistic framework designed for generating radiology reports from 3D brain CT scans using multimodal Large Language Models.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "report_generation",
        "3d_imaging"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/charlierabea/FORTE",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "brain-ct",
        "report-generation",
        "multimodal-llm"
      ],
      "id": 44
    },
    {
      "name": "MediCLIP",
      "one_line_profile": "Adapted CLIP model for few-shot medical image anomaly detection",
      "detailed_description": "A tool that adapts the CLIP vision-language model for the specific task of few-shot anomaly detection in medical images.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "anomaly_detection",
        "few_shot_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/cnulab/MediCLIP",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "clip",
        "anomaly-detection",
        "few-shot"
      ],
      "id": 45
    },
    {
      "name": "FactMM-RAG",
      "one_line_profile": "Fact-aware multimodal RAG for radiology report generation",
      "detailed_description": "A retrieval-augmented generation (RAG) framework specifically designed for radiology report generation, focusing on factual accuracy and multimodal data retrieval.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "report_generation",
        "retrieval_augmented_generation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/cxcscmu/FactMM-RAG",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "radiology-report",
        "fact-checking"
      ],
      "id": 46
    },
    {
      "name": "Kedro Multimodal Healthcare",
      "one_line_profile": "Kedro template for multimodal healthcare machine learning pipelines",
      "detailed_description": "A project template for building multimodal machine learning pipelines in healthcare using Kedro, facilitating the combination of reports, tabular data, and images.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "pipeline_orchestration",
        "multimodal_fusion"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/dermatologist/kedro-multimodal",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "kedro",
        "pipeline",
        "healthcare-ml"
      ],
      "id": 47
    },
    {
      "name": "Expert-CFG",
      "one_line_profile": "Expert-controlled classifier-free guidance for medical VQA",
      "detailed_description": "A method for reliable Medical Visual Question Answering that uses expert knowledge to control classifier-free guidance during generation.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "visual_question_answering",
        "model_guidance"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ecoxial2007/Expert-CFG",
      "help_website": [],
      "license": null,
      "tags": [
        "medvqa",
        "classifier-free-guidance",
        "expert-knowledge"
      ],
      "id": 48
    },
    {
      "name": "ConVIRT-pytorch",
      "one_line_profile": "Contrastive learning framework for medical image-text pretraining",
      "detailed_description": "A PyTorch implementation of ConVIRT (Contrastive VIsual Representation Learning from Text), a method for learning medical visual representations by exploiting paired descriptive text.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "contrastive_learning",
        "representation_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/edreisMD/ConVIRT-pytorch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "contrastive-learning",
        "medical-imaging",
        "pretraining"
      ],
      "id": 49
    },
    {
      "name": "MICFormer",
      "one_line_profile": "Multimodal transformer for medical image segmentation",
      "detailed_description": "A transformer-based model designed for medical image segmentation that effectively interacts and fuses multimodal information.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "image_segmentation",
        "multimodal_fusion"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/fxxJuses/MICFormer",
      "help_website": [],
      "license": null,
      "tags": [
        "transformer",
        "segmentation",
        "multimodal"
      ],
      "id": 50
    },
    {
      "name": "CMSA-MTPT-4-MedicalVQA",
      "one_line_profile": "Medical VQA with multi-task pre-training and cross-modal self-attention",
      "detailed_description": "A framework for Medical Visual Question Answering that leverages multi-task pre-training and a cross-modal self-attention mechanism to improve performance.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "visual_question_answering",
        "pre_training"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/haifangong/CMSA-MTPT-4-MedicalVQA",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "medvqa",
        "self-attention",
        "multi-task-learning"
      ],
      "id": 51
    },
    {
      "name": "EKAGen",
      "one_line_profile": "Knowledge-enhanced radiology report generation model",
      "detailed_description": "A radiology report generation tool that incorporates instance-level expert knowledge and aggregate discriminative attention to improve report quality.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "report_generation",
        "knowledge_enhancement"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/hnjzbss/EKAGen",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "radiology-report",
        "attention-mechanism",
        "expert-knowledge"
      ],
      "id": 52
    },
    {
      "name": "CT2Rep",
      "one_line_profile": "Automated radiology report generation for 3D CT imaging",
      "detailed_description": "A deep learning model specifically designed to generate comprehensive radiology reports from 3D CT volumes.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "report_generation",
        "3d_imaging"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ibrahimethemhamamci/CT2Rep",
      "help_website": [],
      "license": null,
      "tags": [
        "ct-scan",
        "report-generation",
        "3d-medical-imaging"
      ],
      "id": 53
    },
    {
      "name": "OAProgression",
      "one_line_profile": "Multimodal prediction of knee osteoarthritis progression",
      "detailed_description": "A machine learning tool for predicting the progression of knee osteoarthritis by combining plain radiographs with clinical data.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "disease_progression_prediction",
        "multimodal_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/imedslab/OAProgression",
      "help_website": [],
      "license": null,
      "tags": [
        "osteoarthritis",
        "prognosis",
        "multimodal-learning"
      ],
      "id": 54
    },
    {
      "name": "Variational X-Ray Report Gen",
      "one_line_profile": "Variational topic inference for chest X-ray report generation",
      "detailed_description": "A model for generating chest X-ray reports that uses variational inference to model topics within the reports, improving diversity and accuracy.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "report_generation",
        "variational_inference"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ivonajdenkoska/variational-xray-report-gen",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "topic-modeling",
        "variational-inference",
        "chest-x-ray"
      ],
      "id": 55
    },
    {
      "name": "StructuredLight_3DfreehandUS",
      "one_line_profile": "Multimodal 3D freehand ultrasound and structured light imaging tool",
      "detailed_description": "A software tool for a multimodal medical imaging technique that combines 3D freehand ultrasound with structured light scanning.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "image_reconstruction",
        "multimodal_imaging"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/jhacsonmeza/StructuredLight_3DfreehandUS",
      "help_website": [],
      "license": null,
      "tags": [
        "ultrasound",
        "structured-light",
        "3d-reconstruction"
      ],
      "id": 56
    },
    {
      "name": "PeFoMed",
      "one_line_profile": "Parameter-efficient fine-tuning for medical VQA",
      "detailed_description": "A framework for parameter-efficient fine-tuning (PEFT) of multimodal large language models specifically for medical visual question answering tasks.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "visual_question_answering",
        "fine_tuning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/jinlHe/PeFoMed",
      "help_website": [],
      "license": null,
      "tags": [
        "peft",
        "medvqa",
        "multimodal-llm"
      ],
      "id": 57
    },
    {
      "name": "Multimodal Clinical Pretraining",
      "one_line_profile": "Multimodal pretraining framework for medical time series and clinical notes",
      "detailed_description": "Official implementation of the MLHC 2023 paper 'Multimodal Pretraining of Medical Time Series and Notes'. It provides a framework for pretraining models on multimodal clinical data to improve downstream tasks.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "pretraining",
        "representation_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/kingrc15/multimodal-clinical-pretraining",
      "help_website": [],
      "license": null,
      "tags": [
        "multimodal",
        "medical-time-series",
        "clinical-notes",
        "pretraining"
      ],
      "id": 58
    },
    {
      "name": "MHCA",
      "one_line_profile": "Multimodal Multi-Head Convolutional Attention for medical image super-resolution",
      "detailed_description": "Implementation of a multimodal attention mechanism designed to fuse features from different modalities for the task of medical image super-resolution.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "super_resolution",
        "image_fusion"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/lilygeorgescu/MHCA",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "super-resolution",
        "attention-mechanism",
        "multimodal-fusion"
      ],
      "id": 59
    },
    {
      "name": "GLoRIA",
      "one_line_profile": "Multimodal Global-Local Representation Learning for medical image recognition",
      "detailed_description": "A framework for label-efficient medical image recognition that learns global and local representations by contrasting image sub-regions and words in radiology reports.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "representation_learning",
        "image_classification",
        "zero_shot_classification"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/marshuang80/gloria",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "contrastive-learning",
        "multimodal",
        "medical-imaging",
        "representation-learning"
      ],
      "id": 60
    },
    {
      "name": "DeepGuide",
      "one_line_profile": "Deep multimodal guidance framework for medical image classification",
      "detailed_description": "Implementation of a deep multimodal guidance architecture that leverages auxiliary modalities to guide the learning process for medical image classification tasks.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "image_classification",
        "multimodal_learning"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/mayurmallya/DeepGuide",
      "help_website": [],
      "license": null,
      "tags": [
        "medical-classification",
        "multimodal-guidance",
        "deep-learning"
      ],
      "id": 61
    },
    {
      "name": "MIRA",
      "one_line_profile": "Medical RAG framework for multimodal medical reasoning",
      "detailed_description": "A medical Retrieval-Augmented Generation (RAG) framework that fuses image features and retrieved knowledge with dynamic context control to enhance factual accuracy in multimodal medical reasoning.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "visual_question_answering",
        "rag",
        "medical_reasoning"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/mbzuai-oryx/MIRA",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "multimodal-reasoning",
        "medical-vqa"
      ],
      "id": 62
    },
    {
      "name": "UniMed-CLIP",
      "one_line_profile": "Unified image-text pretraining paradigm for diverse medical imaging modalities",
      "detailed_description": "A unified pretraining framework adapting CLIP for the medical domain, designed to handle diverse medical imaging modalities and align them with textual reports.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "pretraining",
        "representation_learning",
        "zero_shot_classification"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/mbzuai-oryx/UniMed-CLIP",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "clip",
        "medical-pretraining",
        "multimodal-alignment"
      ],
      "id": 63
    },
    {
      "name": "CMC",
      "one_line_profile": "Cross-modality collaboration for semi-supervised medical image segmentation",
      "detailed_description": "Implementation of a robust semi-supervised multimodal medical image segmentation framework via Cross Modality Collaboration (CMC), as presented at MICCAI 2024.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "image_segmentation",
        "semi_supervised_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/med-air/CMC",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "segmentation",
        "multimodal",
        "semi-supervised"
      ],
      "id": 64
    },
    {
      "name": "LLaVA-Med",
      "one_line_profile": "Large Language-and-Vision Assistant for Biomedicine",
      "detailed_description": "A large language-and-vision model trained for the biomedical domain, capable of performing multimodal chat and visual question answering on biomedical images.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "visual_question_answering",
        "multimodal_chat",
        "biomedical_reasoning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/LLaVA-Med",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "llm",
        "vlm",
        "biomedicine",
        "multimodal-chat"
      ],
      "id": 65
    },
    {
      "name": "RadFact",
      "one_line_profile": "Factual correctness metric for radiology report generation",
      "detailed_description": "A metric suite leveraging the logical inference capabilities of LLMs to evaluate the factual correctness of generated radiology reports, supporting both grounded and ungrounded evaluation.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "evaluation",
        "report_generation",
        "metric_calculation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/RadFact",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "evaluation-metric",
        "radiology-reports",
        "factual-correctness"
      ],
      "id": 66
    },
    {
      "name": "MLRG",
      "one_line_profile": "Enhanced contrastive learning with multi-view longitudinal data for report generation",
      "detailed_description": "A framework for chest X-ray report generation that leverages multi-view longitudinal data and enhanced contrastive learning to improve report quality.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "report_generation",
        "contrastive_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/mk-runner/MLRG",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "chest-x-ray",
        "report-generation",
        "longitudinal-data"
      ],
      "id": 67
    },
    {
      "name": "SEI",
      "one_line_profile": "Structural entities extraction and patient indications incorporation for report generation",
      "detailed_description": "A model for chest X-ray report generation that explicitly extracts structural entities and incorporates patient indications to generate more accurate and clinically relevant reports.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "report_generation",
        "entity_extraction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/mk-runner/SEI",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "chest-x-ray",
        "report-generation",
        "clinical-indications"
      ],
      "id": 68
    },
    {
      "name": "DCL",
      "one_line_profile": "Dynamic graph enhanced contrastive learning for chest X-ray report generation",
      "detailed_description": "Official implementation of the CVPR 2023 paper proposing a dynamic graph enhanced contrastive learning approach for automated chest X-ray report generation.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "report_generation",
        "contrastive_learning",
        "graph_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/mlii0117/DCL",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "chest-x-ray",
        "report-generation",
        "dynamic-graph"
      ],
      "id": 69
    },
    {
      "name": "CDGPT2-CXR",
      "one_line_profile": "Automated radiology report generation using conditioned transformers",
      "detailed_description": "Implementation of the CDGPT2 model for automated radiology report generation, utilizing conditioned transformers to generate reports from chest X-ray images.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "report_generation",
        "text_generation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/omar-mohamed/GPT2-Chest-X-Ray-Report-Generation",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "gpt2",
        "radiology-report",
        "chest-x-ray"
      ],
      "id": 70
    },
    {
      "name": "MIU-VL",
      "one_line_profile": "Medical image understanding with pretrained vision-language models",
      "detailed_description": "A comprehensive study and framework for medical image understanding using pretrained vision-language models, covering various downstream tasks and modalities.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "image_understanding",
        "representation_learning",
        "visual_question_answering"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/openmedlab/MIU-VL",
      "help_website": [],
      "license": null,
      "tags": [
        "vision-language-models",
        "medical-image-understanding",
        "pretraining"
      ],
      "id": 71
    },
    {
      "name": "Medical-AI (Template-based Report Gen)",
      "one_line_profile": "Clinically correct report generation from chest X-rays using templates",
      "detailed_description": "Code for the MLMI 2021 paper focusing on generating clinically correct radiology reports from chest X-rays using a template-based approach to ensure factual accuracy.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "report_generation",
        "template_based_generation"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/pdpino/medical-ai",
      "help_website": [],
      "license": null,
      "tags": [
        "chest-x-ray",
        "report-generation",
        "clinical-accuracy"
      ],
      "id": 72
    },
    {
      "name": "M2I2",
      "one_line_profile": "Self-supervised vision-language pretraining for medical visual question answering",
      "detailed_description": "Implementation of a self-supervised vision-language pretraining framework specifically designed to improve performance on medical visual question answering tasks.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "visual_question_answering",
        "pretraining",
        "self_supervised_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/pengfeiliHEU/M2I2",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "medical-vqa",
        "vision-language",
        "pretraining"
      ],
      "id": 73
    },
    {
      "name": "MUMC",
      "one_line_profile": "Masked vision and language pre-training for medical visual question answering",
      "detailed_description": "A framework for medical visual question answering that utilizes masked vision and language pre-training with unimodal and multimodal contrastive losses.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "visual_question_answering",
        "pretraining",
        "contrastive_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/pengfeiliHEU/MUMC",
      "help_website": [],
      "license": null,
      "tags": [
        "medical-vqa",
        "masked-modeling",
        "contrastive-learning"
      ],
      "id": 74
    },
    {
      "name": "PLIP",
      "one_line_profile": "Protein-Ligand Interaction Profiler",
      "detailed_description": "A tool to analyze and visualize non-covalent protein-ligand interactions in PDB files. It detects interactions such as hydrogen bonds, hydrophobic contacts, and pi-stacking.",
      "domains": [
        "H1",
        "H1-01"
      ],
      "subtask_category": [
        "interaction_analysis",
        "structural_biology",
        "visualization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pharmai/plip",
      "help_website": [
        "https://plip-tool.biotec.tu-dresden.de/"
      ],
      "license": "GPL-2.0",
      "tags": [
        "protein-ligand-interaction",
        "bioinformatics",
        "structural-biology"
      ],
      "id": 75
    },
    {
      "name": "ROCO",
      "one_line_profile": "Large-scale multimodal medical image dataset with radiology objects in context",
      "detailed_description": "The Radiology Objects in COntext (ROCO) dataset is a large-scale multimodal medical image dataset designed to support vision-language tasks in the medical domain. It contains image-caption pairs extracted from PubMed Central Open Access articles, enabling research in image retrieval, captioning, and multimodal pre-training.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "dataset",
        "multimodal_learning"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/razorx89/roco-dataset",
      "help_website": [],
      "license": null,
      "tags": [
        "dataset",
        "radiology",
        "multimodal",
        "image-captioning"
      ],
      "id": 76
    },
    {
      "name": "GlorIA",
      "one_line_profile": "Global-Local Representations for Image-text Alignment in medical vision-language pre-training",
      "detailed_description": "GlorIA is a multimodal pre-training framework that learns global and local representations for medical image-text alignment. It is designed to improve performance on downstream tasks such as image retrieval, classification, and segmentation by capturing fine-grained semantic correspondences between radiology images and reports.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "multimodal_pretraining",
        "image_text_alignment"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/rvalgreen/GlorIA",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "vision-language",
        "pre-training",
        "medical-imaging",
        "contrastive-learning"
      ],
      "id": 77
    },
    {
      "name": "PubMedCLIP",
      "one_line_profile": "CLIP model fine-tuned on the ROCO dataset for medical vision-language tasks",
      "detailed_description": "PubMedCLIP adapts the CLIP architecture to the medical domain by fine-tuning on the ROCO dataset (image-caption pairs from PubMed). It serves as a specialized foundation model for medical visual question answering (VQA) and image retrieval tasks.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "multimodal_pretraining",
        "visual_question_answering"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/sarahESL/PubMedCLIP",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "clip",
        "medical-vqa",
        "fine-tuning",
        "roco"
      ],
      "id": 78
    },
    {
      "name": "Consistency-VQA",
      "one_line_profile": "Consistency-preserving Visual Question Answering in Medical Imaging",
      "detailed_description": "A framework for Medical Visual Question Answering (MedVQA) that incorporates a consistency-preserving module. It aims to improve the reliability of VQA models by ensuring that answers to semantically related questions are consistent, addressing a common failure mode in VQA systems.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "visual_question_answering",
        "model_consistency"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/sergiotasconmorales/consistency_vqa",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "vqa",
        "medical-imaging",
        "consistency-learning"
      ],
      "id": 79
    },
    {
      "name": "BrainMVP",
      "one_line_profile": "Multimodal Vision-Language Pre-training for mpMRI brain image analysis",
      "detailed_description": "BrainMVP is a PyTorch implementation of a multimodal pre-training framework specifically designed for multi-parametric MRI (mpMRI) brain image analysis. It leverages vision-language techniques to learn robust representations for tasks like tumor segmentation and classification.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "multimodal_pretraining",
        "mri_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/shaohao011/BrainMVP",
      "help_website": [],
      "license": null,
      "tags": [
        "mri",
        "brain-tumor",
        "vision-language",
        "pre-training"
      ],
      "id": 80
    },
    {
      "name": "H-DenseFormer",
      "one_line_profile": "Hybrid Densely Connected Transformer for multimodal medical image segmentation",
      "detailed_description": "H-DenseFormer is a deep learning model that combines dense connections with transformer mechanisms for tumor segmentation using multimodal medical images. It aims to effectively fuse information from different imaging modalities to improve segmentation accuracy.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "image_segmentation",
        "multimodal_fusion"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/shijun18/H-DenseFormer",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "transformer",
        "segmentation",
        "multimodal",
        "tumor"
      ],
      "id": 81
    },
    {
      "name": "DilRAN",
      "one_line_profile": "Attention-based Multi-Scale Feature Learning Framework for Multimodal Medical Image Fusion",
      "detailed_description": "DilRAN (Dilated Residual Attention Network) is a framework for fusing multimodal medical images (e.g., MRI and CT). It utilizes attention mechanisms and multi-scale feature learning to synthesize fused images that retain complementary information from source modalities.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "image_fusion",
        "multimodal_processing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/simonZhou86/dilran",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "image-fusion",
        "attention-mechanism",
        "medical-imaging"
      ],
      "id": 82
    },
    {
      "name": "R2-LLM",
      "one_line_profile": "Bootstrapping Large Language Models for Radiology Report Generation",
      "detailed_description": "R2-LLM is a framework that leverages Large Language Models (LLMs) for generating radiology reports. It focuses on bootstrapping LLMs to handle the specific domain language and structure of medical reports, integrating visual features from radiology scans.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "report_generation",
        "multimodal_generation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/synlp/R2-LLM",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "radiology-report",
        "generation",
        "bootstrapping"
      ],
      "id": 83
    },
    {
      "name": "R2GenRL",
      "one_line_profile": "Reinforced Cross-modal Alignment for Radiology Report Generation",
      "detailed_description": "R2GenRL applies reinforcement learning techniques to improve cross-modal alignment in radiology report generation. It optimizes the generation process by rewarding the model for better alignment between image regions and generated text, aiming for more accurate and clinically relevant reports.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "report_generation",
        "reinforcement_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/synlp/R2GenRL",
      "help_website": [],
      "license": null,
      "tags": [
        "reinforcement-learning",
        "radiology-report",
        "cross-modal"
      ],
      "id": 84
    },
    {
      "name": "Medical-Report-Generation",
      "one_line_profile": "Multimodal Recurrent Model with Attention for Automated Radiology Report Generation",
      "detailed_description": "A PyTorch implementation of a multimodal recurrent model equipped with attention mechanisms for generating radiology reports. This tool focuses on automatically interpreting medical images and producing descriptive text reports.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "report_generation",
        "image_captioning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/tengfeixue-victor/Medical-Report-Generation",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "radiology",
        "rnn",
        "attention",
        "report-generation"
      ],
      "id": 85
    },
    {
      "name": "RGRG",
      "one_line_profile": "Interactive and Explainable Region-guided Radiology Report Generation",
      "detailed_description": "RGRG (Region-Guided Radiology Report Generation) is a framework that allows for interactive and explainable report generation. It enables users to guide the generation process by focusing on specific regions of interest in the radiology image, enhancing the clinical utility and interpretability of the automated reports.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "report_generation",
        "explainable_ai"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ttanida/rgrg",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "interactive",
        "explainable",
        "radiology",
        "region-guided"
      ],
      "id": 86
    },
    {
      "name": "MATR",
      "one_line_profile": "Multimodal Medical Image Fusion via Multiscale Adaptive Transformer",
      "detailed_description": "MATR is a deep learning model for multimodal medical image fusion that utilizes a multiscale adaptive transformer architecture. It is designed to merge information from different imaging modalities (e.g., MRI, CT, PET) into a single high-quality image for better clinical diagnosis.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "image_fusion",
        "multimodal_processing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/tthinking/MATR",
      "help_website": [],
      "license": null,
      "tags": [
        "transformer",
        "image-fusion",
        "multiscale",
        "medical-imaging"
      ],
      "id": 87
    },
    {
      "name": "ARGON",
      "one_line_profile": "Progressive Transformer-Based Generation of Radiology Reports",
      "detailed_description": "ARGON is a transformer-based model for generating radiology reports in a progressive manner. It aims to improve the quality and coherence of generated reports by refining the generation process through progressive stages.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "report_generation",
        "transformer_modeling"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/uzh-dqbm-cmi/ARGON",
      "help_website": [],
      "license": null,
      "tags": [
        "transformer",
        "radiology",
        "progressive-generation"
      ],
      "id": 88
    },
    {
      "name": "R2GenGPT",
      "one_line_profile": "Radiology Report Generation with Frozen Large Language Models",
      "detailed_description": "R2GenGPT is a framework that utilizes frozen Large Language Models (LLMs) for radiology report generation. It aligns visual features from medical images with the input space of LLMs, allowing the powerful text generation capabilities of LLMs to be applied to medical reporting without extensive fine-tuning of the language model itself.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "report_generation",
        "llm_adaptation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/wang-zhanyu/R2GenGPT",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "gpt",
        "llm",
        "radiology",
        "frozen-model"
      ],
      "id": 89
    },
    {
      "name": "Multimodal-Explanation",
      "one_line_profile": "Generating and Evaluating Post-Hoc Explanations for Multimodal Medical Image Analysis",
      "detailed_description": "This repository provides tools for generating and evaluating post-hoc explanations for deep neural networks used in multimodal medical image analysis. It focuses on interpretability, helping researchers and clinicians understand model decisions in tasks involving multiple imaging modalities.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "explainable_ai",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/weinajin/multimodal_explanation",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "explainability",
        "post-hoc",
        "multimodal",
        "evaluation"
      ],
      "id": 90
    },
    {
      "name": "ORGAN",
      "one_line_profile": "Observation-Guided Radiology Report Generation via Tree Reasoning",
      "detailed_description": "ORGAN is a radiology report generation model that employs observation-guided tree reasoning. It structures the generation process by explicitly reasoning about observations in a tree structure, aiming to produce more logical and clinically accurate reports.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "report_generation",
        "reasoning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/wjhou/ORGan",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "tree-reasoning",
        "radiology",
        "observation-guided"
      ],
      "id": 91
    },
    {
      "name": "RADAR",
      "one_line_profile": "Enhancing Radiology Report Generation with Supplementary Knowledge Injection",
      "detailed_description": "RADAR is a framework for enhancing radiology report generation by injecting supplementary knowledge. It integrates external medical knowledge to support the generation process, improving the factual accuracy and completeness of the reports.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "report_generation",
        "knowledge_injection"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/wjhou/Radar",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "knowledge-graph",
        "radiology",
        "report-generation"
      ],
      "id": 92
    },
    {
      "name": "RECAP",
      "one_line_profile": "Towards Precise Radiology Report Generation via Dynamic Disease Progression Reasoning",
      "detailed_description": "RECAP is a model for radiology report generation that incorporates dynamic disease progression reasoning. It focuses on capturing the temporal and logical progression of diseases to generate more precise and contextually aware reports.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "report_generation",
        "disease_reasoning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/wjhou/Recap",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "disease-progression",
        "reasoning",
        "radiology"
      ],
      "id": 93
    },
    {
      "name": "PMC-VQA",
      "one_line_profile": "Large-scale medical visual question-answering dataset from PubMed Central",
      "detailed_description": "PMC-VQA is a large-scale dataset for medical visual question answering, containing 227k VQA pairs derived from 149k images in PubMed Central. It covers a wide range of modalities and diseases, serving as a benchmark for training and evaluating medical VQA models.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "dataset",
        "visual_question_answering"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/xiaoman-zhang/PMC-VQA",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "vqa",
        "dataset",
        "pubmed",
        "medical-imaging"
      ],
      "id": 94
    },
    {
      "name": "BenchX",
      "one_line_profile": "Unified Benchmark Framework for Medical Vision-Language Pretraining on Chest X-Rays",
      "detailed_description": "BenchX is a comprehensive benchmark framework designed to evaluate medical vision-language pretraining models, specifically on chest X-rays. It provides standardized protocols and metrics to assess the performance of various pretraining strategies on downstream tasks.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "benchmarking",
        "multimodal_pretraining"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/yangzhou12/BenchX",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "benchmark",
        "chest-xray",
        "vision-language",
        "pre-training"
      ],
      "id": 95
    },
    {
      "name": "IFCC",
      "one_line_profile": "Improving Factual Completeness and Consistency of Image-to-text Radiology Report Generation",
      "detailed_description": "IFCC is a method and codebase for improving the factual completeness and consistency of generated radiology reports. It addresses the hallucination problem in report generation by enforcing factual consistency between the image and the generated text.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "report_generation",
        "factual_consistency"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ysmiura/ifcc",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "factual-consistency",
        "radiology",
        "nlp",
        "generation"
      ],
      "id": 96
    },
    {
      "name": "MultiP-R2Gen",
      "one_line_profile": "Enhancing Radiology Report Generation via Multi-Phased Supervision",
      "detailed_description": "MultiP-R2Gen is a framework that enhances radiology report generation using multi-phased supervision. It employs a training strategy that guides the model through different phases of learning to improve the quality and accuracy of the generated medical reports.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "report_generation",
        "supervised_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/zailongchen/MultiP-R2Gen",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "radiology",
        "report-generation",
        "multi-phased",
        "supervision"
      ],
      "id": 97
    },
    {
      "name": "LLM-RG4",
      "one_line_profile": "Flexible and factual radiology report generation framework using LLMs",
      "detailed_description": "An implementation of the LLM-RG4 framework (AAAI 2025) designed for generating radiology reports. It leverages Large Language Models to ensure factual accuracy and flexibility across diverse input contexts, serving as a solver for medical image captioning tasks.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "radiology_report_generation",
        "medical_image_captioning",
        "multimodal_fusion"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/zh-Wang-Med/LLM-RG4",
      "help_website": [],
      "license": null,
      "tags": [
        "radiology-report-generation",
        "llm",
        "medical-imaging",
        "aaai-2025"
      ],
      "id": 98
    },
    {
      "name": "R2GenCMN",
      "one_line_profile": "Cross-modal Memory Networks for radiology report generation",
      "detailed_description": "The official implementation of Cross-modal Memory Networks (CMN) for radiology report generation (ACL 2021). It provides a deep learning model that utilizes shared memory mechanisms to align visual and textual features for generating accurate medical reports from radiology images.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "radiology_report_generation",
        "vision_language_alignment"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/zhjohnchan/R2GenCMN",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "radiology-report",
        "cross-modal",
        "memory-networks",
        "acl-2021"
      ],
      "id": 99
    },
    {
      "name": "WCL",
      "one_line_profile": "Weakly Supervised Contrastive Learning for Chest X-Ray Report Generation",
      "detailed_description": "A PyTorch implementation of Weakly Supervised Contrastive Learning (WCL) for chest X-ray report generation (EMNLP 2021). It serves as a solver for training models to generate medical reports using contrastive learning techniques on weakly labeled data.",
      "domains": [
        "H2",
        "H2-05"
      ],
      "subtask_category": [
        "radiology_report_generation",
        "contrastive_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/zzxslp/WCL",
      "help_website": [],
      "license": null,
      "tags": [
        "chest-x-ray",
        "contrastive-learning",
        "emnlp-2021",
        "medical-report-generation"
      ],
      "id": 100
    }
  ]
}
{
  "generated_at": "2025-12-16T12:05:07.334458+08:00",
  "metadata": {
    "leaf_cluster": {
      "leaf_cluster_id": "AI4",
      "leaf_cluster_name": "科研评测-基准/指标/复现实验生态",
      "domain": "AI Toolchain",
      "typical_objects": "datasets/tasks",
      "task_chain": "任务→指标→自动评测→leaderboard→回归",
      "tool_form": "benchmark + eval harness"
    },
    "unit": {
      "unit_id": "AI4-05",
      "unit_name": "安全/偏差/可信评测",
      "target_scale": "150–300",
      "coverage_tools": "safety/bias toolkits"
    },
    "search": {
      "target_candidates": 300,
      "queries": [
        "[GH] Counterfit",
        "[GH] TruLens",
        "[GH] NeMo Guardrails",
        "[GH] TextAttack",
        "[GH] Garak",
        "[GH] DeepEval",
        "[GH] Giskard",
        "[GH] Adversarial Robustness Toolbox",
        "[GH] Fairlearn",
        "[GH] AI Fairness 360",
        "[GH] ai safety evaluation",
        "[GH] llm bias detection",
        "[GH] fairness metrics",
        "[GH] trustworthy ai benchmark",
        "[GH] red teaming llm",
        "[GH] adversarial robustness",
        "[GH] toxicity detection",
        "[GH] hallucination evaluation",
        "[GH] model guardrails",
        "[GH] jailbreak detection",
        "[GH] responsible ai toolkit",
        "[GH] safety alignment",
        "[WEB] llm safety and bias evaluation framework github",
        "[WEB] ai fairness metrics library github",
        "[WEB] red teaming tools for llm github",
        "[WEB] adversarial robustness toolbox github",
        "[WEB] trustworthy ai benchmark tools github"
      ],
      "total_candidates": 1173,
      "tool_candidates": 463,
      "final_tools": 187
    }
  },
  "tools": [
    {
      "name": "DiaHalu",
      "one_line_profile": "Dialogue-level hallucination evaluation benchmark for LLMs",
      "detailed_description": "A benchmark dataset and evaluation framework designed to assess hallucination in Large Language Models at the dialogue level, focusing on multi-turn interactions.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_evaluation",
        "safety_benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/141forever/DiaHalu",
      "help_website": [],
      "license": null,
      "tags": [
        "hallucination",
        "llm-evaluation",
        "benchmark"
      ],
      "id": 1
    },
    {
      "name": "AgentPoison",
      "one_line_profile": "Red-teaming tool for LLM Agents via backdoor poisoning",
      "detailed_description": "A red-teaming framework that evaluates the robustness of LLM agents by injecting backdoors into their memory or knowledge base to trigger targeted misbehaviors.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "red_teaming",
        "adversarial_attack",
        "agent_security"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/AI-secure/AgentPoison",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "red-teaming",
        "llm-agent",
        "backdoor-attack"
      ],
      "id": 2
    },
    {
      "name": "UDora",
      "one_line_profile": "Unified red teaming framework against LLM Agents",
      "detailed_description": "A comprehensive framework for conducting red teaming operations against Large Language Model agents to identify security vulnerabilities and safety risks.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "red_teaming",
        "safety_evaluation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/AI-secure/UDora",
      "help_website": [],
      "license": null,
      "tags": [
        "red-teaming",
        "llm-agent",
        "safety-framework"
      ],
      "id": 3
    },
    {
      "name": "aixploit",
      "one_line_profile": "Exploitation toolkit for LLM vulnerabilities",
      "detailed_description": "A toolkit designed for red teams and penetration testers to identify and exploit vulnerabilities in Large Language Model solutions.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "penetration_testing",
        "vulnerability_scanning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/AINTRUST-AI/aixploit",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "exploit",
        "llm-security",
        "red-teaming"
      ],
      "id": 4
    },
    {
      "name": "hallucinogen",
      "one_line_profile": "Benchmark for evaluating hallucinations in LVLMs",
      "detailed_description": "A benchmark suite specifically designed to evaluate and quantify hallucination phenomena in Large Visual Language Models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_evaluation",
        "multimodal_benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/AikyamLab/hallucinogen",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "lvlm",
        "hallucination",
        "benchmark"
      ],
      "id": 5
    },
    {
      "name": "Octopus-Family",
      "one_line_profile": "Multi-dimensional safety assessment suite for AI models",
      "detailed_description": "A comprehensive safety testing suite developed by Alibaba-AAIG that provides multi-faceted probing to evaluate the safety and robustness of AI models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_assessment",
        "robustness_testing"
      ],
      "application_level": "framework",
      "primary_language": null,
      "repo_url": "https://github.com/Alibaba-AAIG/Octopus-Family",
      "help_website": [],
      "license": null,
      "tags": [
        "safety-evaluation",
        "robustness",
        "testing-suite"
      ],
      "id": 6
    },
    {
      "name": "GuardBench",
      "one_line_profile": "Library for evaluating guardrail models",
      "detailed_description": "A Python library designed to evaluate the effectiveness and performance of guardrail models used in AI safety systems.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "guardrail_evaluation",
        "safety_metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/AmenRa/GuardBench",
      "help_website": [],
      "license": "EUPL-1.2",
      "tags": [
        "guardrails",
        "evaluation",
        "python-library"
      ],
      "id": 7
    },
    {
      "name": "CognitiveLens",
      "one_line_profile": "Analytics tool for human-AI alignment visualization",
      "detailed_description": "A Streamlit-based analytics tool that visualizes fairness, calibration, and interpretability metrics to explore alignment between human and AI decisions.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "alignment_visualization",
        "fairness_auditing"
      ],
      "application_level": "application",
      "primary_language": "Python",
      "repo_url": "https://github.com/AmirhosseinHonardoust/Cognitivelens-AI-Human-Comparison",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "visualization",
        "alignment",
        "fairness"
      ],
      "id": 8
    },
    {
      "name": "POPE",
      "one_line_profile": "Evaluation method for object hallucination in LVLMs",
      "detailed_description": "A benchmark and evaluation method (Polling-based Object Probing Evaluation) for assessing object hallucination issues in Large Vision-Language Models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_evaluation",
        "object_detection_check"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/AoiDragon/POPE",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "lvlm",
        "hallucination",
        "evaluation-metric"
      ],
      "id": 9
    },
    {
      "name": "rexmex",
      "one_line_profile": "Recommender metrics library for fair evaluation",
      "detailed_description": "A general-purpose library for calculating evaluation metrics for recommender systems, with a focus on fairness and robust evaluation.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "fairness_evaluation",
        "recommender_metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/AstraZeneca/rexmex",
      "help_website": [],
      "license": null,
      "tags": [
        "metrics",
        "recommender-systems",
        "fairness"
      ],
      "id": 10
    },
    {
      "name": "PREPER",
      "one_line_profile": "Dataset for safety evaluation of AI perception systems",
      "detailed_description": "A dataset specifically constructed to evaluate the safety and robustness of AI perception systems under various conditions.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_evaluation",
        "perception_benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/AsymptoticAI/PREPER",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "dataset",
        "safety",
        "perception"
      ],
      "id": 11
    },
    {
      "name": "Counterfit",
      "one_line_profile": "CLI for assessing security of ML models",
      "detailed_description": "A command-line automation tool by Azure for assessing the security of machine learning models, enabling red teaming and vulnerability scanning.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "security_assessment",
        "vulnerability_scanning"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/Azure/counterfit",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "security",
        "ml-assessment",
        "cli"
      ],
      "id": 12
    },
    {
      "name": "ALERT",
      "one_line_profile": "Benchmark for assessing LLM safety via red teaming",
      "detailed_description": "A comprehensive benchmark designed to assess the safety of Large Language Models through simulated red teaming attacks and scenarios.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_benchmarking",
        "red_teaming"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/Babelscape/ALERT",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "benchmark",
        "llm-safety",
        "red-teaming"
      ],
      "id": 13
    },
    {
      "name": "SafeWatch",
      "one_line_profile": "Safety-policy following video guardrail model",
      "detailed_description": "An efficient video guardrail model designed to enforce safety policies with transparent explanations, serving as a tool for content moderation and safety alignment.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "content_moderation",
        "video_safety"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/BillChan226/SafeWatch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "guardrail",
        "video-safety",
        "explainability"
      ],
      "id": 14
    },
    {
      "name": "LitterBox",
      "one_line_profile": "Secure sandbox for malware analysis with LLM integration",
      "detailed_description": "A secure sandbox environment designed for red teamers to test payloads, featuring integration with LLM agents for enhanced analysis capabilities.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "malware_analysis",
        "red_teaming_environment"
      ],
      "application_level": "platform",
      "primary_language": "YARA",
      "repo_url": "https://github.com/BlackSnufkin/LitterBox",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "sandbox",
        "malware-analysis",
        "llm-agent"
      ],
      "id": 15
    },
    {
      "name": "advertorch",
      "one_line_profile": "Toolbox for adversarial robustness research",
      "detailed_description": "A Python toolbox for adversarial robustness research, providing implementations of attacks, defenses, and robust training methods for PyTorch.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_robustness",
        "attack_simulation"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/BorealisAI/advertorch",
      "help_website": [],
      "license": "LGPL-3.0",
      "tags": [
        "adversarial-attacks",
        "robustness",
        "pytorch"
      ],
      "id": 16
    },
    {
      "name": "Bud Runtime",
      "one_line_profile": "Inference stack for AI deployment and optimization",
      "detailed_description": "A comprehensive inference stack and runtime environment for deploying, optimizing, and scaling compound AI systems.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "inference_optimization",
        "model_deployment"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/BudEcosystem/bud-runtime",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "inference",
        "deployment",
        "optimization"
      ],
      "id": 17
    },
    {
      "name": "agent-attack",
      "one_line_profile": "Adversarial robustness benchmark for multimodal agents",
      "detailed_description": "A framework and benchmark for dissecting and evaluating the adversarial robustness of multimodal language model agents.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "robustness_benchmarking",
        "adversarial_attack"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ChenWu98/agent-attack",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "multimodal-agent",
        "robustness",
        "benchmark"
      ],
      "id": 18
    },
    {
      "name": "VidHalluc",
      "one_line_profile": "Benchmark for temporal hallucinations in video LLMs",
      "detailed_description": "A benchmark designed to evaluate temporal hallucinations in Multimodal Large Language Models specifically for video understanding tasks.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_evaluation",
        "video_understanding"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/CyL97/VidHalluc",
      "help_website": [],
      "license": null,
      "tags": [
        "video-llm",
        "hallucination",
        "benchmark"
      ],
      "id": 19
    },
    {
      "name": "CMM",
      "one_line_profile": "Benchmark for evaluating hallucinations across modalities",
      "detailed_description": "The Curse of Multi-Modalities (CMM) is a benchmark for evaluating hallucinations in Large Multimodal Models across language, visual, and audio modalities.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_evaluation",
        "multimodal_benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/DAMO-NLP-SG/CMM",
      "help_website": [],
      "license": null,
      "tags": [
        "multimodal",
        "hallucination",
        "benchmark"
      ],
      "id": 20
    },
    {
      "name": "ToxiCN",
      "one_line_profile": "Benchmark for Chinese toxic language detection",
      "detailed_description": "A fine-grained benchmark and resource for detecting toxic language in Chinese, including a hierarchical taxonomy and dataset.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "toxicity_detection",
        "safety_benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/DUT-lujunyu/ToxiCN",
      "help_website": [],
      "license": null,
      "tags": [
        "toxicity",
        "chinese-nlp",
        "benchmark"
      ],
      "id": 21
    },
    {
      "name": "AISecurity",
      "one_line_profile": "AI Firewall for protecting LLMs",
      "detailed_description": "A security tool acting as an AI firewall with multiple detection engines to protect Large Language Models from jailbreaks, injections, and adversarial attacks.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "security_defense",
        "attack_detection"
      ],
      "application_level": "service",
      "primary_language": "HTML",
      "repo_url": "https://github.com/DmitrL-dev/AISecurity",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "firewall",
        "llm-security",
        "jailbreak-detection"
      ],
      "id": 22
    },
    {
      "name": "DiaHalu (ECNU)",
      "one_line_profile": "Dialogue-level hallucination evaluation benchmark",
      "detailed_description": "A benchmark for evaluating dialogue-level hallucinations in LLMs (Mirror/Fork of the main DiaHalu project).",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": null,
      "repo_url": "https://github.com/ECNU-ICALK/DiaHalu",
      "help_website": [],
      "license": null,
      "tags": [
        "hallucination",
        "benchmark"
      ],
      "id": 23
    },
    {
      "name": "Robust3DOD",
      "one_line_profile": "Benchmark and robustness evaluation toolkit for LiDAR-based 3D object detectors",
      "detailed_description": "A comprehensive study and toolkit for evaluating the robustness of LiDAR-based 3D object detectors against adversarial attacks, providing benchmark datasets and attack implementations.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "adversarial_attack",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Eaphan/Robust3DOD",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "3d-object-detection",
        "adversarial-robustness",
        "lidar",
        "autonomous-driving"
      ],
      "id": 24
    },
    {
      "name": "FAIR Metrics",
      "one_line_profile": "Reference implementation of FAIR (Findable, Accessible, Interoperable, Reusable) metrics",
      "detailed_description": "A Ruby-based library implementing the metrics defined by the FAIR Metrics Group to evaluate the FAIRness of digital resources, supporting scientific data management and quality control.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "data_quality_control",
        "fair_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Ruby",
      "repo_url": "https://github.com/FAIRMetrics/Metrics",
      "help_website": [
        "http://fairmetrics.org"
      ],
      "license": "MIT",
      "tags": [
        "fair-principles",
        "data-quality",
        "metrics"
      ],
      "id": 25
    },
    {
      "name": "LRV-Instruction",
      "one_line_profile": "Dataset and instruction tuning method for mitigating hallucination in LMMs",
      "detailed_description": "A framework and dataset designed to evaluate and mitigate hallucinations in Large Multi-Modal Models (LMMs) via robust instruction tuning.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_mitigation",
        "instruction_tuning",
        "dataset_generation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/FuxiaoLiu/LRV-Instruction",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "hallucination",
        "multimodal-models",
        "instruction-tuning"
      ],
      "id": 26
    },
    {
      "name": "Giskard Client",
      "one_line_profile": "Python client for the Giskard AI testing platform",
      "detailed_description": "The official API client for interacting with the Giskard platform, enabling programmatic testing, monitoring, and evaluation of AI models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "model_testing",
        "api_client"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Giskard-AI/giskard-client",
      "help_website": [
        "https://docs.giskard.ai"
      ],
      "license": "Apache-2.0",
      "tags": [
        "ai-testing",
        "mlops",
        "client"
      ],
      "id": 27
    },
    {
      "name": "Giskard Hub SDK",
      "one_line_profile": "SDK for enterprise LLM agent testing and red teaming",
      "detailed_description": "A software development kit for the Giskard Hub, facilitating collaborative testing, continuous red teaming, and evaluation of LLM agents in enterprise environments.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "red_teaming",
        "agent_testing"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/Giskard-AI/giskard-hub",
      "help_website": [
        "https://docs.giskard.ai"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm-testing",
        "red-teaming",
        "sdk"
      ],
      "id": 28
    },
    {
      "name": "Giskard",
      "one_line_profile": "Open-source testing and evaluation library for LLMs and ML models",
      "detailed_description": "A comprehensive testing framework for AI models, focusing on detecting performance issues, bias, security vulnerabilities, and hallucinations in LLMs and tabular models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "model_evaluation",
        "bias_detection",
        "security_testing"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/Giskard-AI/giskard-oss",
      "help_website": [
        "https://docs.giskard.ai"
      ],
      "license": "Apache-2.0",
      "tags": [
        "ai-safety",
        "testing",
        "llm",
        "bias"
      ],
      "id": 29
    },
    {
      "name": "Giskard Vision",
      "one_line_profile": "Evaluation and testing library for Computer Vision models",
      "detailed_description": "A specialized extension of the Giskard framework dedicated to testing and evaluating computer vision AI systems for robustness and correctness.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "vision_evaluation",
        "robustness_testing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Giskard-AI/giskard-vision",
      "help_website": [
        "https://docs.giskard.ai"
      ],
      "license": "Apache-2.0",
      "tags": [
        "computer-vision",
        "testing",
        "evaluation"
      ],
      "id": 30
    },
    {
      "name": "Phare",
      "one_line_profile": "Benchmark for evaluating LLM security and safety dimensions",
      "detailed_description": "A benchmark suite designed to evaluate Large Language Models across key security and safety dimensions, providing standardized metrics for model comparison.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_benchmarking",
        "security_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/Giskard-AI/phare",
      "help_website": [],
      "license": null,
      "tags": [
        "llm-benchmark",
        "safety",
        "security"
      ],
      "id": 31
    },
    {
      "name": "Giskard Prompt Injections",
      "one_line_profile": "Dataset of prompt injections for LLM security scanning",
      "detailed_description": "A curated collection of prompt injection attacks used by the Giskard Scan tool to test the vulnerability of Large Language Models to adversarial inputs.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_attack",
        "security_scanning"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/Giskard-AI/prompt-injections",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "prompt-injection",
        "llm-security",
        "dataset"
      ],
      "id": 32
    },
    {
      "name": "ChatGPT Evaluation",
      "one_line_profile": "Multitask benchmark for evaluating ChatGPT on reasoning and hallucination",
      "detailed_description": "A repository containing test samples and scripts for a multitask, multilingual, and multimodal evaluation of ChatGPT, focusing on reasoning capabilities, hallucination, and interactivity.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "model_benchmarking",
        "hallucination_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/HLTCHKUST/chatgpt-evaluation",
      "help_website": [],
      "license": null,
      "tags": [
        "chatgpt",
        "evaluation",
        "hallucination",
        "multilingual"
      ],
      "id": 33
    },
    {
      "name": "Smoothing Adversarial",
      "one_line_profile": "Reference implementation for Randomized Smoothing robustness certification",
      "detailed_description": "The official implementation of 'Provably Robust Deep Learning via Adversarially Trained Smoothed Classifiers', serving as a standard solver for certifying the robustness of deep learning models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "robustness_certification",
        "adversarial_training"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Hadisalman/smoothing-adversarial",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "randomized-smoothing",
        "certified-robustness",
        "adversarial-defense"
      ],
      "id": 34
    },
    {
      "name": "TrustLLM",
      "one_line_profile": "Comprehensive benchmark for trustworthiness in Large Language Models",
      "detailed_description": "A benchmark toolkit evaluating LLM trustworthiness across multiple dimensions including truthfulness, safety, fairness, robustness, privacy, and machine ethics.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "trustworthiness_evaluation",
        "safety_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/HowieHwong/TrustLLM",
      "help_website": [
        "https://trustllmbenchmark.github.io/TrustLLM-Website/"
      ],
      "license": "MIT",
      "tags": [
        "llm",
        "trustworthiness",
        "benchmark",
        "safety"
      ],
      "id": 35
    },
    {
      "name": "UHGEval",
      "one_line_profile": "Framework for evaluating hallucinations in Large Language Models",
      "detailed_description": "A user-friendly evaluation framework and suite of benchmarks (including UHGEval, HaluEval, HalluQA) designed to assess and quantify hallucinations in LLM generation.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_evaluation",
        "benchmarking"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/IAAR-Shanghai/UHGEval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "hallucination",
        "llm-evaluation",
        "benchmark"
      ],
      "id": 36
    },
    {
      "name": "UHGEval-dataset",
      "one_line_profile": "Pipeline for creating hallucination evaluation datasets",
      "detailed_description": "The data generation pipeline used to create the UHGEval hallucination dataset, supporting the creation of custom evaluation benchmarks for LLMs.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "dataset_generation",
        "hallucination_research"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/IAAR-Shanghai/UHGEval-dataset",
      "help_website": [],
      "license": null,
      "tags": [
        "dataset-creation",
        "hallucination",
        "llm"
      ],
      "id": 37
    },
    {
      "name": "HEART",
      "one_line_profile": "Hardened Extension of the Adversarial Robustness Toolbox",
      "detailed_description": "A library extending the Adversarial Robustness Toolbox (ART) to support the assessment of adversarial AI vulnerabilities specifically in Test & Evaluation workflows.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "vulnerability_assessment",
        "adversarial_robustness"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/IBM/heart-library",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "adversarial-robustness",
        "security",
        "testing"
      ],
      "id": 38
    },
    {
      "name": "Infosys Responsible AI Toolkit",
      "one_line_profile": "Toolkit for AI safety, security, fairness, and explainability",
      "detailed_description": "A comprehensive toolkit incorporating features for safety, security, explainability, fairness, bias detection, and hallucination detection to ensure trustworthy AI solutions.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "responsible_ai",
        "bias_detection",
        "hallucination_detection"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/Infosys/Infosys-Responsible-AI-Toolkit",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "responsible-ai",
        "fairness",
        "explainability",
        "security"
      ],
      "id": 39
    },
    {
      "name": "LAION-SAFETY",
      "one_line_profile": "Toolbox for NSFW and toxicity detection in datasets",
      "detailed_description": "An open toolbox providing models and scripts for detecting NSFW content and toxicity, primarily used for filtering and cleaning large-scale datasets like LAION.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "data_filtering",
        "toxicity_detection",
        "nsfw_detection"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/LAION-AI/LAION-SAFETY",
      "help_website": [],
      "license": null,
      "tags": [
        "safety",
        "content-moderation",
        "dataset-cleaning"
      ],
      "id": 40
    },
    {
      "name": "LLMs-Finetuning-Safety",
      "one_line_profile": "Resources for demonstrating safety guardrail breaches via fine-tuning",
      "detailed_description": "A research toolkit and dataset demonstrating how fine-tuning on a small number of adversarial examples can compromise the safety guardrails of LLMs like GPT-3.5 Turbo.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_evaluation",
        "adversarial_attack",
        "jailbreaking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/LLM-Tuning-Safety/LLMs-Finetuning-Safety",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "jailbreak",
        "fine-tuning",
        "llm-safety"
      ],
      "id": 41
    },
    {
      "name": "fair-test",
      "one_line_profile": "Library to build and deploy FAIR metrics tests APIs",
      "detailed_description": "A Python library facilitating the creation and deployment of FAIR (Findable, Accessible, Interoperable, Reusable) metrics tests, compatible with FAIR evaluation services.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "fair_evaluation",
        "data_management"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/MaastrichtU-IDS/fair-test",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fair-principles",
        "metrics",
        "api"
      ],
      "id": 42
    },
    {
      "name": "CIFAR10 Challenge",
      "one_line_profile": "Benchmark challenge for adversarial robustness on CIFAR10",
      "detailed_description": "A benchmark and challenge framework designed to evaluate and compare the adversarial robustness of neural networks on the CIFAR10 dataset.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "robustness_benchmarking",
        "adversarial_defense"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/MadryLab/cifar10_challenge",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "cifar10",
        "adversarial-robustness",
        "benchmark"
      ],
      "id": 43
    },
    {
      "name": "MNIST Challenge",
      "one_line_profile": "Benchmark challenge for adversarial robustness on MNIST",
      "detailed_description": "A benchmark and challenge framework designed to evaluate and compare the adversarial robustness of neural networks on the MNIST dataset.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "robustness_benchmarking",
        "adversarial_defense"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/MadryLab/mnist_challenge",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mnist",
        "adversarial-robustness",
        "benchmark"
      ],
      "id": 44
    },
    {
      "name": "Robustness",
      "one_line_profile": "Library for training and evaluating robust neural networks",
      "detailed_description": "A comprehensive library for experimenting with, training, and evaluating neural networks with a specific focus on adversarial robustness, providing standard implementations of adversarial training.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_training",
        "robustness_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/MadryLab/robustness",
      "help_website": [
        "https://robustness.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "adversarial-training",
        "deep-learning",
        "robustness"
      ],
      "id": 45
    },
    {
      "name": "HaluMem",
      "one_line_profile": "Hallucination evaluation benchmark for agent memory systems",
      "detailed_description": "HaluMem is an operation-level hallucination evaluation benchmark specifically designed for agent memory systems. It provides a framework to assess and detect hallucinations in the memory retrieval and utilization processes of AI agents.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_detection",
        "benchmark"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/MemTensor/HaluMem",
      "help_website": [],
      "license": null,
      "tags": [
        "hallucination",
        "agent-memory",
        "benchmark"
      ],
      "id": 46
    },
    {
      "name": "Dingo",
      "one_line_profile": "Comprehensive AI quality evaluation tool for data, models, and applications",
      "detailed_description": "Dingo is a comprehensive evaluation tool designed to assess the quality of AI data, models, and applications. It supports various evaluation metrics and scenarios to ensure the reliability and performance of AI systems.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "quality_evaluation",
        "model_assessment"
      ],
      "application_level": "solver",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/MigoXLab/dingo",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "evaluation",
        "quality-assurance",
        "ai-testing"
      ],
      "id": 47
    },
    {
      "name": "Gandalf LLM Pentester",
      "one_line_profile": "Automated red-teaming toolkit for stress-testing LLM defenses",
      "detailed_description": "Gandalf LLM Pentester is an automated toolkit designed for red-teaming Large Language Models. It focuses on stress-testing LLM defenses through vector attacks and provides insights into potential vulnerabilities and alignment failures.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "red_teaming",
        "adversarial_attack"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/MrMoshkovitz/gandalf-llm-pentester",
      "help_website": [],
      "license": null,
      "tags": [
        "red-teaming",
        "llm-security",
        "penetration-testing"
      ],
      "id": 48
    },
    {
      "name": "Summarization Eval",
      "one_line_profile": "Reference-free automatic summarization evaluation with hallucination detection",
      "detailed_description": "This tool provides a reference-free method for evaluating automatic summarization. It includes features for detecting potential hallucinations in generated summaries, aiming to improve the reliability of summarization metrics without requiring ground truth references.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_detection",
        "summarization_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Muhtasham/summarization-eval",
      "help_website": [],
      "license": null,
      "tags": [
        "summarization",
        "evaluation",
        "hallucination"
      ],
      "id": 49
    },
    {
      "name": "NRP",
      "one_line_profile": "Self-supervised approach for adversarial robustness in vision models",
      "detailed_description": "NRP (Neural Representation Purification) is an implementation of a self-supervised approach for enhancing adversarial robustness. It focuses on purifying adversarial perturbations from input images to protect vision models against attacks.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_defense",
        "robustness"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Muzammal-Naseer/NRP",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "adversarial-robustness",
        "computer-vision",
        "self-supervised-learning"
      ],
      "id": 50
    },
    {
      "name": "Hallu-PI",
      "one_line_profile": "Benchmark for evaluating hallucination in Multi-modal LLMs with perturbed inputs",
      "detailed_description": "Hallu-PI is a benchmark and dataset designed to evaluate hallucinations in Multi-modal Large Language Models (MLLMs). It specifically focuses on scenarios involving perturbed inputs to assess the robustness and faithfulness of model generations.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_evaluation",
        "multimodal_benchmark"
      ],
      "application_level": "dataset",
      "primary_language": null,
      "repo_url": "https://github.com/NJUNLP/Hallu-PI",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "hallucination",
        "mllm",
        "benchmark"
      ],
      "id": 51
    },
    {
      "name": "NeMo Guardrails",
      "one_line_profile": "Toolkit for adding programmable guardrails to LLM-based systems",
      "detailed_description": "NeMo Guardrails is an open-source toolkit that allows developers to add programmable guardrails to Large Language Model (LLM) based conversational systems. It helps ensure that the models behave within defined safety and topical boundaries.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_guardrails",
        "alignment"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NVIDIA-NeMo/Guardrails",
      "help_website": [
        "https://github.com/NVIDIA-NeMo/Guardrails"
      ],
      "license": null,
      "tags": [
        "guardrails",
        "llm-safety",
        "dialogue-systems"
      ],
      "id": 52
    },
    {
      "name": "garak",
      "one_line_profile": "Vulnerability scanner for Large Language Models",
      "detailed_description": "garak is a vulnerability scanner specifically designed for Large Language Models (LLMs). It probes LLMs for a wide range of weaknesses, including hallucination, data leakage, prompt injection, and toxicity, acting as an automated red-teaming tool.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "vulnerability_scanning",
        "red_teaming"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/NVIDIA/garak",
      "help_website": [
        "https://garak.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm-security",
        "vulnerability-scanner",
        "red-teaming"
      ],
      "id": 53
    },
    {
      "name": "ToXCL",
      "one_line_profile": "Unified framework for toxic speech detection and explanation",
      "detailed_description": "ToXCL is a framework designed for the detection and explanation of toxic speech. It integrates methods to identify toxic content and provide explanations for the detection, facilitating research into interpretable safety measures for NLP models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "toxicity_detection",
        "explainable_ai"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NhatHoang2002/ToXCL",
      "help_website": [],
      "license": null,
      "tags": [
        "toxicity-detection",
        "nlp",
        "explainability"
      ],
      "id": 54
    },
    {
      "name": "HalluQA",
      "one_line_profile": "Benchmark for evaluating hallucinations in Chinese Large Language Models",
      "detailed_description": "HalluQA is a dataset and evaluation framework specifically tailored for assessing hallucinations in Chinese Large Language Models. It provides a set of questions and evaluation scripts to measure the factual correctness and hallucination rates of models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_evaluation",
        "benchmark"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenMOSS/HalluQA",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "hallucination",
        "chinese-llm",
        "benchmark"
      ],
      "id": 55
    },
    {
      "name": "SafeVLA",
      "one_line_profile": "Safety alignment framework for Vision-Language-Action models",
      "detailed_description": "SafeVLA is a framework for the safety alignment of Vision-Language-Action (VLA) models via constrained learning. It addresses safety concerns in embodied AI agents by enforcing constraints during the learning process.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_alignment",
        "embodied_ai"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/PKU-Alignment/SafeVLA",
      "help_website": [],
      "license": null,
      "tags": [
        "safety-alignment",
        "vla",
        "constrained-learning"
      ],
      "id": 56
    },
    {
      "name": "BeaverTails",
      "one_line_profile": "Dataset collection for safety alignment in Large Language Models",
      "detailed_description": "BeaverTails is a comprehensive collection of datasets designed to support research on safety alignment in Large Language Models (LLMs). It includes data for training and evaluating models on helpfulness and harmlessness.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_alignment",
        "dataset"
      ],
      "application_level": "dataset",
      "primary_language": "Makefile",
      "repo_url": "https://github.com/PKU-Alignment/beavertails",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "safety-alignment",
        "rlhf",
        "dataset"
      ],
      "id": 57
    },
    {
      "name": "Safe-RLHF",
      "one_line_profile": "Library for constrained value alignment via Safe Reinforcement Learning",
      "detailed_description": "Safe-RLHF is a library that implements Safe Reinforcement Learning from Human Feedback. It enables the alignment of LLMs with human values while enforcing safety constraints, decoupling helpfulness and harmlessness objectives.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_alignment",
        "rlhf"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/PKU-Alignment/safe-rlhf",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rlhf",
        "safety",
        "alignment"
      ],
      "id": 58
    },
    {
      "name": "SafeSora",
      "one_line_profile": "Human preference dataset for safety alignment in text-to-video generation",
      "detailed_description": "SafeSora is a dataset of human preferences designed for safety alignment research in text-to-video generation models. It aims to enhance the helpfulness and harmlessness of Large Vision Models (LVMs) by providing safety-oriented preference data.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_alignment",
        "text-to-video"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/PKU-Alignment/safe-sora",
      "help_website": [],
      "license": null,
      "tags": [
        "text-to-video",
        "safety",
        "preference-dataset"
      ],
      "id": 59
    },
    {
      "name": "FCMI",
      "one_line_profile": "Deep Fair Clustering via Maximizing and Minimizing Mutual Information",
      "detailed_description": "FCMI is a PyTorch implementation of a Deep Fair Clustering algorithm. It utilizes mutual information maximization and minimization to achieve clustering results that are fair with respect to sensitive attributes.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "fairness",
        "clustering"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/PengxinZeng/2023-CVPR-FCMI",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "fair-clustering",
        "mutual-information",
        "fairness"
      ],
      "id": 60
    },
    {
      "name": "SafeWorld",
      "one_line_profile": "Geo-Diverse Safety Alignment framework",
      "detailed_description": "SafeWorld is a framework and dataset for Geo-Diverse Safety Alignment. It addresses the cultural and geographical variations in safety standards for LLMs, enabling the evaluation and improvement of model alignment across different global contexts.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_alignment",
        "cultural_bias"
      ],
      "application_level": "dataset",
      "primary_language": null,
      "repo_url": "https://github.com/PlusLabNLP/SafeWorld",
      "help_website": [],
      "license": null,
      "tags": [
        "safety",
        "geo-diversity",
        "alignment"
      ],
      "id": 61
    },
    {
      "name": "ASTRA",
      "one_line_profile": "Adversarial attack framework for AI safety competitions",
      "detailed_description": "ASTRA is an adversarial attack framework developed for AI safety competitions. It includes methods for generating effective adversarial prompts and attacks to evaluate the robustness of AI models against red-teaming efforts.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_attack",
        "red_teaming"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/PurCL/ASTRA",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "adversarial-attack",
        "ai-safety",
        "red-teaming"
      ],
      "id": 62
    },
    {
      "name": "Decepticon",
      "one_line_profile": "Autonomous Multi-Agent Based Red Team Testing Service",
      "detailed_description": "Decepticon is an autonomous red-teaming service that utilizes a multi-agent system to test AI models. It simulates various attack vectors and interactions to uncover vulnerabilities in LLM deployments.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "red_teaming",
        "multi_agent_simulation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/PurpleAILAB/Decepticon",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "red-teaming",
        "multi-agent",
        "security-testing"
      ],
      "id": 63
    },
    {
      "name": "Reevaluating NLP Adversarial Examples",
      "one_line_profile": "Code for reevaluating adversarial examples in Natural Language Processing",
      "detailed_description": "This repository contains the code and resources for reevaluating the effectiveness and validity of adversarial examples in NLP. It provides tools to analyze and benchmark different adversarial attack methods.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_evaluation",
        "nlp_robustness"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/QData/Reevaluating-NLP-Adversarial-Examples",
      "help_website": [],
      "license": null,
      "tags": [
        "adversarial-examples",
        "nlp",
        "evaluation"
      ],
      "id": 64
    },
    {
      "name": "TextAttack",
      "one_line_profile": "Framework for adversarial attacks, data augmentation, and model training in NLP",
      "detailed_description": "TextAttack is a Python framework for adversarial attacks, data augmentation, and adversarial training in NLP. It allows researchers to easily construct attacks, benchmark model robustness, and improve model performance through augmentation.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_attack",
        "data_augmentation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/QData/TextAttack",
      "help_website": [
        "https://textattack.readthedocs.io/en/master/"
      ],
      "license": "MIT",
      "tags": [
        "adversarial-attacks",
        "nlp",
        "robustness"
      ],
      "id": 65
    },
    {
      "name": "TextAttack-A2T",
      "one_line_profile": "Tools for improving adversarial training of NLP models",
      "detailed_description": "TextAttack-A2T provides implementations for improving adversarial training in NLP models. It builds upon TextAttack to offer specific methods for enhancing model robustness against textual adversarial examples.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_training",
        "robustness"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/QData/TextAttack-A2T",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "adversarial-training",
        "nlp",
        "robustness"
      ],
      "id": 66
    },
    {
      "name": "TextAttack Search Benchmark",
      "one_line_profile": "Benchmark for search algorithms in generating NLP adversarial examples",
      "detailed_description": "This repository benchmarks various search algorithms used for generating adversarial examples in NLP. It provides a comparative analysis of search strategies within the context of adversarial attacks.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_benchmark",
        "search_algorithms"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/QData/TextAttack-Search-Benchmark",
      "help_website": [],
      "license": null,
      "tags": [
        "benchmark",
        "adversarial-search",
        "nlp"
      ],
      "id": 67
    },
    {
      "name": "Qwen3Guard",
      "one_line_profile": "Multilingual guardrail model series for AI safety",
      "detailed_description": "Qwen3Guard is a series of multilingual guardrail models designed to ensure the safety of AI interactions. It detects and mitigates unsafe content across multiple languages, serving as a safety layer for LLM deployments.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_guardrails",
        "content_moderation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/QwenLM/Qwen3Guard",
      "help_website": [],
      "license": null,
      "tags": [
        "guardrails",
        "multilingual",
        "safety-model"
      ],
      "id": 68
    },
    {
      "name": "HaluEval",
      "one_line_profile": "Large-scale hallucination evaluation benchmark for LLMs",
      "detailed_description": "HaluEval is a large-scale benchmark designed to evaluate hallucinations in Large Language Models. It includes a diverse set of generated and human-annotated samples to assess the factual consistency and hallucination tendencies of LLMs.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_evaluation",
        "benchmark"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/RUCAIBox/HaluEval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "hallucination",
        "benchmark",
        "llm-evaluation"
      ],
      "id": 69
    },
    {
      "name": "RedTeamingforLLMs",
      "one_line_profile": "Framework for executing positive red-teaming experiments on LLMs",
      "detailed_description": "This framework is designed for conducting positive red-teaming experiments on Large Language Models. It provides a structure for testing model behaviors and identifying failure modes in a controlled environment.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "red_teaming",
        "safety_testing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/RedTeamingforLLMs/RedTeamingforLLMs",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "red-teaming",
        "llm",
        "experiment-framework"
      ],
      "id": 70
    },
    {
      "name": "HalluDetect",
      "one_line_profile": "Token probability approach for detecting hallucinations in LLM generation",
      "detailed_description": "HalluDetect implements a method for detecting hallucinations in LLM generations using token probability features. It uses logistic regression and MLP classifiers trained on features extracted from the text to identify hallucinated content.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_detection",
        "uncertainty_estimation"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/Rivas-AI/HalluDetect",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "hallucination-detection",
        "token-probability",
        "llm"
      ],
      "id": 71
    },
    {
      "name": "RobustBench",
      "one_line_profile": "Standardized adversarial robustness benchmark",
      "detailed_description": "RobustBench is a standardized benchmark for evaluating the adversarial robustness of image classification models. It provides a leaderboard and a model zoo of robust models to facilitate comparison and progress in adversarial defense research.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "robustness_benchmark",
        "adversarial_defense"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/RobustBench/robustbench",
      "help_website": [
        "https://robustbench.github.io/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "robustness",
        "benchmark",
        "computer-vision"
      ],
      "id": 72
    },
    {
      "name": "CipherChat",
      "one_line_profile": "Framework to evaluate generalization capability of safety alignment for LLMs",
      "detailed_description": "CipherChat is a framework designed to evaluate the generalization of safety alignment in LLMs, particularly in the context of cipher-based or encrypted conversations. It tests whether safety mechanisms hold up under non-standard input formats.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_evaluation",
        "generalization_testing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/RobustNLP/CipherChat",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "safety-alignment",
        "llm",
        "evaluation"
      ],
      "id": 73
    },
    {
      "name": "LangBiTe",
      "one_line_profile": "Bias Tester framework for Large Language Models",
      "detailed_description": "LangBiTe is a framework for testing bias in Large Language Models. It provides a structured approach to generate test cases and evaluate models for various types of social biases.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "bias_detection",
        "fairness_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/SOM-Research/LangBiTe",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bias-testing",
        "llm",
        "fairness"
      ],
      "id": 74
    },
    {
      "name": "PRISM",
      "one_line_profile": "Robust VLM Alignment with Principled Reasoning",
      "detailed_description": "PRISM is a framework for the robust alignment of Vision-Language Models (VLMs). It incorporates principled reasoning to improve the safety and reliability of multimodal systems.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_alignment",
        "vlm_robustness"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/SaFo-Lab/PRISM",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "vlm",
        "alignment",
        "robustness"
      ],
      "id": 75
    },
    {
      "name": "LLM Testlab",
      "one_line_profile": "Comprehensive testing tool for Large Language Models",
      "detailed_description": "LLM Testlab is a tool designed for the comprehensive testing of Large Language Models. It likely includes features for evaluating performance, safety, and other quality metrics of LLMs.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "model_testing",
        "quality_assurance"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Saivineeth147/llm-testlab",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm-testing",
        "evaluation",
        "qa"
      ],
      "id": 76
    },
    {
      "name": "RedesignAutonomy",
      "one_line_profile": "AI safety evaluation framework for LLM-assisted software engineering",
      "detailed_description": "RedesignAutonomy is a safety evaluation framework specifically for LLM-assisted software engineering. It assesses risks such as security flaws, overtrust, and misinterpretation in code generated by AI.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_evaluation",
        "code_generation_security"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Satyamkumarnavneet/RedesignAutonomy",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ai-safety",
        "software-engineering",
        "risk-assessment"
      ],
      "id": 77
    },
    {
      "name": "giskardpy",
      "one_line_profile": "Core library for constraint- and optimization-based robot motion control",
      "detailed_description": "giskardpy is the core Python library of the Giskard framework, designed for robot motion control using constraint-based and optimization-based methods. It allows for the specification and execution of complex robot behaviors.",
      "domains": [
        "AI4",
        "Robotics"
      ],
      "subtask_category": [
        "motion_control",
        "trajectory_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/SemRoCo/giskardpy",
      "help_website": [],
      "license": "LGPL-3.0",
      "tags": [
        "robotics",
        "motion-control",
        "optimization"
      ],
      "id": 78
    },
    {
      "name": "Face-Robustness-Benchmark",
      "one_line_profile": "Adversarial robustness evaluation library for face recognition",
      "detailed_description": "A comprehensive library and benchmark designed to evaluate the adversarial robustness of face recognition models against various attack methods.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "adversarial_attack"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ShawnXYang/Face-Robustness-Benchmark",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "face-recognition",
        "adversarial-robustness",
        "benchmark"
      ],
      "id": 79
    },
    {
      "name": "Graph Robustness Benchmark (GRB)",
      "one_line_profile": "Scalable benchmark for evaluating graph machine learning robustness",
      "detailed_description": "A unified, modular, and reproducible benchmark framework for evaluating the adversarial robustness of Graph Machine Learning (GML) models against various attacks.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "graph_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/THUDM/grb",
      "help_website": [
        "https://grb.readthedocs.io"
      ],
      "license": "MIT",
      "tags": [
        "graph-neural-networks",
        "adversarial-robustness",
        "benchmark"
      ],
      "id": 80
    },
    {
      "name": "AI-Infra-Guard",
      "one_line_profile": "Comprehensive AI Red Teaming platform",
      "detailed_description": "A comprehensive and intelligent AI Red Teaming platform designed to evaluate and enhance the safety and security of AI infrastructure and models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "red_teaming",
        "safety_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/Tencent/AI-Infra-Guard",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "red-teaming",
        "ai-safety",
        "security"
      ],
      "id": 81
    },
    {
      "name": "Trust & Safety Evals",
      "one_line_profile": "Reference stack for AI model trust and safety evaluation",
      "detailed_description": "A project by The AI Alliance defining a reference stack for AI model and system evaluation, providing benchmarks and tools for assessing trust and safety.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_evaluation",
        "benchmarking"
      ],
      "application_level": "workflow",
      "primary_language": "Makefile",
      "repo_url": "https://github.com/The-AI-Alliance/trust-safety-evals",
      "help_website": [],
      "license": null,
      "tags": [
        "ai-safety",
        "evaluation",
        "benchmarks"
      ],
      "id": 82
    },
    {
      "name": "AI Fairness 360 (AIF360)",
      "one_line_profile": "Fairness metrics and bias mitigation library",
      "detailed_description": "A comprehensive open-source toolkit containing metrics to check for unwanted bias in datasets and machine learning models, and algorithms to mitigate such bias.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "bias_mitigation",
        "fairness_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Trusted-AI/AIF360",
      "help_website": [
        "https://aif360.mybluemix.net/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "fairness",
        "bias-mitigation",
        "machine-learning"
      ],
      "id": 83
    },
    {
      "name": "Adversarial Robustness Toolbox (ART)",
      "one_line_profile": "Python library for machine learning security and robustness",
      "detailed_description": "A Python library for machine learning security, providing tools for evasion, poisoning, extraction, and inference attacks, as well as defenses and robustness certification.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "adversarial_defense"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox",
      "help_website": [
        "https://adversarial-robustness-toolbox.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "adversarial-ml",
        "security",
        "robustness"
      ],
      "id": 84
    },
    {
      "name": "AdvBox",
      "one_line_profile": "Adversarial example generation and robustness benchmarking toolbox",
      "detailed_description": "A toolbox to generate adversarial examples that fool neural networks across multiple frameworks (PaddlePaddle, PyTorch, etc.) and benchmark the robustness of machine learning models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_attack",
        "robustness_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/advboxes/AdvBox",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "adversarial-examples",
        "robustness",
        "paddlepaddle"
      ],
      "id": 85
    },
    {
      "name": "PromptInject",
      "one_line_profile": "Framework for evaluating LLM robustness to adversarial prompt attacks",
      "detailed_description": "A modular framework that assembles prompts to provide a quantitative analysis of the robustness of Large Language Models (LLMs) to adversarial prompt injection attacks.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "prompt_injection",
        "robustness_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/agencyenterprise/PromptInject",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "prompt-injection",
        "security"
      ],
      "id": 86
    },
    {
      "name": "Moonshot",
      "one_line_profile": "Modular framework for evaluating and red-teaming LLM applications",
      "detailed_description": "Moonshot is a tool designed to evaluate and red-team Large Language Model (LLM) applications. It provides a modular architecture to test for safety, security, and performance issues.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "red_teaming",
        "safety_evaluation",
        "llm_testing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aiverify-foundation/moonshot",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "red-teaming",
        "llm-evaluation",
        "ai-safety"
      ],
      "id": 87
    },
    {
      "name": "TurboFuzzLLM",
      "one_line_profile": "Mutation-based fuzzing tool for jailbreaking Large Language Models",
      "detailed_description": "TurboFuzzLLM is a tool that enhances mutation-based fuzzing techniques to effectively jailbreak Large Language Models (LLMs), aiding in safety testing and red teaming.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "fuzzing",
        "jailbreaking",
        "red_teaming"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/amazon-science/TurboFuzzLLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "fuzzing",
        "llm-safety",
        "jailbreak"
      ],
      "id": 88
    },
    {
      "name": "last_layer",
      "one_line_profile": "High-performance library for LLM prompt injection and jailbreak detection",
      "detailed_description": "last_layer is an ultra-fast, low-latency Python library designed to detect prompt injections and jailbreak attempts in Large Language Models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "prompt_injection_detection",
        "safety_monitoring"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/arekusandr/last_layer",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "prompt-injection",
        "jailbreak-detection",
        "security"
      ],
      "id": 89
    },
    {
      "name": "Arthur Bench",
      "one_line_profile": "Evaluation tool for comparing and testing LLMs",
      "detailed_description": "Arthur Bench is an open-source tool for evaluating Large Language Models (LLMs) to compare performance across different models, prompts, and hyperparameters.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "llm_evaluation",
        "model_comparison"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/arthur-ai/bench",
      "help_website": [
        "https://docs.arthur.ai/bench"
      ],
      "license": "MIT",
      "tags": [
        "llm-eval",
        "benchmarking",
        "observability"
      ],
      "id": 90
    },
    {
      "name": "Fairness.jl",
      "one_line_profile": "Julia toolkit for fairness metrics and bias mitigation",
      "detailed_description": "Fairness.jl is a Julia library providing a collection of fairness metrics and bias mitigation algorithms for machine learning models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "bias_mitigation",
        "fairness_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/ashryaagr/Fairness.jl",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fairness",
        "bias",
        "julia"
      ],
      "id": 91
    },
    {
      "name": "MLIP Arena",
      "one_line_profile": "Benchmark platform for machine learning interatomic potentials",
      "detailed_description": "MLIP Arena is a fair and transparent benchmark framework for evaluating machine learning interatomic potentials (MLIPs), going beyond basic error metrics to assess physical properties.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "model_benchmarking",
        "interatomic_potentials"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/atomind-ai/mlip-arena",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "mlip",
        "materials-science",
        "benchmarking"
      ],
      "id": 92
    },
    {
      "name": "FaithScore",
      "one_line_profile": "Evaluation metric for hallucinations in Large Vision-Language Models",
      "detailed_description": "FaithScore is a tool for fine-grained evaluation of hallucinations in Large Vision-Language Models (LVLMs), assessing the faithfulness of generated text to the visual input.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_detection",
        "vlm_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/bcdnlp/FAITHSCORE",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "hallucination",
        "vlm",
        "evaluation-metric"
      ],
      "id": 93
    },
    {
      "name": "nn_robust_attacks",
      "one_line_profile": "Library of robust evasion attacks against neural networks",
      "detailed_description": "A foundational library implementing robust evasion attacks (including the Carlini & Wagner attack) to evaluate the adversarial robustness of neural networks.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_attack",
        "robustness_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/carlini/nn_robust_attacks",
      "help_website": [],
      "license": "BSD-2-Clause",
      "tags": [
        "adversarial-attacks",
        "robustness",
        "neural-networks"
      ],
      "id": 94
    },
    {
      "name": "RedEval",
      "one_line_profile": "Library for red-teaming LLM applications",
      "detailed_description": "RedEval is a Python library designed for red-teaming LLM applications using other LLMs to generate adversarial inputs and evaluate safety.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "red_teaming",
        "safety_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/chziakas/redeval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "red-teaming",
        "llm",
        "security"
      ],
      "id": 95
    },
    {
      "name": "TedEval",
      "one_line_profile": "Fair evaluation metric for scene text detectors",
      "detailed_description": "TedEval provides a fair evaluation metric for scene text detectors, addressing issues with edit distance-based metrics in complex scenarios.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "metric_calculation",
        "text_detection_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/clovaai/TedEval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ocr",
        "evaluation-metric",
        "scene-text"
      ],
      "id": 96
    },
    {
      "name": "DeepEval",
      "one_line_profile": "Comprehensive evaluation framework for LLMs",
      "detailed_description": "DeepEval is an open-source evaluation framework for Large Language Models (LLMs), offering a suite of metrics to test for hallucinations, bias, toxicity, and other performance indicators.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "llm_evaluation",
        "unit_testing",
        "quality_assurance"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/confident-ai/deepeval",
      "help_website": [
        "https://docs.confident-ai.com"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm-evaluation",
        "testing",
        "metrics"
      ],
      "id": 97
    },
    {
      "name": "DeepTeam",
      "one_line_profile": "Framework for red teaming LLM systems",
      "detailed_description": "DeepTeam is a framework designed to red team LLMs and LLM systems, automating the process of finding vulnerabilities and safety issues.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "red_teaming",
        "vulnerability_scanning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/confident-ai/deepteam",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "red-teaming",
        "llm-security",
        "automation"
      ],
      "id": 98
    },
    {
      "name": "LangFair",
      "one_line_profile": "Library for LLM bias and fairness assessment",
      "detailed_description": "LangFair is a Python library for conducting use-case level assessments of bias and fairness in Large Language Models (LLMs), providing metrics and evaluation tools.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "bias_assessment",
        "fairness_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/cvs-health/langfair",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "fairness",
        "bias",
        "llm"
      ],
      "id": 99
    },
    {
      "name": "VerifyML",
      "one_line_profile": "Toolkit for responsible AI workflows and model verification",
      "detailed_description": "VerifyML is an open-source toolkit designed to help implement responsible AI workflows, enabling model verification, documentation, and fairness checks.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "responsible_ai",
        "model_verification",
        "documentation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/cylynx/verifyml",
      "help_website": [
        "https://verifyml.com"
      ],
      "license": "Apache-2.0",
      "tags": [
        "responsible-ai",
        "governance",
        "fairness"
      ],
      "id": 100
    },
    {
      "name": "PHUDGE",
      "one_line_profile": "Phi-3 based scalable judge for LLM evaluation and hallucination detection",
      "detailed_description": "A framework using Phi-3 as a scalable judge to evaluate Large Language Models (LLMs). It includes tools and methods for detecting hallucinations, grading responses, and performing evaluations with or without custom rubrics and reference answers.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_detection",
        "llm_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/deshwalmahesh/PHUDGE",
      "help_website": [],
      "license": null,
      "tags": [
        "llm-judge",
        "hallucination",
        "evaluation"
      ],
      "id": 101
    },
    {
      "name": "AIRTBench",
      "one_line_profile": "Benchmark for measuring autonomous AI red teaming capabilities",
      "detailed_description": "A code repository for AIRTBench, designed to evaluate and measure the capabilities of autonomous AI red teaming agents in language models, focusing on safety and robustness testing.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "red_teaming",
        "safety_benchmark"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/dreadnode/AIRTBench-Code",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "red-teaming",
        "benchmark",
        "llm-safety"
      ],
      "id": 102
    },
    {
      "name": "Aequitas",
      "one_line_profile": "Open-source bias auditing and fair machine learning toolkit",
      "detailed_description": "A toolkit for auditing bias and fairness in machine learning models. It enables developers and researchers to evaluate models for various bias metrics and visualize the results to ensure equitable outcomes.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "bias_auditing",
        "fairness_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/dssg/aequitas",
      "help_website": [
        "http://aequitas.dssg.io"
      ],
      "license": "MIT",
      "tags": [
        "fairness",
        "bias-audit",
        "machine-learning"
      ],
      "id": 103
    },
    {
      "name": "Robust-Semantic-Segmentation",
      "one_line_profile": "Dynamic divide-and-conquer adversarial training for robust segmentation",
      "detailed_description": "Implementation of the Dynamic Divide-and-Conquer Adversarial Training (DDCAT) method to improve the robustness of semantic segmentation models against adversarial attacks.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_training",
        "robust_segmentation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/dvlab-research/Robust-Semantic-Segmentation",
      "help_website": [],
      "license": null,
      "tags": [
        "adversarial-defense",
        "semantic-segmentation",
        "robustness"
      ],
      "id": 104
    },
    {
      "name": "ChatProtect",
      "one_line_profile": "Evaluation, detection, and mitigation of self-contradictory hallucinations in LLMs",
      "detailed_description": "Code implementation for detecting and mitigating self-contradictory hallucinations in Large Language Models. It provides a framework for evaluating consistency and improving model reliability.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_detection",
        "consistency_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/eth-sri/ChatProtect",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "hallucination",
        "llm",
        "reliability"
      ],
      "id": 105
    },
    {
      "name": "DiffAI",
      "one_line_profile": "Certifiable defense against adversarial examples via provable robustness training",
      "detailed_description": "A system for training neural networks to be provably robust against adversarial examples. It implements differentiable abstract interpretation to certify the robustness of deep learning models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "robustness_verification",
        "adversarial_defense"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/eth-sri/diffai",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "certified-robustness",
        "adversarial-defense",
        "abstract-interpretation"
      ],
      "id": 106
    },
    {
      "name": "RigorLLM",
      "one_line_profile": "Resilient guardrails for LLMs against undesired content",
      "detailed_description": "Implementation of RigorLLM, a framework for creating resilient guardrails to prevent Large Language Models from generating undesired or harmful content, enhancing safety in deployment.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_guardrail",
        "content_moderation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/eurekayuan/RigorLLM",
      "help_website": [],
      "license": null,
      "tags": [
        "guardrails",
        "llm-safety",
        "moderation"
      ],
      "id": 107
    },
    {
      "name": "ImageNet-Adversarial-Training",
      "one_line_profile": "State-of-the-art adversarial training for ImageNet classifiers",
      "detailed_description": "A repository providing code and pre-trained models for adversarially robust ImageNet classifiers. It serves as a benchmark and toolkit for researching adversarial robustness in large-scale computer vision.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_training",
        "robust_model"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/ImageNet-Adversarial-Training",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "adversarial-robustness",
        "imagenet",
        "computer-vision"
      ],
      "id": 108
    },
    {
      "name": "Fairlearn",
      "one_line_profile": "Toolkit to assess and improve fairness of machine learning models",
      "detailed_description": "A Python package that empowers developers of artificial intelligence systems to assess their systems' fairness and mitigate any observed unfairness issues. It offers metrics for evaluation and algorithms for bias mitigation.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "fairness_assessment",
        "bias_mitigation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/fairlearn/fairlearn",
      "help_website": [
        "https://fairlearn.org"
      ],
      "license": "MIT",
      "tags": [
        "fairness",
        "machine-learning",
        "bias-mitigation"
      ],
      "id": 109
    },
    {
      "name": "Jurity",
      "one_line_profile": "Fairness and evaluation library for AI systems",
      "detailed_description": "A Python library for evaluating the fairness of AI models. It provides a set of metrics and tools to detect bias and ensure compliance with fairness standards in machine learning applications.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "fairness_evaluation",
        "bias_detection"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/fidelity/jurity",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "fairness",
        "evaluation",
        "metrics"
      ],
      "id": 110
    },
    {
      "name": "AutoAttack",
      "one_line_profile": "Ensemble of diverse parameter-free attacks for robust evaluation",
      "detailed_description": "A standard benchmark tool for reliably evaluating the adversarial robustness of machine learning models. It utilizes an ensemble of parameter-free attacks to provide a rigorous assessment of model security.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_attack",
        "robustness_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/fra31/auto-attack",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "adversarial-robustness",
        "benchmark",
        "attack-ensemble"
      ],
      "id": 111
    },
    {
      "name": "Booster",
      "one_line_profile": "Defense against harmful fine-tuning attacks on LLMs",
      "detailed_description": "Implementation of the Booster method, designed to tackle harmful fine-tuning attacks on Large Language Models by attenuating harmful perturbations, thereby enhancing model safety.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "alignment_defense",
        "fine_tuning_safety"
      ],
      "application_level": "solver",
      "primary_language": "Shell",
      "repo_url": "https://github.com/git-disl/Booster",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-safety",
        "fine-tuning",
        "defense"
      ],
      "id": 112
    },
    {
      "name": "Lisa",
      "one_line_profile": "Lazy safety alignment for LLMs against harmful fine-tuning",
      "detailed_description": "Code for the 'Lazy Safety Alignment' (Lisa) method, which protects Large Language Models against harmful fine-tuning attacks by employing strategic alignment techniques.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "alignment_defense",
        "fine_tuning_safety"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/git-disl/Lisa",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "safety-alignment",
        "llm",
        "defense"
      ],
      "id": 113
    },
    {
      "name": "Safety-Tax",
      "one_line_profile": "Evaluation of trade-offs between safety alignment and reasoning in LLMs",
      "detailed_description": "A tool/codebase for analyzing the 'Safety Tax', quantifying how safety alignment processes may impact the reasoning capabilities of Large Language Models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_alignment_evaluation",
        "model_performance_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/git-disl/Safety-Tax",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "safety-alignment",
        "reasoning",
        "evaluation"
      ],
      "id": 114
    },
    {
      "name": "Virus",
      "one_line_profile": "Harmful fine-tuning attack framework for bypassing LLM guardrails",
      "detailed_description": "Implementation of the 'Virus' attack method, used for red teaming and evaluating the vulnerability of Large Language Models to harmful fine-tuning that bypasses guardrail moderation.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_attack",
        "red_teaming"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/git-disl/Virus",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "red-teaming",
        "jailbreak",
        "fine-tuning"
      ],
      "id": 115
    },
    {
      "name": "RobNets",
      "one_line_profile": "Neural Architecture Search for robust architectures against adversarial attacks",
      "detailed_description": "A framework combining Neural Architecture Search (NAS) with adversarial robustness to discover network architectures that are inherently more resistant to adversarial attacks.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "robust_architecture_search",
        "adversarial_defense"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/gmh14/RobNets",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nas",
        "robustness",
        "architecture-search"
      ],
      "id": 116
    },
    {
      "name": "RETVec",
      "one_line_profile": "Efficient, multilingual, and adversarially-robust text vectorizer",
      "detailed_description": "RETVec (Resilient and Efficient Text Vectorizer) is a text processing layer designed to be robust against adversarial attacks (like typos and character perturbations) while remaining efficient and multilingual.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "robust_text_vectorization",
        "adversarial_defense"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/google-research/retvec",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "text-vectorization",
        "robustness",
        "nlp"
      ],
      "id": 117
    },
    {
      "name": "Guardrails",
      "one_line_profile": "Framework for adding data validation and safety guardrails to LLMs",
      "detailed_description": "A Python package that lets users add structure, type checking, and quality assurance to the outputs of large language models. It enforces safety policies and validates LLM responses against defined specifications.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_guardrail",
        "output_validation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/guardrails-ai/guardrails",
      "help_website": [
        "https://www.guardrailsai.com/docs/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "guardrails",
        "validation",
        "llm-safety"
      ],
      "id": 118
    },
    {
      "name": "Gyroscopic Diagnostics",
      "one_line_profile": "AI safety diagnostics and alignment evaluation lab",
      "detailed_description": "A toolkit for diagnosing AI safety issues and evaluating alignment. It provides methods to assess how well AI models adhere to intended safety guidelines and alignment principles.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "alignment_evaluation",
        "safety_diagnostics"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/gyrogovernance/diagnostics",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ai-safety",
        "alignment",
        "diagnostics"
      ],
      "id": 119
    },
    {
      "name": "HolisticAI",
      "one_line_profile": "Open-source tool to assess and improve AI trustworthiness",
      "detailed_description": "A library designed to assess the trustworthiness of AI systems across multiple dimensions including bias, efficacy, robustness, and explainability. It helps in auditing and mitigating risks in AI deployments.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "trustworthiness_assessment",
        "bias_mitigation"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/holistic-ai/holisticai",
      "help_website": [
        "https://holisticai.readthedocs.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "trustworthiness",
        "audit",
        "risk-management"
      ],
      "id": 120
    },
    {
      "name": "SaLoRA",
      "one_line_profile": "Safety-Alignment Preserved Low-Rank Adaptation for LLMs",
      "detailed_description": "Implementation of SaLoRA, a method for fine-tuning Large Language Models using Low-Rank Adaptation (LoRA) while preserving safety alignment, preventing the degradation of safety features during adaptation.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_alignment",
        "peft_safety"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/homles11/SaLoRA",
      "help_website": [],
      "license": null,
      "tags": [
        "lora",
        "safety-alignment",
        "fine-tuning"
      ],
      "id": 121
    },
    {
      "name": "Circular Bias Detection",
      "one_line_profile": "Statistical framework for detecting circular reasoning bias in AI evaluation",
      "detailed_description": "A comprehensive statistical framework designed to detect circular reasoning bias in the evaluation of AI algorithms, ensuring that evaluation metrics do not unfairly favor certain models due to data leakage or self-reinforcing patterns.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "bias_detection",
        "evaluation_framework"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/hongping-zh/circular-bias-detection",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bias-detection",
        "circular-reasoning",
        "evaluation"
      ],
      "id": 122
    },
    {
      "name": "CROWN-IBP",
      "one_line_profile": "Certified defense against adversarial examples using CROWN and IBP",
      "detailed_description": "A certified defense framework for neural networks that uses CROWN (bound propagation) and IBP (Interval Bound Propagation) to verify robustness against adversarial examples and train provably robust models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "robustness_verification",
        "certified_defense"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/huanzhang12/CROWN-IBP",
      "help_website": [],
      "license": "BSD-2-Clause",
      "tags": [
        "certified-robustness",
        "verification",
        "neural-networks"
      ],
      "id": 123
    },
    {
      "name": "AISploit",
      "one_line_profile": "Package for red teaming and exploiting LLM AI solutions",
      "detailed_description": "A Python package designed to assist red teams and penetration testers in identifying vulnerabilities in Large Language Model (LLM) solutions. It provides tools for exploiting and testing the security of AI deployments.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "red_teaming",
        "adversarial_exploitation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/hupe1980/aisploit",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "red-teaming",
        "exploit",
        "llm-security"
      ],
      "id": 124
    },
    {
      "name": "Adversarial Robustness PyTorch",
      "one_line_profile": "PyTorch implementation of adversarial training and robustness methods",
      "detailed_description": "An unofficial but widely used implementation of key DeepMind papers on adversarial training and data augmentation for improving adversarial robustness in PyTorch.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_training",
        "robustness_defense"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/imrahulr/adversarial_robustness_pytorch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "adversarial-training",
        "pytorch",
        "robustness"
      ],
      "id": 125
    },
    {
      "name": "PatchGuard",
      "one_line_profile": "Provably robust defense against adversarial patches",
      "detailed_description": "Code for PatchGuard, a defense mechanism that provides provable robustness against adversarial patch attacks by utilizing small receptive fields and masking techniques in vision models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_defense",
        "robust_vision"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/inspire-group/PatchGuard",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "adversarial-patch",
        "defense",
        "computer-vision"
      ],
      "id": 126
    },
    {
      "name": "HYDRA",
      "one_line_profile": "Pruning technique for creating adversarially robust neural networks",
      "detailed_description": "A PyTorch implementation of HYDRA (Pruning Adversarially Robust Neural Networks), a technique to compress neural networks while maintaining their robustness against adversarial attacks. It provides code for training, pruning, and evaluating robust models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_robustness",
        "model_compression"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/inspire-group/hydra",
      "help_website": [
        "https://arxiv.org/abs/2002.10509"
      ],
      "license": null,
      "tags": [
        "adversarial-robustness",
        "pruning",
        "neural-network-compression"
      ],
      "id": 127
    },
    {
      "name": "IMMUNE",
      "one_line_profile": "Inference-time alignment defense against jailbreaks in Multi-modal LLMs",
      "detailed_description": "Official implementation of the CVPR 2025 paper 'IMMUNE'. It provides a defense mechanism to improve the safety of Multi-modal Large Language Models (MLLMs) against jailbreak attacks during inference time.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "jailbreak_defense",
        "safety_alignment"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/itsvaibhav01/Immune",
      "help_website": [],
      "license": null,
      "tags": [
        "jailbreak-defense",
        "mllm",
        "safety-alignment"
      ],
      "id": 128
    },
    {
      "name": "PhD (Prompted Hallucination Dataset)",
      "one_line_profile": "Large-scale visual hallucination evaluation dataset for LVLMs",
      "detailed_description": "A ChatGPT-Prompted Visual hallucination Evaluation Dataset (PhD) for Large Vision-Language Models. It features over 100,000 data samples with extensive contextual descriptions and counterintuitive images to evaluate hallucination.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_evaluation",
        "dataset"
      ],
      "application_level": "dataset",
      "primary_language": null,
      "repo_url": "https://github.com/jiazhen-code/PhD",
      "help_website": [],
      "license": null,
      "tags": [
        "hallucination",
        "lvlm",
        "benchmark"
      ],
      "id": 129
    },
    {
      "name": "ToLD-Br",
      "one_line_profile": "Toxic language detection dataset for Brazilian Portuguese",
      "detailed_description": "A dataset and analysis code for toxic language detection in Brazilian Portuguese social media posts. It supports multilingual analysis and benchmarking of toxicity detection models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "toxicity_detection",
        "dataset"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/joaoaleite/ToLD-Br",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "toxicity-detection",
        "nlp",
        "portuguese"
      ],
      "id": 130
    },
    {
      "name": "AMBER",
      "one_line_profile": "Multi-dimensional benchmark for multi-modal hallucination evaluation",
      "detailed_description": "An LLM-free, multi-dimensional benchmark designed to evaluate hallucinations in Multi-modal Large Language Models (MLLMs). It covers various types of hallucinations and provides a standardized evaluation framework.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_evaluation",
        "benchmark"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/junyangwang0410/AMBER",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "hallucination",
        "mllm",
        "benchmark"
      ],
      "id": 131
    },
    {
      "name": "ToxVidLM",
      "one_line_profile": "Dataset for toxicity detection in code-mixed Hinglish video content",
      "detailed_description": "Code and datasets from the ACL 2024 paper focusing on toxicity detection in code-mixed Hinglish (Hindi-English) video content, addressing multimodal toxicity challenges.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "toxicity_detection",
        "dataset"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/justaguyalways/ToxVidLM_ACL_2024",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "toxicity",
        "multimodal",
        "hinglish"
      ],
      "id": 132
    },
    {
      "name": "MobileSafetyBench",
      "one_line_profile": "Benchmark for evaluating safety of autonomous agents in mobile device control",
      "detailed_description": "A benchmark for evaluating the safety of autonomous agents when controlling mobile devices, presented at AAAI 2026 AI Alignment Track. It assesses risks associated with agentic actions on mobile platforms.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_evaluation",
        "agent_safety"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/jylee425/mobilesafetybench",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "agent-safety",
        "benchmark",
        "mobile-agents"
      ],
      "id": 133
    },
    {
      "name": "AVHBench",
      "one_line_profile": "Cross-modal hallucination evaluation for Audio-Visual LLMs",
      "detailed_description": "Official repository for the ICLR 2025 paper 'AVHBench'. It is a benchmark designed to evaluate hallucinations in Audio-Visual Large Language Models, focusing on cross-modal inconsistencies.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_evaluation",
        "benchmark"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/kaist-ami/AVHBench",
      "help_website": [],
      "license": null,
      "tags": [
        "audio-visual",
        "hallucination",
        "benchmark"
      ],
      "id": 134
    },
    {
      "name": "BEAF",
      "one_line_profile": "Evaluation method for hallucination in Vision-Language Models",
      "detailed_description": "Official repository for the ECCV 2024 paper 'BEAF'. It proposes a method to evaluate hallucinations in Vision-Language Models by observing Before-After changes in visual inputs.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_evaluation",
        "methodology"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/kaist-ami/BEAF",
      "help_website": [],
      "license": null,
      "tags": [
        "hallucination",
        "vlm",
        "evaluation"
      ],
      "id": 135
    },
    {
      "name": "Kereva Scanner",
      "one_line_profile": "Code scanner for detecting issues in prompts and LLM calls",
      "detailed_description": "A static analysis tool and scanner designed to check for security issues, PII leakage, and safety concerns in LLM prompts and API calls within codebases.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_scanning",
        "prompt_security"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/kereva-dev/kereva-scanner",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "security-scanner",
        "llm-safety",
        "pii-detection"
      ],
      "id": 136
    },
    {
      "name": "fairness",
      "one_line_profile": "R package for computing and visualizing fair ML metrics",
      "detailed_description": "An R package that provides functions to compute and visualize various fairness metrics for machine learning models, helping researchers evaluate algorithmic bias.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "fairness_evaluation",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/kozodoi/fairness",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "fairness",
        "r-package",
        "bias-evaluation"
      ],
      "id": 137
    },
    {
      "name": "LangEvals",
      "one_line_profile": "Platform for aggregating LLM evaluators and guardrails",
      "detailed_description": "A platform and library that aggregates various language model evaluators, providing a standard interface for scoring and implementing guardrails to benchmark and protect LLM pipelines.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "llm_evaluation",
        "guardrails"
      ],
      "application_level": "platform",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/langwatch/langevals",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm-evaluation",
        "guardrails",
        "benchmarking"
      ],
      "id": 138
    },
    {
      "name": "convex_adversarial",
      "one_line_profile": "Method for training provably robust neural networks",
      "detailed_description": "A Python library implementing methods for training neural networks that are provably robust to adversarial attacks, using convex relaxation techniques.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_robustness",
        "robust_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/locuslab/convex_adversarial",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "adversarial-robustness",
        "verification",
        "neural-networks"
      ],
      "id": 139
    },
    {
      "name": "smoothing",
      "one_line_profile": "Randomized smoothing for provable adversarial robustness",
      "detailed_description": "Code for 'Provable adversarial robustness at ImageNet scale', implementing randomized smoothing techniques to certify the robustness of deep learning models against adversarial perturbations.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_robustness",
        "certified_robustness"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/locuslab/smoothing",
      "help_website": [],
      "license": null,
      "tags": [
        "randomized-smoothing",
        "robustness",
        "certification"
      ],
      "id": 140
    },
    {
      "name": "Square Attack",
      "one_line_profile": "Query-efficient black-box adversarial attack",
      "detailed_description": "Implementation of Square Attack, a score-based black-box adversarial attack that does not require gradient information, suitable for evaluating model robustness.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_attack",
        "robustness_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/max-andr/square-attack",
      "help_website": [
        "https://arxiv.org/abs/1912.00049"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "adversarial-attack",
        "black-box",
        "robustness"
      ],
      "id": 141
    },
    {
      "name": "FairBench",
      "one_line_profile": "Comprehensive benchmark framework for exploring and evaluating AI fairness",
      "detailed_description": "FairBench is a comprehensive framework designed for the exploration and evaluation of fairness in AI models. It provides a suite of tools and metrics to assess bias and ensure equitable outcomes across different demographic groups in machine learning applications.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "fairness_evaluation",
        "bias_detection"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/mever-team/FairBench",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "fairness",
        "bias",
        "benchmark",
        "ai-ethics"
      ],
      "id": 142
    },
    {
      "name": "AI Agent Evals",
      "one_line_profile": "Evaluation framework for AI agents using model-as-judge and safety metrics",
      "detailed_description": "A tool designed to evaluate AI agent applications, focusing on performance, content safety, and mathematical metrics. It utilizes a 'model as the judge' approach to assess the quality and safety of agent outputs, suitable for integration into development workflows.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "agent_evaluation",
        "safety_metrics",
        "model_as_judge"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/ai-agent-evals",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ai-agents",
        "evaluation",
        "safety",
        "metrics"
      ],
      "id": 143
    },
    {
      "name": "R-Bench",
      "one_line_profile": "Benchmark for evaluating relationship hallucinations in Large Vision-Language Models",
      "detailed_description": "R-Bench is a benchmark specifically designed to evaluate and analyze relationship hallucinations in Large Vision-Language Models (LVLMs). It provides a dataset and evaluation scripts to assess how well models perceive and describe relationships between objects in images.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_detection",
        "vision_language_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/mrwu-mac/R-Bench",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "hallucination",
        "lvlm",
        "benchmark",
        "computer-vision"
      ],
      "id": 144
    },
    {
      "name": "Agentic Security",
      "one_line_profile": "Vulnerability scanner and red teaming toolkit for Agentic LLMs",
      "detailed_description": "Agentic Security is a comprehensive vulnerability scanner and red teaming toolkit designed for Agentic Large Language Models. It helps identify security flaws, safety issues, and potential exploits in AI agents through automated testing and evaluation.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "red_teaming",
        "vulnerability_scanning",
        "safety_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/msoedov/agentic_security",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "red-teaming",
        "llm-security",
        "vulnerability-scanner",
        "ai-safety"
      ],
      "id": 145
    },
    {
      "name": "GenAIEval",
      "one_line_profile": "Evaluation benchmark and scorecard for Generative AI performance and safety",
      "detailed_description": "GenAIEval is a project under OPEA that provides tools for evaluating Generative AI models. It includes benchmarks and scorecards for assessing throughput, latency, accuracy, safety, and hallucination rates, facilitating comprehensive model performance analysis.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "performance_evaluation",
        "safety_benchmark",
        "hallucination_detection"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/opea-project/GenAIEval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "genai",
        "evaluation",
        "benchmark",
        "safety"
      ],
      "id": 146
    },
    {
      "name": "OpenAI Evals",
      "one_line_profile": "Standard framework for evaluating LLMs and an open registry of benchmarks",
      "detailed_description": "Evals is a framework developed by OpenAI for evaluating Large Language Models (LLMs) and LLM systems. It provides a registry of benchmarks and tools to test models against various tasks, ensuring performance, safety, and alignment standards are met.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "llm_evaluation",
        "benchmark_registry",
        "model_testing"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/openai/evals",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "llm",
        "evaluation",
        "benchmark",
        "openai"
      ],
      "id": 147
    },
    {
      "name": "OpenGuardrails",
      "one_line_profile": "Open-source customizable AI guardrails for inference pipeline security",
      "detailed_description": "OpenGuardrails is an open-source framework for implementing customizable guardrails in AI applications. It protects the AI inference pipeline by scanning prompts, models, agents, and outputs, allowing users to define custom scanners and security policies.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_guardrails",
        "inference_security",
        "input_output_scanning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/openguardrails/openguardrails",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "guardrails",
        "ai-security",
        "inference-protection"
      ],
      "id": 148
    },
    {
      "name": "HAI Guardrails",
      "one_line_profile": "TypeScript library providing safety guards for LLM applications",
      "detailed_description": "HAI Guardrails is a TypeScript library that provides a set of guardrails for Large Language Model (LLM) applications. It helps developers implement safety checks and content filtering mechanisms to ensure responsible AI usage.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_guardrails",
        "content_filtering"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/presidio-oss/hai-guardrails",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "guardrails",
        "llm",
        "typescript",
        "safety"
      ],
      "id": 149
    },
    {
      "name": "NoMIRACL",
      "one_line_profile": "Multilingual hallucination evaluation dataset for RAG robustness",
      "detailed_description": "NoMIRACL is a dataset designed to evaluate the robustness of Large Language Models (LLMs) in Retrieval-Augmented Generation (RAG) settings. It focuses on hallucination detection across 18 languages, specifically targeting scenarios with first-stage retrieval errors.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_evaluation",
        "rag_robustness",
        "multilingual_benchmark"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/project-miracl/nomiracl",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "hallucination",
        "rag",
        "multilingual",
        "dataset"
      ],
      "id": 150
    },
    {
      "name": "Promptfoo",
      "one_line_profile": "CLI tool for testing, red teaming, and evaluating LLM prompts and agents",
      "detailed_description": "Promptfoo is a command-line tool and library for evaluating Large Language Models (LLMs). It supports testing prompts, agents, and RAG pipelines, offering features for AI red teaming, pentesting, and vulnerability scanning with declarative configuration.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "red_teaming",
        "prompt_evaluation",
        "vulnerability_scanning"
      ],
      "application_level": "solver",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/promptfoo/promptfoo",
      "help_website": [
        "https://www.promptfoo.dev/"
      ],
      "license": "MIT",
      "tags": [
        "red-teaming",
        "llm-testing",
        "prompt-engineering",
        "evaluation"
      ],
      "id": 151
    },
    {
      "name": "Hallucination Index",
      "one_line_profile": "Ranking and evaluation initiative for LLM hallucination propensity",
      "detailed_description": "The Hallucination Index is an initiative to evaluate and rank popular Large Language Models (LLMs) based on their propensity to hallucinate across various task types. It provides benchmarks and data to help users choose reliable models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_ranking",
        "model_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/rungalileo/hallucination-index",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "hallucination",
        "leaderboard",
        "llm-ranking"
      ],
      "id": 152
    },
    {
      "name": "OpenAgentSafety",
      "one_line_profile": "Framework for evaluating AI agent safety in realistic environments",
      "detailed_description": "OpenAgentSafety is a framework designed to evaluate the safety of AI agents within realistic environments. It provides methodologies and tools to assess how agents behave in complex scenarios, ensuring they operate safely and reliably.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "agent_safety",
        "environment_simulation",
        "behavioral_evaluation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/sani903/OpenAgentSafety",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ai-agents",
        "safety",
        "evaluation",
        "simulation"
      ],
      "id": 153
    },
    {
      "name": "frAI",
      "one_line_profile": "Open-source toolkit for responsible AI with scanning and reporting capabilities",
      "detailed_description": "frAI is an open-source toolkit for responsible AI that includes a CLI and SDK. It allows users to scan code, collect evidence, and generate model cards, risk files, and evaluations, facilitating compliance and transparency in AI development.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "responsible_ai",
        "model_card_generation",
        "risk_assessment"
      ],
      "application_level": "toolkit",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/sebuzdugan/frai",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "responsible-ai",
        "compliance",
        "model-cards",
        "risk-management"
      ],
      "id": 154
    },
    {
      "name": "STATE-ToxiCN",
      "one_line_profile": "Benchmark for span-level target-aware toxicity extraction in Chinese hate speech",
      "detailed_description": "A benchmark dataset and evaluation framework designed for span-level target-aware toxicity extraction in Chinese hate speech detection, supporting research in safety and bias evaluation.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "toxicity_detection",
        "benchmark_dataset"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/shenmeyemeifashengguo/STATE-ToxiCN",
      "help_website": [],
      "license": null,
      "tags": [
        "toxicity-detection",
        "chinese-nlp",
        "benchmark"
      ],
      "id": 155
    },
    {
      "name": "LLM-Detector-Robustness",
      "one_line_profile": "Red teaming framework for language model detectors",
      "detailed_description": "Code implementation for red teaming language model detectors using other language models, focusing on evaluating the robustness of AI-generated text detectors.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "red_teaming",
        "robustness_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/shizhouxing/LLM-Detector-Robustness",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "red-teaming",
        "llm-detection",
        "robustness"
      ],
      "id": 156
    },
    {
      "name": "Red-Teaming-Language-Models",
      "one_line_profile": "Implementation of red teaming language models with language models",
      "detailed_description": "A re-implementation of the 'Red Teaming Language Models with Language Models' paper, providing tools to generate adversarial test cases for evaluating LLM safety.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "red_teaming",
        "safety_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/shreyansh26/Red-Teaming-Language-Models-with-Language-Models",
      "help_website": [],
      "license": null,
      "tags": [
        "red-teaming",
        "llm",
        "adversarial-generation"
      ],
      "id": 157
    },
    {
      "name": "SIUO",
      "one_line_profile": "Cross-modality safety alignment framework",
      "detailed_description": "A framework for cross-modality safety alignment in multimodal models, providing methods to evaluate and improve safety across different data modalities.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_alignment",
        "multimodal_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "HTML",
      "repo_url": "https://github.com/sinwang20/SIUO",
      "help_website": [],
      "license": null,
      "tags": [
        "safety-alignment",
        "multimodal",
        "llm"
      ],
      "id": 158
    },
    {
      "name": "Kov.jl",
      "one_line_profile": "Black-box red teaming of LLMs using MDPs",
      "detailed_description": "A Julia package for black-box red teaming and jailbreaking of large language models using Markov Decision Processes (MDPs) to discover failure modes.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "red_teaming",
        "jailbreaking"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/sisl/Kov.jl",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "red-teaming",
        "julia",
        "mdp",
        "jailbreak"
      ],
      "id": 159
    },
    {
      "name": "ASTRA",
      "one_line_profile": "Adaptive Stress Testing for Robust AI toolbox",
      "detailed_description": "The Adaptive Stress Testing for Robust AI (ASTRA) toolbox provides tooling to support model developers in making more robust AI systems through adaptive stress testing and adversarial training.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "stress_testing",
        "robustness_evaluation"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/sisl/astra-rl",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "stress-testing",
        "robustness",
        "adversarial-training"
      ],
      "id": 160
    },
    {
      "name": "toxic-comments-detection-in-russian",
      "one_line_profile": "Toxic comments detection model for Russian language",
      "detailed_description": "A tool and model for detecting toxic comments specifically in the Russian language, useful for content moderation and safety evaluation.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "toxicity_detection"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/sismetanin/toxic-comments-detection-in-russian",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "toxicity-detection",
        "russian-nlp",
        "bert"
      ],
      "id": 161
    },
    {
      "name": "FairClassifier",
      "one_line_profile": "Neural Network fairness evaluation package",
      "detailed_description": "An open source package that evaluates the fairness of a Neural Network using the p% rule fairness metric.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "fairness_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/suraz09/FairClassifier",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "fairness",
        "neural-networks",
        "metrics"
      ],
      "id": 162
    },
    {
      "name": "Re-Align",
      "one_line_profile": "Alignment framework to mitigate hallucinations in VLMs",
      "detailed_description": "A novel alignment framework that leverages image retrieval to mitigate hallucinations in Vision Language Models (VLMs).",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_mitigation",
        "alignment"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/taco-group/Re-Align",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "vlm",
        "hallucination",
        "alignment"
      ],
      "id": 163
    },
    {
      "name": "rGAN",
      "one_line_profile": "Label-Noise Robust Generative Adversarial Networks",
      "detailed_description": "Implementation of rGAN, a Generative Adversarial Network designed to be robust against label noise.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "robust_modeling",
        "generative_models"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/takuhirok/rGAN",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gan",
        "robustness",
        "label-noise"
      ],
      "id": 164
    },
    {
      "name": "ToxiBenchCN",
      "one_line_profile": "Benchmark for multimodal toxic Chinese detection",
      "detailed_description": "A benchmark and taxonomy for exploring multimodal challenges in toxic Chinese detection, providing data and evaluation standards.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "toxicity_detection",
        "multimodal_benchmark"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/thomasyyyoung/ToxiBenchCN",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "toxicity",
        "multimodal",
        "benchmark",
        "chinese"
      ],
      "id": 165
    },
    {
      "name": "AISafetyLab",
      "one_line_profile": "Comprehensive framework for AI safety attack, defense, and evaluation",
      "detailed_description": "A comprehensive framework covering safety attack, defense, and evaluation for AI systems, facilitating research in AI safety.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_evaluation",
        "adversarial_attack",
        "defense"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/thu-coai/AISafetyLab",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ai-safety",
        "evaluation",
        "attack-defense"
      ],
      "id": 166
    },
    {
      "name": "cotk",
      "one_line_profile": "Toolkit for fast development and fair evaluation of text generation",
      "detailed_description": "Conversational Toolkit (cotk) is an open-source toolkit designed for fast development and fair evaluation of text generation models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "text_generation_evaluation",
        "fairness"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/thu-coai/cotk",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "text-generation",
        "evaluation",
        "nlp"
      ],
      "id": 167
    },
    {
      "name": "MMTrustEval",
      "one_line_profile": "Toolbox for benchmarking trustworthiness of multimodal LLMs",
      "detailed_description": "A toolbox for benchmarking the trustworthiness of multimodal large language models, covering various safety and reliability dimensions.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "trustworthiness_evaluation",
        "multimodal_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/thu-ml/MMTrustEval",
      "help_website": [],
      "license": "CC-BY-SA-4.0",
      "tags": [
        "multimodal",
        "trustworthiness",
        "benchmark"
      ],
      "id": 168
    },
    {
      "name": "STAIR",
      "one_line_profile": "Safety alignment with introspective reasoning",
      "detailed_description": "Codebase for improving safety alignment in language models using introspective reasoning techniques.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_alignment",
        "reasoning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/thu-ml/STAIR",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "safety-alignment",
        "llm",
        "reasoning"
      ],
      "id": 169
    },
    {
      "name": "ares",
      "one_line_profile": "Library for benchmarking adversarial robustness",
      "detailed_description": "A Python library for adversarial machine learning focusing on benchmarking adversarial robustness of models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_robustness",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/thu-ml/ares",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "adversarial-ml",
        "robustness",
        "benchmark"
      ],
      "id": 170
    },
    {
      "name": "understanding-fast-adv-training",
      "one_line_profile": "Implementation of Fast Adversarial Training",
      "detailed_description": "Code for understanding and improving fast adversarial training methods to enhance model robustness.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_training",
        "robustness"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/tml-epfl/understanding-fast-adv-training",
      "help_website": [],
      "license": null,
      "tags": [
        "adversarial-training",
        "robustness"
      ],
      "id": 171
    },
    {
      "name": "redteam-ai-benchmark",
      "one_line_profile": "Benchmark for evaluating uncensored LLMs for offensive security",
      "detailed_description": "A benchmark suite designed to evaluate uncensored Large Language Models in the context of offensive security and red teaming.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "red_teaming",
        "security_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/toxy4ny/redteam-ai-benchmark",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "red-teaming",
        "llm",
        "security"
      ],
      "id": 172
    },
    {
      "name": "trulens",
      "one_line_profile": "Evaluation and tracking for LLM experiments",
      "detailed_description": "A library for evaluating and tracking Large Language Model (LLM) experiments, providing feedback functions to assess relevance, groundedness, and other metrics.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "llm_evaluation",
        "experiment_tracking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/truera/trulens",
      "help_website": [
        "https://www.trulens.org/"
      ],
      "license": "MIT",
      "tags": [
        "llm-evaluation",
        "observability",
        "metrics"
      ],
      "id": 173
    },
    {
      "name": "image-crop-analysis",
      "one_line_profile": "Analysis tools for image cropping fairness",
      "detailed_description": "Code and tools for analyzing fairness metrics in image cropping algorithms, specifically addressing representation and bias.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "fairness_analysis",
        "bias_detection"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/twitter-research/image-crop-analysis",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "fairness",
        "image-processing",
        "bias"
      ],
      "id": 174
    },
    {
      "name": "armory",
      "one_line_profile": "Adversarial Robustness Evaluation Test Bed",
      "detailed_description": "A test bed for evaluating the adversarial robustness of machine learning models, providing a standardized environment for testing defenses.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "adversarial_defense"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/twosixlabs/armory",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "adversarial-robustness",
        "evaluation",
        "testbed"
      ],
      "id": 175
    },
    {
      "name": "Metric-Fairness",
      "one_line_profile": "Evaluation of social bias in text generation metrics",
      "detailed_description": "Tools and code for analyzing social bias in language model-based metrics (like BERTScore) for text generation.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "metric_evaluation",
        "bias_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/txsun1997/Metric-Fairness",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fairness",
        "metrics",
        "bias"
      ],
      "id": 176
    },
    {
      "name": "Quantus",
      "one_line_profile": "Explainable AI toolkit for evaluating neural network explanations",
      "detailed_description": "Quantus is an eXplainable AI (XAI) toolkit designed for the responsible evaluation of neural network explanations using various metrics.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "xai_evaluation",
        "interpretability"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/understandable-machine-intelligence-lab/Quantus",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "xai",
        "evaluation",
        "interpretability"
      ],
      "id": 177
    },
    {
      "name": "Oversight",
      "one_line_profile": "Modular LLM Red Teaming and Vulnerability Research Framework",
      "detailed_description": "A modular framework for reverse engineering, red teaming, and vulnerability research on Large Language Models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "red_teaming",
        "vulnerability_research"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/user1342/Oversight",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "red-teaming",
        "llm",
        "security"
      ],
      "id": 178
    },
    {
      "name": "configurable-safety-tuning",
      "one_line_profile": "Safety tuning of LMs with synthetic preference data",
      "detailed_description": "Code and data for configurable safety tuning of language models using synthetic preference data to align models with safety guidelines.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_tuning",
        "alignment"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/vicgalle/configurable-safety-tuning",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "safety-tuning",
        "alignment",
        "synthetic-data"
      ],
      "id": 179
    },
    {
      "name": "MagnetLoss-PyTorch",
      "one_line_profile": "PyTorch implementation of Magnet Loss for metric learning",
      "detailed_description": "A PyTorch implementation of Magnet Loss, a deep metric learning technique, useful for learning embeddings with specific structural properties.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "metric_learning",
        "loss_function"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/vithursant/MagnetLoss-PyTorch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "metric-learning",
        "pytorch",
        "loss-function"
      ],
      "id": 180
    },
    {
      "name": "Trojan-Activation-Attack",
      "one_line_profile": "Trojan attack on LLMs using activation steering",
      "detailed_description": "Implementation of Trojan Activation Attack, a method to attack Large Language Models using activation steering to bypass safety alignment.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_attack",
        "safety_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/wang2226/Trojan-Activation-Attack",
      "help_website": [],
      "license": null,
      "tags": [
        "trojan-attack",
        "llm",
        "activation-steering"
      ],
      "id": 181
    },
    {
      "name": "CHALE",
      "one_line_profile": "Controlled Hallucination-Evaluation Dataset",
      "detailed_description": "A dataset designed for the controlled evaluation of hallucinations in Question-Answering systems.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_evaluation",
        "dataset"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/weijiaheng/CHALE",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "hallucination",
        "dataset",
        "qa"
      ],
      "id": 182
    },
    {
      "name": "circle-guard-bench",
      "one_line_profile": "Benchmark for evaluating LLM guard systems",
      "detailed_description": "A benchmark for evaluating the protection capabilities of large language model (LLM) guard systems, including guardrails and safeguards.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_evaluation",
        "guardrails_benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/whitecircle-ai/circle-guard-bench",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "guardrails",
        "benchmark",
        "llm-security"
      ],
      "id": 183
    },
    {
      "name": "drug_det_ro",
      "one_line_profile": "Toxic and narcotic medication detection with rotated object detector",
      "detailed_description": "Source code for detecting toxic and narcotic medications using rotated object detectors, applicable in medical safety and monitoring.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "object_detection",
        "safety_monitoring"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/woodywff/drug_det_ro",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "object-detection",
        "medical-safety",
        "rotated-bbox"
      ],
      "id": 184
    },
    {
      "name": "Tri-HE",
      "one_line_profile": "Unified triplet-level hallucination evaluation for LVLMs",
      "detailed_description": "Code and data for evaluating hallucinations in Large Vision-Language Models (LVLMs) at the triplet level.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_evaluation",
        "vlm"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/wujunjie1998/Tri-HE",
      "help_website": [],
      "license": null,
      "tags": [
        "hallucination",
        "vlm",
        "evaluation"
      ],
      "id": 185
    },
    {
      "name": "Adversarial_Long-Tail",
      "one_line_profile": "Adversarial robustness under long-tailed distribution",
      "detailed_description": "PyTorch implementation for improving adversarial robustness in scenarios with long-tailed data distributions.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_robustness",
        "long_tail_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/wutong16/Adversarial_Long-Tail",
      "help_website": [],
      "license": null,
      "tags": [
        "robustness",
        "long-tail",
        "adversarial-training"
      ],
      "id": 186
    },
    {
      "name": "veiled-toxicity-detection",
      "one_line_profile": "Detection of veiled toxicity in speech",
      "detailed_description": "Tools and methods for fortifying toxic speech detectors against veiled or subtle toxicity.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "toxicity_detection",
        "robustness"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/xhan77/veiled-toxicity-detection",
      "help_website": [],
      "license": null,
      "tags": [
        "toxicity",
        "nlp",
        "safety"
      ],
      "id": 187
    }
  ]
}
{
  "generated_at": "2025-12-16T09:11:25.796472+08:00",
  "metadata": {
    "leaf_cluster": {
      "leaf_cluster_id": "AI4",
      "leaf_cluster_name": "科研评测-基准/指标/复现实验生态",
      "domain": "AI Toolchain",
      "typical_objects": "datasets/tasks",
      "task_chain": "任务→指标→自动评测→leaderboard→回归",
      "tool_form": "benchmark + eval harness"
    },
    "unit": {
      "unit_id": "AI4-01",
      "unit_name": "基准集合与任务定义",
      "target_scale": "150–350",
      "coverage_tools": "benchmark repos、task specs"
    },
    "search": {
      "target_candidates": 350,
      "queries": [
        "[GH] OpenCatalyst",
        "[GH] Matbench",
        "[GH] HuggingFace Evaluate",
        "[GH] SciBench",
        "[GH] MoleculeNet",
        "[GH] PDEBench",
        "[GH] OpenCompass",
        "[GH] BIG-bench",
        "[GH] HELM",
        "[GH] lm-evaluation-harness",
        "[GH] benchmark suite",
        "[GH] evaluation harness",
        "[GH] task definition",
        "[GH] model evaluation framework",
        "[GH] dataset collection",
        "[GH] leaderboard source",
        "[GH] scientific benchmark",
        "[GH] AI4S benchmark",
        "[GH] standardized datasets",
        "[GH] reproducibility framework",
        "[GH] LLM evaluation",
        "[GH] metric collection",
        "[GH] PDE benchmark",
        "[GH] molecular benchmark",
        "[WEB] LLM evaluation harness github",
        "[WEB] AI for science benchmark suite github",
        "[WEB] machine learning task specification github",
        "[WEB] scientific dataset collection github",
        "[WEB] model leaderboard framework github",
        "[WEB] reproducible research benchmark github"
      ],
      "total_candidates": 1117,
      "tool_candidates": 767,
      "final_tools": 320
    }
  },
  "tools": [
    {
      "name": "SLM4Mol",
      "one_line_profile": "Quantitative benchmark and analysis suite for molecular large language models",
      "detailed_description": "A comprehensive benchmark and analysis framework designed for Molecular Large Language Models (MolLLMs). It provides quantitative evaluation metrics and datasets to assess the performance of LLMs on molecular tasks, facilitating the development of AI for molecular science.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "molecular_modeling"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/AI-HPC-Research-Team/SLM4Mol",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "molecular-llm",
        "benchmark",
        "chemistry",
        "ai4science"
      ],
      "id": 1
    },
    {
      "name": "AISBench Benchmark",
      "one_line_profile": "Model evaluation tool extending OpenCompass for service-based models",
      "detailed_description": "An AI model evaluation tool built upon the OpenCompass framework. It is compatible with OpenCompass's configuration system and dataset structure but extends capabilities to support service-based models, facilitating comprehensive benchmarking of AI systems.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/AISBench/benchmark",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "benchmarking",
        "model-evaluation",
        "opencompass",
        "ai-infrastructure"
      ],
      "id": 2
    },
    {
      "name": "ThinkingAgent",
      "one_line_profile": "Evaluation framework for rating overthinking behavior in LLMs",
      "detailed_description": "A systematic evaluation framework designed to automatically rate and analyze 'overthinking' behavior in Large Language Models. It serves as a tool for behavioral analysis and interpretability of LLM reasoning processes.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "interpretability"
      ],
      "application_level": "solver",
      "primary_language": "Shell",
      "repo_url": "https://github.com/AlexCuadron/ThinkingAgent",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm-evaluation",
        "reasoning",
        "behavioral-analysis"
      ],
      "id": 3
    },
    {
      "name": "DocGenome",
      "one_line_profile": "Large-scale scientific document benchmark for multi-modal large models",
      "detailed_description": "An open, large-scale benchmark specifically designed for training and testing Multi-modal Large Models (MLLMs) on scientific documents. It facilitates the evaluation of AI models' capabilities in understanding and processing complex scientific literature.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "scientific_literature_mining"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/Alpha-Innovator/DocGenome",
      "help_website": [],
      "license": "CC-BY-4.0",
      "tags": [
        "scientific-documents",
        "multimodal-llm",
        "benchmark",
        "dataset"
      ],
      "id": 4
    },
    {
      "name": "FedHeteroBench",
      "one_line_profile": "Benchmark framework for data heterogeneity in federated learning",
      "detailed_description": "A repository of frameworks and unified implementations for handling data heterogeneity in federated learning (Non-IID settings). It provides reproducible experiments and benchmarks to evaluate FL algorithms under heterogeneous data conditions.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "federated_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/AntonioZC666/FedHeteroBench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "federated-learning",
        "benchmark",
        "non-iid",
        "data-heterogeneity"
      ],
      "id": 5
    },
    {
      "name": "EmotionCircuits-LLM",
      "one_line_profile": "Framework for discovering and controlling emotion circuits in LLMs",
      "detailed_description": "A complete and reproducible framework for analyzing, discovering, and controlling 'emotion circuits' within Large Language Models. It provides tools for interpretability and mechanistic analysis of LLM internal states.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "interpretability",
        "model_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Aurora-cx/EmotionCircuits-LLM",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "llm-interpretability",
        "mechanistic-interpretability",
        "emotion-analysis"
      ],
      "id": 6
    },
    {
      "name": "ai4sci-2021-denovo-benchmarks",
      "one_line_profile": "Benchmarks for de novo molecular design from NeurIPS AI4Sci workshop",
      "detailed_description": "Code and benchmark definitions for the 'A Fresh Look at De Novo Molecular Design Benchmarks' paper presented at the NeurIPS 2021 AI for Science Workshop. It provides a standardized environment for evaluating molecular generation models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "molecular_design"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/AustinT/ai4sci-2021-denovo-benchmarks",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "molecular-design",
        "benchmark",
        "neurips-workshop",
        "drug-discovery"
      ],
      "id": 7
    },
    {
      "name": "BigVectorBench",
      "one_line_profile": "Benchmark suite for vector database embedding performance",
      "detailed_description": "A benchmark suite designed to evaluate the embedding performance of vector databases. It supports heterogeneous data and abstracts compound queries (multimodal or single-modal) to assess vector search capabilities critical for AI applications.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "vector_search"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/BenchCouncil/BigVectorBench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "vector-database",
        "benchmark",
        "embedding",
        "ai-infrastructure"
      ],
      "id": 8
    },
    {
      "name": "ko-lm-evaluation-harness",
      "one_line_profile": "Evaluation harness for Korean language models",
      "detailed_description": "A specialized fork of the lm-evaluation-harness adapted for Korean Language Models. It provides a framework for few-shot evaluation of autoregressive language models on Korean datasets.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "nlp_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Beomi/ko-lm-evaluation-harness",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm-evaluation",
        "korean-nlp",
        "benchmark"
      ],
      "id": 9
    },
    {
      "name": "MolGenBench",
      "one_line_profile": "Benchmark for molecular generative models from de novo design to lead optimization",
      "detailed_description": "A codebase and benchmark suite for evaluating the real-world applicability of molecular generative models. It covers tasks ranging from de novo design to lead optimization, providing metrics for chemical validity and optimization success.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "molecular_generation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/CAODH/MolGenBench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "molecular-generation",
        "drug-design",
        "benchmark",
        "lead-optimization"
      ],
      "id": 10
    },
    {
      "name": "ctf4science",
      "one_line_profile": "Benchmarking framework for modeling dynamic systems (ODEs/PDEs)",
      "detailed_description": "A modular and extensible framework for benchmarking modeling methods on dynamic systems. It supports the evaluation of models for ordinary differential equations (ODEs) and partial differential equations (PDEs) using standardized datasets and metrics.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "dynamic_systems_modeling"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/CTF-for-Science/ctf4science",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pde-solving",
        "ode-solving",
        "benchmark",
        "scientific-modeling"
      ],
      "id": 11
    },
    {
      "name": "Matcha (variationanalysis)",
      "one_line_profile": "Framework for training and evaluating deep learning models for genomic variation calling",
      "detailed_description": "The Matcha framework, part of the variationanalysis repository, is designed to help train and evaluate deep learning models for calling genomic variations. It supports semi-simulation workflows for generating training data and assessing model performance.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "variant_calling"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/CampagneLaboratory/variationanalysis",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "genomics",
        "variant-calling",
        "deep-learning",
        "bioinformatics"
      ],
      "id": 12
    },
    {
      "name": "PoseX",
      "one_line_profile": "Benchmark for molecular docking algorithms",
      "detailed_description": "A benchmark suite specifically designed for evaluating molecular docking methods. It provides datasets and metrics to assess the accuracy of pose prediction and binding affinity estimation in drug discovery contexts.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "molecular_docking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/CataAI/PoseX",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "molecular-docking",
        "benchmark",
        "drug-discovery",
        "pose-prediction"
      ],
      "id": 13
    },
    {
      "name": "jetson-orin-matmul-analysis",
      "one_line_profile": "Scientific CUDA benchmarking framework for matrix multiplication on Jetson Orin",
      "detailed_description": "A scientific benchmarking framework for evaluating CUDA matrix multiplication performance on Jetson Orin edge devices. It includes implementations for different power modes and matrix sizes, providing performance (GFLOPS) and power efficiency analysis.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "hpc_performance"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Cre4T3Tiv3/jetson-orin-matmul-analysis",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "cuda",
        "benchmarking",
        "edge-ai",
        "matrix-multiplication"
      ],
      "id": 14
    },
    {
      "name": "Benchmark-Gemma-Models",
      "one_line_profile": "Customizable Python suite for evaluating Gemma and LLaMA-based LLMs",
      "detailed_description": "A lightweight and customizable framework designed for benchmarking Large Language Models (LLMs) like Gemma and LLaMA. It supports custom scripts for models, datasets, tasks, and metrics, facilitating reproducible evaluation experiments.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/D0men1c0/Benchmark-Gemma-Models",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "evaluation",
        "gemma",
        "llama"
      ],
      "id": 15
    },
    {
      "name": "ABC (Annotated Beethoven Corpus)",
      "one_line_profile": "Dataset of harmonic analyses for Beethoven's string quartets",
      "detailed_description": "The Annotated Beethoven Corpus (ABC) provides expert harmonic analyses of Beethoven's string quartets. It serves as a benchmark dataset for symbolic music analysis and computational musicology tasks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "music_analysis",
        "harmonic_analysis"
      ],
      "application_level": "dataset",
      "primary_language": "None",
      "repo_url": "https://github.com/DCMLab/ABC",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "musicology",
        "dataset",
        "beethoven",
        "harmonics"
      ],
      "id": 16
    },
    {
      "name": "DeepDIVA",
      "one_line_profile": "Framework for reproducible deep learning experiments in document image analysis",
      "detailed_description": "A Python framework designed to enable quick and reproducible experiments in Deep Learning, specifically for Document Image Analysis (DIA). Although deprecated, it represents a tool for managing experimental workflows and hyperparameters.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "experiment_reproduction",
        "document_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/DIVA-DIA/DeepDIVA",
      "help_website": [
        "https://diva-dia.github.io/DeepDIVA/"
      ],
      "license": "LGPL-3.0",
      "tags": [
        "reproducibility",
        "deep-learning",
        "document-analysis"
      ],
      "id": 17
    },
    {
      "name": "DMind-Benchmark",
      "one_line_profile": "Evaluation framework for LLMs on blockchain and Web3 knowledge",
      "detailed_description": "A comprehensive benchmark framework designed to evaluate Large Language Models (LLMs) on domain-specific knowledge related to blockchain, cryptocurrency, and Web3 technologies.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "domain_specific_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/DMindAI/DMind-Benchmark",
      "help_website": [],
      "license": "None",
      "tags": [
        "llm",
        "blockchain",
        "benchmark",
        "web3"
      ],
      "id": 18
    },
    {
      "name": "DDRL4NAV",
      "one_line_profile": "Reproducible reinforcement learning training framework based on OpenAI Five",
      "detailed_description": "A lightweight reinforcement learning training framework that reproduces the architecture used in OpenAI Five. It separates Forward, Backward, and Env modules to facilitate RL research and reproduction of navigation tasks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "reproduction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/DRL-Navigation/DDRL4NAV",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "reinforcement-learning",
        "openai-five",
        "navigation"
      ],
      "id": 19
    },
    {
      "name": "DeepSpark",
      "one_line_profile": "Evaluation platform for industrial AI algorithms and models",
      "detailed_description": "An open platform that provides a multi-dimensional evaluation system for open-source application algorithms and models. It focuses on industrial applications and supports mainstream frameworks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "industrial_ai"
      ],
      "application_level": "platform",
      "primary_language": "None",
      "repo_url": "https://github.com/Deep-Spark/DeepSpark",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "evaluation",
        "industrial-ai",
        "benchmark"
      ],
      "id": 20
    },
    {
      "name": "SPDEBench",
      "one_line_profile": "Benchmark for learning regular and singular stochastic PDEs",
      "detailed_description": "SPDEBench is an extensive benchmark suite designed for evaluating machine learning models on regular and singular Stochastic Partial Differential Equations (SPDEs), facilitating research in scientific machine learning (SciML).",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "pde_solving",
        "sciml_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/DeepIntoStreams/SPDE_hackathon",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "spde",
        "pde",
        "benchmark",
        "sciml"
      ],
      "id": 21
    },
    {
      "name": "Khmer OCR Benchmark Dataset",
      "one_line_profile": "Standardized benchmark dataset for Khmer Optical Character Recognition",
      "detailed_description": "A standardized dataset designed to benchmark Optical Character Recognition (OCR) engines specifically for the Khmer language, facilitating evaluation and comparison of OCR models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "ocr_evaluation",
        "dataset"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/EKYCSolutions/khmer-ocr-benchmark-dataset",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ocr",
        "khmer",
        "benchmark",
        "dataset"
      ],
      "id": 22
    },
    {
      "name": "EvoxBench",
      "one_line_profile": "Benchmark suite for evolutionary neural architecture search",
      "detailed_description": "A benchmark suite that transforms Neural Architecture Search (NAS) into multi-objective optimization problems, designed for testing evolutionary algorithms in deep learning contexts.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "neural_architecture_search",
        "evolutionary_algorithms"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/EMI-Group/evoxbench",
      "help_website": [
        "https://evoxbench.readthedocs.io/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "nas",
        "evolutionary-computation",
        "benchmark"
      ],
      "id": 23
    },
    {
      "name": "LM Evaluation Harness",
      "one_line_profile": "Framework for few-shot evaluation of language models",
      "detailed_description": "A widely used framework for evaluating autoregressive language models (LLMs) on a large number of tasks. It provides a unified interface for few-shot evaluation and benchmarking.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "nlp_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/EleutherAI/lm-evaluation-harness",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "evaluation",
        "few-shot",
        "benchmark"
      ],
      "id": 24
    },
    {
      "name": "JamAIBase",
      "one_line_profile": "Collaborative spreadsheet platform for LLM experimentation and evaluation",
      "detailed_description": "A platform that combines a spreadsheet interface with LLM capabilities, allowing users to chain cells into pipelines, experiment with prompts, and evaluate LLM responses in real-time.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "prompt_engineering",
        "model_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/EmbeddedLLM/JamAIBase",
      "help_website": [
        "https://jamaibase.com/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "spreadsheet",
        "no-code",
        "evaluation"
      ],
      "id": 25
    },
    {
      "name": "MoSQITo",
      "one_line_profile": "Modular framework for sound quality metrics and psychoacoustics",
      "detailed_description": "A unified and modular development framework for calculating key sound quality metrics. It supports reproducible science in acoustics and psychoacoustics by providing standardized implementations of metrics.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "signal_processing",
        "psychoacoustics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Eomys/MoSQITo",
      "help_website": [
        "https://github.com/Eomys/MoSQITo"
      ],
      "license": "Apache-2.0",
      "tags": [
        "acoustics",
        "sound-quality",
        "metrics"
      ],
      "id": 26
    },
    {
      "name": "WEHUB (Water Environmental Hub)",
      "one_line_profile": "Cloud-based platform for accessing and sharing environmental data",
      "detailed_description": "An open-source web platform designed to facilitate the search, access, and sharing of water and environmental data. It supports international data standards and provides tools for data translation and cataloging.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "data_management",
        "environmental_science"
      ],
      "application_level": "platform",
      "primary_language": "Ruby",
      "repo_url": "https://github.com/ExplorusDataSolutions/WaterEnvironmentalHub-WEHUB",
      "help_website": [],
      "license": "None",
      "tags": [
        "environmental-data",
        "water",
        "data-sharing"
      ],
      "id": 27
    },
    {
      "name": "openFDA",
      "one_line_profile": "API and data access service for FDA public datasets",
      "detailed_description": "A project providing open APIs, raw data downloads, and documentation for a large collection of FDA public datasets, enabling researchers and developers to access health and drug-related data programmatically.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "data_access",
        "public_health"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/FDA/openfda",
      "help_website": [
        "https://open.fda.gov/"
      ],
      "license": "CC0-1.0",
      "tags": [
        "fda",
        "health-data",
        "api"
      ],
      "id": 28
    },
    {
      "name": "FluxBench.jl",
      "one_line_profile": "Benchmark suite for the FluxML ecosystem and scientific machine learning",
      "detailed_description": "A benchmarking tool for the FluxML ecosystem, covering deep learning, scientific machine learning (SciML), differentiable programming, and CUDA-accelerated workloads in Julia.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "sciml"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/FluxML/FluxBench.jl",
      "help_website": [],
      "license": "None",
      "tags": [
        "julia",
        "fluxml",
        "benchmark",
        "sciml"
      ],
      "id": 29
    },
    {
      "name": "LLM Zoo",
      "one_line_profile": "Repository of data, models, and benchmarks for LLMs",
      "detailed_description": "A project providing a collection of instruction-tuning data, models, and evaluation benchmarks for Large Language Models, facilitating the development and assessment of LLMs.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "dataset"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/FreedomIntelligence/LLMZoo",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "benchmark",
        "instruction-tuning"
      ],
      "id": 30
    },
    {
      "name": "LAB-Bench",
      "one_line_profile": "Benchmark dataset for AI capabilities in biological research",
      "detailed_description": "An evaluation dataset designed to benchmark AI systems on capabilities foundational to scientific research in biology, such as literature retrieval, protocol planning, and data interpretation.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "biology_evaluation",
        "scientific_reasoning"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/Future-House/LAB-Bench",
      "help_website": [],
      "license": "CC-BY-SA-4.0",
      "tags": [
        "biology",
        "benchmark",
        "ai4science"
      ],
      "id": 31
    },
    {
      "name": "MAYE",
      "one_line_profile": "Framework for evaluating RL scaling in Vision Language Models",
      "detailed_description": "A framework and comprehensive evaluation scheme for Reinforcement Learning (RL) scaling in Vision Language Models (VLMs), providing tools for transparent and from-scratch training and assessment.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "rl_evaluation",
        "vlm_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/GAIR-NLP/MAYE",
      "help_website": [],
      "license": "None",
      "tags": [
        "vlm",
        "reinforcement-learning",
        "evaluation"
      ],
      "id": 32
    },
    {
      "name": "Molecules Dataset Collection",
      "one_line_profile": "Collection of molecular datasets for property inference validation",
      "detailed_description": "A curated collection of datasets containing molecular structures and properties, intended for the validation and benchmarking of molecular property inference models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "molecular_property_prediction",
        "dataset"
      ],
      "application_level": "dataset",
      "primary_language": "None",
      "repo_url": "https://github.com/GLambard/Molecules_Dataset_Collection",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "molecules",
        "dataset",
        "chemistry"
      ],
      "id": 33
    },
    {
      "name": "GPT-Fathom",
      "one_line_profile": "Reproducible evaluation suite for LLMs on curated benchmarks",
      "detailed_description": "An open-source LLM evaluation suite that benchmarks leading open-source and closed-source models on over 20 curated benchmarks under aligned settings to ensure reproducibility.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/GPT-Fathom/GPT-Fathom",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "evaluation",
        "reproducibility"
      ],
      "id": 34
    },
    {
      "name": "MolCap-Arena",
      "one_line_profile": "Benchmark for language-enhanced molecular property prediction",
      "detailed_description": "A comprehensive captioning benchmark designed for evaluating language-enhanced molecular property prediction models, facilitating research at the intersection of chemistry and NLP.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "molecular_captioning",
        "property_prediction"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/Genentech/molcap-arena",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "molecular-captioning",
        "benchmark",
        "chemistry"
      ],
      "id": 35
    },
    {
      "name": "MultimodalHugs",
      "one_line_profile": "Framework for training and evaluating multimodal AI models",
      "detailed_description": "An extension of Hugging Face Transformers that provides a generalized framework for training, evaluating, and using multimodal AI models with unified code interfaces.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "multimodal_training",
        "evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/GerrySant/multimodalhugs",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "multimodal",
        "huggingface",
        "framework"
      ],
      "id": 36
    },
    {
      "name": "Giskard",
      "one_line_profile": "Evaluation and testing library for LLM agents and AI models",
      "detailed_description": "An open-source library for testing and evaluating AI models and LLM agents. It helps detect vulnerabilities, hallucinations, and performance issues through automated tests.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_testing",
        "quality_assurance"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Giskard-AI/giskard-oss",
      "help_website": [
        "https://docs.giskard.ai/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "testing",
        "llm",
        "evaluation",
        "qa"
      ],
      "id": 37
    },
    {
      "name": "GeSS",
      "one_line_profile": "Benchmark for Geometric Deep Learning under distribution shifts",
      "detailed_description": "A benchmarking suite for Geometric Deep Learning (GDL) focusing on scientific applications and robustness against distribution shifts in geometric data.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "geometric_deep_learning",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Graph-COM/GESS",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gdl",
        "benchmark",
        "scientific-applications"
      ],
      "id": 38
    },
    {
      "name": "SciKnowEval",
      "one_line_profile": "Benchmark for evaluating scientific knowledge of LLMs",
      "detailed_description": "A benchmark designed to evaluate the multi-level scientific knowledge capabilities of Large Language Models, assessing their proficiency in various scientific domains.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "scientific_knowledge_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/HICAI-ZJU/SciKnowEval",
      "help_website": [],
      "license": "None",
      "tags": [
        "llm",
        "science",
        "evaluation"
      ],
      "id": 39
    },
    {
      "name": "NewtonBench",
      "one_line_profile": "Benchmark for scientific law discovery in LLM agents",
      "detailed_description": "A benchmark suite for evaluating the ability of LLM-based agents to discover generalizable scientific laws, testing their reasoning and discovery capabilities.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "scientific_discovery",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/HKUST-KnowComp/NewtonBench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "scientific-discovery",
        "llm-agent",
        "benchmark"
      ],
      "id": 40
    },
    {
      "name": "MD-Bench",
      "one_line_profile": "Prototyping harness for Molecular Dynamics algorithms",
      "detailed_description": "A performance-oriented benchmarking harness for developing and testing state-of-the-art Molecular Dynamics (MD) algorithms, focusing on computational efficiency.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "molecular_dynamics",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "C",
      "repo_url": "https://github.com/HPC-Dwarfs/MD-Bench",
      "help_website": [],
      "license": "LGPL-3.0",
      "tags": [
        "molecular-dynamics",
        "hpc",
        "benchmark"
      ],
      "id": 41
    },
    {
      "name": "BubbleML",
      "one_line_profile": "Dataset and benchmarks for multiphase multiphysics SciML",
      "detailed_description": "A dataset and benchmark suite for scientific machine learning (SciML) focused on multiphase multiphysics simulations, providing data for training and evaluating physics-informed models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "multiphysics_simulation",
        "sciml_benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/HPCForge/BubbleML",
      "help_website": [],
      "license": "None",
      "tags": [
        "sciml",
        "multiphysics",
        "dataset"
      ],
      "id": 42
    },
    {
      "name": "GMNS Plus Dataset",
      "one_line_profile": "Standardized transportation network dataset collection",
      "detailed_description": "A standardized collection of transportation network datasets based on the General Modeling Network Specification (GMNS), supporting reproducible research in transportation modeling and planning.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "transportation_modeling",
        "dataset"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/HanZhengIntelliTransport/GMNS_Plus_Dataset",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "transportation",
        "gmns",
        "dataset"
      ],
      "id": 43
    },
    {
      "name": "Helicone",
      "one_line_profile": "Observability and evaluation platform for LLM applications",
      "detailed_description": "An open-source platform for monitoring, evaluating, and experimenting with Large Language Models (LLMs). It provides tools for logging, caching, and analyzing model performance.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "llm_observability",
        "model_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/Helicone/helicone",
      "help_website": [
        "https://docs.helicone.ai/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "observability",
        "llm",
        "evaluation"
      ],
      "id": 44
    },
    {
      "name": "DL-based-MI-EEG-models",
      "one_line_profile": "Deep learning models and leaderboard for Motor Imagery EEG analysis",
      "detailed_description": "A repository collecting source code of representative deep learning-based Motor Imagery EEG (MI-EEG) models and running a leaderboard for fair comparison.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "eeg_analysis",
        "model_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Henrywang621/DL-based-MI-EEG-models",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "eeg",
        "brain-computer-interface",
        "deep-learning"
      ],
      "id": 45
    },
    {
      "name": "SDE-Harness",
      "one_line_profile": "Framework for evaluating scientific discovery capabilities of agents",
      "detailed_description": "SDE-Harness (Scientific Discovery Evaluation Framework) is designed to evaluate AI agents on scientific discovery tasks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "scientific_discovery_evaluation",
        "agent_benchmarking"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/HowieHwong/sde-harness",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "scientific-discovery",
        "evaluation-framework",
        "agents"
      ],
      "id": 46
    },
    {
      "name": "Multimodal 3D Image Segmentation",
      "one_line_profile": "Frameworks for multimodal 3D medical image segmentation",
      "detailed_description": "Source code for multimodal image segmentation frameworks, providing network architectures and training procedures for 3D medical imaging analysis.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "medical_image_segmentation",
        "3d_imaging"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/IBM/multimodal-3d-image-segmentation",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "medical-imaging",
        "segmentation",
        "multimodal"
      ],
      "id": 47
    },
    {
      "name": "VLM4Bio",
      "one_line_profile": "Benchmark dataset of scientific QA pairs for biological images",
      "detailed_description": "A benchmark dataset of scientific question-answer pairs used to evaluate pretrained Vision-Language Models (VLMs) for trait discovery from biological images.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "biological_image_analysis",
        "visual_question_answering"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/Imageomics/VLM4Bio",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "biology",
        "vlm",
        "benchmark"
      ],
      "id": 48
    },
    {
      "name": "InternManip",
      "one_line_profile": "Robot manipulation learning suite for policy training and evaluation",
      "detailed_description": "An all-in-one robot manipulation learning suite for policy models training and evaluation on various datasets and benchmarks, relevant to embodied AI in science.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "robotic_manipulation",
        "policy_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/InternRobotics/InternManip",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "robotics",
        "manipulation",
        "embodied-ai"
      ],
      "id": 49
    },
    {
      "name": "SGI-Bench",
      "one_line_profile": "Benchmark defining and evaluating Scientific General Intelligence",
      "detailed_description": "A benchmark suite designed to define and evaluate Scientific General Intelligence capabilities in AI models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "scientific_reasoning_evaluation",
        "general_intelligence"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/InternScience/SGI-Bench",
      "help_website": [],
      "license": null,
      "tags": [
        "scientific-intelligence",
        "benchmark",
        "reasoning"
      ],
      "id": 50
    },
    {
      "name": "Computer-Generated-Hologram",
      "one_line_profile": "Simulation framework for digital holography and computer-generated holograms",
      "detailed_description": "A computational framework for simulating the production process of computer holography, recording, and reproducing holograms using MATLAB and Python.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "holography_simulation",
        "optical_computing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/JackHCC/Computer-Generated-Hologram",
      "help_website": [],
      "license": null,
      "tags": [
        "holography",
        "optics",
        "simulation"
      ],
      "id": 51
    },
    {
      "name": "Science-T2I",
      "one_line_profile": "Benchmark for addressing scientific illusions in text-to-image synthesis",
      "detailed_description": "A benchmark designed to evaluate and address scientific illusions in image synthesis models, ensuring scientific accuracy in generated imagery.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "scientific_image_synthesis",
        "hallucination_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/Jialuo-Li/Science-T2I",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "image-synthesis",
        "scientific-accuracy",
        "benchmark"
      ],
      "id": 52
    },
    {
      "name": "FAERS Data Toolkit",
      "one_line_profile": "Tools for processing FDA Adverse Event Reporting System datasets",
      "detailed_description": "Script tools for downloading, data preprocessing, data merging, and standardizing the FDA Adverse Event Reporting System (FAERS) dataset for pharmacovigilance research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "pharmacovigilance",
        "data_preprocessing"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/Judenpech/FAERS-data-toolkit",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "faers",
        "pharmacovigilance",
        "medical-data"
      ],
      "id": 53
    },
    {
      "name": "LLM-MAP",
      "one_line_profile": "Bimanual robot task planning using Large Language Models",
      "detailed_description": "Implementation of LLM+MAP for bimanual robot task planning using LLMs and PDDL, relevant to lab automation and embodied AI.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "robot_task_planning",
        "embodied_ai"
      ],
      "application_level": "library",
      "primary_language": null,
      "repo_url": "https://github.com/Kchu/LLM-MAP",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "robotics",
        "task-planning",
        "llm"
      ],
      "id": 54
    },
    {
      "name": "MolPuzzle",
      "one_line_profile": "Multimodal benchmark for molecular structure elucidation using LLMs",
      "detailed_description": "A multimodal benchmark designed to evaluate the capability of Large Language Models in solving molecule puzzles and elucidating molecular structures.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "molecular_structure_prediction",
        "chemistry_reasoning"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/KehanGuo2/MolPuzzle",
      "help_website": [],
      "license": null,
      "tags": [
        "chemistry",
        "molecular-structure",
        "benchmark"
      ],
      "id": 55
    },
    {
      "name": "Guaranteed-Non-Local-Molecular-Dataset",
      "one_line_profile": "Dataset for benchmarking non-local capabilities of geometric ML models",
      "detailed_description": "A dataset specifically created to benchmark the non-local capabilities of geometric machine learning models in molecular contexts.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "molecular_modeling",
        "geometric_deep_learning"
      ],
      "application_level": "dataset",
      "primary_language": null,
      "repo_url": "https://github.com/LarsSchaaf/Guaranteed-Non-Local-Molecular-Dataset",
      "help_website": [],
      "license": null,
      "tags": [
        "molecular-dataset",
        "geometric-ml",
        "benchmark"
      ],
      "id": 56
    },
    {
      "name": "Spyglass",
      "one_line_profile": "Neuroscience data analysis framework for reproducible research",
      "detailed_description": "A neuroscience data analysis framework built for reproducible research, facilitating the management and analysis of complex neurophysiological data.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "neuroscience_data_analysis",
        "reproducible_research"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/LorenFrankLab/spyglass",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "neuroscience",
        "data-analysis",
        "framework"
      ],
      "id": 57
    },
    {
      "name": "MolData",
      "one_line_profile": "Molecular benchmark for disease and target-based machine learning",
      "detailed_description": "A benchmark suite for evaluating machine learning models on molecular datasets focused on diseases and targets.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "drug_discovery",
        "molecular_property_prediction"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/LumosBio/MolData",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "molecular-benchmark",
        "drug-discovery",
        "machine-learning"
      ],
      "id": 58
    },
    {
      "name": "Proteomics Lab Agent",
      "one_line_profile": "Multimodal agentic AI framework for automating proteomics laboratory work",
      "detailed_description": "A multimodal, agentic AI framework that links written instructions to real-world laboratory work in proteomics, using video analysis for documentation and guidance.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "lab_automation",
        "proteomics"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/MannLabs/proteomics_lab_agent",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "proteomics",
        "lab-automation",
        "agentic-ai"
      ],
      "id": 59
    },
    {
      "name": "MedMNIST",
      "one_line_profile": "Collection of standardized datasets for biomedical image classification",
      "detailed_description": "A large-scale MNIST-like collection of standardized biomedical images, including 2D and 3D datasets, designed for lightweight benchmarking of biomedical image analysis models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "biomedical_image_classification",
        "benchmark_suite"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/MedMNIST/MedMNIST",
      "help_website": [
        "https://medmnist.com/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "biomedical-imaging",
        "classification",
        "dataset"
      ],
      "id": 60
    },
    {
      "name": "PaperArena",
      "one_line_profile": "Benchmark for tool-augmented agentic reasoning on scientific literature",
      "detailed_description": "An evaluation benchmark designed to assess the performance of tool-augmented agents in reasoning over scientific literature.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "scientific_literature_mining",
        "agent_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/Melmaphother/PaperArena",
      "help_website": [],
      "license": null,
      "tags": [
        "scientific-literature",
        "agents",
        "benchmark"
      ],
      "id": 61
    },
    {
      "name": "MobiSurvStd",
      "one_line_profile": "Standardization tool for French mobility survey datasets",
      "detailed_description": "A Python tool designed to standardize various French mobility survey datasets (such as EMC², EGT, EMP) into a unified format to facilitate transportation research and social science analysis.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "data_standardization",
        "data_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/MobiSurvStd/MobiSurvStd",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mobility-surveys",
        "data-standardization",
        "transportation-science"
      ],
      "id": 62
    },
    {
      "name": "Modalities",
      "one_line_profile": "Distributed and reproducible foundation model training framework",
      "detailed_description": "A PyTorch-native framework designed for the distributed and reproducible training of foundation models, emphasizing scientific rigor in model development experiments.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_training",
        "reproducibility"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/Modalities/modalities",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "foundation-models",
        "distributed-training",
        "reproducibility"
      ],
      "id": 63
    },
    {
      "name": "smiles-featurizers",
      "one_line_profile": "Molecular SMILES embedding extractor using language models",
      "detailed_description": "A tool to extract molecular embeddings from SMILES strings using various pre-trained language model architectures, facilitating downstream cheminformatics and drug discovery tasks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "feature_extraction",
        "molecular_representation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/MoleculeTransformers/smiles-featurizers",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "smiles",
        "molecular-embeddings",
        "cheminformatics"
      ],
      "id": 64
    },
    {
      "name": "MolScore",
      "one_line_profile": "Automated scoring function for de novo molecular design",
      "detailed_description": "An automated scoring framework to facilitate and standardize the evaluation of goal-directed generative models for de novo molecular design, supporting various objective functions.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "molecular_design"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/MorganCThomas/MolScore",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "molecular-design",
        "generative-models",
        "scoring-function"
      ],
      "id": 65
    },
    {
      "name": "BiomedSQL",
      "one_line_profile": "Text-to-SQL benchmark for biomedical scientific reasoning",
      "detailed_description": "A benchmark dataset and task definition for evaluating Text-to-SQL capabilities specifically within the domain of biomedical scientific reasoning.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "scientific_reasoning"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/NIH-CARD/biomedsql",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "text-to-sql",
        "biomedical",
        "benchmark"
      ],
      "id": 66
    },
    {
      "name": "ComputeEval",
      "one_line_profile": "Evaluation framework for CUDA code generation models",
      "detailed_description": "A framework designed to generate and evaluate CUDA code from Large Language Models, targeting high-performance computing and scientific code generation tasks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "code_generation",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NVIDIA/compute-eval",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "cuda",
        "llm-evaluation",
        "hpc"
      ],
      "id": 67
    },
    {
      "name": "Framework Reproducibility",
      "one_line_profile": "Tools for ensuring reproducibility in deep learning frameworks",
      "detailed_description": "A collection of tools and methodologies to ensure deterministic and reproducible results across deep learning frameworks, essential for scientific validity of AI experiments.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "reproducibility",
        "experiment_control"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NVIDIA/framework-reproducibility",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "reproducibility",
        "deep-learning",
        "determinism"
      ],
      "id": 68
    },
    {
      "name": "Rapidae",
      "one_line_profile": "Framework for developing and comparing autoencoder models",
      "detailed_description": "A back-end agnostic framework to explore, compare, and develop autoencoder models, facilitating research into representation learning and generative modeling.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_development",
        "autoencoders"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NahuelCostaCortez/rapidae",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "autoencoder",
        "model-comparison",
        "deep-learning"
      ],
      "id": 69
    },
    {
      "name": "CanarySEFI",
      "one_line_profile": "Robustness evaluation framework for image recognition models",
      "detailed_description": "A comprehensive framework for evaluating the robustness of deep learning-based image recognition models against adversarial attacks, including metrics for attack and defense effectiveness.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "adversarial_testing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NeoSunJZ/Canary_Master",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "robustness",
        "adversarial-attacks",
        "model-evaluation"
      ],
      "id": 70
    },
    {
      "name": "Atropos",
      "one_line_profile": "LLM reinforcement learning environment and evaluation framework",
      "detailed_description": "A framework for collecting and evaluating Large Language Model trajectories through diverse environments, supporting reinforcement learning research and agent evaluation.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "agent_evaluation",
        "reinforcement_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NousResearch/atropos",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "reinforcement-learning",
        "evaluation"
      ],
      "id": 71
    },
    {
      "name": "ScienceBoard",
      "one_line_profile": "Benchmark for multimodal autonomous agents in scientific workflows",
      "detailed_description": "A benchmark suite and environment designed to evaluate the performance of multimodal autonomous agents in realistic scientific workflows and tasks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "agent_benchmarking",
        "scientific_workflow"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/OS-Copilot/ScienceBoard",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "autonomous-agents",
        "scientific-benchmark",
        "multimodal"
      ],
      "id": 72
    },
    {
      "name": "OneIG-Bench",
      "one_line_profile": "Fine-grained evaluation benchmark for Text-to-Image models",
      "detailed_description": "A comprehensive benchmark framework for evaluating Text-to-Image models across dimensions like subject-element alignment, reasoning, and scientific/technical accuracy in generation.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "text-to-image"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/OneIG-Bench/OneIG-Benchmark",
      "help_website": [],
      "license": null,
      "tags": [
        "text-to-image",
        "benchmark",
        "generative-ai"
      ],
      "id": 73
    },
    {
      "name": "OlympiadBench",
      "one_line_profile": "Benchmark for Olympiad-level scientific problems",
      "detailed_description": "A challenging benchmark dataset and evaluation framework featuring Olympiad-level bilingual multimodal scientific problems to promote AGI research in scientific reasoning.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "scientific_reasoning",
        "benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenBMB/OlympiadBench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "scientific-reasoning",
        "olympiad",
        "multimodal"
      ],
      "id": 74
    },
    {
      "name": "UltraEval",
      "one_line_profile": "Open source framework for evaluating foundation models",
      "detailed_description": "A comprehensive framework for evaluating foundation models across various capabilities, providing a standardized interface for model assessment.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenBMB/UltraEval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "foundation-models",
        "evaluation-framework",
        "llm"
      ],
      "id": 75
    },
    {
      "name": "OpenBioLink",
      "one_line_profile": "Evaluation framework for biomedical link prediction",
      "detailed_description": "A resource and evaluation framework designed for assessing link prediction models on heterogeneous biomedical graph data, facilitating network biology research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "link_prediction",
        "graph_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenBioLink/OpenBioLink",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "biomedical-graph",
        "link-prediction",
        "benchmark"
      ],
      "id": 76
    },
    {
      "name": "BioSimSpace",
      "one_line_profile": "Interoperable framework for biomolecular simulation",
      "detailed_description": "A Python framework that provides an interoperable interface for various biomolecular simulation tools, facilitating the setup, execution, and analysis of molecular dynamics simulations.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "molecular_simulation",
        "workflow_management"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenBioSim/biosimspace",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "molecular-dynamics",
        "simulation",
        "interoperability"
      ],
      "id": 77
    },
    {
      "name": "MULTI-Benchmark",
      "one_line_profile": "Multimodal understanding leaderboard with text and images",
      "detailed_description": "A benchmark suite for evaluating multimodal AI models on their understanding of complex text and image data.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "multimodal_evaluation",
        "benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenDFM/MULTI-Benchmark",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "multimodal",
        "benchmark",
        "leaderboard"
      ],
      "id": 78
    },
    {
      "name": "SciEval",
      "one_line_profile": "Multi-level LLM evaluation benchmark for scientific research",
      "detailed_description": "A benchmark designed to evaluate Large Language Models specifically on their capability to perform scientific research tasks across multiple levels of complexity.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "scientific_evaluation",
        "benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenDFM/SciEval",
      "help_website": [],
      "license": null,
      "tags": [
        "scientific-research",
        "llm-evaluation",
        "benchmark"
      ],
      "id": 79
    },
    {
      "name": "OpenHands Benchmarks",
      "one_line_profile": "Evaluation harness for OpenHands agents",
      "detailed_description": "The evaluation harness and benchmark suite for OpenHands, enabling the assessment of autonomous coding and task-solving agents.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "agent_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenHands/benchmarks",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "autonomous-agents",
        "evaluation",
        "harness"
      ],
      "id": 80
    },
    {
      "name": "GAOKAO-Bench",
      "one_line_profile": "LLM evaluation framework using GAOKAO questions",
      "detailed_description": "An evaluation framework that utilizes questions from the Chinese National College Entrance Examination (GAOKAO) to assess the knowledge and reasoning capabilities of Large Language Models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "knowledge_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenLMLab/GAOKAO-Bench",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "gaokao",
        "llm-evaluation",
        "reasoning"
      ],
      "id": 81
    },
    {
      "name": "OG-Core",
      "one_line_profile": "Overlapping generations model framework for fiscal policy evaluation",
      "detailed_description": "A Python framework for modeling overlapping generations (OG) to evaluate the economic effects of fiscal policies, serving as a tool for economic simulation and analysis.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "economic_modeling",
        "policy_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/PSLmodels/OG-Core",
      "help_website": [],
      "license": "CC0-1.0",
      "tags": [
        "economics",
        "simulation",
        "fiscal-policy"
      ],
      "id": 82
    },
    {
      "name": "GBBS",
      "one_line_profile": "Graph Based Benchmark Suite",
      "detailed_description": "A comprehensive benchmark suite for evaluating the performance of graph algorithms and systems, essential for research in high-performance graph processing.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "graph_benchmarking",
        "algorithm_evaluation"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/ParAlg/gbbs",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "graph-algorithms",
        "benchmark",
        "hpc"
      ],
      "id": 83
    },
    {
      "name": "SkillMetricsToolbox",
      "one_line_profile": "Matlab toolbox for model skill assessment",
      "detailed_description": "A collection of Matlab functions for calculating statistical metrics (like Taylor diagrams) to evaluate the skill of model predictions against observational data.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_verification",
        "statistical_analysis"
      ],
      "application_level": "library",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/PeterRochford/SkillMetricsToolbox",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "model-evaluation",
        "statistics",
        "matlab"
      ],
      "id": 84
    },
    {
      "name": "The Well",
      "one_line_profile": "Physics simulation dataset collection and access tools",
      "detailed_description": "A large-scale collection of physics simulation datasets accompanied by tools/loaders to facilitate research in physics-informed machine learning and simulation surrogates.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "physics_simulation",
        "dataset_access"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/PolymathicAI/the_well",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "physics",
        "simulation",
        "dataset"
      ],
      "id": 85
    },
    {
      "name": "LIKWID",
      "one_line_profile": "Performance monitoring and benchmarking suite for HPC",
      "detailed_description": "A tool suite for performance oriented programmers to measure and benchmark hardware performance counters, critical for optimizing scientific computing applications.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "performance_benchmarking",
        "hpc_optimization"
      ],
      "application_level": "solver",
      "primary_language": "C",
      "repo_url": "https://github.com/RRZE-HPC/likwid",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "hpc",
        "performance-monitoring",
        "benchmarking"
      ],
      "id": 86
    },
    {
      "name": "LLMBox",
      "one_line_profile": "Unified library for LLM training and evaluation",
      "detailed_description": "A comprehensive library for implementing Large Language Models, featuring a unified pipeline for training and rigorous model evaluation.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_training",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/RUCAIBox/LLMBox",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "training-pipeline",
        "evaluation"
      ],
      "id": 87
    },
    {
      "name": "STAVER",
      "one_line_profile": "Algorithm for variation reduction in DIA MS data",
      "detailed_description": "A standardized dataset-based algorithm designed to efficiently reduce variation in large-scale Data-Independent Acquisition (DIA) Mass Spectrometry data.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "data_processing",
        "mass_spectrometry"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/Ran485/STAVER",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "proteomics",
        "mass-spectrometry",
        "data-normalization"
      ],
      "id": 88
    },
    {
      "name": "RobustBench",
      "one_line_profile": "Standardized adversarial robustness benchmark",
      "detailed_description": "A standardized benchmark and library for evaluating the adversarial robustness of image classification models, providing a leaderboard and model zoo.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/RobustBench/robustbench",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "adversarial-robustness",
        "benchmark",
        "computer-vision"
      ],
      "id": 89
    },
    {
      "name": "RouterArena",
      "one_line_profile": "Evaluation framework for LLM routers",
      "detailed_description": "An open framework for evaluating LLM routing strategies with standardized datasets and metrics, facilitating the optimization of compound AI systems.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_routing",
        "system_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/RouteWorks/RouterArena",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-routing",
        "evaluation",
        "benchmark"
      ],
      "id": 90
    },
    {
      "name": "RAG Evaluation Harnesses",
      "one_line_profile": "Evaluation suite for Retrieval-Augmented Generation",
      "detailed_description": "A comprehensive evaluation suite designed to assess the performance of Retrieval-Augmented Generation (RAG) systems across various metrics.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "rag_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/RulinShao/RAG-evaluation-harnesses",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "evaluation",
        "retrieval"
      ],
      "id": 91
    },
    {
      "name": "ServerlessBench",
      "one_line_profile": "Benchmark suite for serverless computing",
      "detailed_description": "A benchmark suite designed to evaluate the performance and characteristics of serverless computing platforms, aiding systems research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "system_benchmarking",
        "serverless"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/SJTU-IPADS/ServerlessBench",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "serverless",
        "benchmark",
        "systems-research"
      ],
      "id": 92
    },
    {
      "name": "Guardians MT Eval",
      "one_line_profile": "Machine Translation meta-evaluation metrics",
      "detailed_description": "Implementation of metrics for the meta-evaluation of Machine Translation systems, as proposed in the ACL 2024 paper 'Guardians of the Machine Translation Meta-Evaluation'.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "mt_evaluation",
        "meta_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/SapienzaNLP/guardians-mt-eval",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "machine-translation",
        "evaluation-metrics",
        "nlp"
      ],
      "id": 93
    },
    {
      "name": "SceMQA",
      "one_line_profile": "Benchmark for scientific multimodal question answering",
      "detailed_description": "A scientific college entrance level multimodal question answering benchmark designed to evaluate the reasoning capabilities of multimodal models in scientific domains.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "question_answering",
        "scientific_reasoning"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/SceMQA/SceMQA",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "multimodal",
        "qa",
        "scientific-benchmark"
      ],
      "id": 94
    },
    {
      "name": "DiffEqDevTools.jl",
      "one_line_profile": "Benchmarking and testing tools for differential equations",
      "detailed_description": "A Julia library providing tools for benchmarking, testing, and developing solvers for differential equations and scientific machine learning (SciML).",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "differential_equations"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/SciML/DiffEqDevTools.jl",
      "help_website": [
        "https://docs.sciml.ai/DiffEqDevTools/stable/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "sciml",
        "differential-equations",
        "benchmarking"
      ],
      "id": 95
    },
    {
      "name": "SciMLBenchmarks.jl",
      "one_line_profile": "Scientific machine learning and differential equation solver benchmarks",
      "detailed_description": "A comprehensive benchmark suite for scientific machine learning (SciML) and differential equation solvers, covering Julia, Python, MATLAB, and R implementations.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "solver_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/SciML/SciMLBenchmarks.jl",
      "help_website": [
        "https://benchmarks.sciml.ai/"
      ],
      "license": "MIT",
      "tags": [
        "sciml",
        "benchmarks",
        "ode-solvers"
      ],
      "id": 96
    },
    {
      "name": "AstroVisBench",
      "one_line_profile": "Benchmark for astronomy scientific computing and visualization",
      "detailed_description": "The first benchmark designed to evaluate both scientific computing and visualization capabilities specifically within the astronomy domain.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "visualization",
        "scientific_computing"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/SebaJoe/AstroVisBench",
      "help_website": [],
      "license": null,
      "tags": [
        "astronomy",
        "visualization",
        "benchmark"
      ],
      "id": 97
    },
    {
      "name": "nonbonded",
      "one_line_profile": "Framework for optimizing and benchmarking molecular force fields",
      "detailed_description": "A management system designed for optimizing and benchmarking molecular force fields against physical property data, facilitating computational chemistry research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "force_field_optimization",
        "molecular_modeling"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/SimonBoothroyd/nonbonded",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "molecular-dynamics",
        "force-fields",
        "chemistry"
      ],
      "id": 98
    },
    {
      "name": "CSVQA",
      "one_line_profile": "Benchmark for scientific reasoning in VLMs",
      "detailed_description": "A multimodal benchmark specifically designed to evaluate the scientific reasoning capabilities of Vision-Language Models (VLMs).",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "scientific_reasoning",
        "multimodal_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/SkyworkAI/CSVQA",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "vlm",
        "scientific-reasoning",
        "benchmark"
      ],
      "id": 99
    },
    {
      "name": "EmoLLM",
      "one_line_profile": "LLM framework for mental health evaluation and therapy",
      "detailed_description": "A comprehensive framework including datasets, evaluation, and deployment tools for Large Language Models focused on mental health and psychology applications.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "mental_health_analysis",
        "psychological_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/SmartFlowAI/EmoLLM",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mental-health",
        "llm",
        "psychology"
      ],
      "id": 100
    },
    {
      "name": "OmixBench",
      "one_line_profile": "Evaluation framework for LLMs in multi-omics analysis",
      "detailed_description": "A systematic evaluation framework designed to assess the performance of Large Language Models in the context of multi-omics data analysis.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "multi_omics",
        "bioinformatics_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "R",
      "repo_url": "https://github.com/SolvingLab/OmixBench",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "multi-omics",
        "llm-evaluation",
        "bioinformatics"
      ],
      "id": 101
    },
    {
      "name": "StreetView-NatureVisibility",
      "one_line_profile": "Framework for greenness visibility modelling using street view data",
      "detailed_description": "A scalable and reproducible framework for utilising Mapillary street view data to model nature visibility, supporting environmental and urban science research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "environmental_modelling",
        "urban_science"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/Spatial-Data-Science-and-GEO-AI-Lab/StreetView-NatureVisibility",
      "help_website": [],
      "license": null,
      "tags": [
        "geo-ai",
        "environmental-science",
        "street-view"
      ],
      "id": 102
    },
    {
      "name": "SMDG-19",
      "one_line_profile": "Standardized multi-channel dataset for glaucoma",
      "detailed_description": "A collection and standardization of 19 public full-fundus glaucoma images and associated metadata, serving as a benchmark for medical imaging analysis.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "medical_imaging",
        "disease_diagnosis"
      ],
      "application_level": "dataset",
      "primary_language": null,
      "repo_url": "https://github.com/TheBeastCoding/standardized-multichannel-dataset-glaucoma",
      "help_website": [],
      "license": null,
      "tags": [
        "medical-imaging",
        "glaucoma",
        "dataset"
      ],
      "id": 103
    },
    {
      "name": "TSB-UAD",
      "one_line_profile": "Benchmark suite for univariate time-series anomaly detection",
      "detailed_description": "An end-to-end benchmark suite for univariate time-series anomaly detection, including datasets from scientific and engineering domains (e.g., NASA spacecraft, water treatment).",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "anomaly_detection",
        "time_series_analysis"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/TheDatumOrg/TSB-UAD",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "time-series",
        "anomaly-detection",
        "scientific-data"
      ],
      "id": 104
    },
    {
      "name": "AgML-CY-Bench",
      "one_line_profile": "Crop yield forecasting benchmark dataset",
      "detailed_description": "A comprehensive dataset and benchmark for forecasting crop yields at the subnational level, harmonizing public yield statistics with relevant predictors for agricultural research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "crop_yield_forecasting",
        "agricultural_modeling"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/WUR-AI/AgML-CY-Bench",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "agriculture",
        "crop-yield",
        "benchmark"
      ],
      "id": 105
    },
    {
      "name": "SciTab",
      "one_line_profile": "Benchmark for reasoning on scientific tables",
      "detailed_description": "A challenging benchmark designed for compositional reasoning and claim verification specifically on scientific tables.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "scientific_reasoning",
        "table_verification"
      ],
      "application_level": "dataset",
      "primary_language": null,
      "repo_url": "https://github.com/XinyuanLu00/SciTab",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "scientific-tables",
        "reasoning",
        "benchmark"
      ],
      "id": 106
    },
    {
      "name": "EHRStruct",
      "one_line_profile": "Benchmark for LLMs on structured electronic health records",
      "detailed_description": "A comprehensive benchmark framework for evaluating Large Language Models on tasks involving structured Electronic Health Records (EHR).",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "ehr_analysis",
        "medical_informatics"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/YXNTU/EHRStruct",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "ehr",
        "medical-benchmark",
        "llm"
      ],
      "id": 107
    },
    {
      "name": "mimic3-benchmarks",
      "one_line_profile": "Benchmark datasets from MIMIC-III clinical database",
      "detailed_description": "A Python suite to construct benchmark machine learning datasets from the MIMIC-III clinical database for healthcare research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "clinical_prediction",
        "medical_dataset_construction"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/YerevaNN/mimic3-benchmarks",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mimic-iii",
        "clinical-data",
        "benchmark"
      ],
      "id": 108
    },
    {
      "name": "MUBen",
      "one_line_profile": "Benchmark for uncertainty of molecular representation models",
      "detailed_description": "A benchmark suite for evaluating the uncertainty estimation capabilities of molecular representation models in cheminformatics.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "molecular_representation",
        "uncertainty_estimation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Yinghao-Li/MUBen",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "molecular-modeling",
        "uncertainty",
        "benchmark"
      ],
      "id": 109
    },
    {
      "name": "math-evaluation-harness",
      "one_line_profile": "Toolkit for benchmarking LLMs on mathematical reasoning tasks",
      "detailed_description": "A specialized evaluation framework designed to assess the performance of Large Language Models (LLMs) on mathematical reasoning problems, providing a standardized harness for metrics calculation and result comparison.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "math_reasoning",
        "model_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ZubinGou/math-evaluation-harness",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "benchmark",
        "mathematics",
        "reasoning"
      ],
      "id": 110
    },
    {
      "name": "AidanBench",
      "one_line_profile": "Benchmark suite for measuring specific behavioral metrics in LLMs",
      "detailed_description": "A benchmarking tool aimed at measuring 'big_model_smell' and other behavioral characteristics in Large Language Models, contributing to the evaluation ecology of AI models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "behavioral_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/aidanmclaughlin/AidanBench",
      "help_website": [],
      "license": null,
      "tags": [
        "llm",
        "benchmark",
        "evaluation"
      ],
      "id": 111
    },
    {
      "name": "PeerRead",
      "one_line_profile": "Dataset and code for analyzing scientific peer reviews",
      "detailed_description": "A dataset and accompanying code for the analysis of scientific peer reviews, enabling NLP research into scientific literature assessment and review processes.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "scientific_literature_mining",
        "nlp_dataset"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/allenai/PeerRead",
      "help_website": [],
      "license": null,
      "tags": [
        "nlp",
        "peer-review",
        "scientific-literature"
      ],
      "id": 112
    },
    {
      "name": "tau2-bench-verified",
      "one_line_profile": "Verified benchmark for evaluating AI agents on database tasks",
      "detailed_description": "A corrected and verified version of the τ²-bench benchmark, providing reliable task definitions and evaluation criteria for assessing the performance of AI agents in database interaction scenarios.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "agent_evaluation",
        "database_interaction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/amazon-agi/tau2-bench-verified",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "benchmark",
        "agents",
        "database"
      ],
      "id": 113
    },
    {
      "name": "SARosPerceptionKitti",
      "one_line_profile": "ROS package for KITTI vision benchmark perception tasks",
      "detailed_description": "A ROS-based framework for executing and evaluating perception tasks (detection, tracking) using the KITTI Vision Benchmark Suite, facilitating robotics and computer vision research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "object_detection",
        "tracking",
        "benchmark_implementation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/appinho/SARosPerceptionKitti",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ros",
        "kitti",
        "computer-vision",
        "robotics"
      ],
      "id": 114
    },
    {
      "name": "AD-ML",
      "one_line_profile": "Framework for reproducible classification of Alzheimer's disease",
      "detailed_description": "A machine learning framework designed for the reproducible classification of Alzheimer's disease using neuroimaging data, serving as a benchmark for medical AI methods.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "disease_classification",
        "medical_imaging"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/aramis-lab/AD-ML",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "alzheimers",
        "machine-learning",
        "medical-imaging"
      ],
      "id": 115
    },
    {
      "name": "clinicadl",
      "one_line_profile": "Deep learning framework for neuroimaging data processing",
      "detailed_description": "A framework for reproducible processing and analysis of neuroimaging data using deep learning, supporting tasks like classification, reconstruction, and segmentation in medical research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "medical_imaging",
        "deep_learning_framework"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aramis-lab/clinicadl",
      "help_website": [
        "https://clinicadl.readthedocs.io"
      ],
      "license": "MIT",
      "tags": [
        "neuroimaging",
        "deep-learning",
        "medical-research"
      ],
      "id": 116
    },
    {
      "name": "BigCodeBench-X",
      "one_line_profile": "Multilingual programming task benchmark for LLMs",
      "detailed_description": "A benchmark suite for evaluating Large Language Models on programming tasks across multiple programming languages, assessing code generation and reasoning capabilities.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "code_generation",
        "model_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/arjunguha/BigCodeBench-X",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "benchmark",
        "code-generation"
      ],
      "id": 117
    },
    {
      "name": "Tartarus",
      "one_line_profile": "Benchmark platform for inverse molecular design",
      "detailed_description": "A benchmarking platform designed for realistic and practical inverse molecular design, evaluating generative models in chemistry and materials science contexts.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "molecular_design",
        "generative_chemistry"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/aspuru-guzik-group/Tartarus",
      "help_website": [],
      "license": null,
      "tags": [
        "molecular-design",
        "chemistry",
        "benchmark"
      ],
      "id": 118
    },
    {
      "name": "File-Format-Testing",
      "one_line_profile": "Benchmark for scientific file formats (HDF5, netCDF4, Zarr)",
      "detailed_description": "A configurable benchmarking tool for comparing the performance of common scientific file formats (HDF5, netCDF4, Zarr) across various I/O operations.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "io_benchmarking",
        "scientific_data_management"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/asriniket/File-Format-Testing",
      "help_website": [],
      "license": null,
      "tags": [
        "hdf5",
        "netcdf",
        "zarr",
        "benchmark"
      ],
      "id": 119
    },
    {
      "name": "adaptrapezoid_benchmark",
      "one_line_profile": "Benchmark for adaptive trapezoid numeric integration algorithms",
      "detailed_description": "A benchmarking tool to evaluate the performance of different programming languages for scientific computing tasks, specifically using the adaptive trapezoid numeric integration algorithm.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "numeric_integration",
        "performance_benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Scala",
      "repo_url": "https://github.com/astrojhgu/adaptrapezoid_benchmark",
      "help_website": [],
      "license": null,
      "tags": [
        "numeric-integration",
        "scientific-computing",
        "benchmark"
      ],
      "id": 120
    },
    {
      "name": "TerjamaBench",
      "one_line_profile": "Evaluation code for the TerjamaBench dataset",
      "detailed_description": "Code repository for running evaluations on the TerjamaBench dataset, facilitating benchmarking of NLP models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "nlp_benchmark"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/atlasia-ma/TerjamaBench",
      "help_website": [],
      "license": null,
      "tags": [
        "nlp",
        "benchmark",
        "evaluation"
      ],
      "id": 121
    },
    {
      "name": "fm-leaderboarder",
      "one_line_profile": "Tool for creating custom LLM evaluation leaderboards",
      "detailed_description": "A utility to create custom leaderboards for evaluating Large Language Models (LLMs) based on specific data, tasks, and prompts, enabling tailored model selection for business or research use cases.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "leaderboard_generation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/aws-samples/fm-leaderboarder",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "leaderboard",
        "evaluation"
      ],
      "id": 122
    },
    {
      "name": "RT-CUDA",
      "one_line_profile": "Optimizing compiler for CUDA-based scientific computing",
      "detailed_description": "A restructuring compiler and optimization tool designed to bridge high-level languages and CUDA for scientific applications, supporting linear algebra solvers and simulations (e.g., reservoir, molecular dynamics).",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "scientific_computing",
        "hpc_optimization",
        "linear_algebra"
      ],
      "application_level": "solver",
      "primary_language": "C",
      "repo_url": "https://github.com/ayazhassan/RT-CUDA-GUI-Development",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "cuda",
        "hpc",
        "compiler",
        "scientific-simulation"
      ],
      "id": 123
    },
    {
      "name": "MolNet Geometric Lightning",
      "one_line_profile": "Benchmarking framework for molecular property prediction using PyTorch Lightning and Torch Geometric",
      "detailed_description": "A repository implementing MoleculeNet benchmarks specifically adapted for Graph Neural Networks using PyTorch Lightning and Torch Geometric, facilitating reproducible evaluation of molecular machine learning models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "molecular_property_prediction",
        "graph_representation_learning"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/bayer-science-for-a-better-life/molnet-geometric-lightning",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "molecular-graphs",
        "drug-discovery",
        "benchmark"
      ],
      "id": 124
    },
    {
      "name": "BEIR",
      "one_line_profile": "Heterogeneous benchmark for zero-shot information retrieval including scientific corpora",
      "detailed_description": "A heterogeneous benchmark for information retrieval that includes diverse datasets, significantly covering scientific domains (BioASQ, TREC-COVID, NFCorpus, SciFact), enabling the evaluation of retrieval models for scientific literature mining.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "information_retrieval",
        "scientific_literature_mining"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/beir-cellar/beir",
      "help_website": [
        "https://github.com/beir-cellar/beir/wiki"
      ],
      "license": "Apache-2.0",
      "tags": [
        "information-retrieval",
        "biomedical-ir",
        "benchmark"
      ],
      "id": 125
    },
    {
      "name": "CiteME",
      "one_line_profile": "Benchmark for evaluating citation recommendation in scientific texts",
      "detailed_description": "A benchmark designed to test the abilities of language models in identifying and retrieving papers that are cited in scientific texts, supporting the development of AI assistants for scientific writing.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "citation_recommendation",
        "scientific_literature_mining"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/bethgelab/CiteME",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "nlp-for-science",
        "citation-analysis",
        "benchmark"
      ],
      "id": 126
    },
    {
      "name": "MDBenchmark",
      "one_line_profile": "Tool for generating and analyzing molecular dynamics simulation benchmarks",
      "detailed_description": "A command-line tool and Python library to quickly generate, start, and analyze benchmarks for molecular dynamics simulations (e.g., GROMACS, NAMD), helping researchers optimize simulation performance on HPC systems.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "molecular_dynamics_simulation",
        "performance_benchmarking"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/bio-phys/MDBenchmark",
      "help_website": [
        "https://mdbenchmark.readthedocs.io/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "molecular-dynamics",
        "hpc",
        "benchmarking"
      ],
      "id": 127
    },
    {
      "name": "Brain-Score Vision",
      "one_line_profile": "Framework for evaluating vision models against brain and behavioral data",
      "detailed_description": "A framework that evaluates artificial neural networks on their alignment with biological brain measurements (neural recordings) and behavioral data, serving as a key benchmark in computational neuroscience and vision science.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "neural_alignment_evaluation",
        "computational_neuroscience"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/brain-score/vision",
      "help_website": [
        "http://www.brain-score.org/"
      ],
      "license": "MIT",
      "tags": [
        "neuroscience",
        "computer-vision",
        "brain-alignment"
      ],
      "id": 128
    },
    {
      "name": "ConvolutionalNeuralOperator",
      "one_line_profile": "Implementation of Convolutional Neural Operators for PDE solving",
      "detailed_description": "Official implementation of Convolutional Neural Operators (CNO), a deep learning framework for robust and accurate learning of Partial Differential Equations (PDEs), used in physics simulations and fluid dynamics.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "pde_solving",
        "fluid_dynamics_simulation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/camlab-ethz/ConvolutionalNeuralOperator",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pde",
        "neural-operators",
        "scientific-machine-learning"
      ],
      "id": 129
    },
    {
      "name": "gee_brazil_sv",
      "one_line_profile": "Benchmark maps and code for secondary forest age in Brazil",
      "detailed_description": "Code repository and dataset for generating benchmark maps of secondary forest age in Brazil using Google Earth Engine, supporting ecological research and carbon stock estimation.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "forest_age_mapping",
        "remote_sensing"
      ],
      "application_level": "dataset",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/celsohlsj/gee_brazil_sv",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "ecology",
        "remote-sensing",
        "google-earth-engine"
      ],
      "id": 130
    },
    {
      "name": "PDF Text Extraction Benchmark",
      "one_line_profile": "Benchmark for evaluating PDF text extraction tools on scientific articles",
      "detailed_description": "A benchmarking suite designed to evaluate the semantic capabilities of PDF extraction tools specifically on scientific articles, addressing the challenge of parsing complex scientific document layouts.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "document_parsing",
        "scientific_literature_mining"
      ],
      "application_level": "library",
      "primary_language": "TeX",
      "repo_url": "https://github.com/ckorzen/pdf-text-extraction-benchmark",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pdf-extraction",
        "scientific-documents",
        "benchmark"
      ],
      "id": 131
    },
    {
      "name": "Quantum-PDE-Benchmark",
      "one_line_profile": "Benchmark for near-term quantum algorithms solving PDEs",
      "detailed_description": "A repository for benchmarking near-term quantum algorithms designed to solve Partial Differential Equations (PDEs), facilitating the evaluation of quantum computing applications in computational physics.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "quantum_algorithm_benchmarking",
        "pde_solving"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/comp-physics/Quantum-PDE-Benchmark",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "quantum-computing",
        "pde",
        "computational-physics"
      ],
      "id": 132
    },
    {
      "name": "DIEF_BTS",
      "one_line_profile": "Building TimeSeries dataset with Brick schema standardization",
      "detailed_description": "A dataset containing time-series data from buildings, standardized using the Brick schema, designed for benchmarking algorithms in building energy modeling and smart building applications.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "building_energy_modeling",
        "time_series_forecasting"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/cruiseresearchgroup/DIEF_BTS",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "smart-buildings",
        "energy-data",
        "brick-schema"
      ],
      "id": 133
    },
    {
      "name": "GBM_Benchmarking",
      "one_line_profile": "Benchmarking Gradient Boosting for molecular property prediction",
      "detailed_description": "Scripts and guidelines for reproducing benchmarks of Gradient Boosting models on molecular property prediction tasks, providing a baseline for chemoinformatics research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "molecular_property_prediction",
        "model_benchmarking"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/dahvida/GBM_Benchmarking",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "chemoinformatics",
        "gradient-boosting",
        "drug-discovery"
      ],
      "id": 134
    },
    {
      "name": "FixaTons",
      "one_line_profile": "Datasets and metrics for evaluating human fixation scanpath similarity",
      "detailed_description": "A collection of datasets and metrics specifically designed to calculate and evaluate the similarity of human eye-fixation scanpaths, supporting research in visual attention and saliency modeling.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "metric_calculation",
        "evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/dariozanca/FixaTons",
      "help_website": [],
      "license": null,
      "tags": [
        "scanpath",
        "eye-tracking",
        "metrics",
        "saliency"
      ],
      "id": 135
    },
    {
      "name": "zk-benchmark-r1cs",
      "one_line_profile": "Tools for benchmarking Zero-Knowledge provers using R1CS",
      "detailed_description": "A set of tools to serialize R1CS (Rank-1 Constraint Systems) and witness representations into standardized datasets, enabling the benchmarking of different Zero-Knowledge provers.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "data_generation"
      ],
      "application_level": "tool",
      "primary_language": "Rust",
      "repo_url": "https://github.com/dcbuild3r/zk-benchmark-r1cs",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "zero-knowledge",
        "cryptography",
        "benchmarking",
        "r1cs"
      ],
      "id": 136
    },
    {
      "name": "WEFE",
      "one_line_profile": "Word Embeddings Fairness Evaluation Framework",
      "detailed_description": "An open source framework for measuring and mitigating bias in word embedding models, providing standardized metrics for fairness evaluation in NLP.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "fairness_evaluation",
        "bias_measurement"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/dccuchile/wefe",
      "help_website": [
        "https://wefe.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "nlp",
        "fairness",
        "bias",
        "word-embeddings"
      ],
      "id": 137
    },
    {
      "name": "FunctionBench",
      "one_line_profile": "Workload suite for benchmarking serverless cloud functions",
      "detailed_description": "A suite of workloads designed to benchmark serverless cloud function services, evaluating performance across different providers and configurations for systems research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_evaluation"
      ],
      "application_level": "benchmark_suite",
      "primary_language": "Python",
      "repo_url": "https://github.com/ddps-lab/serverless-faas-workbench",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "serverless",
        "cloud-computing",
        "benchmark",
        "faas"
      ],
      "id": 138
    },
    {
      "name": "LLM-SRBench",
      "one_line_profile": "Benchmark for Scientific Equation Discovery with LLMs",
      "detailed_description": "A benchmark designed to evaluate Large Language Models on the task of scientific equation discovery, assessing their ability to recover symbolic relationships from data.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "equation_discovery"
      ],
      "application_level": "benchmark_suite",
      "primary_language": "Python",
      "repo_url": "https://github.com/deep-symbolic-mathematics/llm-srbench",
      "help_website": [],
      "license": null,
      "tags": [
        "symbolic-regression",
        "llm",
        "scientific-discovery",
        "benchmark"
      ],
      "id": 139
    },
    {
      "name": "SciAssess",
      "one_line_profile": "Benchmark for LLMs in scientific literature analysis",
      "detailed_description": "A comprehensive benchmark for evaluating Large Language Models' proficiency in scientific literature analysis, covering memorization, comprehension, and analysis across various scientific fields.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "literature_analysis"
      ],
      "application_level": "benchmark_suite",
      "primary_language": "Python",
      "repo_url": "https://github.com/deepmodeling/SciAssess",
      "help_website": [],
      "license": "LGPL-3.0",
      "tags": [
        "llm",
        "scientific-literature",
        "benchmark",
        "evaluation"
      ],
      "id": 140
    },
    {
      "name": "DeathStarBench",
      "one_line_profile": "Benchmark suite for cloud microservices",
      "detailed_description": "An open-source benchmark suite for cloud microservices, designed to evaluate the performance and implications of microservices architectures in systems research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "systems_evaluation"
      ],
      "application_level": "benchmark_suite",
      "primary_language": "Lua",
      "repo_url": "https://github.com/delimitrou/DeathStarBench",
      "help_website": [
        "http://microservices.csl.cornell.edu/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "microservices",
        "cloud",
        "benchmark",
        "systems"
      ],
      "id": 141
    },
    {
      "name": "pyhpc-benchmarks",
      "one_line_profile": "Benchmarks for Python high-performance computing libraries",
      "detailed_description": "A suite of benchmarks for evaluating the CPU and GPU performance of popular high-performance computing libraries in the Python ecosystem, aiding in library selection for scientific computing.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_profiling"
      ],
      "application_level": "benchmark_suite",
      "primary_language": "Python",
      "repo_url": "https://github.com/dionhaefner/pyhpc-benchmarks",
      "help_website": [],
      "license": "Unlicense",
      "tags": [
        "hpc",
        "python",
        "benchmark",
        "gpu",
        "cpu"
      ],
      "id": 142
    },
    {
      "name": "AL4PDE",
      "one_line_profile": "Benchmark for Active Learning in Neural PDE Solvers",
      "detailed_description": "A benchmark suite specifically designed for evaluating active learning strategies applied to neural partial differential equation (PDE) solvers.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "active_learning",
        "pde_solving"
      ],
      "application_level": "benchmark_suite",
      "primary_language": "Python",
      "repo_url": "https://github.com/dmusekamp/al4pde",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "pde",
        "active-learning",
        "neural-solver",
        "benchmark"
      ],
      "id": 143
    },
    {
      "name": "dockstring",
      "one_line_profile": "Dataset and benchmark tasks for molecular docking",
      "detailed_description": "A package providing a curated dataset and realistic benchmark tasks for molecular docking, facilitating the evaluation of machine learning models in drug discovery.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "molecular_docking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/dockstring/dockstring",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "molecular-docking",
        "drug-discovery",
        "benchmark",
        "dataset-loader"
      ],
      "id": 144
    },
    {
      "name": "docling-eval",
      "one_line_profile": "Evaluation framework for document processing models",
      "detailed_description": "A framework for evaluating document processing models and services, providing metrics and tools to assess performance on document understanding tasks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "document_processing"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/docling-project/docling-eval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "document-processing",
        "evaluation",
        "metrics"
      ],
      "id": 145
    },
    {
      "name": "ADaM Standard Code",
      "one_line_profile": "Standardized code for creating ADaM clinical datasets",
      "detailed_description": "A repository of standardized SAS and R code designed to create ADaM (Analysis Data Model) datasets from SDTM data, supporting clinical trial data analysis and submission.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "data_processing",
        "standardization"
      ],
      "application_level": "library",
      "primary_language": "SAS",
      "repo_url": "https://github.com/dominodatalab/ADaM_Standard_Code",
      "help_website": [],
      "license": null,
      "tags": [
        "clinical-trials",
        "adam",
        "sdtm",
        "sas",
        "r"
      ],
      "id": 146
    },
    {
      "name": "dpbench",
      "one_line_profile": "Dataplane benchmarking suite",
      "detailed_description": "A benchmarking suite for evaluating dataplane performance, useful for systems research and network optimization.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_evaluation"
      ],
      "application_level": "benchmark_suite",
      "primary_language": "Shell",
      "repo_url": "https://github.com/dpbench/dpbench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "dataplane",
        "networking",
        "benchmark",
        "systems"
      ],
      "id": 147
    },
    {
      "name": "Blind Image Quality Toolbox",
      "one_line_profile": "Collection of blind image quality metrics",
      "detailed_description": "A MATLAB toolbox containing various blind (no-reference) image quality metrics, used for evaluating image processing algorithms and quality assessment.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "metric_calculation",
        "image_quality_assessment"
      ],
      "application_level": "library",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/dsoellinger/blind_image_quality_toolbox",
      "help_website": [],
      "license": null,
      "tags": [
        "image-quality",
        "matlab",
        "metrics",
        "blind-assessment"
      ],
      "id": 148
    },
    {
      "name": "SAR-ShipDet-Dataset-Processor",
      "one_line_profile": "Processing tool for SAR ship detection datasets",
      "detailed_description": "A unified tool for processing various Synthetic Aperture Radar (SAR) ship detection datasets (HRSID, SSDD, etc.) into standardized formats for research and model training.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "data_processing",
        "normalization"
      ],
      "application_level": "tool",
      "primary_language": "Python",
      "repo_url": "https://github.com/egshkim/SAR-ShipDet-Dataset-Processor",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "sar",
        "remote-sensing",
        "dataset-processing",
        "ship-detection"
      ],
      "id": 149
    },
    {
      "name": "AdaTime",
      "one_line_profile": "Benchmarking suite for domain adaptation on time series",
      "detailed_description": "A benchmarking suite designed to evaluate domain adaptation algorithms specifically on time series data, facilitating reproducible research in temporal distribution shifts.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "domain_adaptation"
      ],
      "application_level": "benchmark_suite",
      "primary_language": "Python",
      "repo_url": "https://github.com/emadeldeen24/AdaTime",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "time-series",
        "domain-adaptation",
        "benchmark",
        "machine-learning"
      ],
      "id": 150
    },
    {
      "name": "repurpose",
      "one_line_profile": "Framework for drug repurposing classifiers",
      "detailed_description": "A Python-based framework for building and evaluating drug-disease association classifiers, supporting reproducible research in drug repurposing.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "modeling",
        "drug_repurposing"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/emreg00/repurpose",
      "help_website": [],
      "license": null,
      "tags": [
        "drug-repurposing",
        "bioinformatics",
        "machine-learning",
        "drug-discovery"
      ],
      "id": 151
    },
    {
      "name": "text-to-image-eval",
      "one_line_profile": "Evaluation metrics for text-to-image models",
      "detailed_description": "A tool to evaluate custom and HuggingFace text-to-image and zero-shot image classification models using metrics like Zero-shot accuracy, Linear Probe, and Image retrieval.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "metric_calculation"
      ],
      "application_level": "tool",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/encord-team/text-to-image-eval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "text-to-image",
        "evaluation",
        "metrics",
        "computer-vision"
      ],
      "id": 152
    },
    {
      "name": "Kurtis",
      "one_line_profile": "Fine-tuning and evaluation tool for Small Language Models",
      "detailed_description": "A tool designed for fine-tuning, inference, and evaluation of Small Language Models (SLMs), streamlining the development lifecycle for smaller AI models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "fine_tuning"
      ],
      "application_level": "tool",
      "primary_language": "Python",
      "repo_url": "https://github.com/ethicalabs-ai/kurtis",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "slm",
        "fine-tuning",
        "evaluation",
        "inference"
      ],
      "id": 153
    },
    {
      "name": "EvalPlus",
      "one_line_profile": "Rigorous evaluation framework for LLM-synthesized code",
      "detailed_description": "A framework for rigorously evaluating code synthesized by Large Language Models, providing enhanced test generation and benchmarking capabilities.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "code_generation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/evalplus/evalplus",
      "help_website": [
        "https://evalplus.github.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "code-generation",
        "evaluation",
        "benchmark"
      ],
      "id": 154
    },
    {
      "name": "lammps-bulk-benchmark",
      "one_line_profile": "Benchmark for LAMMPS molecular dynamics simulations",
      "detailed_description": "A benchmarking tool for evaluating CPU and GPU performance by running bulk molecular dynamics simulations using LAMMPS.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_evaluation"
      ],
      "application_level": "benchmark_suite",
      "primary_language": "Shell",
      "repo_url": "https://github.com/evenmn/lammps-bulk-benchmark",
      "help_website": [],
      "license": null,
      "tags": [
        "lammps",
        "molecular-dynamics",
        "benchmark",
        "hpc"
      ],
      "id": 155
    },
    {
      "name": "Evidently",
      "one_line_profile": "ML and LLM observability and evaluation framework",
      "detailed_description": "An open-source framework to evaluate, test, and monitor ML models and LLMs, providing over 100 metrics for data quality, drift detection, and model performance.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "monitoring",
        "quality_control"
      ],
      "application_level": "framework",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/evidentlyai/evidently",
      "help_website": [
        "https://docs.evidentlyai.com/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "observability",
        "evaluation",
        "mlops",
        "drift-detection"
      ],
      "id": 156
    },
    {
      "name": "HydroNet",
      "one_line_profile": "Benchmark tasks for molecular data modeling",
      "detailed_description": "A set of benchmark tasks designed to evaluate predictive and generative models for molecular data, focusing on preserving long-range interactions and structural motifs.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "molecular_modeling"
      ],
      "application_level": "benchmark_suite",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/exalearn/hydronet",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "molecular-data",
        "benchmark",
        "generative-models",
        "chemistry"
      ],
      "id": 157
    },
    {
      "name": "datamaestro",
      "one_line_profile": "Scripts for automated dataset handling and standardization",
      "detailed_description": "A library to automatize and standardize the handling of datasets for experiments, facilitating reproducible data pipelines.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "data_management",
        "workflow_automation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/experimaestro/datamaestro",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "dataset-management",
        "reproducibility",
        "workflow"
      ],
      "id": 158
    },
    {
      "name": "BenchMARL",
      "one_line_profile": "Benchmarking library for Multi-Agent Reinforcement Learning",
      "detailed_description": "A library for benchmarking Multi-Agent Reinforcement Learning (MARL) algorithms, enabling standardized comparison of tasks, models, and algorithms.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "reinforcement_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/BenchMARL",
      "help_website": [
        "https://benchmarl.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "marl",
        "reinforcement-learning",
        "benchmark",
        "torchrl"
      ],
      "id": 159
    },
    {
      "name": "DCPerf",
      "one_line_profile": "Benchmark suite for hyperscale cloud applications",
      "detailed_description": "A benchmark suite designed to evaluate the performance of hyperscale cloud applications and datacenter workloads, useful for systems research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "systems_evaluation"
      ],
      "application_level": "benchmark_suite",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/DCPerf",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "cloud",
        "benchmark",
        "datacenter",
        "performance"
      ],
      "id": 160
    },
    {
      "name": "ParlAI",
      "one_line_profile": "A unified framework for training and evaluating dialogue models",
      "detailed_description": "A python framework for sharing, training and testing dialogue models, from open-domain chitchat to task-oriented dialogue. It provides access to many popular datasets and a wide set of reference models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "dialogue_system",
        "dataset_access"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/ParlAI",
      "help_website": [
        "https://parl.ai/"
      ],
      "license": "MIT",
      "tags": [
        "nlp",
        "dialogue",
        "evaluation-framework",
        "chatbot"
      ],
      "id": 161
    },
    {
      "name": "video-transformers",
      "one_line_profile": "Streamlined fine-tuning interface for video classification models",
      "detailed_description": "A wrapper tool designed to simplify the fine-tuning process of HuggingFace video classification models, providing an accessible interface for model training and experimentation.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_training",
        "fine_tuning",
        "video_classification"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/fcakyon/video-transformers",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "video-classification",
        "transformers",
        "fine-tuning"
      ],
      "id": 162
    },
    {
      "name": "transfer-nlp",
      "one_line_profile": "Experiment management library for reproducible NLP research",
      "detailed_description": "A library designed to manage NLP experiments, ensuring reproducibility and easing the process of transfer learning and model configuration.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "experiment_management",
        "reproducibility",
        "nlp"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/feedly/transfer-nlp",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nlp",
        "experiment-tracking",
        "reproducibility"
      ],
      "id": 163
    },
    {
      "name": "FlagEvalMM",
      "one_line_profile": "Comprehensive evaluation framework for multimodal models",
      "detailed_description": "A flexible framework designed to evaluate multimodal AI models across various dimensions, supporting the assessment of model capabilities in processing diverse data types.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "multimodal_learning",
        "benchmarking"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/flageval-baai/FlagEvalMM",
      "help_website": [],
      "license": null,
      "tags": [
        "multimodal",
        "evaluation",
        "benchmark"
      ],
      "id": 164
    },
    {
      "name": "FluidML",
      "one_line_profile": "Lightweight framework for developing machine learning pipelines",
      "detailed_description": "A framework for building ML pipelines that supports caching, parallel execution, and experiment tracking, facilitating efficient model development and scientific experimentation.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "pipeline_orchestration",
        "experiment_management"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/fluidml/fluidml",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "pipeline",
        "machine-learning",
        "experimentation"
      ],
      "id": 165
    },
    {
      "name": "frictionless-js",
      "one_line_profile": "Library for standardized access to tabular scientific data",
      "detailed_description": "A JavaScript library implementing the Frictionless Data specifications, providing standardized methods to access, validate, and process tabular datasets (CSV, Excel) often used in open science.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "data_access",
        "data_validation",
        "interoperability"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/frictionlessdata/frictionless-js",
      "help_website": [
        "https://frictionlessdata.io/"
      ],
      "license": null,
      "tags": [
        "data-standard",
        "csv",
        "open-science"
      ],
      "id": 166
    },
    {
      "name": "pLitter",
      "one_line_profile": "Dataset and model for plastic litter detection",
      "detailed_description": "A standardized dataset and pre-trained deep learning model specifically designed for the detection of plastic litter in environmental settings, aiding in environmental science research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "object_detection",
        "environmental_monitoring",
        "dataset"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/gicait/pLitter",
      "help_website": [],
      "license": null,
      "tags": [
        "dataset",
        "environmental-science",
        "plastic-detection"
      ],
      "id": 167
    },
    {
      "name": "language-table",
      "one_line_profile": "Benchmark for open vocabulary visuolinguomotor learning",
      "detailed_description": "A suite of datasets and a multi-task continuous control benchmark designed for researching open vocabulary visual-language-motor learning in robotics.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "robotics_benchmark",
        "visuomotor_learning",
        "dataset"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/google-research/language-table",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "robotics",
        "benchmark",
        "multimodal"
      ],
      "id": 168
    },
    {
      "name": "realworldrl_suite",
      "one_line_profile": "Benchmark suite for Real-World Reinforcement Learning",
      "detailed_description": "A collection of reinforcement learning tasks designed to capture the challenges of real-world control problems, serving as a benchmark for RL algorithm evaluation.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "benchmarking",
        "control_systems"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/google-research/realworldrl_suite",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "reinforcement-learning",
        "benchmark",
        "real-world-rl"
      ],
      "id": 169
    },
    {
      "name": "BIG-bench",
      "one_line_profile": "Collaborative benchmark for large language models",
      "detailed_description": "The Beyond the Imitation Game Benchmark (BIG-bench) is a collaborative initiative to measure and extrapolate the capabilities of large language models across a diverse range of tasks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "llm_benchmark",
        "task_definition"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/google/BIG-bench",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "benchmark",
        "evaluation"
      ],
      "id": 170
    },
    {
      "name": "Google ADK (Go)",
      "one_line_profile": "Toolkit for building and evaluating AI agents (Go)",
      "detailed_description": "An open-source toolkit for building, evaluating, and deploying sophisticated AI agents, providing frameworks for agentic workflows and evaluation.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "agent_evaluation",
        "agent_development"
      ],
      "application_level": "framework",
      "primary_language": "Go",
      "repo_url": "https://github.com/google/adk-go",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ai-agents",
        "toolkit",
        "evaluation"
      ],
      "id": 171
    },
    {
      "name": "Google ADK (Java)",
      "one_line_profile": "Toolkit for building and evaluating AI agents (Java)",
      "detailed_description": "An open-source toolkit for building, evaluating, and deploying sophisticated AI agents, providing frameworks for agentic workflows and evaluation.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "agent_evaluation",
        "agent_development"
      ],
      "application_level": "framework",
      "primary_language": "Java",
      "repo_url": "https://github.com/google/adk-java",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ai-agents",
        "toolkit",
        "evaluation"
      ],
      "id": 172
    },
    {
      "name": "Google ADK (Python)",
      "one_line_profile": "Toolkit for building and evaluating AI agents (Python)",
      "detailed_description": "An open-source toolkit for building, evaluating, and deploying sophisticated AI agents, providing frameworks for agentic workflows and evaluation.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "agent_evaluation",
        "agent_development"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/google/adk-python",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ai-agents",
        "toolkit",
        "evaluation"
      ],
      "id": 173
    },
    {
      "name": "h2o-LLM-eval",
      "one_line_profile": "LLM evaluation framework with Elo leaderboard",
      "detailed_description": "A framework for evaluating Large Language Models, featuring an Elo rating system and A/B testing capabilities to assess model performance.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "llm_ranking",
        "ab_testing"
      ],
      "application_level": "framework",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/h2oai/h2o-LLM-eval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "evaluation",
        "leaderboard"
      ],
      "id": 174
    },
    {
      "name": "pytorch-worker",
      "one_line_profile": "Framework for training and evaluating PyTorch models",
      "detailed_description": "A lightweight framework designed to streamline the training, evaluation, and testing processes for deep learning models built with PyTorch.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_training",
        "model_evaluation",
        "workflow_automation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/haoxizhong/pytorch-worker",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pytorch",
        "training-framework",
        "evaluation"
      ],
      "id": 175
    },
    {
      "name": "SeisFlowBench",
      "one_line_profile": "Benchmark for seismic wave propagation simulations",
      "detailed_description": "A reproducible scientific project and benchmark suite for seismic wave propagation and fluid flow simulations in porous media, built using Julia.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "seismic_simulation",
        "benchmarking",
        "geophysics"
      ],
      "application_level": "solver",
      "primary_language": "Julia",
      "repo_url": "https://github.com/haoyunl2/SeisFlowBench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "seismology",
        "simulation",
        "julia"
      ],
      "id": 176
    },
    {
      "name": "dlio_benchmark",
      "one_line_profile": "I/O benchmark for scientific deep learning workloads",
      "detailed_description": "A benchmark suite designed to represent and evaluate the I/O patterns and performance of scientific deep learning applications.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "io_benchmarking",
        "scientific_computing",
        "performance_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/hariharan-devarajan/dlio_benchmark",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "hpc",
        "deep-learning",
        "io-benchmark"
      ],
      "id": 177
    },
    {
      "name": "big-ann-benchmarks",
      "one_line_profile": "Benchmark for billion-scale approximate nearest neighbor search",
      "detailed_description": "A framework for evaluating Approximate Nearest Neighbor Search (ANNS) algorithms on billion-scale datasets, critical for vector search in AI applications.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "algorithm_evaluation",
        "vector_search",
        "benchmarking"
      ],
      "application_level": "framework",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/harsha-simhadri/big-ann-benchmarks",
      "help_website": [
        "http://big-ann-benchmarks.com/"
      ],
      "license": "MIT",
      "tags": [
        "ann",
        "vector-search",
        "benchmark"
      ],
      "id": 178
    },
    {
      "name": "EvalView",
      "one_line_profile": "Test harness for AI agents evaluation",
      "detailed_description": "A pytest-style test harness for evaluating AI agents, supporting YAML scenarios, tool-call checks, and reporting on cost, latency, and safety.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "agent_evaluation",
        "testing_framework",
        "safety_eval"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/hidai25/eval-view",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ai-agents",
        "evaluation",
        "testing"
      ],
      "id": 179
    },
    {
      "name": "This-is-not-a-Dataset",
      "one_line_profile": "Dataset for evaluating negation and commonsense in LLMs",
      "detailed_description": "A large semi-automatically generated dataset of descriptive sentences about commonsense knowledge, focusing on negation, used for evaluating Large Language Models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "commonsense_reasoning",
        "dataset"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/hitz-zentroa/This-is-not-a-Dataset",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-evaluation",
        "dataset",
        "commonsense"
      ],
      "id": 180
    },
    {
      "name": "Latxa",
      "one_line_profile": "Language model and evaluation suite for Basque",
      "detailed_description": "An open language model and a comprehensive evaluation suite specifically designed for the Basque language, facilitating NLP research in low-resource languages.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "nlp_benchmark",
        "low_resource_language"
      ],
      "application_level": "framework",
      "primary_language": "Shell",
      "repo_url": "https://github.com/hitz-zentroa/latxa",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nlp",
        "basque",
        "evaluation-suite"
      ],
      "id": 181
    },
    {
      "name": "ColorTransferLib",
      "one_line_profile": "Library for color transfer algorithms and evaluation metrics",
      "detailed_description": "A collection of algorithms for color and style transfer, accompanied by objective evaluation metrics for quantitative assessment of image processing results.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "image_processing",
        "algorithm_evaluation",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/hpotechius/ColorTransferLib",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "color-transfer",
        "image-processing",
        "metrics"
      ],
      "id": 182
    },
    {
      "name": "hf_benchmarks",
      "one_line_profile": "Starter kit for evaluating benchmarks on HuggingFace Hub",
      "detailed_description": "A toolkit designed to facilitate the evaluation of benchmarks hosted on the HuggingFace Hub, providing starter code and utilities for model assessment.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "model_evaluation",
        "huggingface_hub"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/huggingface/hf_benchmarks",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "benchmark",
        "evaluation",
        "huggingface"
      ],
      "id": 183
    },
    {
      "name": "LightEval",
      "one_line_profile": "Toolkit for evaluating LLMs across multiple backends",
      "detailed_description": "A comprehensive toolkit for evaluating Large Language Models, supporting multiple backends and providing a unified interface for running various evaluation tasks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "llm",
        "inference_backend"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/huggingface/lighteval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "evaluation",
        "toolkit"
      ],
      "id": 184
    },
    {
      "name": "ScreenSuite",
      "one_line_profile": "Benchmarking suite for GUI Agents",
      "detailed_description": "A comprehensive benchmarking suite designed to evaluate the performance of GUI Agents, enabling standardized testing of agent interactions with graphical user interfaces.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "agent_evaluation",
        "gui_automation",
        "benchmark"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/huggingface/screensuite",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "gui-agents",
        "benchmark",
        "evaluation"
      ],
      "id": 185
    },
    {
      "name": "PINNacle",
      "one_line_profile": "Benchmark for Physics-Informed Neural Networks",
      "detailed_description": "A comprehensive benchmark suite for Physics-Informed Neural Networks (PINNs) applied to solving Partial Differential Equations (PDEs), facilitating fair comparison of different PINN methods.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "pde_solving",
        "model_benchmarking",
        "scientific_ml"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/i207M/PINNacle",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pinns",
        "pde",
        "benchmark",
        "ai4science"
      ],
      "id": 186
    },
    {
      "name": "transformers-lightning",
      "one_line_profile": "Integration library for PyTorch Lightning and Transformers",
      "detailed_description": "A collection of utilities, metrics, and modules to seamlessly integrate HuggingFace Transformers with PyTorch Lightning, facilitating efficient model training and evaluation workflows.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_training",
        "integration",
        "workflow_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/iKernels/transformers-lightning",
      "help_website": [],
      "license": "GPL-2.0",
      "tags": [
        "pytorch-lightning",
        "transformers",
        "integration"
      ],
      "id": 187
    },
    {
      "name": "ChainForge",
      "one_line_profile": "Visual programming environment for LLM prompt evaluation",
      "detailed_description": "An open-source visual programming environment designed for battle-testing and evaluating prompts for Large Language Models, enabling systematic comparison of model outputs.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "prompt_engineering",
        "model_evaluation",
        "visual_programming"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/ianarawjo/ChainForge",
      "help_website": [
        "https://chainforge.ai/"
      ],
      "license": "MIT",
      "tags": [
        "prompt-engineering",
        "llm",
        "evaluation",
        "visualization"
      ],
      "id": 188
    },
    {
      "name": "coref-data",
      "one_line_profile": "Standardized collection of coreference resolution datasets",
      "detailed_description": "A repository containing a collection of coreference datasets formatted in a standardized way to facilitate training and evaluation of coreference resolution models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "dataset_management",
        "evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/ianporada/coref-data",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "nlp",
        "coreference-resolution",
        "dataset"
      ],
      "id": 189
    },
    {
      "name": "PPTAgent",
      "one_line_profile": "Framework for generating and evaluating presentation slides",
      "detailed_description": "A research framework designed to generate and evaluate presentations, moving beyond simple text-to-slides by incorporating multimodal reasoning and evaluation metrics.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "generation"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/icip-cas/PPTAgent",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "multimodal",
        "evaluation-framework",
        "presentation-generation"
      ],
      "id": 190
    },
    {
      "name": "scaling-resnets",
      "one_line_profile": "Framework investigating scaling limits of ResNets vs Neural ODEs",
      "detailed_description": "A research framework used to investigate the scaling limits of ResNets and compare them to Neural ODEs, tested on synthetic and standardized datasets for scientific modeling analysis.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "modeling",
        "evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/instadeepai/scaling-resnets",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "neural-odes",
        "resnet",
        "scaling-laws"
      ],
      "id": 191
    },
    {
      "name": "itu-p1203-open-dataset",
      "one_line_profile": "Open dataset for ITU-T P.1203 video quality standardization",
      "detailed_description": "An open dataset used for the standardization of ITU-T P.1203, facilitating the evaluation and benchmarking of video quality assessment models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "quality_control"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/itu-p1203/open-dataset",
      "help_website": [],
      "license": null,
      "tags": [
        "video-quality",
        "standardization",
        "dataset"
      ],
      "id": 192
    },
    {
      "name": "mallet",
      "one_line_profile": "Evaluation harness for VLMs controlling robots",
      "detailed_description": "Cloud-based tools and an evaluation harness designed to test and benchmark Vision-Language Models (VLMs) for controlling real-world robots.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "robotics"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/jacobphillips99/mallet",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "vlm",
        "robotics",
        "evaluation-harness"
      ],
      "id": 193
    },
    {
      "name": "matbench-discovery",
      "one_line_profile": "Evaluation framework for high-throughput materials discovery",
      "detailed_description": "An evaluation framework for machine learning models that simulates high-throughput materials discovery, providing metrics and benchmarks for materials science.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "modeling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/janosh/matbench-discovery",
      "help_website": [
        "https://matbench-discovery.materialsproject.org"
      ],
      "license": "MIT",
      "tags": [
        "materials-science",
        "discovery",
        "benchmark"
      ],
      "id": 194
    },
    {
      "name": "41-llms-evaluated-on-19-benchmarks",
      "one_line_profile": "Benchmark suite and results for 41 LLMs across 19 tasks",
      "detailed_description": "A project benchmarking 41 open-source large language models across 19 evaluation tasks using the lm-evaluation-harness library, providing reproducible evaluation scripts and data.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "benchmarking"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/jayminban/41-llms-evaluated-on-19-benchmarks",
      "help_website": [],
      "license": null,
      "tags": [
        "llm",
        "benchmark",
        "evaluation"
      ],
      "id": 195
    },
    {
      "name": "text2sql-data",
      "one_line_profile": "Collection of Text-to-SQL datasets",
      "detailed_description": "A collection of datasets that pair natural language questions with SQL queries, serving as a benchmark resource for semantic parsing and NLP tasks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "dataset_management",
        "evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/jkkummerfeld/text2sql-data",
      "help_website": [
        "https://jkk.name/text2sql-data/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "nlp",
        "text-to-sql",
        "dataset"
      ],
      "id": 196
    },
    {
      "name": "MicroVQA",
      "one_line_profile": "Multimodal reasoning benchmark for microscopy",
      "detailed_description": "Evaluation code and benchmark for MicroVQA, a multimodal reasoning task focused on microscopy-based scientific research, including the RefineBot method.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "image_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/jmhb0/microvqa",
      "help_website": [],
      "license": null,
      "tags": [
        "microscopy",
        "vqa",
        "benchmark"
      ],
      "id": 197
    },
    {
      "name": "julia-pde-benchmark",
      "one_line_profile": "Benchmark for PDE integration algorithms in Julia",
      "detailed_description": "A benchmarking tool for evaluating the performance of simple Partial Differential Equation (PDE) integration algorithms in Julia compared to other languages.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "modeling"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/johnfgibson/julia-pde-benchmark",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pde",
        "julia",
        "benchmark"
      ],
      "id": 198
    },
    {
      "name": "SciFIBench",
      "one_line_profile": "Benchmark for scientific figure interpretation",
      "detailed_description": "A benchmark suite for evaluating Large Multimodal Models (LMMs) on their ability to interpret and reason about scientific figures.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "image_analysis"
      ],
      "application_level": "dataset",
      "primary_language": null,
      "repo_url": "https://github.com/jonathan-roberts1/SciFIBench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "scientific-figures",
        "multimodal",
        "benchmark"
      ],
      "id": 199
    },
    {
      "name": "ScientificComputingBenchmarks.jl",
      "one_line_profile": "Benchmarks for scientific computing in Julia",
      "detailed_description": "A collection of benchmarks designed to evaluate the performance of various scientific computing tasks and algorithms in the Julia programming language.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_analysis"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/jonathanBieler/ScientificComputingBenchmarks.jl",
      "help_website": [],
      "license": null,
      "tags": [
        "scientific-computing",
        "julia",
        "benchmark"
      ],
      "id": 200
    },
    {
      "name": "entity-recognition-datasets",
      "one_line_profile": "Collection of Named Entity Recognition (NER) corpora",
      "detailed_description": "A comprehensive collection of annotated datasets for named entity recognition tasks across various languages and domains, formatted for research use.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "dataset_management",
        "evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/juand-r/entity-recognition-datasets",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ner",
        "nlp",
        "dataset"
      ],
      "id": 201
    },
    {
      "name": "eva",
      "one_line_profile": "Evaluation framework for oncology foundation models",
      "detailed_description": "A framework designed for the evaluation of oncology foundation models, providing metrics and workflows for assessing model performance in cancer research contexts.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "medical_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/kaiko-ai/eva",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "oncology",
        "foundation-models",
        "evaluation"
      ],
      "id": 202
    },
    {
      "name": "MalDataGen",
      "one_line_profile": "Synthetic tabular dataset generation and evaluation framework",
      "detailed_description": "An advanced framework for generating and evaluating synthetic tabular datasets using generative models like diffusion and adversarial architectures, applicable to scientific data augmentation.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "data_generation",
        "evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/kayua/MalDataGen",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "synthetic-data",
        "generative-models",
        "tabular-data"
      ],
      "id": 203
    },
    {
      "name": "Mars-Bench",
      "one_line_profile": "Benchmark for vision models on Martian imagery",
      "detailed_description": "A standardized benchmark for evaluating computer vision models on Martian surface and orbital imagery, covering classification, segmentation, and detection tasks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "image_analysis"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/kerner-lab/Mars-Bench",
      "help_website": [],
      "license": null,
      "tags": [
        "planetary-science",
        "mars",
        "computer-vision"
      ],
      "id": 204
    },
    {
      "name": "DROP-Fixed-Income",
      "one_line_profile": "Java libraries for fixed income analytics and curve construction",
      "detailed_description": "A collection of libraries for quantitative finance, including multi-curve construction, valuation, and stochastic evolution, serving as a scientific modeling tool for financial mathematics.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "modeling",
        "analysis"
      ],
      "application_level": "library",
      "primary_language": "HTML",
      "repo_url": "https://github.com/lakshmiDRIP/DROP-Fixed-Income",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "quantitative-finance",
        "numerical-analysis",
        "modeling"
      ],
      "id": 205
    },
    {
      "name": "deeplearning-benchmark",
      "one_line_profile": "Benchmark suite for deep learning infrastructure",
      "detailed_description": "A suite of scripts and tools for benchmarking the performance of deep learning models on various hardware configurations.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "Shell",
      "repo_url": "https://github.com/lambdal/deeplearning-benchmark",
      "help_website": [],
      "license": null,
      "tags": [
        "deep-learning",
        "benchmark",
        "gpu"
      ],
      "id": 206
    },
    {
      "name": "lamindb",
      "one_line_profile": "Data framework for biology and scientific data management",
      "detailed_description": "A data framework specifically designed for biology, enabling queryable, traceable, and reproducible data management (FAIR principles) integrating lakehouse and lineage tracking.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "data_management",
        "reproducibility"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/laminlabs/lamindb",
      "help_website": [
        "https://lamin.ai"
      ],
      "license": "Apache-2.0",
      "tags": [
        "biology",
        "data-management",
        "fair-data"
      ],
      "id": 207
    },
    {
      "name": "langfuse",
      "one_line_profile": "LLM engineering platform for observability and evaluation",
      "detailed_description": "An open-source platform for LLM engineering that includes tools for observability, metrics collection, dataset management, and model evaluation.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "monitoring"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/langfuse/langfuse",
      "help_website": [
        "https://langfuse.com"
      ],
      "license": "NOASSERTION",
      "tags": [
        "llm-ops",
        "evaluation",
        "observability"
      ],
      "id": 208
    },
    {
      "name": "langwatch",
      "one_line_profile": "LLM Ops platform for traces, analytics, and evaluations",
      "detailed_description": "A platform for LLM operations focusing on traces, analytics, and evaluations, allowing for the optimization of prompts and assessment of model performance.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "analysis"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/langwatch/langwatch",
      "help_website": [
        "https://langwatch.ai"
      ],
      "license": "NOASSERTION",
      "tags": [
        "llm-ops",
        "evaluation",
        "analytics"
      ],
      "id": 209
    },
    {
      "name": "latitude-llm",
      "one_line_profile": "Prompt engineering and evaluation platform",
      "detailed_description": "An open-source platform designed to build, evaluate, and refine prompts for Large Language Models, facilitating systematic prompt engineering and testing.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "prompt_engineering"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/latitude-dev/latitude-llm",
      "help_website": [
        "https://latitude.so"
      ],
      "license": "LGPL-3.0",
      "tags": [
        "prompt-engineering",
        "evaluation",
        "llm"
      ],
      "id": 210
    },
    {
      "name": "BenchmarkDatasetCreator",
      "one_line_profile": "Pipeline for creating standardized bioacoustic datasets",
      "detailed_description": "A standardized pipeline for creating, storing, sharing, and using bioacoustic datasets, designed to facilitate the training and testing of AI models in bioacoustics.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "data_generation",
        "dataset_management"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/leabouffaut/BenchmarkDatasetCreator",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "bioacoustics",
        "dataset-creation",
        "standardization"
      ],
      "id": 211
    },
    {
      "name": "les-audits-affaires-eval-harness",
      "one_line_profile": "Evaluation harness for French business law LLMs",
      "detailed_description": "A lightweight CLI tool for benchmarking French Large Language Models specifically in the domain of business law, evaluating aspects like action, delay, and risk assessment.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/legml-ai/les-audits-affaires-eval-harness",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "legal-tech",
        "llm-evaluation",
        "french-law"
      ],
      "id": 212
    },
    {
      "name": "llm_benchmarks",
      "one_line_profile": "Collection of benchmarks and datasets for evaluating Large Language Models",
      "detailed_description": "A comprehensive collection of benchmarks and datasets designed for the evaluation of Large Language Models (LLMs), facilitating comparative analysis and performance assessment.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/leobeeson/llm_benchmarks",
      "help_website": [],
      "license": null,
      "tags": [
        "llm",
        "benchmark",
        "dataset",
        "evaluation"
      ],
      "id": 213
    },
    {
      "name": "Blades",
      "one_line_profile": "Unified benchmark suite for attacks and defenses in Federated Learning",
      "detailed_description": "A unified benchmark suite designed to evaluate attacks and defenses in Federated Learning systems, providing a standard environment for security and robustness research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "federated_learning",
        "security_evaluation",
        "benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/lishenghui/blades",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "federated-learning",
        "security",
        "benchmark",
        "attacks",
        "defenses"
      ],
      "id": 214
    },
    {
      "name": "llm-jp-eval-mm",
      "one_line_profile": "Lightweight framework for evaluating visual-language models",
      "detailed_description": "A lightweight evaluation framework specifically designed for visual-language models, supporting various metrics and datasets for multimodal performance assessment.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "multimodal_evaluation",
        "vlm_benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/llm-jp/llm-jp-eval-mm",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "vlm",
        "evaluation",
        "multimodal",
        "benchmark"
      ],
      "id": 215
    },
    {
      "name": "RouteLLM",
      "one_line_profile": "Framework for serving and evaluating LLM routers",
      "detailed_description": "A framework designed to serve and evaluate Large Language Model (LLM) routers, enabling researchers to optimize the trade-off between cost and response quality in LLM deployments.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_routing",
        "cost_optimization",
        "evaluation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/lm-sys/RouteLLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-routing",
        "evaluation",
        "inference-optimization"
      ],
      "id": 216
    },
    {
      "name": "SWT-Bench",
      "one_line_profile": "Evaluation harness for benchmarking LLM repository-level test generation",
      "detailed_description": "An evaluation harness for SWT-Bench, a benchmark designed to assess the capabilities of Large Language Models in generating repository-level tests, specifically for software testing tasks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "code_generation",
        "test_generation",
        "benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/logic-star-ai/swt-bench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "software-testing",
        "benchmark",
        "code-generation"
      ],
      "id": 217
    },
    {
      "name": "Loghub",
      "one_line_profile": "Large collection of system log datasets for AI-driven log analytics",
      "detailed_description": "A comprehensive collection of system log datasets designed to support research in AI-driven log analytics, including tasks such as anomaly detection and failure prediction.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "log_analysis",
        "anomaly_detection",
        "dataset"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/logpai/loghub",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "log-analysis",
        "aiops",
        "dataset",
        "anomaly-detection"
      ],
      "id": 218
    },
    {
      "name": "Matbench",
      "one_line_profile": "Benchmarks for materials science property prediction",
      "detailed_description": "A benchmark suite for evaluating machine learning models on materials science property prediction tasks, providing a standardized set of datasets and evaluation metrics.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "materials_science",
        "property_prediction",
        "benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/materialsproject/matbench",
      "help_website": [
        "https://matbench.materialsproject.org/"
      ],
      "license": "MIT",
      "tags": [
        "materials-science",
        "benchmark",
        "property-prediction"
      ],
      "id": 219
    },
    {
      "name": "pysaliency",
      "one_line_profile": "Python Framework for Saliency Modeling and Evaluation",
      "detailed_description": "A Python framework designed for the modeling and evaluation of visual saliency, providing tools to handle datasets, models, and standard metrics in saliency research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "saliency_prediction",
        "model_evaluation",
        "computer_vision"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/matthias-k/pysaliency",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "saliency",
        "evaluation",
        "computer-vision",
        "neuroscience"
      ],
      "id": 220
    },
    {
      "name": "Video-ChatGPT",
      "one_line_profile": "Video conversation model with quantitative evaluation benchmarking",
      "detailed_description": "A video conversation model capable of generating meaningful conversations about videos, accompanied by a rigorous quantitative evaluation benchmark for assessing video-based conversational models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "video_understanding",
        "multimodal_conversation",
        "benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/mbzuai-oryx/Video-ChatGPT",
      "help_website": [],
      "license": "CC-BY-4.0",
      "tags": [
        "video-llm",
        "benchmark",
        "multimodal"
      ],
      "id": 221
    },
    {
      "name": "Torcheval",
      "one_line_profile": "Library for performant PyTorch model metrics and evaluation",
      "detailed_description": "A library providing a rich collection of performant PyTorch model metrics and tools to facilitate metric computation in distributed training and model evaluation.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "metrics_computation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/meta-pytorch/torcheval",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "pytorch",
        "metrics",
        "evaluation"
      ],
      "id": 222
    },
    {
      "name": "micarraylib",
      "one_line_profile": "Software for aggregation and processing of microphone array datasets",
      "detailed_description": "A software tool for the reproducible aggregation, standardization, and signal processing of microphone array datasets, facilitating research in acoustics and audio processing.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "signal_processing",
        "dataset_standardization",
        "acoustics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/micarraylib/micarraylib",
      "help_website": [],
      "license": "CC-BY-4.0",
      "tags": [
        "audio-processing",
        "microphone-array",
        "dataset-tools"
      ],
      "id": 223
    },
    {
      "name": "OpenTTDLab",
      "one_line_profile": "Framework for running reproducible experiments using OpenTTD",
      "detailed_description": "A Python framework designed to run reproducible experiments using the OpenTTD game engine, suitable for research in reinforcement learning and simulation.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "simulation",
        "reinforcement_learning",
        "experiment_management"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/michalc/OpenTTDLab",
      "help_website": [],
      "license": "GPL-2.0",
      "tags": [
        "simulation",
        "reinforcement-learning",
        "openttd"
      ],
      "id": 224
    },
    {
      "name": "FS-Mol",
      "one_line_profile": "Few-Shot Learning Dataset and Benchmark for Molecules",
      "detailed_description": "A dataset and evaluation benchmark for few-shot learning in molecular property prediction, containing molecular compounds with activity measurements against various protein targets.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "molecular_property_prediction",
        "few_shot_learning",
        "benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/FS-Mol",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "drug-discovery",
        "molecules",
        "few-shot-learning",
        "benchmark"
      ],
      "id": 225
    },
    {
      "name": "Eureka ML Insights",
      "one_line_profile": "Framework for standardizing evaluations of large foundation models",
      "detailed_description": "A framework designed to standardize the evaluation of large foundation models, moving beyond single-score reporting to provide deeper insights into model capabilities.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "foundation_models"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/eureka-ml-insights",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "evaluation",
        "foundation-models",
        "metrics"
      ],
      "id": 226
    },
    {
      "name": "PromptBench",
      "one_line_profile": "Unified evaluation framework for large language models",
      "detailed_description": "A unified evaluation framework for Large Language Models (LLMs) that supports various tasks, datasets, and metrics to assess model performance and robustness.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "prompt_engineering",
        "benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/promptbench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "evaluation",
        "benchmark",
        "prompting"
      ],
      "id": 227
    },
    {
      "name": "Clustering-Datasets",
      "one_line_profile": "Collection of datasets for clustering algorithm benchmarking",
      "detailed_description": "A collection of UCI real-life and synthetic datasets, formatted and ready for use in benchmarking and evaluating clustering algorithms.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "clustering",
        "benchmarking",
        "dataset"
      ],
      "application_level": "dataset",
      "primary_language": null,
      "repo_url": "https://github.com/milaan9/Clustering-Datasets",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "clustering",
        "dataset",
        "machine-learning"
      ],
      "id": 228
    },
    {
      "name": "SPECTRA",
      "one_line_profile": "Spectral framework for evaluation of biomedical AI models",
      "detailed_description": "A framework utilizing spectral analysis methods for the evaluation of biomedical AI models, providing specialized metrics for this domain.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "biomedical_ai",
        "model_evaluation",
        "spectral_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/mims-harvard/SPECTRA",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "biomedical",
        "evaluation",
        "spectral-analysis"
      ],
      "id": 229
    },
    {
      "name": "GenAI Energy Leaderboard",
      "one_line_profile": "Benchmark and measurements for Generative AI energy consumption",
      "detailed_description": "A canonical source and benchmark suite for measuring and evaluating the energy consumption of Generative AI models, providing standardized metrics for efficiency analysis.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "energy_efficiency_analysis"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/ml-energy/leaderboard-v2",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "genai",
        "energy-efficiency",
        "benchmark"
      ],
      "id": 230
    },
    {
      "name": "CK-MLOps",
      "one_line_profile": "Portable workflows and automation recipes for MLOps",
      "detailed_description": "A collection of portable workflows, automation recipes, and components for MLOps in the Unified Collective Knowledge (CK) format, facilitating reproducible ML experiments and benchmarking.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "workflow_automation",
        "reproducibility"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/mlcommons/ck-mlops",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "mlops",
        "reproducibility",
        "workflow"
      ],
      "id": 231
    },
    {
      "name": "MLPerf Storage",
      "one_line_profile": "Benchmark suite for ML storage performance",
      "detailed_description": "The MLPerf Storage Benchmark Suite measures the performance of storage systems in the context of machine learning workloads, providing standardized metrics for system evaluation.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "system_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/mlcommons/storage",
      "help_website": [
        "https://mlcommons.org"
      ],
      "license": "Apache-2.0",
      "tags": [
        "mlperf",
        "storage",
        "benchmark"
      ],
      "id": 232
    },
    {
      "name": "MLPerf Tiny",
      "one_line_profile": "ML benchmark suite for low-power embedded systems",
      "detailed_description": "MLPerf Tiny is a machine learning benchmark suite specifically designed for extremely low-power systems such as microcontrollers, evaluating inference performance on constrained hardware.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "inference_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "C",
      "repo_url": "https://github.com/mlcommons/tiny",
      "help_website": [
        "https://mlcommons.org"
      ],
      "license": "Apache-2.0",
      "tags": [
        "tinyml",
        "embedded-systems",
        "benchmark"
      ],
      "id": 233
    },
    {
      "name": "EvalScope",
      "one_line_profile": "Framework for large model evaluation and benchmarking",
      "detailed_description": "A streamlined and customizable framework for efficient evaluation and performance benchmarking of large models (LLM, VLM, AIGC), supporting various datasets and metrics.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/modelscope/evalscope",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-evaluation",
        "benchmark",
        "modelscope"
      ],
      "id": 234
    },
    {
      "name": "MOSES",
      "one_line_profile": "Benchmarking platform for molecular generation models",
      "detailed_description": "Molecular Sets (MOSES) is a benchmarking platform for molecular generation models, providing standard datasets and metrics to evaluate the quality and diversity of generated chemical structures.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "molecular_generation",
        "benchmarking"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/molecularsets/moses",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "drug-discovery",
        "molecular-generation",
        "benchmark"
      ],
      "id": 235
    },
    {
      "name": "eICU Benchmark Updated",
      "one_line_profile": "Benchmark for clinical tasks on eICU dataset",
      "detailed_description": "An updated version of the eICU Benchmark providing problem definitions and evaluation frameworks for clinical tasks such as Length of Stay (LoS) prediction and Decompensation detection.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "clinical_prediction",
        "benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/mostafaalishahi/eICU_Benchmark_updated",
      "help_website": [],
      "license": null,
      "tags": [
        "healthcare",
        "clinical-benchmark",
        "eicu"
      ],
      "id": 236
    },
    {
      "name": "SciREX",
      "one_line_profile": "Benchmark for scientific information extraction",
      "detailed_description": "A benchmark dataset and evaluation framework for document-level information extraction from scientific articles, focusing on identifying entities and relationships in scientific text.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "information_extraction",
        "benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/n0w0f/scirex",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nlp",
        "scientific-extraction",
        "benchmark"
      ],
      "id": 237
    },
    {
      "name": "NASA Prognostic Algorithms",
      "one_line_profile": "Framework for model-based prognostics of engineering systems",
      "detailed_description": "A Python framework for model-based prognostics, providing algorithms for state estimation, uncertainty propagation, and remaining useful life (RUL) computation for engineering systems.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "prognostics",
        "state_estimation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/nasa/prog_algs",
      "help_website": [],
      "license": null,
      "tags": [
        "prognostics",
        "nasa",
        "engineering-systems"
      ],
      "id": 238
    },
    {
      "name": "latrend",
      "one_line_profile": "R package for clustering longitudinal datasets",
      "detailed_description": "An R package designed for clustering longitudinal datasets in a standardized way, providing interfaces to various clustering methods and facilitating method evaluation.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "clustering",
        "longitudinal_analysis"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/niekdt/latrend",
      "help_website": [],
      "license": "GPL-2.0",
      "tags": [
        "r-package",
        "clustering",
        "longitudinal-data"
      ],
      "id": 239
    },
    {
      "name": "ProkEvo",
      "one_line_profile": "Framework for bacterial population genomics analyses",
      "detailed_description": "An automated, reproducible, and scalable framework for high-throughput bacterial population genomics analyses, facilitating large-scale bioinformatics workflows.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "genomics",
        "bioinformatics_workflow"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/npavlovikj/ProkEvo",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "genomics",
        "bioinformatics",
        "pipeline"
      ],
      "id": 240
    },
    {
      "name": "pcapML",
      "one_line_profile": "Tool for standardizing network traffic analysis datasets",
      "detailed_description": "pcapML standardizes network traffic analysis datasets by directly encoding metadata information into raw traffic captures, facilitating machine learning on network data.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "data_standardization",
        "network_analysis"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/nprint/pcapml",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "network-traffic",
        "dataset-creation",
        "pcap"
      ],
      "id": 241
    },
    {
      "name": "Jury",
      "one_line_profile": "Comprehensive NLP evaluation system",
      "detailed_description": "A comprehensive evaluation system for Natural Language Processing (NLP) tasks, providing a unified interface for various metrics to assess model performance.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "nlp_evaluation",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/obss/jury",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nlp",
        "evaluation",
        "metrics"
      ],
      "id": 242
    },
    {
      "name": "mteval",
      "one_line_profile": "Evaluation metrics for machine translation",
      "detailed_description": "A collection of evaluation metrics and algorithms specifically designed for Machine Translation tasks, implemented in C++ for efficiency.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "machine_translation_evaluation",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/odashi/mteval",
      "help_website": [],
      "license": "LGPL-3.0",
      "tags": [
        "machine-translation",
        "evaluation",
        "cpp"
      ],
      "id": 243
    },
    {
      "name": "blurr",
      "one_line_profile": "Integration library for Transformers and fastai",
      "detailed_description": "A library that integrates Hugging Face Transformers with the fastai framework, enabling efficient training, evaluation, and deployment of transformer-based models for scientific and general NLP tasks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_training",
        "integration"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/ohmeow/blurr",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "fastai",
        "transformers",
        "deep-learning"
      ],
      "id": 244
    },
    {
      "name": "SRSD Benchmark",
      "one_line_profile": "Symbolic Regression datasets and benchmarks",
      "detailed_description": "A benchmark suite rethinking Symbolic Regression datasets for scientific discovery, providing standardized tasks to evaluate algorithms that discover mathematical expressions from data.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "symbolic_regression",
        "scientific_discovery"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/omron-sinicx/srsd-benchmark",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "symbolic-regression",
        "scientific-discovery",
        "benchmark"
      ],
      "id": 245
    },
    {
      "name": "GenAIEval",
      "one_line_profile": "Evaluation and benchmark for Generative AI",
      "detailed_description": "A comprehensive evaluation framework, benchmark, and scorecard for Generative AI, targeting performance (throughput/latency), accuracy, safety, and hallucination metrics.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "platform",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/opea-project/GenAIEval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "genai",
        "evaluation",
        "safety"
      ],
      "id": 246
    },
    {
      "name": "ATLAS",
      "one_line_profile": "Benchmark for frontier scientific reasoning",
      "detailed_description": "ATLAS is a high-difficulty, multidisciplinary benchmark designed to evaluate the scientific reasoning capabilities of AI models across various scientific domains.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "scientific_reasoning",
        "benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": null,
      "repo_url": "https://github.com/open-compass/ATLAS",
      "help_website": [],
      "license": null,
      "tags": [
        "scientific-reasoning",
        "benchmark",
        "llm"
      ],
      "id": 247
    },
    {
      "name": "CompassJudger",
      "one_line_profile": "Judge models for automated evaluation",
      "detailed_description": "A collection of 'Judge Models' introduced by OpenCompass, designed to act as automated evaluators for assessing the performance of other large language models on various benchmarks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "automated_evaluation",
        "model_judging"
      ],
      "application_level": "solver",
      "primary_language": null,
      "repo_url": "https://github.com/open-compass/CompassJudger",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-judge",
        "evaluation",
        "opencompass"
      ],
      "id": 248
    },
    {
      "name": "VLMEvalKit",
      "one_line_profile": "Evaluation toolkit for large multi-modality models",
      "detailed_description": "An open-source evaluation toolkit for Large Multi-modality Models (LMMs), supporting over 220 models and 80 benchmarks, facilitating comprehensive performance assessment.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "multimodal_evaluation",
        "benchmarking"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/open-compass/VLMEvalKit",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "vlm",
        "multimodal",
        "evaluation"
      ],
      "id": 249
    },
    {
      "name": "OpenCompass",
      "one_line_profile": "Comprehensive LLM evaluation platform",
      "detailed_description": "OpenCompass is a comprehensive platform for evaluating Large Language Models (LLMs), supporting a wide range of models and over 100 datasets to assess capabilities across various domains.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/open-compass/opencompass",
      "help_website": [
        "https://opencompass.org.cn/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm-evaluation",
        "benchmark",
        "platform"
      ],
      "id": 250
    },
    {
      "name": "OpenAI Evals",
      "one_line_profile": "Framework for evaluating LLMs and registry of benchmarks",
      "detailed_description": "Evals is a framework for evaluating Large Language Models (LLMs) and LLM systems, providing an open-source registry of benchmarks to test model performance on diverse tasks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/openai/evals",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "llm",
        "evaluation",
        "benchmark"
      ],
      "id": 251
    },
    {
      "name": "Oumi",
      "one_line_profile": "Open-source framework for fine-tuning, evaluating, and deploying LLMs/VLMs",
      "detailed_description": "Oumi is a comprehensive platform that simplifies the lifecycle of large language models (LLMs) and vision-language models (VLMs), providing tools for fine-tuning, evaluation, and deployment, facilitating AI research and application development.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_training",
        "model_evaluation",
        "inference"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/oumi-ai/oumi",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "fine-tuning",
        "evaluation",
        "vlm"
      ],
      "id": 252
    },
    {
      "name": "ToR[e]cSys",
      "one_line_profile": "PyTorch framework for recommendation system algorithms",
      "detailed_description": "ToR[e]cSys is a PyTorch-based framework designed to implement, experiment with, and reproduce recommendation system algorithms, including CTR prediction and learning-to-rank models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "modeling",
        "recommendation_system"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/p768lwy3/torecsys",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "recsys",
        "pytorch",
        "ctr-prediction"
      ],
      "id": 253
    },
    {
      "name": "CloudSuite",
      "one_line_profile": "Benchmark suite for cloud services",
      "detailed_description": "CloudSuite is a benchmark suite for cloud services, covering a wide range of applications such as data analytics, web serving, and media streaming, used for evaluating system performance in cloud environments.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "C",
      "repo_url": "https://github.com/parsa-epfl/cloudsuite",
      "help_website": [
        "http://cloudsuite.ch"
      ],
      "license": "NOASSERTION",
      "tags": [
        "cloud-computing",
        "benchmark",
        "systems-research"
      ],
      "id": 254
    },
    {
      "name": "PDEBench",
      "one_line_profile": "Extensive benchmark for Scientific Machine Learning (SciML)",
      "detailed_description": "PDEBench is a comprehensive benchmark suite for Scientific Machine Learning, specifically focused on solving Partial Differential Equations (PDEs). It provides diverse datasets and tasks to evaluate ML models in scientific contexts.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "sciml",
        "pde_solving"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/pdebench/PDEBench",
      "help_website": [
        "https://arxiv.org/abs/2210.07182"
      ],
      "license": "NOASSERTION",
      "tags": [
        "sciml",
        "pde",
        "benchmark",
        "physics"
      ],
      "id": 255
    },
    {
      "name": "Japanese LM Financial Harness",
      "one_line_profile": "Evaluation harness for Japanese financial language models",
      "detailed_description": "A specialized evaluation framework for assessing the performance of Japanese Language Models on financial domain tasks, facilitating domain-specific AI research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "benchmarking",
        "nlp"
      ],
      "application_level": "framework",
      "primary_language": "Shell",
      "repo_url": "https://github.com/pfnet-research/japanese-lm-fin-harness",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "finance",
        "japanese",
        "evaluation"
      ],
      "id": 256
    },
    {
      "name": "Phoronix Test Suite",
      "one_line_profile": "Automated testing and benchmarking software",
      "detailed_description": "The Phoronix Test Suite is a comprehensive, open-source testing and benchmarking platform for Linux, Windows, and macOS, widely used in computer science research for system performance evaluation.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_testing"
      ],
      "application_level": "solver",
      "primary_language": "PHP",
      "repo_url": "https://github.com/phoronix-test-suite/phoronix-test-suite",
      "help_website": [
        "https://www.phoronix-test-suite.com/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "benchmark",
        "hardware",
        "performance"
      ],
      "id": 257
    },
    {
      "name": "OpenCGRA",
      "one_line_profile": "Framework for modeling and evaluating CGRAs",
      "detailed_description": "OpenCGRA is an open-source framework designed for modeling, testing, and evaluating Coarse-Grained Reconfigurable Architectures (CGRAs), supporting computer architecture research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "modeling",
        "architecture_evaluation"
      ],
      "application_level": "framework",
      "primary_language": "Verilog",
      "repo_url": "https://github.com/pnnl/OpenCGRA",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "cgra",
        "computer-architecture",
        "modeling"
      ],
      "id": 258
    },
    {
      "name": "QASMBench",
      "one_line_profile": "Low-level OpenQASM benchmark suite for NISQ evaluation",
      "detailed_description": "QASMBench is a benchmark suite for evaluating Noisy Intermediate-Scale Quantum (NISQ) devices and simulators using OpenQASM, facilitating quantum computing research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "quantum_computing"
      ],
      "application_level": "dataset",
      "primary_language": "OpenQASM",
      "repo_url": "https://github.com/pnnl/QASMBench",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "quantum",
        "benchmark",
        "openqasm"
      ],
      "id": 259
    },
    {
      "name": "LitSearch",
      "one_line_profile": "Retrieval benchmark for scientific literature search",
      "detailed_description": "LitSearch is a benchmark designed to evaluate retrieval systems on the task of scientific literature search, supporting research in NLP and meta-science.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "information_retrieval"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/princeton-nlp/LitSearch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nlp",
        "retrieval",
        "scientific-literature"
      ],
      "id": 260
    },
    {
      "name": "PhysGym",
      "one_line_profile": "Benchmark suite for LLM-based interactive scientific reasoning",
      "detailed_description": "PhysGym is a benchmark suite for evaluating the capabilities of Large Language Models in interactive scientific reasoning tasks, specifically in physics.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "scientific_reasoning"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/principia-ai/PhysGym",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "physics",
        "reasoning",
        "benchmark"
      ],
      "id": 261
    },
    {
      "name": "Etalon",
      "one_line_profile": "LLM serving performance evaluation harness",
      "detailed_description": "Etalon is a framework for evaluating the performance of LLM serving systems, providing metrics for throughput, latency, and cost, essential for systems research in AI.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_evaluation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/project-etalon/etalon",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-serving",
        "performance",
        "benchmark"
      ],
      "id": 262
    },
    {
      "name": "Prometheus-Eval",
      "one_line_profile": "Library for evaluating LLM responses using Prometheus and GPT-4",
      "detailed_description": "Prometheus-Eval is a Python library that facilitates the evaluation of Large Language Model outputs by leveraging the Prometheus model and GPT-4 as judges, supporting research in automated evaluation metrics.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/prometheus-eval/prometheus-eval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-evaluation",
        "metrics",
        "nlp"
      ],
      "id": 263
    },
    {
      "name": "pyperformance",
      "one_line_profile": "Python performance benchmark suite",
      "detailed_description": "pyperformance is the standard benchmark suite for the Python programming language, used to evaluate and track the performance of Python implementations, supporting computer science and programming language research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/python/pyperformance",
      "help_website": [
        "https://pyperformance.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "python",
        "benchmark",
        "performance"
      ],
      "id": 264
    },
    {
      "name": "psychopy_ext",
      "one_line_profile": "Framework for behavioral neuroscience and psychology experiments",
      "detailed_description": "psychopy_ext is a framework built on top of PsychoPy to facilitate rapid, reproducible design, analysis, and plotting of experiments in behavioral neuroscience and psychology.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "experiment_control",
        "data_analysis"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/qbilius/psychopy_ext",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "neuroscience",
        "psychology",
        "experiment-design"
      ],
      "id": 265
    },
    {
      "name": "Quaterion Models",
      "one_line_profile": "Building blocks for fine-tunable metric learning models",
      "detailed_description": "Quaterion Models provides a collection of pre-trained models and building blocks for metric learning, enabling researchers to build and fine-tune models for similarity search and related tasks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "modeling",
        "metric_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/qdrant/quaterion-models",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "metric-learning",
        "similarity-search",
        "embeddings"
      ],
      "id": 266
    },
    {
      "name": "Renaissance",
      "one_line_profile": "Benchmark suite for the JVM",
      "detailed_description": "Renaissance is a modern benchmark suite for the Java Virtual Machine (JVM), focusing on parallelism and concurrency, used in systems research to evaluate JVM performance.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "SMT",
      "repo_url": "https://github.com/renaissance-benchmarks/renaissance",
      "help_website": [
        "http://renaissance.dev"
      ],
      "license": "GPL-3.0",
      "tags": [
        "jvm",
        "benchmark",
        "concurrency"
      ],
      "id": 267
    },
    {
      "name": "Phylociraptor",
      "one_line_profile": "Rapid phylogenomic tree calculator",
      "detailed_description": "Phylociraptor is a highly customizable and reproducible framework for phylogenomic inference, automating the calculation of phylogenetic trees from genomic data.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "inference",
        "phylogenetics"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/reslp/phylociraptor",
      "help_website": [
        "https://phylociraptor.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "phylogenomics",
        "bioinformatics",
        "tree-inference"
      ],
      "id": 268
    },
    {
      "name": "Auto-Evaluator",
      "one_line_profile": "Evaluation tool for LLM QA chains",
      "detailed_description": "Auto-Evaluator is a tool designed to evaluate the quality of Question Answering (QA) chains powered by LLMs, providing automated metrics for assessing model performance.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "qa_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/rlancemartin/auto-evaluator",
      "help_website": [],
      "license": null,
      "tags": [
        "llm",
        "evaluation",
        "qa"
      ],
      "id": 269
    },
    {
      "name": "RobotPerf Benchmarks",
      "one_line_profile": "Benchmarking suite for robotics computing performance",
      "detailed_description": "RobotPerf is a vendor-neutral benchmarking suite designed to evaluate robotics computing performance, covering both grey-box and black-box approaches for robotics systems.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "robotics"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/robotperf/benchmarks",
      "help_website": [
        "https://robotperf.org"
      ],
      "license": "Apache-2.0",
      "tags": [
        "robotics",
        "benchmark",
        "performance"
      ],
      "id": 270
    },
    {
      "name": "YAIB",
      "one_line_profile": "Yet Another ICU Benchmark for clinical prediction models",
      "detailed_description": "YAIB is a holistic framework for standardizing clinical prediction model experiments, providing datasets, cohorts, tasks, and preprocessing pipelines for ICU data.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "clinical_prediction"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/rvandewater/YAIB",
      "help_website": [
        "https://arxiv.org/abs/2306.05109"
      ],
      "license": "MIT",
      "tags": [
        "clinical-ml",
        "benchmark",
        "icu"
      ],
      "id": 271
    },
    {
      "name": "RXN Reaction Preprocessing",
      "one_line_profile": "Preprocessing library for chemical reaction datasets",
      "detailed_description": "A Python library for the standardization, filtering, augmentation, and tokenization of chemical reaction datasets, developed by IBM Research for chemical AI tasks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "data_processing",
        "chemistry"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/rxn4chemistry/rxn-reaction-preprocessing",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "chemistry",
        "preprocessing",
        "reaction-prediction"
      ],
      "id": 272
    },
    {
      "name": "DialogStudio",
      "one_line_profile": "Unified dataset collection and instruction-aware models for conversational AI",
      "detailed_description": "A large-scale collection of unified datasets for conversational AI, designed to facilitate research in dialogue systems and instruction tuning.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "dataset_collection",
        "conversational_ai"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/salesforce/DialogStudio",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "nlp",
        "dialogue-systems",
        "dataset"
      ],
      "id": 273
    },
    {
      "name": "RePlay",
      "one_line_profile": "Framework for building and evaluating recommendation systems",
      "detailed_description": "A comprehensive framework for building, training, and evaluating recommendation systems, supporting various state-of-the-art models and data preprocessing pipelines.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "recommendation_system",
        "model_evaluation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/sb-ai-lab/RePlay",
      "help_website": [
        "https://replay.readthedocs.io/en/latest/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "recsys",
        "pytorch",
        "evaluation"
      ],
      "id": 274
    },
    {
      "name": "GAP Benchmark Suite",
      "one_line_profile": "Benchmark suite for graph processing algorithms",
      "detailed_description": "A benchmark suite designed to standardize the evaluation of graph processing algorithms and systems, providing reference implementations and datasets.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "graph_processing",
        "performance_benchmarking"
      ],
      "application_level": "benchmark",
      "primary_language": "C++",
      "repo_url": "https://github.com/sbeamer/gapbs",
      "help_website": [
        "http://gap.cs.berkeley.edu/benchmark.html"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "graph-algorithms",
        "hpc",
        "benchmark"
      ],
      "id": 275
    },
    {
      "name": "SciCode",
      "one_line_profile": "Benchmark for code generation in scientific domains",
      "detailed_description": "A benchmark designed to evaluate the capability of language models to generate code for solving scientific problems across various disciplines.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "code_generation",
        "model_evaluation"
      ],
      "application_level": "benchmark",
      "primary_language": "Python",
      "repo_url": "https://github.com/scicode-bench/SciCode",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "scientific-computing",
        "benchmark"
      ],
      "id": 276
    },
    {
      "name": "TorchSpatial",
      "one_line_profile": "Framework and benchmark for Spatial Representation Learning",
      "detailed_description": "A comprehensive framework and benchmark suite designed to advance Spatial Representation Learning (SRL), providing tools for modeling and evaluating spatial data.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "spatial_representation_learning",
        "model_evaluation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/seai-lab/TorchSpatial",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "spatial-data",
        "representation-learning",
        "benchmark"
      ],
      "id": 277
    },
    {
      "name": "selva86/datasets",
      "one_line_profile": "Collection of standard datasets for machine learning",
      "detailed_description": "A repository hosting a variety of standard datasets commonly used for machine learning problems, facilitating easy access for experimentation and benchmarking.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "dataset_collection"
      ],
      "application_level": "dataset",
      "primary_language": "R",
      "repo_url": "https://github.com/selva86/datasets",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "machine-learning",
        "datasets",
        "statistics"
      ],
      "id": 278
    },
    {
      "name": "numpy-benchmarks",
      "one_line_profile": "Collection of scientific kernels for NumPy benchmarking",
      "detailed_description": "A set of scientific computing kernels implemented using NumPy, designed for benchmarking and performance analysis of numerical operations.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "performance_benchmarking",
        "numerical_computing"
      ],
      "application_level": "benchmark",
      "primary_language": "Python",
      "repo_url": "https://github.com/serge-sans-paille/numpy-benchmarks",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "numpy",
        "hpc",
        "benchmark"
      ],
      "id": 279
    },
    {
      "name": "gnn-benchmark",
      "one_line_profile": "Benchmarking framework for Graph Neural Networks",
      "detailed_description": "A framework for evaluating Graph Neural Network (GNN) models on semi-supervised node classification tasks, ensuring fair and reproducible comparisons.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "graph_neural_networks",
        "model_evaluation"
      ],
      "application_level": "benchmark",
      "primary_language": "Python",
      "repo_url": "https://github.com/shchur/gnn-benchmark",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gnn",
        "graph-learning",
        "benchmark"
      ],
      "id": 280
    },
    {
      "name": "ChemBench",
      "one_line_profile": "Benchmark datasets for molecular machine learning",
      "detailed_description": "A collection of benchmark datasets including MoleculeNet and MolMapNet, designed for evaluating machine learning models in chemistry and drug discovery.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "molecular_modeling",
        "dataset_collection"
      ],
      "application_level": "dataset",
      "primary_language": "HTML",
      "repo_url": "https://github.com/shenwanxiang/ChemBench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "cheminformatics",
        "moleculenet",
        "benchmark"
      ],
      "id": 281
    },
    {
      "name": "DataRec",
      "one_line_profile": "Library for reproducible data management in recommender systems",
      "detailed_description": "A Python library designed to standardize and ensure reproducibility in data management and preprocessing for recommender system research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "data_management",
        "reproducibility"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/sisinflab/DataRec",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "recsys",
        "data-processing",
        "reproducibility"
      ],
      "id": 282
    },
    {
      "name": "Elliot",
      "one_line_profile": "Framework for reproducible recommender systems evaluation",
      "detailed_description": "A comprehensive and rigorous framework for the evaluation of recommender systems, focusing on reproducibility and standardized metrics.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "recommendation_system"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/sisinflab/elliot",
      "help_website": [
        "https://elliot.readthedocs.io/en/latest/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "recsys",
        "evaluation",
        "benchmark"
      ],
      "id": 283
    },
    {
      "name": "Seamless",
      "one_line_profile": "Framework for reproducible and interactive computations",
      "detailed_description": "A framework to set up reproducible computations and visualizations that respond to changes in data or code, suitable for scientific workflows.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "workflow_management",
        "reproducibility"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/sjdv1982/seamless",
      "help_website": [
        "http://sjdv1982.github.io/seamless/"
      ],
      "license": "MIT",
      "tags": [
        "reproducibility",
        "workflow",
        "interactive-computing"
      ],
      "id": 284
    },
    {
      "name": "ZSC-Eval",
      "one_line_profile": "Evaluation toolkit for multi-agent zero-shot coordination",
      "detailed_description": "An evaluation toolkit and benchmark designed for assessing multi-agent reinforcement learning agents in zero-shot coordination scenarios.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "multi_agent_rl",
        "model_evaluation"
      ],
      "application_level": "benchmark",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/sjtu-marl/ZSC-Eval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "marl",
        "reinforcement-learning",
        "benchmark"
      ],
      "id": 285
    },
    {
      "name": "SpeechLLM",
      "one_line_profile": "Training and evaluation framework for SpeechLLM models",
      "detailed_description": "A repository containing code for training, inference, and evaluation of SpeechLLM models, facilitating research in speech-language modeling.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "speech_processing",
        "model_training",
        "evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/skit-ai/SpeechLLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "speech-recognition",
        "llm",
        "audio-processing"
      ],
      "id": 286
    },
    {
      "name": "matbench-genmetrics",
      "one_line_profile": "Metrics for generative materials benchmarking",
      "detailed_description": "A Python library providing benchmarking metrics for generative models in materials science, inspired by Guacamol and CDVAE.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "materials_science",
        "generative_models",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/sparks-baird/matbench-genmetrics",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "materials-informatics",
        "generative-ai",
        "benchmark"
      ],
      "id": 287
    },
    {
      "name": "SeBS",
      "one_line_profile": "Serverless benchmarking suite for performance analysis",
      "detailed_description": "A benchmarking suite for automatic performance analysis of FaaS platforms, useful for systems research and infrastructure evaluation.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "performance_benchmarking",
        "systems_research"
      ],
      "application_level": "benchmark",
      "primary_language": "Python",
      "repo_url": "https://github.com/spcl/serverless-benchmarks",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "serverless",
        "benchmark",
        "faas"
      ],
      "id": 288
    },
    {
      "name": "HELM",
      "one_line_profile": "Holistic Evaluation of Language Models framework",
      "detailed_description": "A framework for holistic, reproducible, and transparent evaluation of foundation models, including LLMs, covering a wide range of metrics and scenarios.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "llm_benchmarking"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/stanford-crfm/helm",
      "help_website": [
        "https://crfm.stanford.edu/helm/latest/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "evaluation",
        "benchmark"
      ],
      "id": 289
    },
    {
      "name": "SciML Bench",
      "one_line_profile": "Benchmarking suite for AI for Science",
      "detailed_description": "A benchmarking suite specifically designed for evaluating AI models and methods in scientific domains (AI for Science).",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "ai4science",
        "model_evaluation"
      ],
      "application_level": "benchmark",
      "primary_language": "Python",
      "repo_url": "https://github.com/stfc-sciml/sciml-bench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "sciml",
        "benchmark",
        "scientific-computing"
      ],
      "id": 290
    },
    {
      "name": "BIG-Bench-Hard",
      "one_line_profile": "Challenging subset of BIG-Bench tasks for LLMs",
      "detailed_description": "A collection of challenging tasks from the BIG-Bench suite, designed to evaluate the reasoning capabilities of large language models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "reasoning"
      ],
      "application_level": "benchmark",
      "primary_language": "Python",
      "repo_url": "https://github.com/suzgunmirac/BIG-Bench-Hard",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "benchmark",
        "reasoning"
      ],
      "id": 291
    },
    {
      "name": "tdoku",
      "one_line_profile": "Fast Sudoku solver and benchmark suite",
      "detailed_description": "A highly optimized Sudoku solver and generator, including a benchmark suite for comparing solver performance, relevant for constraint satisfaction research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "constraint_satisfaction",
        "solver_benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/t-dillon/tdoku",
      "help_website": [],
      "license": "BSD-2-Clause",
      "tags": [
        "sudoku",
        "optimization",
        "benchmark"
      ],
      "id": 292
    },
    {
      "name": "NPF",
      "one_line_profile": "Network Performance Framework for automated experiments",
      "detailed_description": "An experiment manager for network performance testing, providing automated execution, result collection, and graphing for reproducible research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "network_experiments",
        "performance_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/tbarbette/npf",
      "help_website": [
        "https://npf.readthedocs.io/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "networking",
        "reproducibility",
        "benchmark"
      ],
      "id": 293
    },
    {
      "name": "bigint-benchmark-rs",
      "one_line_profile": "Benchmarks for Rust big integer implementations",
      "detailed_description": "A benchmarking suite for comparing the performance of various big integer arithmetic implementations in Rust, useful for cryptography and number theory research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "numerical_computing",
        "performance_benchmarking"
      ],
      "application_level": "benchmark",
      "primary_language": "Rust",
      "repo_url": "https://github.com/tczajka/bigint-benchmark-rs",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rust",
        "bigint",
        "benchmark"
      ],
      "id": 294
    },
    {
      "name": "GPTeacher",
      "one_line_profile": "Modular datasets generated by GPT-4 for instruction tuning",
      "detailed_description": "A collection of datasets generated by GPT-4, including General-Instruct, Roleplay-Instruct, Code-Instruct, and Toolformer, for training and evaluating models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "dataset_collection",
        "instruction_tuning"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/teknium1/GPTeacher",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "dataset",
        "synthetic-data"
      ],
      "id": 295
    },
    {
      "name": "TensorFlow Datasets",
      "one_line_profile": "Collection of ready-to-use datasets for TensorFlow",
      "detailed_description": "A library of datasets ready to use with TensorFlow, JAX, and other machine learning frameworks, handling downloading and preparing data.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "dataset_access",
        "data_loading"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/tensorflow/datasets",
      "help_website": [
        "https://www.tensorflow.org/datasets"
      ],
      "license": "Apache-2.0",
      "tags": [
        "tensorflow",
        "datasets",
        "machine-learning"
      ],
      "id": 296
    },
    {
      "name": "Safety-Prompts",
      "one_line_profile": "Chinese safety prompts dataset for evaluating LLM safety",
      "detailed_description": "A collection of Chinese safety prompts designed to evaluate and improve the safety of Large Language Models (LLMs), covering various safety categories.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "safety_evaluation",
        "benchmark_dataset"
      ],
      "application_level": "dataset",
      "primary_language": "No Code",
      "repo_url": "https://github.com/thu-coai/Safety-Prompts",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-safety",
        "benchmark",
        "prompts"
      ],
      "id": 297
    },
    {
      "name": "TransformerLab",
      "one_line_profile": "Desktop application for LLM engineering and evaluation",
      "detailed_description": "An open-source application designed for advanced Large Language Model (LLM) engineering, enabling users to interact with, train, fine-tune, and evaluate models locally.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "fine_tuning",
        "training_platform"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/transformerlab/transformerlab-app",
      "help_website": [
        "https://transformerlab.ai"
      ],
      "license": "AGPL-3.0",
      "tags": [
        "llm",
        "evaluation",
        "fine-tuning",
        "desktop-app"
      ],
      "id": 298
    },
    {
      "name": "uci_datasets",
      "one_line_profile": "Python library for loading standardized UCI regression datasets",
      "detailed_description": "A Python package that provides easy access to regression datasets from the UCI Machine Learning Repository with standardized train-test splits for benchmarking.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "dataset_loading",
        "benchmark_data"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/treforevans/uci_datasets",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "uci",
        "datasets",
        "regression",
        "benchmark"
      ],
      "id": 299
    },
    {
      "name": "TruLens",
      "one_line_profile": "Evaluation and tracking library for LLM experiments",
      "detailed_description": "A library for evaluating and tracking Large Language Model (LLM) experiments and AI agents, providing feedback functions to measure performance and quality.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "experiment_tracking",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/truera/trulens",
      "help_website": [
        "https://www.trulens.org"
      ],
      "license": "MIT",
      "tags": [
        "llm-evaluation",
        "observability",
        "metrics"
      ],
      "id": 300
    },
    {
      "name": "ApeBench",
      "one_line_profile": "Benchmark suite for autoregressive neural emulation of PDEs",
      "detailed_description": "A comprehensive benchmark suite for evaluating autoregressive neural emulators for Partial Differential Equations (PDEs), covering 1D, 2D, and 3D problems with differentiable physics metrics.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "pde_solving",
        "neural_operator_benchmark",
        "scientific_machine_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/tum-pbs/apebench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pde",
        "benchmark",
        "neural-operators",
        "physics-ml"
      ],
      "id": 301
    },
    {
      "name": "Petastorm",
      "one_line_profile": "Library for deep learning data access from Apache Parquet",
      "detailed_description": "A library enabling single-machine or distributed training and evaluation of deep learning models directly from datasets in Apache Parquet format, supporting TensorFlow and PyTorch.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "data_loading",
        "training_infrastructure"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/uber/petastorm",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "parquet",
        "data-loading",
        "deep-learning"
      ],
      "id": 302
    },
    {
      "name": "RADIal",
      "one_line_profile": "High-definition radar dataset for multi-task learning",
      "detailed_description": "A raw high-definition radar dataset designed for multi-task learning in autonomous driving, including object detection and segmentation tasks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "autonomous_driving_benchmark",
        "radar_perception"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/valeoai/RADIal",
      "help_website": [],
      "license": "No Assertion",
      "tags": [
        "radar",
        "dataset",
        "autonomous-driving"
      ],
      "id": 303
    },
    {
      "name": "Whisper Finetune",
      "one_line_profile": "Tools for fine-tuning and evaluating Whisper ASR models",
      "detailed_description": "A collection of scripts and tools to fine-tune and evaluate OpenAI's Whisper models for Automatic Speech Recognition (ASR) on custom or Hugging Face datasets.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_finetuning",
        "asr_evaluation"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/vasistalodagala/whisper-finetune",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "whisper",
        "asr",
        "fine-tuning"
      ],
      "id": 304
    },
    {
      "name": "SHOC",
      "one_line_profile": "Scalable Heterogeneous Computing Benchmark Suite",
      "detailed_description": "A benchmark suite designed to test the performance and stability of scalable heterogeneous computing systems, including GPUs and multi-core processors, for scientific workloads.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "hpc_benchmark",
        "system_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Makefile",
      "repo_url": "https://github.com/vetter/shoc",
      "help_website": [],
      "license": "No Assertion",
      "tags": [
        "hpc",
        "benchmark",
        "gpu",
        "opencl",
        "cuda"
      ],
      "id": 305
    },
    {
      "name": "Ragas",
      "one_line_profile": "Evaluation framework for RAG pipelines",
      "detailed_description": "A framework for evaluating Retrieval Augmented Generation (RAG) pipelines, providing metrics to assess the faithfulness, answer relevance, and context retrieval quality of LLM applications.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "rag_evaluation",
        "llm_metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/vibrantlabsai/ragas",
      "help_website": [
        "https://docs.ragas.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "evaluation",
        "llm",
        "metrics"
      ],
      "id": 306
    },
    {
      "name": "Rdatasets",
      "one_line_profile": "Collection of standard datasets from R packages",
      "detailed_description": "A comprehensive collection of datasets originally distributed in R packages, made available as CSV files for use in other data analysis environments and benchmarks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "dataset_collection",
        "benchmark_data"
      ],
      "application_level": "dataset",
      "primary_language": "HTML",
      "repo_url": "https://github.com/vincentarelbundock/Rdatasets",
      "help_website": [
        "https://vincentarelbundock.github.io/Rdatasets"
      ],
      "license": "No Assertion",
      "tags": [
        "datasets",
        "r",
        "csv",
        "benchmark"
      ],
      "id": 307
    },
    {
      "name": "WfCommons",
      "one_line_profile": "Framework for scientific workflow research and trace generation",
      "detailed_description": "A framework for enabling research and development in scientific workflows, providing tools to generate synthetic workflow traces and analyze workflow execution logs.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "workflow_research",
        "trace_generation",
        "simulation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/wfcommons/WfCommons",
      "help_website": [
        "https://wfcommons.org"
      ],
      "license": "LGPL-3.0",
      "tags": [
        "scientific-workflows",
        "simulation",
        "benchmark"
      ],
      "id": 308
    },
    {
      "name": "PhaseLLM",
      "one_line_profile": "Framework for LLM evaluation and workflow management",
      "detailed_description": "A framework designed to streamline the evaluation and workflow management of Large Language Models (LLMs), helping developers build more robust AI applications.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "workflow_management"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/wgryc/phasellm",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "evaluation",
        "workflow"
      ],
      "id": 309
    },
    {
      "name": "whylogs",
      "one_line_profile": "Data logging and quality monitoring library for ML",
      "detailed_description": "An open-source library for logging data profiles, enabling data quality monitoring, model performance tracking, and drift detection in machine learning pipelines.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "data_quality",
        "model_monitoring",
        "logging"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/whylabs/whylogs",
      "help_website": [
        "https://whylabs.ai/whylogs"
      ],
      "license": "Apache-2.0",
      "tags": [
        "data-quality",
        "mlops",
        "monitoring"
      ],
      "id": 310
    },
    {
      "name": "ControlGym",
      "one_line_profile": "Benchmark environments for reinforcement learning in control",
      "detailed_description": "A large-scale benchmark suite of control environments for evaluating Reinforcement Learning algorithms, focusing on control theory and dynamic systems.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "rl_benchmark",
        "control_systems"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/xiangyuan-zhang/controlgym",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "reinforcement-learning",
        "control-theory",
        "benchmark"
      ],
      "id": 311
    },
    {
      "name": "Multiphysics-Bench",
      "one_line_profile": "Benchmark for scientific machine learning on multiphysics PDEs",
      "detailed_description": "A benchmark suite for investigating and evaluating Scientific Machine Learning (SciML) methods for solving multiphysics Partial Differential Equations (PDEs).",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "pde_benchmark",
        "sciml",
        "multiphysics"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/xie-lab-ml/multiphysics-bench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "sciml",
        "pde",
        "benchmark",
        "multiphysics"
      ],
      "id": 312
    },
    {
      "name": "GAN-Metrics",
      "one_line_profile": "Collection of evaluation metrics for GAN models",
      "detailed_description": "A Python library implementing various metrics for evaluating the performance and quality of Generative Adversarial Networks (GANs).",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "gan_evaluation",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/yhlleo/GAN-Metrics",
      "help_website": [],
      "license": "No Assertion",
      "tags": [
        "gan",
        "metrics",
        "evaluation"
      ],
      "id": 313
    },
    {
      "name": "RaLLe",
      "one_line_profile": "Framework for developing and evaluating Retrieval-Augmented Large Language Models (RAG)",
      "detailed_description": "RaLLe is a framework designed to facilitate the development and evaluation of Retrieval-Augmented Generation (RAG) systems for Large Language Models. It provides tools for assessing the performance of RAG pipelines, ensuring the reliability and accuracy of retrieved information in AI-driven workflows.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "rag_framework"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/yhoshi3/RaLLe",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "llm",
        "evaluation",
        "benchmark"
      ],
      "id": 314
    },
    {
      "name": "MGBench",
      "one_line_profile": "Benchmark suite for molecular glue ternary structure prediction methods",
      "detailed_description": "MGBench is a benchmarking tool specifically designed for evaluating cofolding methods used in molecular glue ternary structure prediction. It provides a standardized environment and datasets to assess the accuracy and performance of computational models in drug discovery and structural biology contexts.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "structure_prediction",
        "molecular_modeling"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/yiyanliao/MGBench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "molecular-glue",
        "protein-folding",
        "drug-discovery",
        "benchmark"
      ],
      "id": 315
    },
    {
      "name": "torchdistill",
      "one_line_profile": "Reproducible deep learning framework for knowledge distillation",
      "detailed_description": "torchdistill is a coding-free framework built on PyTorch designed to facilitate reproducible deep learning studies, specifically focusing on knowledge distillation. It offers implementations of numerous state-of-the-art distillation methods and provides configurations to ensure consistent benchmarking and experimental reproduction.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_optimization",
        "reproducibility"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/yoshitomo-matsubara/torchdistill",
      "help_website": [
        "https://torchdistill.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "knowledge-distillation",
        "pytorch",
        "reproducibility",
        "deep-learning"
      ],
      "id": 316
    },
    {
      "name": "EasyLM",
      "one_line_profile": "One-stop solution for pre-training, finetuning, and serving LLMs using JAX/Flax",
      "detailed_description": "EasyLM is a comprehensive library for Large Language Models (LLMs) built on JAX and Flax. It simplifies the process of pre-training, fine-tuning, evaluating, and serving LLMs, providing a scalable and efficient toolchain for AI model development.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_training",
        "inference"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/young-geng/EasyLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "jax",
        "flax",
        "model-serving"
      ],
      "id": 317
    },
    {
      "name": "MT-Consistency",
      "one_line_profile": "Benchmark for evaluating acquiescence bias and consistency in LLMs",
      "detailed_description": "MT-Consistency is a research framework and benchmark designed to investigate and mitigate acquiescence bias in Large Language Models during sequential QA interactions. It includes datasets and evaluation scripts to assess the robustness and conversational consistency of AI models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "bias_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/yubol-bobo/MT-Consistency",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "bias",
        "consistency",
        "benchmark"
      ],
      "id": 318
    },
    {
      "name": "fret",
      "one_line_profile": "Framework for Reproducible ExperimenTs in scientific computing",
      "detailed_description": "fret is a lightweight framework designed to simplify the management and reproduction of scientific experiments. It provides tools to configure, run, and track experiments, ensuring that research results are reproducible and manageable.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "experiment_management",
        "reproducibility"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/yxonic/fret",
      "help_website": [
        "https://fret.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "reproducibility",
        "experiment-tracking",
        "workflow"
      ],
      "id": 319
    },
    {
      "name": "MedSegBench",
      "one_line_profile": "Standardized benchmark collection for medical image segmentation",
      "detailed_description": "MedSegBench is a comprehensive benchmarking library that provides access to standardized medical segmentation datasets across various modalities. It facilitates the evaluation and comparison of medical image segmentation models by offering a unified interface for data loading and processing.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "image_segmentation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/zekikus/MedSegBench",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "medical-imaging",
        "segmentation",
        "benchmark",
        "datasets"
      ],
      "id": 320
    }
  ]
}
{
  "generated_at": "2025-12-16T10:43:48.786476+08:00",
  "metadata": {
    "leaf_cluster": {
      "leaf_cluster_id": "AI4",
      "leaf_cluster_name": "科研评测-基准/指标/复现实验生态",
      "domain": "AI Toolchain",
      "typical_objects": "datasets/tasks",
      "task_chain": "任务→指标→自动评测→leaderboard→回归",
      "tool_form": "benchmark + eval harness"
    },
    "unit": {
      "unit_id": "AI4-03",
      "unit_name": "自动评测 harness（可复现）",
      "target_scale": "200–450",
      "coverage_tools": "eval runners、CI regression"
    },
    "search": {
      "target_candidates": 450,
      "queries": [
        "[GH] truelens",
        "[GH] lighteval",
        "[GH] ragas",
        "[GH] deepeval",
        "[GH] promptfoo",
        "[GH] big-bench",
        "[GH] huggingface evaluate",
        "[GH] helm",
        "[GH] lm-evaluation-harness",
        "[GH] evaluation harness",
        "[GH] benchmark runner",
        "[GH] model evaluation framework",
        "[GH] llm evaluation",
        "[GH] automated metrics",
        "[GH] reproducibility wrapper",
        "[GH] continuous evaluation",
        "[GH] regression testing ml",
        "[GH] leaderboard backend",
        "[GH] performance assessment",
        "[GH] eval suite",
        "[GH] model validator",
        "[GH] metric calculation",
        "[WEB] llm evaluation harness github",
        "[WEB] automated model benchmark runner github",
        "[WEB] ml continuous integration evaluation github",
        "[WEB] reproducible ai experiments framework github",
        "[WEB] deep learning regression testing tools github"
      ],
      "total_candidates": 1241,
      "tool_candidates": 703,
      "final_tools": 248
    }
  },
  "tools": [
    {
      "name": "GraphRAG Agent Framework",
      "one_line_profile": "Integrated framework for GraphRAG construction, search, and custom evaluation",
      "detailed_description": "A comprehensive framework integrating GraphRAG, LightRAG, and Neo4j for knowledge graph construction and search, featuring a custom evaluation framework for assessing GraphRAG performance.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "evaluation",
        "rag_framework"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/1517005260/graph-rag-agent",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "graph-rag",
        "evaluation-framework"
      ],
      "id": 1
    },
    {
      "name": "ATLAS",
      "one_line_profile": "Test harness for evaluating LLM RAG in Humanities & Social Science",
      "detailed_description": "A test harness designed for the evaluation of Large Language Model (LLM) Retrieval Augmented Generation (RAG) specifically for Humanities & Social Science (HASS) research contexts.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "evaluation",
        "rag_testing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/AI-as-Infrastructure/aiinfra-atlas",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "rag",
        "evaluation",
        "humanities"
      ],
      "id": 2
    },
    {
      "name": "lm-evaluation",
      "one_line_profile": "Evaluation suite for large-scale language models",
      "detailed_description": "A suite of tools and metrics for evaluating the performance of large-scale language models, developed by AI21 Labs.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "nlp_metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/AI21Labs/lm-evaluation",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "evaluation",
        "nlp"
      ],
      "id": 3
    },
    {
      "name": "AutoQuant",
      "one_line_profile": "Automation framework for ML, forecasting, and model evaluation",
      "detailed_description": "An R framework for automating machine learning tasks, time series forecasting, model evaluation, and interpretation.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "automl",
        "forecasting",
        "model_evaluation"
      ],
      "application_level": "framework",
      "primary_language": "R",
      "repo_url": "https://github.com/AdrianAntico/AutoQuant",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "automl",
        "forecasting",
        "evaluation"
      ],
      "id": 4
    },
    {
      "name": "Agenta",
      "one_line_profile": "Open-source LLMOps platform for prompt management and evaluation",
      "detailed_description": "A platform for LLM operations that includes features for prompt playground, management, LLM evaluation, and observability.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "llmops",
        "model_evaluation",
        "prompt_engineering"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/Agenta-AI/agenta",
      "help_website": [
        "https://agenta.ai"
      ],
      "license": "NOASSERTION",
      "tags": [
        "llmops",
        "evaluation",
        "observability"
      ],
      "id": 5
    },
    {
      "name": "ThinkingAgent",
      "one_line_profile": "Evaluation framework for rating overthinking behavior in LLMs",
      "detailed_description": "A systematic evaluation framework designed to automatically rate and analyze overthinking behaviors in large language models.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "behavioral_evaluation",
        "llm_analysis"
      ],
      "application_level": "framework",
      "primary_language": "Shell",
      "repo_url": "https://github.com/AlexCuadron/ThinkingAgent",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "evaluation",
        "behavior-analysis"
      ],
      "id": 6
    },
    {
      "name": "Bjontegaard Metric",
      "one_line_profile": "Calculation tool for Bjontegaard metric (BD-PSNR and BD-rate)",
      "detailed_description": "A Python implementation for calculating the Bjontegaard metric, including BD-PSNR and BD-rate, commonly used for evaluating video compression performance.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "metric_calculation",
        "video_compression_eval"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Anserw/Bjontegaard_metric",
      "help_website": [],
      "license": null,
      "tags": [
        "video-coding",
        "metrics",
        "bd-rate"
      ],
      "id": 7
    },
    {
      "name": "cell-eval",
      "one_line_profile": "Evaluation suite for perturbation prediction models",
      "detailed_description": "A comprehensive suite for evaluating models designed to predict cellular perturbations, developed by Arc Institute.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "bioinformatics_eval",
        "perturbation_prediction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ArcInstitute/cell-eval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "biology",
        "perturbation",
        "evaluation"
      ],
      "id": 8
    },
    {
      "name": "BigDataBench MicroBenchmark",
      "one_line_profile": "Micro-benchmark suite for Big Data systems",
      "detailed_description": "A micro-benchmark suite from BigDataBench V5.0 for evaluating the performance of big data systems and components.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "benchmarking",
        "system_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Java",
      "repo_url": "https://github.com/BenchCouncil/BigDataBench_V5.0_BigData_MicroBenchmark",
      "help_website": [
        "http://www.benchcouncil.org/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "big-data",
        "benchmark",
        "microbenchmark"
      ],
      "id": 9
    },
    {
      "name": "BigVectorBench",
      "one_line_profile": "Benchmark for vector database embedding performance",
      "detailed_description": "A benchmark tool for evaluating the embedding performance of vector databases with heterogeneous data and compound queries.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "benchmarking",
        "vector_database_eval"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/BenchCouncil/BigVectorBench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "vector-database",
        "benchmark",
        "embeddings"
      ],
      "id": 10
    },
    {
      "name": "ko-lm-evaluation-harness",
      "one_line_profile": "Evaluation harness for Korean language models",
      "detailed_description": "A framework for evaluating Korean language models, forked and adapted from EleutherAI's lm-evaluation-harness.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "nlp_korean"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/Beomi/ko-lm-evaluation-harness",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nlp",
        "korean",
        "evaluation"
      ],
      "id": 11
    },
    {
      "name": "Ethereum Economic Model",
      "one_line_profile": "Dynamical-systems model of Ethereum validator economics",
      "detailed_description": "A modular dynamical-systems model for simulating and analyzing the economics of Ethereum validators.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "economic_simulation",
        "system_modeling"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/CADLabs/ethereum-economic-model",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "ethereum",
        "economics",
        "simulation"
      ],
      "id": 12
    },
    {
      "name": "Gadget",
      "one_line_profile": "Benchmark harness for streaming state stores",
      "detailed_description": "A benchmark harness designed for the systematic and robust evaluation of streaming state stores in data systems.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "benchmarking",
        "streaming_systems"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/CASP-Systems-BU/Gadget",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "streaming",
        "benchmark",
        "state-store"
      ],
      "id": 13
    },
    {
      "name": "CS-Eval",
      "one_line_profile": "Evaluation suite for cybersecurity models",
      "detailed_description": "A comprehensive evaluation suite for assessing the cybersecurity capabilities of fundamental models or large language models.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "cybersecurity_eval"
      ],
      "application_level": "framework",
      "primary_language": null,
      "repo_url": "https://github.com/CS-EVAL/CS-Eval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "cybersecurity",
        "llm",
        "evaluation"
      ],
      "id": 14
    },
    {
      "name": "Matcha (VariationAnalysis)",
      "one_line_profile": "Framework for training and evaluating genomic variation models",
      "detailed_description": "The Matcha framework, part of the VariationAnalysis project, used to train and evaluate deep learning models for calling genomic variations.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "genomics_eval",
        "model_training"
      ],
      "application_level": "framework",
      "primary_language": "Java",
      "repo_url": "https://github.com/CampagneLaboratory/variationanalysis",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "genomics",
        "deep-learning",
        "variation-calling"
      ],
      "id": 15
    },
    {
      "name": "SURE",
      "one_line_profile": "Library for assessing synthetic tabular data utility and privacy",
      "detailed_description": "An open-source Python library for evaluating the utility and privacy performance of tabular synthetic datasets.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "data_evaluation",
        "synthetic_data"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Clearbox-AI/SURE",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "synthetic-data",
        "evaluation",
        "privacy"
      ],
      "id": 16
    },
    {
      "name": "AI Agents Reality Check",
      "one_line_profile": "Mathematical benchmark for AI agent performance",
      "detailed_description": "A benchmark suite that exposes the performance gap between real agents and LLM wrappers through rigorous multi-dimensional evaluation and statistical validation.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "agent_evaluation",
        "benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Cre4T3Tiv3/ai-agents-reality-check",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ai-agents",
        "benchmark",
        "evaluation"
      ],
      "id": 17
    },
    {
      "name": "DMind Benchmark",
      "one_line_profile": "Benchmark for LLMs on blockchain and Web3 knowledge",
      "detailed_description": "A comprehensive framework for evaluating large language models on their knowledge of blockchain, cryptocurrency, and Web3 domains.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "domain_evaluation",
        "llm_benchmark"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/DMindAI/DMind-Benchmark",
      "help_website": [],
      "license": null,
      "tags": [
        "blockchain",
        "llm",
        "benchmark"
      ],
      "id": 18
    },
    {
      "name": "Benchmarking Big Streams Systems",
      "one_line_profile": "Benchmark suite for big streaming systems",
      "detailed_description": "An extension of Yahoo!'s benchmarking suite designed for evaluating the performance of big streaming data systems.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "benchmarking",
        "streaming_systems"
      ],
      "application_level": "solver",
      "primary_language": "Java",
      "repo_url": "https://github.com/DataSystemsGroupUT/Benchmarking-Big-Streams-Systems",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "streaming",
        "benchmark",
        "big-data"
      ],
      "id": 19
    },
    {
      "name": "DeepSpark",
      "one_line_profile": "Platform for evaluating industrial deep learning models",
      "detailed_description": "An open platform that provides a multi-dimensional evaluation system for deep learning algorithms and models coupled with industrial applications.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "industrial_ai"
      ],
      "application_level": "platform",
      "primary_language": null,
      "repo_url": "https://github.com/Deep-Spark/DeepSpark",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "deep-learning",
        "evaluation",
        "industrial"
      ],
      "id": 20
    },
    {
      "name": "DreamLayer",
      "one_line_profile": "Benchmarking and evaluation automation tool for diffusion models",
      "detailed_description": "A tool designed to benchmark diffusion models faster by automating evaluations, seed management, and metric calculation to ensure reproducible results.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking",
        "reproducibility"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/DreamLayer-AI/DreamLayer",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "diffusion-models",
        "benchmarking",
        "evaluation-harness"
      ],
      "id": 21
    },
    {
      "name": "E3SM",
      "one_line_profile": "Energy Exascale Earth System Model",
      "detailed_description": "The Energy Exascale Earth System Model (E3SM) is a state-of-the-art earth system modeling project designed to simulate the earth's climate system at high resolution.",
      "domains": [
        "AI4",
        "Earth Science"
      ],
      "subtask_category": [
        "climate_modeling",
        "simulation"
      ],
      "application_level": "solver",
      "primary_language": "Fortran",
      "repo_url": "https://github.com/E3SM-Project/E3SM",
      "help_website": [
        "https://e3sm.org/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "earth-system-model",
        "climate-simulation",
        "exascale"
      ],
      "id": 22
    },
    {
      "name": "lm-evaluation-harness",
      "one_line_profile": "Few-shot evaluation framework for language models",
      "detailed_description": "A framework for few-shot evaluation of language models, providing a unified interface to benchmark models on a wide variety of tasks.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/EleutherAI/lm-evaluation-harness",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "evaluation",
        "few-shot",
        "nlp"
      ],
      "id": 23
    },
    {
      "name": "JamAIBase",
      "one_line_profile": "Collaborative spreadsheet interface for AI pipeline evaluation",
      "detailed_description": "A platform that combines spreadsheet-like interaction with AI pipelines, allowing users to experiment with prompts and models, and evaluate LLM responses in real-time.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "prompt_engineering",
        "experiment_tracking"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/EmbeddedLLM/JamAIBase",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-ops",
        "evaluation",
        "spreadsheet-ui"
      ],
      "id": 24
    },
    {
      "name": "fdsvismap",
      "one_line_profile": "Visibility verification tool for fire safety assessment",
      "detailed_description": "A tool for waypoint-based verification of visibility within the scope of performance-based fire safety assessment, likely interfacing with Fire Dynamics Simulator (FDS) data.",
      "domains": [
        "AI4",
        "Physics"
      ],
      "subtask_category": [
        "safety_assessment",
        "simulation_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/FireDynamics/fdsvismap",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fire-dynamics",
        "safety-engineering",
        "visibility-analysis"
      ],
      "id": 25
    },
    {
      "name": "llm-benchmarker-suite",
      "one_line_profile": "Leaderboard and benchmarking suite for LLM evaluations",
      "detailed_description": "A suite designed for benchmarking Large Language Models, providing tools to run evaluations and generate leaderboard rankings.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/FormulaMonks/llm-benchmarker-suite",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "leaderboard",
        "benchmarking"
      ],
      "id": 26
    },
    {
      "name": "LLMZoo",
      "one_line_profile": "Data, models, and evaluation benchmarks for LLMs",
      "detailed_description": "A project providing a collection of data, models, and evaluation benchmarks specifically for Large Language Models, facilitating comparative analysis.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "dataset_management"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/FreedomIntelligence/LLMZoo",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "benchmark",
        "model-zoo"
      ],
      "id": 27
    },
    {
      "name": "MAYE",
      "one_line_profile": "Evaluation scheme for Vision Language Models",
      "detailed_description": "A framework and comprehensive evaluation scheme for Vision Language Models (VLMs), focusing on rethinking RL scaling and transparency.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "vlm_benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/GAIR-NLP/MAYE",
      "help_website": [],
      "license": null,
      "tags": [
        "vlm",
        "reinforcement-learning",
        "evaluation"
      ],
      "id": 28
    },
    {
      "name": "MultimodalHugs",
      "one_line_profile": "Framework for training and evaluating multimodal AI models",
      "detailed_description": "An extension of Hugging Face that offers a generalized framework for training, evaluating, and using multimodal AI models with minimal code differences.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_training",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/GerrySant/multimodalhugs",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "multimodal",
        "huggingface",
        "training-framework"
      ],
      "id": 29
    },
    {
      "name": "Giskard",
      "one_line_profile": "Evaluation and testing library for LLM Agents",
      "detailed_description": "An open-source testing and evaluation library for Large Language Models and ML models, focusing on detecting hallucinations, biases, and performance issues.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_testing",
        "quality_assurance",
        "bias_detection"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Giskard-AI/giskard-oss",
      "help_website": [
        "https://docs.giskard.ai/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm-testing",
        "quality-control",
        "mlops"
      ],
      "id": 30
    },
    {
      "name": "Helicone",
      "one_line_profile": "LLM observability and evaluation platform",
      "detailed_description": "An open-source observability platform for Large Language Models that allows developers to monitor, evaluate, and experiment with model outputs and performance.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "observability",
        "model_evaluation",
        "monitoring"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/Helicone/helicone",
      "help_website": [
        "https://www.helicone.ai/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm-ops",
        "observability",
        "evaluation"
      ],
      "id": 31
    },
    {
      "name": "gee_landcover_metrics",
      "one_line_profile": "Land cover metrics calculation for Google Earth Engine",
      "detailed_description": "A tool for calculating landscape metrics on land cover data within the Google Earth Engine platform, supporting spatial analysis in earth sciences.",
      "domains": [
        "AI4",
        "Earth Science"
      ],
      "subtask_category": [
        "spatial_analysis",
        "metrics_calculation"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/Helmholtz-UFZ/gee_landcover_metrics",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "google-earth-engine",
        "land-cover",
        "landscape-metrics"
      ],
      "id": 32
    },
    {
      "name": "SDE-Harness",
      "one_line_profile": "Scientific Discovery Evaluation Framework",
      "detailed_description": "A framework designed to evaluate agents or models on scientific discovery tasks, assessing their capability in hypothesis generation and experimental design.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "agent_evaluation",
        "scientific_discovery"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/HowieHwong/sde-harness",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "scientific-discovery",
        "agent-evaluation",
        "benchmark"
      ],
      "id": 33
    },
    {
      "name": "UHGEval",
      "one_line_profile": "Evaluation framework for hallucination in LLMs",
      "detailed_description": "A user-friendly evaluation framework and benchmark suite (including HaluEval, HalluQA) designed to assess hallucination generation in Large Language Models.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "hallucination_detection",
        "model_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/IAAR-Shanghai/UHGEval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "hallucination",
        "llm-evaluation",
        "benchmark"
      ],
      "id": 34
    },
    {
      "name": "GraFiTe",
      "one_line_profile": "Platform for tracking domain-specific model issues",
      "detailed_description": "A platform to track and manage domain-specific model issues for continuous evaluation of Large Language Models.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "issue_tracking",
        "continuous_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/IBM/grafite",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-evaluation",
        "model-governance",
        "issue-tracking"
      ],
      "id": 35
    },
    {
      "name": "FakeFinder",
      "one_line_profile": "Framework for evaluating deepfake detection models",
      "detailed_description": "A modular framework for evaluating various deepfake detection models, providing tools to assess model performance against manipulated media.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "deepfake_detection",
        "model_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/IQTLabs/FakeFinder",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "deepfake",
        "evaluation-framework",
        "media-forensics"
      ],
      "id": 36
    },
    {
      "name": "CGM_Performance_Assessment",
      "one_line_profile": "Statistical performance assessment for CGM systems",
      "detailed_description": "A collection of software packages for the statistical performance assessment of Continuous Glucose Monitoring (CGM) systems, used in medical data analysis.",
      "domains": [
        "AI4",
        "Life Science"
      ],
      "subtask_category": [
        "statistical_analysis",
        "performance_assessment"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/IfDTUlm/CGM_Performance_Assessment",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "cgm",
        "diabetes-technology",
        "statistical-analysis"
      ],
      "id": 37
    },
    {
      "name": "Deepmark",
      "one_line_profile": "Testing environment for LLM assessment",
      "detailed_description": "A testing environment enabling the assessment of language models (LLMs) on task-specific metrics and custom data for predictable performance.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "performance_testing"
      ],
      "application_level": "platform",
      "primary_language": "PHP",
      "repo_url": "https://github.com/IngestAI/deepmark",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "llm-testing",
        "metrics",
        "evaluation"
      ],
      "id": 38
    },
    {
      "name": "Factorio Learning Environment",
      "one_line_profile": "Environment for evaluating LLMs in Factorio",
      "detailed_description": "A non-saturating, open-ended environment designed for evaluating Large Language Models and Reinforcement Learning agents within the Factorio game simulation.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "rl_environment",
        "agent_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/JackHopkins/factorio-learning-environment",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "reinforcement-learning",
        "factorio",
        "benchmark-environment"
      ],
      "id": 39
    },
    {
      "name": "ModelMetrics",
      "one_line_profile": "R package for rapid calculation of model metrics",
      "detailed_description": "An R library designed to facilitate the rapid calculation of various statistical model performance metrics.",
      "domains": [
        "AI4",
        "Statistics"
      ],
      "subtask_category": [
        "statistical_analysis",
        "metrics_calculation"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/JackStat/ModelMetrics",
      "help_website": [],
      "license": null,
      "tags": [
        "r-package",
        "statistics",
        "model-evaluation"
      ],
      "id": 40
    },
    {
      "name": "DetectionMetrics",
      "one_line_profile": "Evaluation tool for perception models",
      "detailed_description": "A tool designed to unify and streamline the evaluation of perception (object detection) models across different frameworks and datasets.",
      "domains": [
        "AI4",
        "Computer Vision"
      ],
      "subtask_category": [
        "object_detection",
        "model_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/JdeRobot/DetectionMetrics",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "computer-vision",
        "object-detection",
        "metrics"
      ],
      "id": 41
    },
    {
      "name": "MixEval",
      "one_line_profile": "Evaluation suite for MixEval benchmark",
      "detailed_description": "The official evaluation suite and dynamic data release for the MixEval benchmark, designed for assessing language model performance.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/JinjieNi/MixEval",
      "help_website": [],
      "license": null,
      "tags": [
        "llm",
        "benchmark",
        "evaluation-suite"
      ],
      "id": 42
    },
    {
      "name": "LLM-Writing-Assessment-Psychometric-Framework",
      "one_line_profile": "Psychometric framework for evaluating LLMs as raters",
      "detailed_description": "A repository and framework for evaluating large language models when used as raters in large-scale writing assessments, focusing on reliability and validity from a psychometric perspective.",
      "domains": [
        "AI4",
        "Social Science"
      ],
      "subtask_category": [
        "psychometrics",
        "model_evaluation",
        "reliability_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/John-Wang-0809/LLM-Writing-Assessment-Psychometric-Framework",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "psychometrics",
        "llm-evaluation",
        "automated-scoring"
      ],
      "id": 43
    },
    {
      "name": "multihop-edit-eval",
      "one_line_profile": "Fine-grained evaluation framework for multi-hop knowledge editing in LLMs",
      "detailed_description": "A specialized evaluation framework designed to assess the performance of Large Language Models in multi-hop knowledge editing tasks. It provides metrics and datasets to measure how well models can update their knowledge base across connected facts.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "knowledge_editing_assessment"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/KUNLP/multihop-edit-eval",
      "help_website": [],
      "license": null,
      "tags": [
        "llm-evaluation",
        "knowledge-editing",
        "multi-hop-reasoning"
      ],
      "id": 44
    },
    {
      "name": "q-evaluation-harness",
      "one_line_profile": "Evaluation framework for LLMs on Q/kdb+ code generation tasks",
      "detailed_description": "An open-source framework developed by KX Systems to evaluate the performance of Large Language Models specifically on generating Q/kdb+ code. It serves as a domain-specific benchmark harness for financial time-series database languages.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "code_generation_evaluation",
        "llm_benchmark"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/KxSystems/q-evaluation-harness",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "q",
        "kdb+",
        "llm-evaluation",
        "code-generation"
      ],
      "id": 45
    },
    {
      "name": "aac-metrics",
      "one_line_profile": "Metrics library for evaluating Automated Audio Captioning systems",
      "detailed_description": "A Python library designed for PyTorch that implements various metrics for evaluating Automated Audio Captioning (AAC) systems. It provides a standardized way to measure the quality of generated audio captions against reference annotations.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "audio_captioning_evaluation",
        "metrics_calculation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Labbeti/aac-metrics",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "audio-captioning",
        "evaluation-metrics",
        "pytorch"
      ],
      "id": 46
    },
    {
      "name": "eurybia",
      "one_line_profile": "Model drift monitoring and data validation library",
      "detailed_description": "A Python library designed to monitor machine learning model drift over time and secure model deployment through rigorous data validation. It helps in maintaining the reliability of AI systems in production environments.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_monitoring",
        "drift_detection",
        "data_validation"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/MAIF/eurybia",
      "help_website": [
        "https://eurybia.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "model-drift",
        "data-validation",
        "mlops"
      ],
      "id": 47
    },
    {
      "name": "VideoEval",
      "one_line_profile": "Benchmark suite for evaluation of Video Foundation Models",
      "detailed_description": "A comprehensive benchmark suite designed for the low-cost and efficient evaluation of Video Foundation Models. It provides a set of tasks and metrics to assess the capabilities of video understanding and generation models.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "video_model_evaluation",
        "benchmark_suite"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/MCG-NJU/VideoEval",
      "help_website": [],
      "license": null,
      "tags": [
        "video-understanding",
        "foundation-models",
        "benchmark"
      ],
      "id": 48
    },
    {
      "name": "MHKiT-MATLAB",
      "one_line_profile": "Marine and Hydrokinetic Toolkit (MATLAB)",
      "detailed_description": "A MATLAB toolkit providing standardized data processing, visualization, quality control, and resource assessment tools for the marine renewable energy community. It supports wave, tidal, and river energy research.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "data_processing",
        "resource_assessment",
        "quality_control"
      ],
      "application_level": "library",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/MHKiT-Software/MHKiT-MATLAB",
      "help_website": [
        "https://mhkit-software.github.io/MHKiT/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "marine-energy",
        "hydrokinetic",
        "data-analysis"
      ],
      "id": 49
    },
    {
      "name": "MHKiT-Python",
      "one_line_profile": "Marine and Hydrokinetic Toolkit (Python)",
      "detailed_description": "A Python toolkit providing standardized data processing, visualization, quality control, and resource assessment tools for the marine renewable energy community. It serves as the Python counterpart to MHKiT-MATLAB.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "data_processing",
        "resource_assessment",
        "quality_control"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/MHKiT-Software/MHKiT-Python",
      "help_website": [
        "https://mhkit-software.github.io/MHKiT/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "marine-energy",
        "hydrokinetic",
        "data-analysis"
      ],
      "id": 50
    },
    {
      "name": "nlg-eval",
      "one_line_profile": "Evaluation metrics for Natural Language Generation",
      "detailed_description": "A library containing code for various unsupervised automated metrics for Natural Language Generation (NLG), including BLEU, METEOR, ROUGE, CIDEr, and others. It facilitates the standardized evaluation of text generation models.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "nlg_evaluation",
        "metrics_calculation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Maluuba/nlg-eval",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "nlg",
        "evaluation-metrics",
        "bleu",
        "rouge"
      ],
      "id": 51
    },
    {
      "name": "AutoRAG",
      "one_line_profile": "AutoML-style framework for RAG evaluation and optimization",
      "detailed_description": "An open-source framework designed to evaluate and optimize Retrieval-Augmented Generation (RAG) pipelines. It applies AutoML principles to automatically find the best RAG configuration for a given dataset and use case.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "rag_evaluation",
        "pipeline_optimization",
        "automl"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/Marker-Inc-Korea/AutoRAG",
      "help_website": [
        "https://docs.autorag.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "evaluation",
        "automl",
        "optimization"
      ],
      "id": 52
    },
    {
      "name": "ContainerInception (Gerber)",
      "one_line_profile": "Generalized Easy Reproducible Bioinformatics Environment wRapper",
      "detailed_description": "A tool developed during NCBI Hackathons to facilitate reproducible bioinformatics research by wrapping environments in containers. It aims to simplify the creation and sharing of reproducible scientific workflows.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "reproducibility",
        "workflow_management",
        "containerization"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/NCBI-Hackathons/ContainerInception",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bioinformatics",
        "reproducibility",
        "docker"
      ],
      "id": 53
    },
    {
      "name": "OpenOA",
      "one_line_profile": "Framework for assessing wind plant performance",
      "detailed_description": "An open-source framework developed by NREL for assessing wind plant performance using operational assessment (OA) methodologies. It provides data structures and analysis methods for processing time-series data from wind plants.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "performance_assessment",
        "data_analysis",
        "wind_energy"
      ],
      "application_level": "framework",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/NREL/OpenOA",
      "help_website": [
        "https://openoa.readthedocs.io/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "wind-energy",
        "operational-assessment",
        "nrel"
      ],
      "id": 54
    },
    {
      "name": "compute-eval",
      "one_line_profile": "Evaluation framework for LLM-based CUDA code generation",
      "detailed_description": "A framework by NVIDIA designed to generate and evaluate CUDA code produced by Large Language Models. It provides tools to assess the correctness and performance of AI-generated GPU kernels.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "code_generation_evaluation",
        "cuda_benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/NVIDIA/compute-eval",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "cuda",
        "llm",
        "code-generation",
        "evaluation"
      ],
      "id": 55
    },
    {
      "name": "CanarySEFI",
      "one_line_profile": "Robustness evaluation framework for image recognition models",
      "detailed_description": "A comprehensive framework for evaluating the robustness of deep learning-based image recognition models. It includes various attack and defense algorithms and metrics to assess model security and stability.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "adversarial_attack",
        "model_security"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/NeoSunJZ/Canary_Master",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "robustness",
        "adversarial-ml",
        "security-evaluation"
      ],
      "id": 56
    },
    {
      "name": "atropos",
      "one_line_profile": "LLM Reinforcement Learning Environments framework",
      "detailed_description": "A framework for collecting and evaluating Large Language Model (LLM) trajectories through diverse environments. It serves as a testbed for Reinforcement Learning with LLMs.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "rlhf_evaluation",
        "trajectory_collection",
        "environment_simulation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/NousResearch/atropos",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "reinforcement-learning",
        "evaluation-environment"
      ],
      "id": 57
    },
    {
      "name": "gptables",
      "one_line_profile": "Tool for writing consistently formatted statistical tables",
      "detailed_description": "A Python wrapper around XlsxWriter developed by the Office for National Statistics (ONS) to produce consistently formatted statistical tables in Excel, ensuring data reporting standards.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "data_reporting",
        "statistical_formatting"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ONSdigital/gptables",
      "help_website": [
        "https://gptables.readthedocs.io/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "statistics",
        "reporting",
        "excel",
        "ons"
      ],
      "id": 58
    },
    {
      "name": "OneIG-Benchmark",
      "one_line_profile": "Fine-grained evaluation benchmark for Text-to-Image models",
      "detailed_description": "A comprehensive benchmark framework designed for the fine-grained evaluation of Text-to-Image (T2I) models. It assesses dimensions such as subject-element alignment, text rendering precision, reasoning, stylization, and diversity.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "t2i_evaluation",
        "image_generation_benchmark"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/OneIG-Bench/OneIG-Benchmark",
      "help_website": [],
      "license": null,
      "tags": [
        "text-to-image",
        "benchmark",
        "evaluation"
      ],
      "id": 59
    },
    {
      "name": "UltraEval",
      "one_line_profile": "Open source framework for evaluating foundation models",
      "detailed_description": "An open-source framework for evaluating foundation models across various capabilities. It provides a structured way to run benchmarks and assess model performance.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "foundation_model_evaluation",
        "benchmark_framework"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenBMB/UltraEval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-evaluation",
        "foundation-models",
        "benchmark"
      ],
      "id": 60
    },
    {
      "name": "OpenBioLink",
      "one_line_profile": "Evaluation framework for biomedical link prediction",
      "detailed_description": "A resource and evaluation framework designed for evaluating link prediction models on heterogeneous biomedical graph data. It provides benchmarks and datasets to foster research in biomedical knowledge graphs.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "link_prediction_evaluation",
        "biomedical_graph_benchmark"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenBioLink/OpenBioLink",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "biomedical-graph",
        "link-prediction",
        "benchmark"
      ],
      "id": 61
    },
    {
      "name": "llm-colosseum",
      "one_line_profile": "Game-based evaluation harness for LLMs",
      "detailed_description": "A unique benchmarking tool that evaluates Large Language Models by having them compete in the Street Fighter 3 game. It tests the models' ability to make real-time decisions and strategize in a dynamic environment.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "agent_evaluation",
        "game_benchmark"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/OpenGenerativeAI/llm-colosseum",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm-agent",
        "game-benchmark",
        "evaluation"
      ],
      "id": 62
    },
    {
      "name": "OpenHands Benchmarks",
      "one_line_profile": "Evaluation harness for OpenHands agents",
      "detailed_description": "The official evaluation harness for the OpenHands project, designed to benchmark the performance of AI agents on various tasks. It ensures reproducibility and standardized testing for agentic workflows.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "agent_evaluation",
        "benchmark_harness"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenHands/benchmarks",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ai-agent",
        "benchmark",
        "openhands"
      ],
      "id": 63
    },
    {
      "name": "GAOKAO-Bench",
      "one_line_profile": "Evaluation framework using Chinese College Entrance Exam questions",
      "detailed_description": "An evaluation framework that utilizes questions from the Chinese National College Entrance Examination (GAOKAO) to assess the intelligence and knowledge capabilities of Large Language Models.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "llm_evaluation",
        "knowledge_benchmark"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenLMLab/GAOKAO-Bench",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "gaokao",
        "llm-benchmark",
        "chinese-evaluation"
      ],
      "id": 64
    },
    {
      "name": "OG-Core",
      "one_line_profile": "Overlapping generations (OG) model framework for fiscal policy evaluation",
      "detailed_description": "A general equilibrium modeling framework that allows for the simulation and evaluation of fiscal policies using overlapping generations models. It supports economic research and policy analysis.",
      "domains": [
        "Economics",
        "Computational Social Science"
      ],
      "subtask_category": [
        "simulation",
        "modeling",
        "policy_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/PSLmodels/OG-Core",
      "help_website": [],
      "license": "CC0-1.0",
      "tags": [
        "economics",
        "fiscal-policy",
        "simulation",
        "modeling"
      ],
      "id": 65
    },
    {
      "name": "Paddle Continuous Evaluation",
      "one_line_profile": "Continuous evaluation platform for PaddlePaddle framework",
      "detailed_description": "A macro continuous evaluation platform designed for the PaddlePaddle deep learning framework to monitor model performance and regression.",
      "domains": [
        "AI Toolchain",
        "Deep Learning"
      ],
      "subtask_category": [
        "continuous_evaluation",
        "model_regression_testing"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/PaddlePaddle/continuous_evaluation",
      "help_website": [],
      "license": null,
      "tags": [
        "paddlepaddle",
        "ci",
        "evaluation",
        "regression"
      ],
      "id": 66
    },
    {
      "name": "LLMBox",
      "one_line_profile": "Comprehensive library for LLM training and evaluation",
      "detailed_description": "A library designed to standardize the implementation, training, and comprehensive evaluation of Large Language Models (LLMs), supporting unified pipelines.",
      "domains": [
        "AI4",
        "NLP"
      ],
      "subtask_category": [
        "model_training",
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/RUCAIBox/LLMBox",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "evaluation",
        "training-pipeline",
        "nlp"
      ],
      "id": 67
    },
    {
      "name": "RouterArena",
      "one_line_profile": "Evaluation framework for LLM routing strategies",
      "detailed_description": "An open framework for evaluating LLM routers with standardized datasets, metrics, and an automated execution environment.",
      "domains": [
        "AI4",
        "NLP"
      ],
      "subtask_category": [
        "model_routing",
        "evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/RouteWorks/RouterArena",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-routing",
        "evaluation",
        "benchmark"
      ],
      "id": 68
    },
    {
      "name": "RAG-evaluation-harnesses",
      "one_line_profile": "Evaluation suite for Retrieval-Augmented Generation (RAG)",
      "detailed_description": "A specialized evaluation harness designed to assess the performance of Retrieval-Augmented Generation systems.",
      "domains": [
        "AI4",
        "NLP"
      ],
      "subtask_category": [
        "rag_evaluation",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/RulinShao/RAG-evaluation-harnesses",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "evaluation",
        "nlp"
      ],
      "id": 69
    },
    {
      "name": "RuSentEval",
      "one_line_profile": "Probing suite for Russian language models",
      "detailed_description": "A benchmark and probing suite for evaluating the linguistic capabilities of Russian embedding and language models.",
      "domains": [
        "AI4",
        "NLP"
      ],
      "subtask_category": [
        "probing",
        "model_evaluation",
        "linguistics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/RussianNLP/RuSentEval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "russian-nlp",
        "probing-tasks",
        "evaluation"
      ],
      "id": 70
    },
    {
      "name": "MAP (Mapping Accessibility)",
      "one_line_profile": "Urban network modeling and accessibility metric calculation tool",
      "detailed_description": "A software package for creating urban network models and calculating cumulative accessibility metrics and spatial justice indicators for urban planning.",
      "domains": [
        "Urban Planning",
        "Spatial Science"
      ],
      "subtask_category": [
        "network_modeling",
        "accessibility_analysis",
        "spatial_metrics"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/RuthJNelson/MAP-Mapping-Accessibility-for-Ethically-Informed-Urban-Planning",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "urban-planning",
        "accessibility",
        "network-analysis"
      ],
      "id": 71
    },
    {
      "name": "Guardians MT Eval",
      "one_line_profile": "Sentinel metrics for Machine Translation meta-evaluation",
      "detailed_description": "A repository implementing sentinel metrics for the meta-evaluation of Machine Translation systems, as presented in ACL 2024.",
      "domains": [
        "AI4",
        "NLP"
      ],
      "subtask_category": [
        "mt_evaluation",
        "meta_evaluation",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/SapienzaNLP/guardians-mt-eval",
      "help_website": [],
      "license": null,
      "tags": [
        "machine-translation",
        "evaluation",
        "metrics"
      ],
      "id": 72
    },
    {
      "name": "Langtrace",
      "one_line_profile": "Observability and evaluation tool for LLM applications",
      "detailed_description": "An Open Telemetry based tool for tracing, evaluating, and monitoring LLM applications, supporting various LLMs and vector databases.",
      "domains": [
        "AI4",
        "MLOps"
      ],
      "subtask_category": [
        "observability",
        "tracing",
        "model_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/Scale3-Labs/langtrace",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "llm-ops",
        "observability",
        "evaluation",
        "opentelemetry"
      ],
      "id": 73
    },
    {
      "name": "ScienceEval",
      "one_line_profile": "Evaluation suite for scientific foundation models",
      "detailed_description": "An open-source evaluation suite specifically designed for assessing the capabilities of ScienceOne Base models and other scientific LLMs.",
      "domains": [
        "AI4S",
        "Scientific AI"
      ],
      "subtask_category": [
        "model_evaluation",
        "scientific_reasoning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ScienceOne-AI/ScienceEval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "science-llm",
        "evaluation",
        "benchmark"
      ],
      "id": 74
    },
    {
      "name": "Gorilla",
      "one_line_profile": "Benchmark and training framework for LLM function calling",
      "detailed_description": "A framework and benchmark for training and evaluating Large Language Models on their ability to perform function calls (tool use).",
      "domains": [
        "AI4",
        "NLP"
      ],
      "subtask_category": [
        "function_calling",
        "model_evaluation",
        "training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ShishirPatil/gorilla",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "function-calling",
        "benchmark",
        "tool-use"
      ],
      "id": 75
    },
    {
      "name": "OmixBench",
      "one_line_profile": "Evaluation framework for LLMs in Multi-omics Analysis",
      "detailed_description": "A systematic evaluation framework designed to assess the performance of Large Language Models in the context of multi-omics data analysis.",
      "domains": [
        "AI4S",
        "Bioinformatics"
      ],
      "subtask_category": [
        "multi_omics_analysis",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/SolvingLab/OmixBench",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "multi-omics",
        "llm",
        "evaluation",
        "bioinformatics"
      ],
      "id": 76
    },
    {
      "name": "TransformerQuant",
      "one_line_profile": "Framework for deep learning models in quantitative trading",
      "detailed_description": "A framework for training and evaluating deep learning models specifically for the quantitative trading domain.",
      "domains": [
        "Quantitative Finance",
        "AI4"
      ],
      "subtask_category": [
        "model_training",
        "model_evaluation",
        "quantitative_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/StateOfTheArt-quant/transformerquant",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "quantitative-trading",
        "deep-learning",
        "evaluation"
      ],
      "id": 77
    },
    {
      "name": "CellSPA",
      "one_line_profile": "Cell Segmentation Performance Assessment tool",
      "detailed_description": "A tool for assessing the performance of cell segmentation algorithms in biological imaging.",
      "domains": [
        "Bioinformatics",
        "Imaging"
      ],
      "subtask_category": [
        "segmentation_evaluation",
        "image_analysis"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/SydneyBioX/CellSPA",
      "help_website": [],
      "license": null,
      "tags": [
        "cell-segmentation",
        "evaluation",
        "bioinformatics"
      ],
      "id": 78
    },
    {
      "name": "HPCPerfStats",
      "one_line_profile": "HPC resource-usage monitoring and analysis package",
      "detailed_description": "An automated resource-usage monitoring and analysis package for High Performance Computing (HPC) clusters, supporting scientific computing infrastructure.",
      "domains": [
        "HPC",
        "Scientific Computing"
      ],
      "subtask_category": [
        "performance_monitoring",
        "resource_analysis"
      ],
      "application_level": "tool",
      "primary_language": "C",
      "repo_url": "https://github.com/TACC/HPCPerfStats",
      "help_website": [],
      "license": "LGPL-2.1",
      "tags": [
        "hpc",
        "monitoring",
        "performance-analysis"
      ],
      "id": 79
    },
    {
      "name": "AgentBench",
      "one_line_profile": "Comprehensive benchmark to evaluate LLMs as Agents",
      "detailed_description": "A benchmark suite designed to evaluate the capabilities of Large Language Models acting as autonomous agents across various environments.",
      "domains": [
        "AI4",
        "NLP"
      ],
      "subtask_category": [
        "agent_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/THUDM/AgentBench",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-agent",
        "benchmark",
        "evaluation"
      ],
      "id": 80
    },
    {
      "name": "AICGSecEval",
      "one_line_profile": "AI-generated code security evaluation benchmark",
      "detailed_description": "A repository-level benchmark for evaluating the security of code generated by Artificial Intelligence models.",
      "domains": [
        "AI4",
        "Software Security"
      ],
      "subtask_category": [
        "security_evaluation",
        "code_generation_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Tencent/AICGSecEval",
      "help_website": [],
      "license": null,
      "tags": [
        "ai-security",
        "code-generation",
        "evaluation"
      ],
      "id": 81
    },
    {
      "name": "Afrobench Eval Suite",
      "one_line_profile": "LLM evaluation leaderboard for African Languages",
      "detailed_description": "An evaluation suite and leaderboard for assessing Large Language Model performance on African languages.",
      "domains": [
        "AI4",
        "NLP"
      ],
      "subtask_category": [
        "multilingual_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/The-African-Research-Collective/afrobench-eval-suite",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "african-languages",
        "nlp",
        "evaluation"
      ],
      "id": 82
    },
    {
      "name": "TimeCopilot",
      "one_line_profile": "GenAI Forecasting Agent for time series analysis",
      "detailed_description": "An agent-based tool leveraging LLMs and Time Series Foundation Models for forecasting, cross-validation, and anomaly detection in domains like finance and energy.",
      "domains": [
        "AI4S",
        "Time Series Analysis"
      ],
      "subtask_category": [
        "forecasting",
        "anomaly_detection",
        "cross_validation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/TimeCopilot/timecopilot",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "time-series",
        "forecasting",
        "llm-agent",
        "anomaly-detection"
      ],
      "id": 83
    },
    {
      "name": "Inspect",
      "one_line_profile": "Framework for large language model evaluations",
      "detailed_description": "An open-source framework for creating and running evaluations for Large Language Models (LLMs).",
      "domains": [
        "AI4",
        "NLP"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/UKGovernmentBEIS/inspect_ai",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "evaluation",
        "framework"
      ],
      "id": 84
    },
    {
      "name": "VMEvalKit",
      "one_line_profile": "Evaluation framework for reasoning capabilities in foundational video models",
      "detailed_description": "A comprehensive framework designed to evaluate the reasoning abilities of video foundational models, providing metrics and protocols for assessing model performance in understanding complex video content.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "reasoning_assessment"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Video-Reason/VMEvalKit",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "video-understanding",
        "evaluation-framework",
        "reasoning"
      ],
      "id": 85
    },
    {
      "name": "owl-eval",
      "one_line_profile": "Evaluation harness for diffusion world models",
      "detailed_description": "A specialized evaluation harness designed to assess the performance and capabilities of diffusion-based world models, facilitating reproducible benchmarking in generative modeling research.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "diffusion_models"
      ],
      "application_level": "solver",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/Wayfarer-Labs/owl-eval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "evaluation-harness",
        "diffusion-models",
        "world-models"
      ],
      "id": 86
    },
    {
      "name": "EHRStruct",
      "one_line_profile": "Benchmark framework for evaluating LLMs on structured Electronic Health Record tasks",
      "detailed_description": "A comprehensive benchmark framework designed to evaluate Large Language Models on tasks involving structured Electronic Health Records (EHR), facilitating research in medical AI and clinical data processing.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "medical_informatics"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/YXNTU/EHRStruct",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "ehr",
        "llm-evaluation",
        "healthcare-ai"
      ],
      "id": 87
    },
    {
      "name": "OmniBenchmark",
      "one_line_profile": "Benchmark for evaluating pre-trained vision models and contrastive learning",
      "detailed_description": "A benchmark suite and framework for evaluating pre-trained computer vision models, featuring a supervised contrastive learning framework to assess model robustness and transferability.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "computer_vision"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ZhangYuanhan-AI/OmniBenchmark",
      "help_website": [],
      "license": null,
      "tags": [
        "benchmark",
        "pre-trained-models",
        "contrastive-learning"
      ],
      "id": 88
    },
    {
      "name": "math-evaluation-harness",
      "one_line_profile": "Toolkit for benchmarking LLMs on mathematical reasoning tasks",
      "detailed_description": "A lightweight and extensible toolkit designed to evaluate Large Language Models on various mathematical reasoning datasets, supporting the assessment of logical and quantitative capabilities.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "mathematical_reasoning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ZubinGou/math-evaluation-harness",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "math-reasoning",
        "llm-benchmark",
        "evaluation"
      ],
      "id": 89
    },
    {
      "name": "PertEval",
      "one_line_profile": "Evaluation suite for transcriptomic perturbation effect prediction models",
      "detailed_description": "A specialized evaluation suite for assessing models that predict transcriptomic perturbation effects, including support for single-cell foundation models, aiding in computational biology research.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "computational_biology"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/aaronwtr/PertEval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "transcriptomics",
        "perturbation-prediction",
        "single-cell"
      ],
      "id": 90
    },
    {
      "name": "indic_eval",
      "one_line_profile": "Evaluation suite for assessing Indic LLMs across diverse tasks",
      "detailed_description": "A lightweight evaluation suite tailored for benchmarking Large Language Models on Indic language tasks, facilitating the assessment of multilingual and low-resource language models.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "nlp"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/adithya-s-k/indic_eval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "indic-languages",
        "llm-evaluation",
        "nlp-benchmark"
      ],
      "id": 91
    },
    {
      "name": "AidanBench",
      "one_line_profile": "Benchmark tool for measuring specific biases or 'smells' in LLMs",
      "detailed_description": "A benchmarking tool designed to detect and measure 'big model smell' (biases or specific behavioral patterns) in Large Language Models, contributing to model interpretability and safety evaluation.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "bias_detection"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/aidanmclaughlin/AidanBench",
      "help_website": [],
      "license": null,
      "tags": [
        "llm-benchmark",
        "model-behavior",
        "evaluation"
      ],
      "id": 92
    },
    {
      "name": "OLMo-Eval",
      "one_line_profile": "Evaluation suite for Open Language Models (OLMo)",
      "detailed_description": "A comprehensive evaluation suite developed by AllenAI for benchmarking Large Language Models, specifically supporting the OLMo ecosystem and general LLM performance assessment.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "llm_benchmark"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/allenai/OLMo-Eval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "evaluation",
        "allenai"
      ],
      "id": 93
    },
    {
      "name": "deepfabric",
      "one_line_profile": "Platform for dataset curation, training, and evaluation of AI models",
      "detailed_description": "An integrated framework designed to streamline the lifecycle of AI model development, including high-quality dataset curation, model training, and evaluation, facilitating reproducible AI research.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "workflow_automation",
        "model_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/always-further/deepfabric",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "mlops",
        "dataset-curation",
        "evaluation-pipeline"
      ],
      "id": 94
    },
    {
      "name": "Apache Liminal",
      "one_line_profile": "Workflow orchestration for automating machine learning pipelines",
      "detailed_description": "An Apache incubator project that provides a domain-specific language to build, orchestrate, and operationalize machine learning workflows, bridging the gap between experimentation and production inference.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "workflow_automation",
        "pipeline_orchestration"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/apache/incubator-liminal",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "mlops",
        "workflow",
        "apache-airflow"
      ],
      "id": 95
    },
    {
      "name": "BigCodeBench-X",
      "one_line_profile": "Benchmark for evaluating LLMs on programming tasks across multiple languages",
      "detailed_description": "A multi-language benchmark suite designed to evaluate the code generation and reasoning capabilities of Large Language Models, supporting a wide range of programming languages for comprehensive assessment.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "code_generation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/arjunguha/BigCodeBench-X",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "code-benchmark",
        "llm-evaluation",
        "multi-language"
      ],
      "id": 96
    },
    {
      "name": "rcaaqs",
      "one_line_profile": "R package for calculating air quality metrics based on Canadian standards",
      "detailed_description": "An R package developed by the British Columbia Government to facilitate the calculation of air quality metrics (PM2.5, Ozone, NO2, SO2) according to the Canadian Ambient Air Quality Standards (CAAQS). It aids in environmental science data processing and regulatory reporting.",
      "domains": [
        "Environmental Science",
        "Atmospheric Science"
      ],
      "subtask_category": [
        "data_processing",
        "metrics_calculation"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/bcgov/rcaaqs",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "air-quality",
        "environmental-metrics",
        "r-package"
      ],
      "id": 97
    },
    {
      "name": "BEIR",
      "one_line_profile": "Heterogeneous benchmark for zero-shot information retrieval evaluation",
      "detailed_description": "A heterogeneous benchmark for Information Retrieval (IR) that provides a unified framework to evaluate retrieval models across a diverse set of datasets and tasks, focusing on zero-shot performance.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmark"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/beir-cellar/beir",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "information-retrieval",
        "benchmark",
        "zero-shot"
      ],
      "id": 98
    },
    {
      "name": "ASAP-AES Metrics",
      "one_line_profile": "Evaluation metrics for Automated Essay Scoring (Quadratic Weighted Kappa)",
      "detailed_description": "Provides the reference implementation of evaluation metrics, specifically the Quadratic Weighted Kappa, used for the Automated Student Assessment Prize (ASAP) Automated Essay Scoring competition. Useful for evaluating NLP models in scoring tasks.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "metrics_calculation",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/benhamner/ASAP-AES",
      "help_website": [],
      "license": "BSD-2-Clause",
      "tags": [
        "nlp",
        "evaluation-metric",
        "kappa"
      ],
      "id": 99
    },
    {
      "name": "survcomp",
      "one_line_profile": "Performance assessment and comparison for survival analysis models",
      "detailed_description": "An R package providing functions to assess and compare the performance of risk prediction models in survival analysis, widely used in biostatistics and medical research (e.g., cancer prognosis).",
      "domains": [
        "Biostatistics",
        "Medical Science"
      ],
      "subtask_category": [
        "model_evaluation",
        "statistical_analysis"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/bhklab/survcomp",
      "help_website": [
        "https://www.bioconductor.org/packages/release/bioc/html/survcomp.html"
      ],
      "license": "NOASSERTION",
      "tags": [
        "survival-analysis",
        "bioconductor",
        "risk-prediction"
      ],
      "id": 100
    },
    {
      "name": "BigCode Evaluation Harness",
      "one_line_profile": "Evaluation framework for code generation language models",
      "detailed_description": "A framework for the evaluation of autoregressive code generation language models, supporting various coding tasks and metrics. Developed by the BigCode project to standardize code LLM assessment.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmark"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/bigcode-project/bigcode-evaluation-harness",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "code-generation",
        "llm-evaluation",
        "benchmark"
      ],
      "id": 101
    },
    {
      "name": "Realistic SSL Evaluation",
      "one_line_profile": "Benchmark suite for realistic evaluation of Semi-Supervised Learning",
      "detailed_description": "A benchmark suite and codebase for evaluating Deep Semi-Supervised Learning (SSL) algorithms under realistic conditions, developed by Google Brain research.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmark"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/brain-research/realistic-ssl-evaluation",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "semi-supervised-learning",
        "benchmark",
        "deep-learning"
      ],
      "id": 102
    },
    {
      "name": "Brain-Score Vision",
      "one_line_profile": "Framework evaluating vision models against brain and behavioral data",
      "detailed_description": "A framework for evaluating artificial vision models on their alignment to brain (neural) and behavioral measurements, containing over 100 benchmarks to bridge computer vision and neuroscience.",
      "domains": [
        "Neuroscience",
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "alignment_check"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/brain-score/vision",
      "help_website": [
        "http://www.brain-score.org/"
      ],
      "license": "MIT",
      "tags": [
        "neuroscience",
        "computer-vision",
        "brain-alignment"
      ],
      "id": 103
    },
    {
      "name": "BytevalKit-Emb",
      "one_line_profile": "Modular evaluation framework for embedding models",
      "detailed_description": "A modular framework developed by ByteDance for evaluating embedding models, supporting automated performance assessment across multiple task types and standardized processes.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmark"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/bytedance/BytevalKit-Emb",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "embedding-models",
        "evaluation-framework",
        "nlp"
      ],
      "id": 104
    },
    {
      "name": "TMU",
      "one_line_profile": "Library implementing Tsetlin Machine algorithms for logic-based AI",
      "detailed_description": "A library implementing the Tsetlin Machine and its variants (Coalesced, Convolutional, etc.) for interpretable pattern recognition and logic-based machine learning, with CUDA support.",
      "domains": [
        "AI4",
        "Machine Learning"
      ],
      "subtask_category": [
        "modeling",
        "solver"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/cair/tmu",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "tsetlin-machine",
        "logic-learning",
        "interpretable-ai"
      ],
      "id": 105
    },
    {
      "name": "XLM-T",
      "one_line_profile": "Framework for evaluating multilingual language models on Twitter data",
      "detailed_description": "A framework and repository for training and evaluating multilingual language models specifically on Twitter data, enabling consistent benchmarking in social media NLP tasks.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmark"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/cardiffnlp/xlm-t",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "nlp",
        "multilingual",
        "twitter",
        "evaluation"
      ],
      "id": 106
    },
    {
      "name": "Yet Another Applied LLM Benchmark",
      "one_line_profile": "Applied benchmark for evaluating LLMs on practical coding and reasoning tasks",
      "detailed_description": "A benchmark suite developed by Nicholas Carlini to evaluate Large Language Models on a set of practical, applied questions and coding tasks, serving as a personal but influential baseline for model capability assessment.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmark"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/carlini/yet-another-applied-llm-benchmark",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "llm",
        "benchmark",
        "reasoning"
      ],
      "id": 107
    },
    {
      "name": "Opik",
      "one_line_profile": "Platform for evaluating, debugging, and monitoring LLM applications",
      "detailed_description": "Opik is a comprehensive platform designed for the evaluation and monitoring of Large Language Model (LLM) applications, RAG systems, and agentic workflows. It provides tracing capabilities, automated evaluation metrics, and dashboards to ensure production readiness of AI models.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "monitoring",
        "tracing"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/comet-ml/opik",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-evaluation",
        "observability",
        "rag",
        "debugging"
      ],
      "id": 108
    },
    {
      "name": "Compl-AI",
      "one_line_profile": "Compliance-centered evaluation framework for Generative AI models",
      "detailed_description": "Compl-AI is an open-source framework focused on evaluating Generative AI models against compliance standards. It enables the assessment of models for technical and ethical compliance, providing a structured approach to AI safety and regulation adherence.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "compliance_testing",
        "ai_safety"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/compl-ai/compl-ai",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "generative-ai",
        "compliance",
        "evaluation",
        "safety"
      ],
      "id": 109
    },
    {
      "name": "DeepEval",
      "one_line_profile": "Unit testing and evaluation framework for LLMs",
      "detailed_description": "DeepEval is an evaluation framework designed to unit test Large Language Models (LLMs). It offers a suite of metrics to assess RAG pipelines and agents, facilitating continuous integration and regression testing for AI application development.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "unit_testing",
        "rag_evaluation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/confident-ai/deepeval",
      "help_website": [
        "https://docs.confident-ai.com"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "testing",
        "metrics",
        "rag"
      ],
      "id": 110
    },
    {
      "name": "LogiEval",
      "one_line_profile": "Benchmark suite for testing logical reasoning in prompt-based models",
      "detailed_description": "LogiEval is a benchmark suite designed to evaluate the logical reasoning capabilities of prompt-based AI models. It provides a set of tasks and metrics to assess how well models perform on logic-intensive problems.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking",
        "logical_reasoning"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/csitfun/LogiEval",
      "help_website": [],
      "license": null,
      "tags": [
        "benchmark",
        "logical-reasoning",
        "llm",
        "prompting"
      ],
      "id": 111
    },
    {
      "name": "Bisheng",
      "one_line_profile": "Open LLM DevOps and evaluation platform for enterprise AI",
      "detailed_description": "Bisheng is a comprehensive platform for developing and managing LLM applications. It includes features for evaluation, dataset management, supervised fine-tuning (SFT), and RAG workflows, serving as a unified environment for AI model lifecycle management.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_management",
        "evaluation",
        "workflow_orchestration"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/dataelement/bisheng",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llmops",
        "evaluation",
        "rag",
        "fine-tuning"
      ],
      "id": 112
    },
    {
      "name": "WEFE",
      "one_line_profile": "Word Embeddings Fairness Evaluation Framework",
      "detailed_description": "WEFE is a framework dedicated to measuring and mitigating bias in word embedding models. It standardizes fairness evaluation metrics, allowing researchers to assess social biases in pre-trained language models.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "bias_evaluation",
        "fairness_metrics",
        "model_analysis"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/dccuchile/wefe",
      "help_website": [
        "https://wefe.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "fairness",
        "bias",
        "word-embeddings",
        "nlp"
      ],
      "id": 113
    },
    {
      "name": "Deepchecks",
      "one_line_profile": "Continuous validation and testing for ML models and data",
      "detailed_description": "Deepchecks is a holistic open-source solution for testing and validating machine learning models and data. It supports the entire ML lifecycle from research to production, offering checks for data integrity, model performance, and distribution drift.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_validation",
        "data_quality_control",
        "drift_detection"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/deepchecks/deepchecks",
      "help_website": [
        "https://docs.deepchecks.com"
      ],
      "license": "NOASSERTION",
      "tags": [
        "ml-testing",
        "validation",
        "data-quality",
        "model-monitoring"
      ],
      "id": 114
    },
    {
      "name": "Ollama Grid Search",
      "one_line_profile": "Desktop application to evaluate and compare LLM models",
      "detailed_description": "Ollama Grid Search is a multi-platform desktop tool that facilitates the evaluation and comparison of various Large Language Models (LLMs) managed via Ollama. It allows users to run grid searches over prompts and model parameters to assess performance.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "parameter_tuning",
        "comparison"
      ],
      "application_level": "solver",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/dezoito/ollama-grid-search",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "grid-search",
        "evaluation",
        "ollama"
      ],
      "id": 115
    },
    {
      "name": "AB Test Advanced Toolkit",
      "one_line_profile": "Statistical toolkit for advanced A/B testing analysis",
      "detailed_description": "A suite of statistical tools for analyzing A/B tests, featuring advanced methods like CUPED (Controlled-experiment Using Pre-Experiment Data) and Gradient Boosting for variance reduction and faster significance testing.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "statistical_analysis",
        "hypothesis_testing",
        "variance_reduction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/dmitry-brazhenko/ab-test-advanced-toolkit",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ab-testing",
        "statistics",
        "cuped",
        "significance-testing"
      ],
      "id": 116
    },
    {
      "name": "Docling Eval",
      "one_line_profile": "Evaluation framework for document processing models",
      "detailed_description": "Docling Eval is a framework designed to evaluate the performance of document processing models and services. It provides metrics and workflows to assess the accuracy of information extraction and document layout analysis.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "document_processing",
        "ocr_evaluation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/docling-project/docling-eval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "document-ai",
        "evaluation",
        "ocr",
        "layout-analysis"
      ],
      "id": 117
    },
    {
      "name": "Uni-Dock-Benchmarks",
      "one_line_profile": "Benchmark datasets for molecular docking systems",
      "detailed_description": "A curated collection of datasets and benchmarking tests specifically for evaluating the performance and accuracy of the Uni-Dock molecular docking system. It serves as a standard for comparing docking results in drug discovery research.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "benchmarking",
        "molecular_docking",
        "drug_discovery"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/dptech-corp/Uni-Dock-Benchmarks",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "molecular-docking",
        "benchmark",
        "drug-discovery",
        "bioinformatics"
      ],
      "id": 118
    },
    {
      "name": "Encord Active",
      "one_line_profile": "Active learning toolkit for model evaluation and data curation",
      "detailed_description": "Encord Active is an open-source toolkit for testing, validating, and evaluating computer vision models. It focuses on data-centric AI, helping users prioritize data for labeling, detect errors, and analyze model performance through actionable metrics.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "active_learning",
        "data_curation"
      ],
      "application_level": "toolkit",
      "primary_language": "Python",
      "repo_url": "https://github.com/encord-team/encord-active",
      "help_website": [
        "https://docs.encord.com/active/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "computer-vision",
        "active-learning",
        "evaluation",
        "data-centric-ai"
      ],
      "id": 119
    },
    {
      "name": "Text-to-Image Eval",
      "one_line_profile": "Evaluation metrics for text-to-image generation models",
      "detailed_description": "A toolkit for evaluating text-to-image and zero-shot image classification models (e.g., CLIP, SigLIP). It implements metrics such as Zero-shot accuracy, Linear Probe, and Image retrieval to assess model quality and alignment.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "image_generation",
        "zero_shot_classification"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/encord-team/text-to-image-eval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "text-to-image",
        "evaluation",
        "clip",
        "metrics"
      ],
      "id": 120
    },
    {
      "name": "Kurtis",
      "one_line_profile": "Fine-tuning, inference, and evaluation tool for Small Language Models (SLMs)",
      "detailed_description": "Kurtis is a comprehensive tool designed for Small Language Models (SLMs) like SmolLM2, providing capabilities for fine-tuning, inference, and evaluation to streamline the development lifecycle of lightweight models.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "fine_tuning",
        "inference"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ethicalabs-ai/kurtis",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "slm",
        "fine-tuning",
        "evaluation",
        "inference"
      ],
      "id": 121
    },
    {
      "name": "EvalPlus",
      "one_line_profile": "Rigorous evaluation framework for LLM-synthesized code",
      "detailed_description": "EvalPlus is a code synthesis benchmarking framework that augments existing datasets (like HumanEval) with rigorous test cases to accurately evaluate the functional correctness of code generated by Large Language Models.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "code_evaluation",
        "benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/evalplus/evalplus",
      "help_website": [
        "https://evalplus.github.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "code-generation",
        "benchmark",
        "testing"
      ],
      "id": 122
    },
    {
      "name": "Evidently",
      "one_line_profile": "Open-source ML and LLM observability and evaluation framework",
      "detailed_description": "Evidently is a framework to evaluate, test, and monitor ML models and data pipelines. It provides metrics for data drift, model performance, and data quality, supporting both tabular data and LLM text data.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_monitoring",
        "data_drift_detection",
        "evaluation"
      ],
      "application_level": "platform",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/evidentlyai/evidently",
      "help_website": [
        "https://docs.evidentlyai.com/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "observability",
        "ml-monitoring",
        "data-drift",
        "llm-eval"
      ],
      "id": 123
    },
    {
      "name": "OBR",
      "one_line_profile": "Runner for OpenFOAM benchmarks",
      "detailed_description": "OBR (OpenFOAM Benchmark Runner) is a tool designed to automate the execution and management of benchmarks for OpenFOAM, a popular computational fluid dynamics (CFD) software.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "simulation_runner",
        "benchmarking"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/exasim-project/OBR",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "openfoam",
        "cfd",
        "benchmark",
        "hpc"
      ],
      "id": 124
    },
    {
      "name": "ParlAI",
      "one_line_profile": "Framework for training and evaluating dialogue AI models",
      "detailed_description": "ParlAI is a unified platform for training and evaluating dialogue models across many tasks. It provides a standard interface to access datasets and models, facilitating reproduction and benchmarking in conversational AI research.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "dialogue_evaluation",
        "model_training"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/ParlAI",
      "help_website": [
        "https://parl.ai/"
      ],
      "license": "MIT",
      "tags": [
        "nlp",
        "dialogue-systems",
        "chatbot",
        "benchmark"
      ],
      "id": 125
    },
    {
      "name": "LLM Speedrunner",
      "one_line_profile": "Automated benchmark for LLM agents in language modeling innovation",
      "detailed_description": "The Automated LLM Speedrunning Benchmark measures the capability of LLM agents to reproduce previous innovations and discover new ones in the field of language modeling, serving as a metric for agentic research capabilities.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "agent_evaluation",
        "benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/facebookresearch/llm-speedrunner",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "llm-agents",
        "benchmark",
        "research-automation"
      ],
      "id": 126
    },
    {
      "name": "Video Transformers",
      "one_line_profile": "Fine-tuning framework for HuggingFace video classification models",
      "detailed_description": "A lightweight library designed to simplify the fine-tuning process of video classification models from the HuggingFace ecosystem, providing easy-to-use interfaces for training and evaluation.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_finetuning",
        "video_classification"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/fcakyon/video-transformers",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "video-classification",
        "transformers",
        "fine-tuning"
      ],
      "id": 127
    },
    {
      "name": "FlagEvalMM",
      "one_line_profile": "Flexible framework for comprehensive multimodal model evaluation",
      "detailed_description": "FlagEvalMM is a toolkit designed to evaluate multimodal models across various dimensions. It supports diverse datasets and metrics to assess the capabilities of vision-language models.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "multimodal_evaluation",
        "benchmarking"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/flageval-baai/FlagEvalMM",
      "help_website": [],
      "license": null,
      "tags": [
        "multimodal",
        "evaluation",
        "vlm"
      ],
      "id": 128
    },
    {
      "name": "divraster",
      "one_line_profile": "R package for calculating diversity metrics on rasterized data",
      "detailed_description": "divraster is an R package that provides functions to calculate various diversity metrics (e.g., alpha, beta diversity) directly on raster data, commonly used in ecology and biogeography.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "data_analysis",
        "diversity_metrics"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/flaviomoc/divraster",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "ecology",
        "raster",
        "diversity-metrics",
        "r-package"
      ],
      "id": 129
    },
    {
      "name": "js-quantities",
      "one_line_profile": "JavaScript library for quantity calculation and unit conversion",
      "detailed_description": "A library to handle physical quantities and unit conversions in JavaScript, useful for scientific data processing and frontend scientific visualizations.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "unit_conversion",
        "data_processing"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/gentooboontoo/js-quantities",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "unit-conversion",
        "physics",
        "quantities"
      ],
      "id": 130
    },
    {
      "name": "BIG-bench",
      "one_line_profile": "Collaborative benchmark for measuring language model capabilities",
      "detailed_description": "The Beyond the Imitation Game Benchmark (BIG-bench) is a collaborative initiative to create a diverse and challenging benchmark suite for evaluating large language models across a wide range of tasks.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/google/BIG-bench",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "benchmark",
        "nlp"
      ],
      "id": 131
    },
    {
      "name": "Google Agent Development Toolkit (Go)",
      "one_line_profile": "Go toolkit for building and evaluating AI agents",
      "detailed_description": "An open-source toolkit that provides components and infrastructure for building, evaluating, and deploying AI agents, facilitating research and development in agentic AI.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "agent_evaluation",
        "agent_development"
      ],
      "application_level": "library",
      "primary_language": "Go",
      "repo_url": "https://github.com/google/adk-go",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ai-agents",
        "evaluation",
        "toolkit"
      ],
      "id": 132
    },
    {
      "name": "Google Agent Development Toolkit (Java)",
      "one_line_profile": "Java toolkit for building and evaluating AI agents",
      "detailed_description": "An open-source toolkit that provides components and infrastructure for building, evaluating, and deploying AI agents using Java.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "agent_evaluation",
        "agent_development"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/google/adk-java",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ai-agents",
        "evaluation",
        "toolkit"
      ],
      "id": 133
    },
    {
      "name": "Google Agent Development Toolkit (Python)",
      "one_line_profile": "Python toolkit for building and evaluating AI agents",
      "detailed_description": "An open-source toolkit that provides components and infrastructure for building, evaluating, and deploying AI agents using Python, widely used in AI research.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "agent_evaluation",
        "agent_development"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/google/adk-python",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ai-agents",
        "evaluation",
        "toolkit"
      ],
      "id": 134
    },
    {
      "name": "Gromit",
      "one_line_profile": "Decentralized systems benchmarking and experiment runner framework",
      "detailed_description": "Gromit is a framework designed to automate the deployment, execution, and benchmarking of decentralized systems experiments, facilitating reproducible research in distributed computing.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "experiment_runner",
        "system_benchmarking"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/grimadas/gromit",
      "help_website": [],
      "license": null,
      "tags": [
        "distributed-systems",
        "benchmarking",
        "experiment-runner"
      ],
      "id": 135
    },
    {
      "name": "h2o-LLM-eval",
      "one_line_profile": "Large-language Model Evaluation framework with Elo Leaderboard",
      "detailed_description": "A framework for evaluating Large Language Models (LLMs) that includes tools for running benchmarks, calculating metrics, and generating Elo leaderboards and A/B testing results.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "platform",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/h2oai/h2o-LLM-eval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "evaluation",
        "leaderboard"
      ],
      "id": 136
    },
    {
      "name": "haddock-runner",
      "one_line_profile": "Runner for large scale HADDOCK biomolecular simulations",
      "detailed_description": "A utility to automate and manage large-scale docking simulations using HADDOCK, enabling high-throughput structural biology research.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "simulation_runner",
        "molecular_docking"
      ],
      "application_level": "workflow",
      "primary_language": "Go",
      "repo_url": "https://github.com/haddocking/haddock-runner",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "structural-biology",
        "docking",
        "haddock",
        "simulation"
      ],
      "id": 137
    },
    {
      "name": "pytorch-worker",
      "one_line_profile": "Framework for training, evaluating and testing PyTorch models",
      "detailed_description": "A lightweight wrapper framework around PyTorch to streamline the training, evaluation, and testing loops for deep learning models.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_training",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/haoxizhong/pytorch-worker",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pytorch",
        "training-framework",
        "evaluation"
      ],
      "id": 138
    },
    {
      "name": "big-ann-benchmarks",
      "one_line_profile": "Framework for evaluating ANNS algorithms on billion scale datasets",
      "detailed_description": "A benchmarking framework specifically designed to evaluate the performance of Approximate Nearest Neighbor Search (ANNS) algorithms on billion-scale datasets.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "algorithm_benchmarking",
        "anns"
      ],
      "application_level": "platform",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/harsha-simhadri/big-ann-benchmarks",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "anns",
        "benchmark",
        "vector-search"
      ],
      "id": 139
    },
    {
      "name": "EvalView",
      "one_line_profile": "Pytest-style test harness for AI agents",
      "detailed_description": "EvalView is a testing harness for AI agents that supports YAML scenarios, tool-call checks, and cost/latency/safety evaluations, designed to be CI-friendly.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "agent_evaluation",
        "testing_harness"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/hidai25/eval-view",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ai-agents",
        "evaluation",
        "testing",
        "ci"
      ],
      "id": 140
    },
    {
      "name": "This-is-not-a-Dataset",
      "one_line_profile": "Dataset for evaluating commonsense knowledge and negation in LLMs",
      "detailed_description": "A large semi-automatically generated dataset of descriptive sentences about commonsense knowledge, specifically designed to evaluate how Large Language Models handle negation and truthfulness.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "dataset"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/hitz-zentroa/This-is-not-a-Dataset",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-eval",
        "commonsense",
        "negation",
        "dataset"
      ],
      "id": 141
    },
    {
      "name": "Latxa",
      "one_line_profile": "Open Language Model and Evaluation Suite for Basque",
      "detailed_description": "Latxa provides open language models and a comprehensive evaluation suite specifically for the Basque language, facilitating NLP research in this low-resource language.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmark_suite"
      ],
      "application_level": "platform",
      "primary_language": "Shell",
      "repo_url": "https://github.com/hitz-zentroa/latxa",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nlp",
        "basque",
        "evaluation",
        "llm"
      ],
      "id": 142
    },
    {
      "name": "C-Eval",
      "one_line_profile": "Comprehensive Chinese evaluation suite for foundation models",
      "detailed_description": "C-Eval is a comprehensive evaluation suite designed to assess the advanced knowledge and reasoning abilities of foundation models in a Chinese context.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmark_suite"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/hkust-nlp/ceval",
      "help_website": [
        "https://cevalbenchmark.com/"
      ],
      "license": "MIT",
      "tags": [
        "llm",
        "benchmark",
        "chinese",
        "evaluation"
      ],
      "id": 143
    },
    {
      "name": "HMOG Dataset",
      "one_line_profile": "Multimodal dataset for evaluating continuous authentication performance",
      "detailed_description": "A multimodal dataset designed for evaluating continuous authentication performance on smartphones, capturing various sensor data to benchmark security models.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "dataset",
        "security_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": null,
      "repo_url": "https://github.com/hmog-dataset/hmog",
      "help_website": [],
      "license": null,
      "tags": [
        "dataset",
        "authentication",
        "biometrics",
        "mobile"
      ],
      "id": 144
    },
    {
      "name": "Lighteval",
      "one_line_profile": "All-in-one toolkit for evaluating LLMs across multiple backends",
      "detailed_description": "A comprehensive library for evaluating Large Language Models (LLMs) on various benchmarks and tasks. It supports multiple backends and provides a unified interface for assessing model performance, making it a critical component in the AI model development and evaluation lifecycle.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/huggingface/lighteval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "evaluation",
        "nlp",
        "benchmarking"
      ],
      "id": 145
    },
    {
      "name": "ChainForge",
      "one_line_profile": "Visual programming environment for battle-testing prompts to LLMs",
      "detailed_description": "An open-source visual environment designed for prompt engineering and evaluation of Large Language Models. It allows researchers to create data flows, test multiple prompts across different models, and visualize results to assess model robustness and output quality.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "prompt_engineering",
        "model_evaluation",
        "visualization"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/ianarawjo/ChainForge",
      "help_website": [
        "https://chainforge.ai"
      ],
      "license": "MIT",
      "tags": [
        "llm",
        "prompt-engineering",
        "visual-programming",
        "evaluation"
      ],
      "id": 146
    },
    {
      "name": "RAVEN",
      "one_line_profile": "Probabilistic risk analysis, validation, and uncertainty quantification framework",
      "detailed_description": "A flexible framework for probabilistic risk analysis, uncertainty quantification, parameter optimization, and model reduction. It is designed to perform parametric and stochastic analysis of complex system codes, widely used in nuclear engineering and other safety-critical scientific domains.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "uncertainty_quantification",
        "risk_analysis",
        "parameter_optimization"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/idaholab/raven",
      "help_website": [
        "https://raven.inl.gov"
      ],
      "license": "Apache-2.0",
      "tags": [
        "uncertainty-quantification",
        "risk-analysis",
        "simulation",
        "model-validation"
      ],
      "id": 147
    },
    {
      "name": "Probatus",
      "one_line_profile": "SHAP-based validation toolkit for linear and tree-based models",
      "detailed_description": "A Python library for validating binary, multiclass, and regression models using SHAP (SHapley Additive exPlanations) values. It provides tools for feature selection, model analysis, and ensuring model robustness in scientific and financial ML applications.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_validation",
        "feature_selection",
        "interpretability"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ing-bank/probatus",
      "help_website": [
        "https://ing-bank.github.io/probatus/"
      ],
      "license": "MIT",
      "tags": [
        "shap",
        "model-validation",
        "machine-learning",
        "feature-selection"
      ],
      "id": 148
    },
    {
      "name": "ReadabilityMetrics",
      "one_line_profile": "Library and service for computing text readability metrics",
      "detailed_description": "A tool that calculates various readability metrics (e.g., ARI, Coleman-Liau, Flesch-Kincaid) for text data. While implemented as a service, the underlying logic serves as a library for linguistic analysis and text quality assessment in NLP research.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "text_analysis",
        "metric_calculation"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/ipeirotis/ReadabilityMetrics",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "nlp",
        "readability",
        "text-analysis",
        "metrics"
      ],
      "id": 149
    },
    {
      "name": "CML",
      "one_line_profile": "Continuous Machine Learning (CML) for CI/CD of ML experiments",
      "detailed_description": "An open-source library for implementing Continuous Integration/Continuous Delivery (CI/CD) in machine learning projects. It enables researchers to automate model training, evaluation, and report generation (e.g., plots, metrics) directly within pull requests, facilitating reproducible research and regression testing.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "experiment_tracking",
        "ci_regression",
        "reproducibility"
      ],
      "application_level": "workflow",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/iterative/cml",
      "help_website": [
        "https://cml.dev"
      ],
      "license": "Apache-2.0",
      "tags": [
        "mlops",
        "ci-cd",
        "reproducibility",
        "experiment-tracking"
      ],
      "id": 150
    },
    {
      "name": "Matbench Discovery",
      "one_line_profile": "Evaluation framework for ML models in high-throughput materials discovery",
      "detailed_description": "A benchmark and evaluation framework designed to simulate high-throughput materials discovery using machine learning models. It assesses the ability of models to predict stable materials and guide experimental synthesis, serving as a critical tool for materials informatics.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "materials_discovery",
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/janosh/matbench-discovery",
      "help_website": [
        "https://matbench-discovery.materialsproject.org"
      ],
      "license": "MIT",
      "tags": [
        "materials-science",
        "machine-learning",
        "benchmarking",
        "discovery"
      ],
      "id": 151
    },
    {
      "name": "Runcharter",
      "one_line_profile": "Automated run chart analysis for faceted data displays",
      "detailed_description": "An R package for automating the creation and analysis of run charts, which are used for quality control and performance monitoring in healthcare and scientific processes. It supports identifying trends and shifts in data across multiple metrics or locations.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "statistical_analysis",
        "quality_control",
        "visualization"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/johnmackintosh/runcharter",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "r",
        "statistics",
        "quality-improvement",
        "visualization"
      ],
      "id": 152
    },
    {
      "name": "Eva",
      "one_line_profile": "Evaluation framework for oncology foundation models",
      "detailed_description": "A specialized framework for evaluating foundation models in the context of oncology. It provides metrics and workflows to assess the performance of AI models on cancer-related tasks, facilitating the development of reliable medical AI tools.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "medical_ai",
        "oncology"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/kaiko-ai/eva",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "oncology",
        "foundation-models",
        "evaluation",
        "medical-imaging"
      ],
      "id": 153
    },
    {
      "name": "MalDataGen",
      "one_line_profile": "Framework for generating and evaluating synthetic tabular datasets",
      "detailed_description": "A Python framework designed for generating synthetic tabular data using advanced generative models (diffusion, adversarial). It includes tools for evaluating the quality and utility of the generated data, useful for privacy-preserving data sharing and ML model training augmentation.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "data_generation",
        "synthetic_data",
        "data_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/kayua/MalDataGen",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "synthetic-data",
        "generative-models",
        "tabular-data",
        "data-augmentation"
      ],
      "id": 154
    },
    {
      "name": "VSDFLOW",
      "one_line_profile": "Automated RTL-to-GDS flow for semiconductor design",
      "detailed_description": "An automated Electronic Design Automation (EDA) toolchain that converts Register Transfer Level (RTL) designs to GDSII layout. It integrates synthesis, placement, routing, and timing analysis tools, enabling hardware engineers and researchers to produce physical chip designs from logic descriptions.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "chip_design",
        "eda",
        "synthesis"
      ],
      "application_level": "workflow",
      "primary_language": "Verilog",
      "repo_url": "https://github.com/kunalg123/vsdflow",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "eda",
        "vlsi",
        "rtl-to-gds",
        "hardware-design"
      ],
      "id": 155
    },
    {
      "name": "Langfuse",
      "one_line_profile": "Open source LLM engineering platform for observability, metrics, and evaluations",
      "detailed_description": "A comprehensive platform for LLM engineering that includes tools for tracing, dataset management, and running evaluations on model outputs. It supports the lifecycle of developing and refining LLM applications through rigorous metrics and observability.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "observability",
        "dataset_management"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/langfuse/langfuse",
      "help_website": [
        "https://langfuse.com/docs"
      ],
      "license": "NOASSERTION",
      "tags": [
        "llm-ops",
        "evaluation",
        "observability",
        "metrics"
      ],
      "id": 156
    },
    {
      "name": "LangWatch",
      "one_line_profile": "LLM Ops platform for analytics, evaluations, and prompt optimization",
      "detailed_description": "A platform designed for monitoring and evaluating Large Language Models (LLMs). It provides capabilities for tracing execution, analyzing performance analytics, managing datasets, and optimizing prompts through systematic evaluation.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "analytics",
        "prompt_optimization"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/langwatch/langwatch",
      "help_website": [
        "https://docs.langwatch.ai"
      ],
      "license": "NOASSERTION",
      "tags": [
        "llm-ops",
        "analytics",
        "evaluation"
      ],
      "id": 157
    },
    {
      "name": "Latitude LLM",
      "one_line_profile": "Open-source prompt engineering and evaluation platform",
      "detailed_description": "A platform focused on the engineering and refinement of prompts for Large Language Models. It allows users to build, evaluate, and iterate on prompts using AI-assisted feedback and performance metrics.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "prompt_engineering",
        "model_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/latitude-dev/latitude-llm",
      "help_website": [
        "https://docs.latitude.so"
      ],
      "license": "LGPL-3.0",
      "tags": [
        "prompt-engineering",
        "evaluation",
        "llm"
      ],
      "id": 158
    },
    {
      "name": "Les Audits Affaires Eval Harness",
      "one_line_profile": "CLI for benchmarking French LLMs in business law tasks",
      "detailed_description": "A lightweight Python command-line interface designed to benchmark and test French Large Language Models specifically on business law tasks, including action determination, deadline analysis, document processing, cost estimation, and risk assessment.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "domain_specific_evaluation",
        "legal_benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/legml-ai/les-audits-affaires-eval-harness",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "legal-ai",
        "benchmark",
        "llm-evaluation",
        "french-nlp"
      ],
      "id": 159
    },
    {
      "name": "BioMonTools",
      "one_line_profile": "R tools for biomonitoring and bioassessment metric calculation",
      "detailed_description": "A suite of tools for calculating metrics related to biomonitoring and bioassessment, specifically for benthic macroinvertebrates, fish, and periphyton. It aids in the analysis of ecological data for environmental health assessment.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "metric_calculation",
        "bioassessment",
        "ecological_analysis"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/leppott/BioMonTools",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "biomonitoring",
        "ecology",
        "r-package",
        "metrics"
      ],
      "id": 160
    },
    {
      "name": "S3Eval",
      "one_line_profile": "Synthetic, Scalable and Systematic Evaluation Suite for LLMs",
      "detailed_description": "An evaluation suite presented at NAACL 2024 designed to assess Large Language Models. It focuses on providing a synthetic, scalable, and systematic approach to benchmarking LLM performance across various tasks.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/lfy79001/S3Eval",
      "help_website": [],
      "license": null,
      "tags": [
        "llm-evaluation",
        "benchmark",
        "nlp"
      ],
      "id": 161
    },
    {
      "name": "LinearityIQA",
      "one_line_profile": "Norm-in-Norm Loss implementation for Image Quality Assessment",
      "detailed_description": "Implementation of the Norm-in-Norm Loss function for Image Quality Assessment (IQA), as presented at ACM MM 2020. It provides a method for evaluating image quality with faster convergence and better performance compared to standard metrics.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "image_quality_assessment",
        "loss_function"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/lidq92/LinearityIQA",
      "help_website": [],
      "license": null,
      "tags": [
        "computer-vision",
        "iqa",
        "image-quality",
        "metric"
      ],
      "id": 162
    },
    {
      "name": "llm-jp-eval-mm",
      "one_line_profile": "Lightweight framework for evaluating visual-language models",
      "detailed_description": "A framework designed for the evaluation of visual-language models (VLMs), specifically tailored for the Japanese language context (implied by organization) but applicable to VLM benchmarking tasks.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "vlm_evaluation",
        "multimodal_benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/llm-jp/llm-jp-eval-mm",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "vlm",
        "multimodal",
        "evaluation",
        "benchmark"
      ],
      "id": 163
    },
    {
      "name": "RouteLLM",
      "one_line_profile": "Framework for serving and evaluating LLM routers",
      "detailed_description": "A framework designed to evaluate and serve Large Language Model (LLM) routers. It enables researchers and developers to assess routing strategies that balance cost and quality by dynamically selecting appropriate models for different queries.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_routing",
        "cost_optimization",
        "evaluation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/lm-sys/RouteLLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-routing",
        "evaluation",
        "optimization"
      ],
      "id": 164
    },
    {
      "name": "Arena-Hard-Auto",
      "one_line_profile": "Automatic benchmark for Large Language Models",
      "detailed_description": "An automated benchmarking tool for Large Language Models, designed to replicate the difficulty and discrimination power of the Chatbot Arena. It provides a pipeline for evaluating models against hard prompts using judge models.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_benchmarking",
        "automated_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/lmarena/arena-hard-auto",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-benchmark",
        "evaluation",
        "arena-hard"
      ],
      "id": 165
    },
    {
      "name": "SWT-Bench",
      "one_line_profile": "Evaluation harness for LLM repository-level test-generation",
      "detailed_description": "The official evaluation harness for SWT-Bench, a benchmark designed to evaluate the capability of Large Language Models in generating repository-level tests. It facilitates the reproduction of results and assessment of new models on this specific software engineering task.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "code_generation_evaluation",
        "test_generation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/logic-star-ai/swt-bench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm-for-code",
        "benchmark",
        "test-generation"
      ],
      "id": 166
    },
    {
      "name": "LEP-Hybrid-Visual-Odometry",
      "one_line_profile": "Real-time monocular Hybrid Visual Odometry system",
      "detailed_description": "A C++ implementation of a hybrid visual odometry formulation that combines indirect (feature-based) and direct methods. It uses lines, edges, and points (LEP) for robust tracking and mapping, serving as a tool for robotics and computer vision research.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "visual_odometry",
        "slam",
        "robotics_navigation"
      ],
      "application_level": "solver",
      "primary_language": "CMake",
      "repo_url": "https://github.com/maazmb/LEP-Hybrid-Visual-Odometry",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "slam",
        "visual-odometry",
        "computer-vision",
        "robotics"
      ],
      "id": 167
    },
    {
      "name": "FixEval",
      "one_line_profile": "Dataset and test suite for competitive programming bug fixing",
      "detailed_description": "A dataset and evaluation framework for automated program repair in the context of competitive programming. It emphasizes execution-based evaluation over match-based metrics to accurately assess bug-fixing capabilities of models.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "program_repair_evaluation",
        "code_generation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/mahimanzum/FixEval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "automated-program-repair",
        "benchmark",
        "dataset"
      ],
      "id": 168
    },
    {
      "name": "llm_eval_suite",
      "one_line_profile": "Tool to evaluate LLMs using Ollama and Hugging Face datasets",
      "detailed_description": "A Python-based utility to facilitate the evaluation of Large Language Models. It integrates with Ollama for model inference and Hugging Face for dataset retrieval, allowing for streamlined performance testing.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "inference_benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/majesticio/llm_eval_suite",
      "help_website": [],
      "license": null,
      "tags": [
        "llm",
        "evaluation",
        "ollama",
        "huggingface"
      ],
      "id": 169
    },
    {
      "name": "saliency-faithfulness-eval",
      "one_line_profile": "Tests to assess attention faithfulness for explainability",
      "detailed_description": "A suite of tests designed to evaluate the faithfulness of saliency maps and attention mechanisms in explainable AI (XAI). It helps researchers verify if the explanations provided by models accurately reflect their decision-making process.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "xai_evaluation",
        "saliency_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/matteo-rizzo/saliency-faithfulness-eval",
      "help_website": [],
      "license": null,
      "tags": [
        "explainable-ai",
        "saliency-maps",
        "evaluation",
        "faithfulness"
      ],
      "id": 170
    },
    {
      "name": "pysaliency",
      "one_line_profile": "Python Framework for Saliency Modeling and Evaluation",
      "detailed_description": "A comprehensive Python framework for modeling visual saliency and evaluating saliency models. It provides tools for handling datasets, computing metrics (AUC, NSS, KL-Div, etc.), and comparing model predictions against ground truth fixations.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "saliency_modeling",
        "metric_calculation",
        "computer_vision"
      ],
      "application_level": "framework",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/matthias-k/pysaliency",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "saliency",
        "computer-vision",
        "evaluation",
        "metrics"
      ],
      "id": 171
    },
    {
      "name": "Evalite",
      "one_line_profile": "TypeScript library for evaluating LLM-powered applications",
      "detailed_description": "A lightweight TypeScript library designed to help developers and researchers evaluate the performance of LLM-powered applications. It provides a structured way to define and run evaluations on model outputs.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "app_testing"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/mattpocock/evalite",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm-evaluation",
        "typescript",
        "testing"
      ],
      "id": 172
    },
    {
      "name": "CVRR-Evaluation-Suite",
      "one_line_profile": "Complex Video Reasoning and Robustness Evaluation Suite for Video-LMMs",
      "detailed_description": "An evaluation suite for assessing Video Large Multimodal Models (Video-LMMs). It focuses on complex reasoning tasks and robustness, providing a benchmark to measure model performance in challenging video understanding scenarios.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "video_lmm_evaluation",
        "multimodal_benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/mbzuai-oryx/CVRR-Evaluation-Suite",
      "help_website": [],
      "license": "CC-BY-4.0",
      "tags": [
        "video-lmm",
        "benchmark",
        "robustness",
        "reasoning"
      ],
      "id": 173
    },
    {
      "name": "Video-ChatGPT",
      "one_line_profile": "Video conversation model and quantitative evaluation benchmarking",
      "detailed_description": "A repository containing the Video-ChatGPT model and a rigorous quantitative evaluation benchmarking framework for video-based conversational models. It enables the assessment of video understanding and conversation generation capabilities.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "video_understanding",
        "model_benchmarking",
        "multimodal_conversation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/mbzuai-oryx/Video-ChatGPT",
      "help_website": [],
      "license": "CC-BY-4.0",
      "tags": [
        "video-chatgpt",
        "benchmark",
        "multimodal",
        "llm"
      ],
      "id": 174
    },
    {
      "name": "HEAL-T",
      "one_line_profile": "PPG-based Heart Rate and IBI estimation method",
      "detailed_description": "A MATLAB and Bash based implementation for estimating Interbeat-interval (IBI) and Heart-rate (HR) from artifactual Blood Volume Pulse (BVP) signals during physical exercise. It serves as a tool for physiological signal processing.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "signal_processing",
        "physiological_metrics"
      ],
      "application_level": "solver",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/meiyor/HEAL-T-AN-EFFICIENT-PPG-BASED-HEART-RATE-AND-IBI-ESTIMATION-METHOD-DURING-PHYSICAL-EXERCISE",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "ppg",
        "heart-rate",
        "signal-processing",
        "biomedical"
      ],
      "id": 175
    },
    {
      "name": "OpenTTDLab",
      "one_line_profile": "Framework for running reproducible experiments using OpenTTD",
      "detailed_description": "A Python framework that interfaces with the OpenTTD game to allow for reproducible experiments. It turns the game into a simulation environment suitable for research in optimization, logistics, and reinforcement learning.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "simulation_environment",
        "reproducible_research"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/michalc/OpenTTDLab",
      "help_website": [],
      "license": "GPL-2.0",
      "tags": [
        "simulation",
        "openttd",
        "experiments",
        "reproducibility"
      ],
      "id": 176
    },
    {
      "name": "MSMARCO-Conversational-Search",
      "one_line_profile": "Dataset and evaluation paradigm for conversational search",
      "detailed_description": "A dataset and evaluation framework designed to study conversational search behavior. It provides artificial sessions and a methodology to evaluate model performance on real user behavior without compromising privacy.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "dataset_creation",
        "conversational_search_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/MSMARCO-Conversational-Search",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "conversational-search",
        "dataset",
        "evaluation",
        "ir"
      ],
      "id": 177
    },
    {
      "name": "Eureka ML Insights",
      "one_line_profile": "Framework for standardizing evaluations of large foundation models",
      "detailed_description": "A framework developed by Microsoft for standardizing the evaluation of large foundation models. It goes beyond single-score reporting to provide deeper insights into model capabilities and performance.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "standardization"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/eureka-ml-insights",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "foundation-models",
        "evaluation",
        "ml-insights"
      ],
      "id": 178
    },
    {
      "name": "PromptBench",
      "one_line_profile": "Unified evaluation framework for large language models",
      "detailed_description": "A comprehensive framework for evaluating Large Language Models (LLMs), specifically focusing on prompt engineering and robustness. It allows researchers to assess how different prompts affect model performance.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "prompt_evaluation",
        "model_benchmarking"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/promptbench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "prompt-engineering",
        "llm",
        "evaluation",
        "robustness"
      ],
      "id": 179
    },
    {
      "name": "Prompty",
      "one_line_profile": "Tool to create, manage, debug, and evaluate LLM prompts",
      "detailed_description": "An asset class and toolset for managing LLM prompts. It facilitates the creation, debugging, and evaluation of prompts, enhancing observability and portability for AI application development.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "prompt_management",
        "prompt_evaluation"
      ],
      "application_level": "tool",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/prompty",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "prompt-engineering",
        "llm-ops",
        "evaluation"
      ],
      "id": 180
    },
    {
      "name": "SPECTRA",
      "one_line_profile": "Spectral framework for evaluation of biomedical AI models",
      "detailed_description": "A framework designed for the evaluation of biomedical AI models using spectral analysis techniques. It provides a specialized methodology for assessing model performance in the biomedical domain.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "biomedical_ai_evaluation",
        "spectral_analysis"
      ],
      "application_level": "framework",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/mims-harvard/SPECTRA",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "biomedical-ai",
        "evaluation",
        "spectral-analysis"
      ],
      "id": 181
    },
    {
      "name": "minerl-wrappers",
      "one_line_profile": "Standardized wrappers for reproducibility in MineRL environment",
      "detailed_description": "A collection of wrappers for the MineRL environment to standardize code and ensure reproducibility in Reinforcement Learning experiments involving Minecraft.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "rl_environment_wrapper",
        "reproducibility"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/minerl-wrappers/minerl-wrappers",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "reinforcement-learning",
        "minerl",
        "reproducibility"
      ],
      "id": 182
    },
    {
      "name": "ModelBench",
      "one_line_profile": "Safety benchmarks for AI models with detailed reporting",
      "detailed_description": "A tool from MLCommons for running safety benchmarks against AI models. It generates detailed reports on model performance regarding safety metrics, facilitating standardized safety assessments.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "ai_safety_benchmarking",
        "model_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/mlcommons/modelbench",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ai-safety",
        "benchmark",
        "mlcommons"
      ],
      "id": 183
    },
    {
      "name": "FSEB",
      "one_line_profile": "Energy-based Monthly Simulation of Land Surface Temperature",
      "detailed_description": "Code for modeling Land Surface Temperature (LST) using Surface Energy Balance (SEB) principles. It allows for the simulation of environmental factors and evapotranspiration at a monthly temporal scale.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "environmental_modeling",
        "surface_energy_balance"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/mnasserimn/FSEB",
      "help_website": [],
      "license": null,
      "tags": [
        "remote-sensing",
        "land-surface-temperature",
        "modeling"
      ],
      "id": 184
    },
    {
      "name": "EvalScope",
      "one_line_profile": "Framework for efficient large model evaluation and benchmarking",
      "detailed_description": "A streamlined and customizable framework for evaluating Large Language Models (LLMs), Vision-Language Models (VLMs), and AIGC models. It supports efficient performance benchmarking and integrates with the ModelScope ecosystem.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/modelscope/evalscope",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-evaluation",
        "benchmark",
        "modelscope"
      ],
      "id": 185
    },
    {
      "name": "Prognostic Algorithm Package",
      "one_line_profile": "Framework for model-based prognostics and remaining useful life computation of engineering systems",
      "detailed_description": "A Python framework for model-based prognostics that provides algorithms for state estimation, prediction, and uncertainty propagation. It allows for the rapid development and comparative study of prognostics solutions for engineering components and systems.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "prognostics",
        "state_estimation",
        "uncertainty_propagation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/nasa/prog_algs",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "nasa",
        "prognostics",
        "engineering-systems"
      ],
      "id": 186
    },
    {
      "name": "Jury",
      "one_line_profile": "Comprehensive evaluation system for Natural Language Processing (NLP) models",
      "detailed_description": "A comprehensive NLP evaluation system that provides a unified interface for various metrics. It simplifies the process of evaluating NLP models by offering a wide range of automated metrics for tasks like translation, generation, and classification.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "nlp_metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/obss/jury",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nlp",
        "evaluation",
        "metrics"
      ],
      "id": 187
    },
    {
      "name": "blurr",
      "one_line_profile": "Library integrating Hugging Face Transformers with fastai for deep learning workflows",
      "detailed_description": "A library that integrates Hugging Face Transformers with the fastai framework, providing developers with tools to train, evaluate, and deploy transformer-specific models using fastai's API and best practices.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_training",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/ohmeow/blurr",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "fastai",
        "transformers",
        "nlp"
      ],
      "id": 188
    },
    {
      "name": "GenAIEval",
      "one_line_profile": "Evaluation benchmark and scorecard for Generative AI performance and safety",
      "detailed_description": "A framework for evaluating Generative AI models, targeting performance metrics such as throughput and latency, as well as accuracy on popular evaluation harnesses, safety, and hallucination detection.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "safety_check",
        "performance_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/opea-project/GenAIEval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "genai",
        "benchmark",
        "safety"
      ],
      "id": 189
    },
    {
      "name": "VLMEvalKit",
      "one_line_profile": "Open-source evaluation toolkit for large multi-modality models (LMMs)",
      "detailed_description": "An evaluation toolkit designed for large multi-modality models, supporting over 220 LMMs and 80+ benchmarks. It facilitates the assessment of model performance across various multimodal tasks.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "multimodal_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/open-compass/VLMEvalKit",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "vlm",
        "multimodal",
        "benchmark"
      ],
      "id": 190
    },
    {
      "name": "OpenCompass",
      "one_line_profile": "Comprehensive evaluation platform for Large Language Models (LLMs)",
      "detailed_description": "An LLM evaluation platform that supports a wide range of models (including Llama3, GPT-4, Claude) and over 100 datasets. It provides a unified framework for benchmarking the capabilities of large language models.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "llm_benchmarking"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/open-compass/opencompass",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "evaluation",
        "benchmark"
      ],
      "id": 191
    },
    {
      "name": "Evals",
      "one_line_profile": "Framework for evaluating LLMs and an open-source registry of benchmarks",
      "detailed_description": "A framework developed by OpenAI for evaluating Large Language Models (LLMs) and LLM systems. It includes an open-source registry of benchmarks to test model capabilities and ensure quality.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmark_registry"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/openai/evals",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "llm",
        "openai",
        "benchmark"
      ],
      "id": 192
    },
    {
      "name": "OpenLIT",
      "one_line_profile": "OpenTelemetry-native observability and evaluation platform for LLMs and GPUs",
      "detailed_description": "An open-source platform for AI engineering that provides OpenTelemetry-native observability, GPU monitoring, guardrails, and evaluations for LLMs. It integrates with various LLM providers and vector databases.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "observability",
        "monitoring"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/openlit/openlit",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "observability",
        "llm",
        "opentelemetry"
      ],
      "id": 193
    },
    {
      "name": "Oumi",
      "one_line_profile": "Platform to fine-tune, evaluate, and deploy open source LLMs and VLMs",
      "detailed_description": "A platform designed to simplify the fine-tuning, evaluation, and deployment of open-source Large Language Models (LLMs) and Vision Language Models (VLMs), supporting models like Llama, Qwen, and DeepSeek.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_finetuning",
        "model_evaluation",
        "deployment"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/oumi-ai/oumi",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "vlm",
        "fine-tuning"
      ],
      "id": 194
    },
    {
      "name": "pChem",
      "one_line_profile": "Modification-centric assessment tool for chemoproteomic probe performance",
      "detailed_description": "A tool for the assessment of chemoproteomic probes, focusing on modification-centric performance evaluation. It aids in the analysis of probe efficiency and specificity in proteomic studies.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "chemoproteomics",
        "probe_assessment",
        "data_analysis"
      ],
      "application_level": "tool",
      "primary_language": "Python",
      "repo_url": "https://github.com/pFindStudio/pChem",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "proteomics",
        "chemistry",
        "bioinformatics"
      ],
      "id": 195
    },
    {
      "name": "Japanese LM Financial Evaluation Harness",
      "one_line_profile": "Evaluation harness for Japanese language models in the financial domain",
      "detailed_description": "A specialized evaluation harness designed for assessing the performance of Japanese Language Models within the financial domain. It provides benchmarks and metrics tailored to financial texts and tasks.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "domain_specific_benchmarking"
      ],
      "application_level": "harness",
      "primary_language": "Shell",
      "repo_url": "https://github.com/pfnet-research/japanese-lm-fin-harness",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nlp",
        "finance",
        "japanese"
      ],
      "id": 196
    },
    {
      "name": "OpenCGRA",
      "one_line_profile": "Framework for modeling, testing, and evaluating Coarse-Grained Reconfigurable Architectures",
      "detailed_description": "An open-source framework developed by PNNL for modeling, testing, and evaluating Coarse-Grained Reconfigurable Architectures (CGRAs). It supports architectural exploration and performance assessment.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "architecture_modeling",
        "simulation",
        "evaluation"
      ],
      "application_level": "framework",
      "primary_language": "Verilog",
      "repo_url": "https://github.com/pnnl/OpenCGRA",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "cgra",
        "computer-architecture",
        "modeling"
      ],
      "id": 197
    },
    {
      "name": "InstructEval",
      "one_line_profile": "Evaluation suite for instruction selection methods in NLP",
      "detailed_description": "A systematic evaluation suite designed to assess instruction selection methods for Large Language Models (LLMs), supporting the reproduction of findings from NAACL 2024.",
      "domains": [
        "AI4",
        "AI4-03",
        "Natural Language Processing"
      ],
      "subtask_category": [
        "model_evaluation",
        "instruction_tuning_assessment"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/princeton-nlp/InstructEval",
      "help_website": [],
      "license": null,
      "tags": [
        "nlp",
        "llm-evaluation",
        "instruction-following",
        "benchmark"
      ],
      "id": 198
    },
    {
      "name": "skore",
      "one_line_profile": "Python library for ML model evaluation and reporting",
      "detailed_description": "An open-source library that accelerates machine learning model development by providing automated evaluation reports, methodological guidance, and cross-validation analysis.",
      "domains": [
        "AI4",
        "AI4-03",
        "Machine Learning"
      ],
      "subtask_category": [
        "model_evaluation",
        "reporting",
        "cross_validation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/probabl-ai/skore",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "machine-learning",
        "evaluation-metrics",
        "data-science",
        "reporting"
      ],
      "id": 199
    },
    {
      "name": "etalon",
      "one_line_profile": "Performance evaluation harness for LLM serving systems",
      "detailed_description": "A harness designed to benchmark and evaluate the performance (latency, throughput) of Large Language Model serving systems.",
      "domains": [
        "AI4",
        "AI4-03",
        "Machine Learning Systems"
      ],
      "subtask_category": [
        "performance_benchmarking",
        "llm_serving"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/project-etalon/etalon",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "benchmarking",
        "performance",
        "serving"
      ],
      "id": 200
    },
    {
      "name": "prometheus-eval",
      "one_line_profile": "LLM response evaluation tool using Prometheus and GPT-4",
      "detailed_description": "A library for evaluating Large Language Model responses, leveraging Prometheus models and GPT-4 as judges for automated assessment.",
      "domains": [
        "AI4",
        "AI4-03",
        "Natural Language Processing"
      ],
      "subtask_category": [
        "llm_evaluation",
        "automated_grading"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/prometheus-eval/prometheus-eval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "evaluation",
        "prometheus",
        "gpt-4"
      ],
      "id": 201
    },
    {
      "name": "promptfoo",
      "one_line_profile": "CLI tool for testing and evaluating LLM prompts and agents",
      "detailed_description": "A tool for testing prompts, agents, and RAG systems, supporting red teaming, vulnerability scanning, and performance comparison across multiple LLM providers.",
      "domains": [
        "AI4",
        "AI4-03",
        "Natural Language Processing"
      ],
      "subtask_category": [
        "prompt_engineering",
        "model_evaluation",
        "red_teaming"
      ],
      "application_level": "solver",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/promptfoo/promptfoo",
      "help_website": [
        "https://www.promptfoo.dev"
      ],
      "license": "MIT",
      "tags": [
        "llm",
        "testing",
        "prompt-engineering",
        "red-teaming"
      ],
      "id": 202
    },
    {
      "name": "epm",
      "one_line_profile": "R package for eco-phylogenetic metrics calculation",
      "detailed_description": "An R package designed for calculating taxonomic, phenotypic, and phylogenetic metrics across spatial grid cells for ecological research.",
      "domains": [
        "AI4",
        "AI4-03",
        "Ecology",
        "Phylogenetics"
      ],
      "subtask_category": [
        "metric_calculation",
        "spatial_analysis",
        "biodiversity_metrics"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/ptitle/epm",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "ecology",
        "phylogenetics",
        "spatial-analysis",
        "r-package"
      ],
      "id": 203
    },
    {
      "name": "speciesRaster",
      "one_line_profile": "R package for species indexing and community metric calculation",
      "detailed_description": "A tool for indexing species by grid cell and calculating community ecology metrics, serving as a companion to spatial ecological analysis.",
      "domains": [
        "AI4",
        "AI4-03",
        "Ecology"
      ],
      "subtask_category": [
        "community_ecology",
        "metric_calculation"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/ptitle/speciesRaster",
      "help_website": [],
      "license": null,
      "tags": [
        "ecology",
        "biodiversity",
        "r-package"
      ],
      "id": 204
    },
    {
      "name": "aeromancy",
      "one_line_profile": "Framework for reproducible AI and ML experiments",
      "detailed_description": "A framework designed to facilitate reproducible artificial intelligence and machine learning workflows, likely with a focus on environmental or atmospheric sciences given the organization name (quant-aq).",
      "domains": [
        "AI4",
        "AI4-03",
        "Machine Learning"
      ],
      "subtask_category": [
        "reproducibility",
        "experiment_tracking"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/quant-aq/aeromancy",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "reproducibility",
        "machine-learning",
        "framework"
      ],
      "id": 205
    },
    {
      "name": "RagaAI-Catalyst",
      "one_line_profile": "SDK for Agent AI observability and evaluation",
      "detailed_description": "A Python SDK providing a framework for observability, monitoring, and evaluation of AI agents, including tracing and debugging capabilities for multi-agent systems.",
      "domains": [
        "AI4",
        "AI4-03",
        "Artificial Intelligence"
      ],
      "subtask_category": [
        "agent_evaluation",
        "observability",
        "monitoring"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/raga-ai-hub/RagaAI-Catalyst",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ai-agents",
        "observability",
        "evaluation",
        "sdk"
      ],
      "id": 206
    },
    {
      "name": "crossfit",
      "one_line_profile": "GPU-accelerated metric calculation library",
      "detailed_description": "A library from RAPIDS AI for calculating metrics, optimized for GPU execution, typically used in recommender systems or ranking tasks.",
      "domains": [
        "AI4",
        "AI4-03",
        "Data Science"
      ],
      "subtask_category": [
        "metric_calculation",
        "gpu_acceleration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/rapidsai/crossfit",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rapids",
        "gpu",
        "metrics",
        "ranking"
      ],
      "id": 207
    },
    {
      "name": "tensorflow-qnd",
      "one_line_profile": "Command framework for TensorFlow model training and evaluation",
      "detailed_description": "A framework to simplify the creation of TensorFlow commands for training, evaluating, and inferencing with machine learning models.",
      "domains": [
        "AI4",
        "AI4-03",
        "Machine Learning"
      ],
      "subtask_category": [
        "model_training",
        "model_evaluation"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/raviqqe/tensorflow-qnd",
      "help_website": [],
      "license": "Unlicense",
      "tags": [
        "tensorflow",
        "cli",
        "training",
        "inference"
      ],
      "id": 208
    },
    {
      "name": "continuous-eval",
      "one_line_profile": "Data-driven evaluation framework for LLM applications",
      "detailed_description": "A framework for the continuous, data-driven evaluation of Large Language Model (LLM) powered applications, focusing on pipeline performance.",
      "domains": [
        "AI4",
        "AI4-03",
        "Natural Language Processing"
      ],
      "subtask_category": [
        "llm_evaluation",
        "pipeline_assessment"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/relari-ai/continuous-eval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "evaluation",
        "rag",
        "metrics"
      ],
      "id": 209
    },
    {
      "name": "auto-evaluator",
      "one_line_profile": "Evaluation tool for LLM QA chains",
      "detailed_description": "A tool designed to evaluate Question Answering (QA) chains built with Large Language Models, automating the assessment of response quality.",
      "domains": [
        "AI4",
        "AI4-03",
        "Natural Language Processing"
      ],
      "subtask_category": [
        "qa_evaluation",
        "llm_assessment"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/rlancemartin/auto-evaluator",
      "help_website": [],
      "license": null,
      "tags": [
        "llm",
        "qa",
        "evaluation",
        "langchain"
      ],
      "id": 210
    },
    {
      "name": "nanopore_assembly_and_polishing_assessment",
      "one_line_profile": "Pipeline for assessing Nanopore assembly and polishing",
      "detailed_description": "Automated pipelines for evaluating the performance of genome assembly and polishing tools specifically for Nanopore sequencing data, including visualization.",
      "domains": [
        "AI4",
        "AI4-03",
        "Bioinformatics",
        "Genomics"
      ],
      "subtask_category": [
        "assembly_evaluation",
        "quality_assessment",
        "nanopore_sequencing"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/rlorigro/nanopore_assembly_and_polishing_assessment",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "genomics",
        "nanopore",
        "assembly",
        "evaluation"
      ],
      "id": 211
    },
    {
      "name": "TestSuiteEval",
      "one_line_profile": "Semantic evaluation tool for Text-to-SQL models",
      "detailed_description": "Implements the semantic evaluation method for Text-to-SQL tasks using a distilled test suite, as proposed in EMNLP 2020.",
      "domains": [
        "AI4",
        "AI4-03",
        "Natural Language Processing"
      ],
      "subtask_category": [
        "model_evaluation",
        "text_to_sql",
        "semantic_parsing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ruiqi-zhong/TestSuiteEval",
      "help_website": [],
      "license": null,
      "tags": [
        "nlp",
        "text-to-sql",
        "evaluation",
        "emnlp"
      ],
      "id": 212
    },
    {
      "name": "RePlay",
      "one_line_profile": "Framework for building and evaluating recommendation systems",
      "detailed_description": "A comprehensive library for building, training, and evaluating recommendation systems, including state-of-the-art models and evaluation metrics.",
      "domains": [
        "AI4",
        "AI4-03",
        "Recommender Systems"
      ],
      "subtask_category": [
        "model_evaluation",
        "recommendation",
        "benchmarking"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/sb-ai-lab/RePlay",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "recsys",
        "evaluation",
        "machine-learning",
        "framework"
      ],
      "id": 213
    },
    {
      "name": "auto_reports",
      "one_line_profile": "Dashboard for ocean model skill assessment",
      "detailed_description": "An interactive dashboard application for assessing the skill of oceanographic models, specifically for tidal analysis and regional performance evaluation.",
      "domains": [
        "AI4",
        "AI4-03",
        "Oceanography"
      ],
      "subtask_category": [
        "model_assessment",
        "tidal_analysis",
        "visualization"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/seareport/auto_reports",
      "help_website": [],
      "license": "EUPL-1.2",
      "tags": [
        "oceanography",
        "model-evaluation",
        "dashboard",
        "tides"
      ],
      "id": 214
    },
    {
      "name": "bjontegaard2",
      "one_line_profile": "Calculation of Bjontegaard metrics for video coding",
      "detailed_description": "A MATLAB implementation for calculating Bjontegaard metrics (BD-Rate), a standard method for evaluating the coding efficiency of video compression algorithms.",
      "domains": [
        "AI4",
        "AI4-03",
        "Signal Processing",
        "Video Compression"
      ],
      "subtask_category": [
        "metric_calculation",
        "performance_evaluation"
      ],
      "application_level": "library",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/serge-m/bjontegaard2",
      "help_website": [],
      "license": null,
      "tags": [
        "video-coding",
        "metrics",
        "matlab",
        "compression"
      ],
      "id": 215
    },
    {
      "name": "gnn-benchmark",
      "one_line_profile": "Framework for evaluating Graph Neural Networks on semi-supervised classification",
      "detailed_description": "A Python framework designed to provide a standardized evaluation setup for Graph Neural Networks (GNNs), specifically focusing on semi-supervised node classification tasks to ensure fair and reproducible comparisons.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/shchur/gnn-benchmark",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gnn",
        "graph-neural-networks",
        "benchmarking"
      ],
      "id": 216
    },
    {
      "name": "SigOpt EvalSet",
      "one_line_profile": "Benchmark suite of test functions for black-box optimization strategies",
      "detailed_description": "A collection of test functions and benchmarks designed to evaluate and compare the performance of various black-box optimization algorithms, facilitating research in hyperparameter tuning and optimization.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "optimization_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/sigopt/evalset",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "optimization",
        "black-box",
        "benchmarking"
      ],
      "id": 217
    },
    {
      "name": "pg_landmetrics",
      "one_line_profile": "PostgreSQL extension for landscape metrics calculations",
      "detailed_description": "A PostGIS extension that enables the calculation of landscape metrics directly within a database environment, supporting spatial analysis for ecology and geography.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "spatial_analysis",
        "landscape_metrics"
      ],
      "application_level": "library",
      "primary_language": "TSQL",
      "repo_url": "https://github.com/siose-innova/pg_landmetrics",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "gis",
        "landscape-ecology",
        "postgis"
      ],
      "id": 218
    },
    {
      "name": "ZSC-Eval",
      "one_line_profile": "Evaluation toolkit for Multi-agent Zero-shot Coordination",
      "detailed_description": "A benchmark and evaluation toolkit designed for Multi-Agent Reinforcement Learning (MARL), specifically focusing on the zero-shot coordination capabilities of agents.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "reinforcement_learning_evaluation",
        "multi_agent_systems"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/sjtu-marl/ZSC-Eval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "marl",
        "zero-shot",
        "benchmarking"
      ],
      "id": 219
    },
    {
      "name": "dismay",
      "one_line_profile": "R package for calculating distance metrics on matrices",
      "detailed_description": "An R library providing efficient implementations for calculating various distance metrics on matrices, commonly used in statistical analysis and bioinformatics.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "statistical_analysis",
        "distance_metrics"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/skinnider/dismay",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "r-package",
        "statistics",
        "matrices"
      ],
      "id": 220
    },
    {
      "name": "SpeechLLM",
      "one_line_profile": "Training and evaluation framework for SpeechLLM models",
      "detailed_description": "A comprehensive repository containing code for training, inference, and evaluation of Speech Large Language Models, facilitating research in multimodal speech-text processing.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "speech_processing"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/skit-ai/SpeechLLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "speech-llm",
        "evaluation",
        "multimodal"
      ],
      "id": 221
    },
    {
      "name": "HELM",
      "one_line_profile": "Holistic Evaluation of Language Models framework",
      "detailed_description": "A comprehensive framework from Stanford CRFM for the holistic, reproducible, and transparent evaluation of foundation models, covering a wide range of metrics and scenarios.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "llm_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/stanford-crfm/helm",
      "help_website": [
        "https://crfm.stanford.edu/helm/latest/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "evaluation",
        "foundation-models"
      ],
      "id": 222
    },
    {
      "name": "Open Supply Chains",
      "one_line_profile": "Tool for modeling and analyzing supply chain sustainability metrics",
      "detailed_description": "An open-source codebase for visualizing and analyzing supply chains, including modules for calculating evaluation metrics such as carbon footprints, supporting research in sustainable operations.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "sustainability_analysis",
        "supply_chain_modeling"
      ],
      "application_level": "platform",
      "primary_language": "PHP",
      "repo_url": "https://github.com/supplychainstudies/OpenSupplyChains",
      "help_website": [],
      "license": null,
      "tags": [
        "supply-chain",
        "sustainability",
        "carbon-footprint"
      ],
      "id": 223
    },
    {
      "name": "BIG-Bench Hard",
      "one_line_profile": "Challenging subset of BIG-Bench tasks for LLM evaluation",
      "detailed_description": "A benchmark suite focusing on tasks from BIG-Bench where language models previously struggled, used to evaluate the reasoning capabilities of Chain-of-Thought prompting.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "llm_evaluation",
        "reasoning_benchmark"
      ],
      "application_level": "dataset",
      "primary_language": null,
      "repo_url": "https://github.com/suzgunmirac/BIG-Bench-Hard",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "benchmark",
        "chain-of-thought"
      ],
      "id": 224
    },
    {
      "name": "Test Suite SQL Eval",
      "one_line_profile": "Semantic evaluation harness for Text-to-SQL models",
      "detailed_description": "A testing suite and evaluation framework for Text-to-SQL systems, using distilled test suites to provide semantic correctness evaluation beyond simple string matching.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "text_to_sql"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/taoyds/test-suite-sql-eval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "nlp",
        "sql",
        "evaluation"
      ],
      "id": 225
    },
    {
      "name": "AlpacaEval",
      "one_line_profile": "Automatic evaluator for instruction-following language models",
      "detailed_description": "An LLM-based automatic evaluation framework designed to simulate human evaluation of instruction-following models, providing a fast and low-cost alternative to human annotation.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "llm_evaluation",
        "instruction_following"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/tatsu-lab/alpaca_eval",
      "help_website": [
        "https://tatsu-lab.github.io/alpaca_eval/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "evaluation",
        "automation"
      ],
      "id": 226
    },
    {
      "name": "TensorZero",
      "one_line_profile": "Unified stack for LLM engineering, optimization, and evaluation",
      "detailed_description": "An industrial-grade platform for building LLM applications that integrates gateway services with observability, optimization, and evaluation workflows to improve model performance.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "llmops",
        "model_evaluation",
        "optimization"
      ],
      "application_level": "platform",
      "primary_language": "Rust",
      "repo_url": "https://github.com/tensorzero/tensorzero",
      "help_website": [
        "https://www.tensorzero.com/docs"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llmops",
        "evaluation",
        "gateway"
      ],
      "id": 227
    },
    {
      "name": "Safety-Prompts",
      "one_line_profile": "Chinese safety prompts for evaluating LLM safety",
      "detailed_description": "A curated dataset of Chinese safety prompts designed to evaluate and improve the safety alignment of Large Language Models, covering various risk categories.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "safety_evaluation",
        "alignment"
      ],
      "application_level": "dataset",
      "primary_language": null,
      "repo_url": "https://github.com/thu-coai/Safety-Prompts",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-safety",
        "evaluation",
        "chinese-nlp"
      ],
      "id": 228
    },
    {
      "name": "TransformerLab",
      "one_line_profile": "Application for training, fine-tuning, and evaluating LLMs",
      "detailed_description": "An open-source desktop application that provides a GUI for interacting with, training, fine-tuning, and evaluating Large Language Models and Diffusion models locally.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "fine_tuning",
        "experiment_management"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/transformerlab/transformerlab-app",
      "help_website": [
        "https://transformerlab.ai/"
      ],
      "license": "AGPL-3.0",
      "tags": [
        "llm",
        "gui",
        "evaluation"
      ],
      "id": 229
    },
    {
      "name": "TruLens",
      "one_line_profile": "Evaluation and tracking library for LLM experiments",
      "detailed_description": "A software tool for evaluating and tracking the performance of Large Language Model applications, providing feedback functions to assess relevance, groundedness, and other quality metrics.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "llm_evaluation",
        "experiment_tracking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/truera/trulens",
      "help_website": [
        "https://www.trulens.org/"
      ],
      "license": "MIT",
      "tags": [
        "llm",
        "evaluation",
        "observability"
      ],
      "id": 230
    },
    {
      "name": "hlb-gpt",
      "one_line_profile": "Minimalistic and fast researcher's toolbench for training and evaluating GPT models",
      "detailed_description": "A hackable and highly optimized toolbench designed for researchers to train and evaluate GPT-style models. It features extremely fast training speeds (e.g., <100 seconds for wikitext-103 on A100) and a minimalistic codebase to facilitate experimentation and scaling.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_training",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/tysam-code/hlb-gpt",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "gpt",
        "llm-training",
        "research-toolbench",
        "optimization"
      ],
      "id": 231
    },
    {
      "name": "Petastorm",
      "one_line_profile": "Data access library for deep learning training and evaluation from Parquet datasets",
      "detailed_description": "Petastorm is an open-source data access library that enables single-machine or distributed training and evaluation of deep learning models directly from Apache Parquet datasets. It supports major ML frameworks like TensorFlow, PyTorch, and PySpark.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "data_loading",
        "data_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/uber/petastorm",
      "help_website": [
        "https://petastorm.readthedocs.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "parquet",
        "data-loader",
        "deep-learning",
        "distributed-training"
      ],
      "id": 232
    },
    {
      "name": "aibench-llm-endpoints",
      "one_line_profile": "Metric collection runner for LLM inference endpoints",
      "detailed_description": "A runner tool designed to collect and report performance metrics from various LLM inference endpoints. It serves as a component of the Unify Hub ecosystem for benchmarking and monitoring LLM service quality.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "inference_benchmarking",
        "metric_collection"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/unifyai/aibench-llm-endpoints",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-inference",
        "benchmarking",
        "metrics",
        "unify-hub"
      ],
      "id": 233
    },
    {
      "name": "Gale",
      "one_line_profile": "PyTorch framework for reproducible deep learning experiments",
      "detailed_description": "Gale is a framework built on PyTorch designed to facilitate reproducible deep learning experiments. It provides structure and utilities to standardize experimental workflows, ensuring consistency in model training and evaluation.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "experiment_management",
        "reproducibility"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/v7labs/Gale",
      "help_website": [],
      "license": "LGPL-3.0",
      "tags": [
        "pytorch",
        "reproducibility",
        "deep-learning",
        "experiment-framework"
      ],
      "id": 234
    },
    {
      "name": "whisper-finetune",
      "one_line_profile": "Tool for fine-tuning and evaluating Whisper ASR models",
      "detailed_description": "A comprehensive tool for fine-tuning OpenAI's Whisper models on custom or Hugging Face datasets. It includes functionality for evaluating the performance of fine-tuned models for Automatic Speech Recognition (ASR) tasks.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_finetuning",
        "model_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/vasistalodagala/whisper-finetune",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "whisper",
        "asr",
        "fine-tuning",
        "speech-recognition"
      ],
      "id": 235
    },
    {
      "name": "RC-FIAP",
      "one_line_profile": "Virtual platform for seismic vulnerability assessment of reinforced concrete frames",
      "detailed_description": "RC-FIAP is an open virtual platform based on OpenSeesPy for evaluating the seismic vulnerability of reinforced concrete frame archetypes. It automates the design, nonlinear modeling, and pushover analysis to assess structural performance and risk.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "simulation",
        "risk_assessment"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/vfceball/RC-FIAP",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "seismic-analysis",
        "structural-engineering",
        "opensees",
        "simulation"
      ],
      "id": 236
    },
    {
      "name": "Ragas",
      "one_line_profile": "Framework for evaluating Retrieval Augmented Generation (RAG) pipelines",
      "detailed_description": "Ragas is a framework designed to evaluate Retrieval Augmented Generation (RAG) pipelines. It provides a suite of metrics to assess the retrieval and generation components of LLM applications, helping developers optimize their RAG systems.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "rag_evaluation",
        "metric_calculation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/vibrantlabsai/ragas",
      "help_website": [
        "https://docs.ragas.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "evaluation",
        "llm",
        "metrics"
      ],
      "id": 237
    },
    {
      "name": "Tiny QA Benchmark++",
      "one_line_profile": "Micro-benchmark suite and CI-ready eval harness for LLM smoke-testing",
      "detailed_description": "A lightweight benchmark suite and evaluation harness designed for fast smoke-testing and regression detection in Large Language Models. It includes a generator CLI and supports on-demand multilingual synthetic packs.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "benchmark",
        "regression_testing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/vincentkoc/tiny_qa_benchmark_pp",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-benchmark",
        "regression-testing",
        "ci-cd",
        "qa"
      ],
      "id": 238
    },
    {
      "name": "caption-eval",
      "one_line_profile": "Automated metric evaluation tool for image captions",
      "detailed_description": "A Python-based tool for evaluating sentence and image captions using standard automated metrics such as BLEU, METEOR, ROUGE, CIDEr, and SPICE. It facilitates the quantitative assessment of captioning models.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "caption_evaluation",
        "metric_calculation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/vsubhashini/caption-eval",
      "help_website": [],
      "license": null,
      "tags": [
        "nlp-evaluation",
        "image-captioning",
        "metrics",
        "bleu"
      ],
      "id": 239
    },
    {
      "name": "BVQA_Benchmark",
      "one_line_profile": "Benchmark for Blind Video Quality Assessment on User Generated Content",
      "detailed_description": "A resource list and performance benchmark for Blind Video Quality Assessment (BVQA) models, specifically targeting User Generated Content (UGC) datasets. It supports the evaluation and comparison of video quality assessment algorithms.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "video_quality_assessment",
        "benchmark"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/vztu/BVQA_Benchmark",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "video-quality",
        "bvqa",
        "benchmark",
        "ugc"
      ],
      "id": 240
    },
    {
      "name": "PhaseLLM",
      "one_line_profile": "Framework for Large Language Model evaluation and workflow management",
      "detailed_description": "PhaseLLM is a framework designed to streamline the evaluation and workflow creation for Large Language Models (LLMs). It provides tools for building robust LLM applications and assessing their performance.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "llm_evaluation",
        "workflow_management"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/wgryc/phasellm",
      "help_website": [
        "https://phasellm.com"
      ],
      "license": "MIT",
      "tags": [
        "llm",
        "evaluation",
        "workflow",
        "agents"
      ],
      "id": 241
    },
    {
      "name": "RMBench-2022",
      "one_line_profile": "Benchmark for robotic manipulation reinforcement learning",
      "detailed_description": "RMBench is a benchmark suite for robotic manipulation tasks involving high-dimensional continuous action and state spaces. It evaluates reinforcement learning algorithms that use pixel-based observations.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "robotics_benchmark",
        "reinforcement_learning"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/xiangyanfei212/RMBench-2022",
      "help_website": [],
      "license": null,
      "tags": [
        "robotics",
        "reinforcement-learning",
        "benchmark",
        "manipulation"
      ],
      "id": 242
    },
    {
      "name": "thoughtful-agents",
      "one_line_profile": "Framework for building and evaluating proactive LLM agents",
      "detailed_description": "A Python framework for constructing proactive LLM agents that simulate human-like cognitive processes. It enables agents to generate and evaluate thoughts in parallel with conversations, facilitating the development of more autonomous AI systems.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "agent_framework",
        "cognitive_simulation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/xybruceliu/thoughtful-agents",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-agents",
        "cognitive-architecture",
        "proactive-agents"
      ],
      "id": 243
    },
    {
      "name": "RaLLe",
      "one_line_profile": "Framework for developing and evaluating Retrieval-Augmented LLMs",
      "detailed_description": "RaLLe is a framework specifically designed for the development and evaluation of Retrieval-Augmented Large Language Models (RAG). It provides tools to assess the effectiveness of retrieval strategies and generation quality.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "rag_evaluation",
        "retrieval_augmented_generation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/yhoshi3/RaLLe",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "llm",
        "evaluation",
        "retrieval"
      ],
      "id": 244
    },
    {
      "name": "EasyLM",
      "one_line_profile": "JAX/Flax solution for pre-training, fine-tuning, and evaluating LLMs",
      "detailed_description": "EasyLM is a comprehensive library for Large Language Models (LLMs) built on JAX/Flax. It provides a one-stop solution for pre-training, fine-tuning, evaluating, and serving models, designed to be scalable and easy to use.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_training",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/young-geng/EasyLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "jax",
        "flax",
        "llm",
        "training",
        "evaluation"
      ],
      "id": 245
    },
    {
      "name": "AC-EVAL",
      "one_line_profile": "Ancient Chinese evaluation suite for Large Language Models",
      "detailed_description": "AC-EVAL is a specialized benchmark suite designed to evaluate the performance of Large Language Models (LLMs) on tasks involving Ancient Chinese. It serves as a resource for assessing model capabilities in historical language understanding.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "benchmark",
        "nlp_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/yuting-wei/AC-EVAL",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ancient-chinese",
        "llm-benchmark",
        "nlp"
      ],
      "id": 246
    },
    {
      "name": "fastmri-reproducible-benchmark",
      "one_line_profile": "Reproducible benchmark methods for MRI reconstruction on fastMRI dataset",
      "detailed_description": "A repository providing reproducible methods and benchmarks for MRI reconstruction using the fastMRI dataset. It includes implementations of models like XPDNet to facilitate comparison and advancement in medical image reconstruction.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "mri_reconstruction",
        "benchmark"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/zaccharieramzi/fastmri-reproducible-benchmark",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mri-reconstruction",
        "fastmri",
        "medical-imaging",
        "benchmark"
      ],
      "id": 247
    },
    {
      "name": "ZeroEntropy Evals",
      "one_line_profile": "Evaluation suite for benchmarking retrievers and rerankers",
      "detailed_description": "An evaluation suite developed by ZeroEntropy to benchmark the performance of various information retrieval components, specifically retrievers and rerankers. It aids in selecting optimal components for search and RAG systems.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "retrieval_benchmarking",
        "reranker_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/zeroentropy-ai/evals",
      "help_website": [],
      "license": null,
      "tags": [
        "retrieval",
        "reranking",
        "benchmark",
        "search"
      ],
      "id": 248
    }
  ]
}
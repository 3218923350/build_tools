{
  "generated_at": "2025-12-16T01:40:46.122272+08:00",
  "metadata": {
    "leaf_cluster": {
      "leaf_cluster_id": "D1",
      "leaf_cluster_name": "科研数据格式/解析/转换生态",
      "domain": "Data/Workflow",
      "typical_objects": "domain formats",
      "task_chain": "解析→转换→校验→ETL→版本",
      "tool_form": "解析器 + 验证器 + ETL"
    },
    "unit": {
      "unit_id": "D1-01",
      "unit_name": "通用科研格式解析（JSON/YAML/CSV等）",
      "target_scale": "150–300",
      "coverage_tools": "parsers、validators"
    },
    "search": {
      "target_candidates": 300,
      "queries": [
        "[GH] ruamel.yaml",
        "[GH] Marshmallow",
        "[GH] jsonschema",
        "[GH] ujson",
        "[GH] PyYAML",
        "[GH] csvkit",
        "[GH] jq",
        "[GH] Pydantic",
        "[GH] Polars",
        "[GH] Pandas",
        "[GH] csv parser",
        "[GH] json parser",
        "[GH] yaml parser",
        "[GH] toml config",
        "[GH] xml parser",
        "[GH] json schema validator",
        "[GH] data serialization",
        "[GH] fast csv",
        "[GH] config loader",
        "[GH] structured data parsing",
        "[GH] data converter",
        "[GH] csv to json",
        "[GH] schema validation",
        "[GH] data marshalling",
        "[WEB] fast csv parser library github",
        "[WEB] json schema validator tools github",
        "[WEB] yaml configuration loader github",
        "[WEB] python data parsing libraries github",
        "[WEB] high performance csv reader github",
        "[WEB] structured data validation github"
      ],
      "total_candidates": 1340,
      "tool_candidates": 1127,
      "final_tools": 145
    }
  },
  "tools": [
    {
      "name": "telemetry-parser",
      "one_line_profile": "Parser for real-time sensor telemetry metadata from video files and flight logs",
      "detailed_description": "A tool to extract and parse real-time physical measurements (gyroscope, accelerometer, GPS) embedded in video files (GoPro GPMF, Sony, Insta360) and flight logs (Betaflight). This data is essential for drone dynamics analysis, SLAM research, and biomechanics.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_extraction",
        "sensor_processing"
      ],
      "application_level": "solver",
      "primary_language": "Rust",
      "repo_url": "https://github.com/AdrianEddy/telemetry-parser",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "telemetry",
        "gpmf",
        "sensor-data",
        "drone",
        "blackbox"
      ],
      "id": 1
    },
    {
      "name": "xsv",
      "one_line_profile": "A fast CSV command line toolkit for data slicing, indexing, and statistical analysis",
      "detailed_description": "A high-performance command-line toolkit for processing CSV data. It provides commands for indexing, slicing, partitioning, and computing summary statistics (mean, median, frequency) on large tabular datasets without loading them entirely into memory. Widely used in bioinformatics and data science pipelines for preprocessing.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_processing",
        "statistics",
        "filtering"
      ],
      "application_level": "solver",
      "primary_language": "Rust",
      "repo_url": "https://github.com/BurntSushi/xsv",
      "help_website": [],
      "license": "Unlicense",
      "tags": [
        "csv",
        "cli",
        "data-processing",
        "statistics"
      ],
      "id": 2
    },
    {
      "name": "PySysML2",
      "one_line_profile": "Parser for SysML 2.0 textual modeling language for data analysis",
      "detailed_description": "A Python-based parser for the SysML 2.0 textual modeling language. It parses SysML 2.0 models into Python objects to enable data science and analysis on system engineering models, facilitating Model-Based Systems Engineering (MBSE) workflows.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "modeling",
        "systems_engineering"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/DAF-Digital-Transformation-Office/PySysML2",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "sysml",
        "mbse",
        "parsing",
        "systems-engineering"
      ],
      "id": 3
    },
    {
      "name": "configr",
      "one_line_profile": "Configuration file parser (JSON/INI/YAML/TOML) for R language workflows",
      "detailed_description": "An R package that implements parsers for multiple configuration formats (JSON, INI, YAML, TOML) to facilitate the setting and writing of configuration files in R-based scientific workflows and bioinformatics pipelines.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_parsing",
        "workflow_configuration"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/Miachol/configr",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "r-package",
        "configuration",
        "parser",
        "json",
        "yaml"
      ],
      "id": 4
    },
    {
      "name": "dataframely",
      "one_line_profile": "Declarative data frame validation library for Polars and Pandas",
      "detailed_description": "A Python library for validating data frames (Polars and Pandas) using a declarative schema approach. It is used in data science and scientific data processing pipelines to ensure data quality and structural integrity.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_validation",
        "quality_control"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Quantco/dataframely",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "validation",
        "dataframe",
        "polars",
        "pandas",
        "data-quality"
      ],
      "id": 5
    },
    {
      "name": "missingno",
      "one_line_profile": "Missing data visualization module for Python",
      "detailed_description": "A flexible and easy-to-use Python library for visualizing missing data in pandas dataframes. It provides a small toolset of flexible and easy-to-use missing data visualizations and utilities that allow data scientists to get a quick visual summary of the completeness (or lack thereof) of their dataset.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_visualization",
        "quality_control"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ResidentMario/missingno",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "visualization",
        "missing-data",
        "pandas",
        "data-cleaning"
      ],
      "id": 6
    },
    {
      "name": "pyreadstat",
      "one_line_profile": "Reader and writer for SAS, SPSS, and Stata files in Python",
      "detailed_description": "A Python package to read and write SAS (sas7bdat, xport), SPSS (sav, zsav, por), and Stata (dta) data files into/from pandas and polars data frames. It serves as a critical bridge for processing statistical data formats common in social sciences and clinical research within the Python scientific ecosystem.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "format_conversion",
        "data_parsing"
      ],
      "application_level": "library",
      "primary_language": "C",
      "repo_url": "https://github.com/Roche/pyreadstat",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "spss",
        "sas",
        "stata",
        "pandas",
        "polars",
        "converter"
      ],
      "id": 7
    },
    {
      "name": "SPARQL Anything",
      "one_line_profile": "System for querying any data format (JSON, CSV, XML, etc.) with SPARQL",
      "detailed_description": "A system for Semantic Web re-engineering that allows users to query heterogeneous file formats (JSON, CSV, XML, HTML, Markdown, etc.) using SPARQL. It is widely used in scientific data integration and knowledge graph construction to bridge non-RDF data with semantic workflows.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_integration",
        "format_conversion",
        "semantic_query"
      ],
      "application_level": "solver",
      "primary_language": "Java",
      "repo_url": "https://github.com/SPARQL-Anything/sparql.anything",
      "help_website": [
        "http://sparql-anything.cc"
      ],
      "license": "Apache-2.0",
      "tags": [
        "sparql",
        "semantic-web",
        "knowledge-graph",
        "data-integration",
        "rdf"
      ],
      "id": 8
    },
    {
      "name": "StackExchange XML Converter",
      "one_line_profile": "Converter for StackExchange data dumps from XML to CSV format",
      "detailed_description": "A utility tool designed to parse and convert StackExchange data dumps (provided in XML format) into CSV format, facilitating data analysis in social computing and network science research.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_conversion",
        "parsing"
      ],
      "application_level": "solver",
      "primary_language": "Go",
      "repo_url": "https://github.com/SkobelevIgor/stackexchange-xml-converter",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "xml",
        "csv",
        "converter",
        "stackexchange",
        "data-processing"
      ],
      "id": 9
    },
    {
      "name": "jsonschema (Rust)",
      "one_line_profile": "High-performance JSON Schema validator for Rust",
      "detailed_description": "A fast and compliant JSON Schema validator written in Rust. It is essential for validating metadata and data structures in scientific workflows that rely on JSON-based standards.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "validation",
        "parsing"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/Stranger6667/jsonschema",
      "help_website": [
        "https://docs.rs/jsonschema"
      ],
      "license": "MIT",
      "tags": [
        "json-schema",
        "validation",
        "rust",
        "data-integrity"
      ],
      "id": 10
    },
    {
      "name": "Preswald",
      "one_line_profile": "WASM-based packager for interactive scientific data applications",
      "detailed_description": "A tool to bundle Python-based data analysis and visualization workflows (using Pandas, DuckDB, Plotly) into single-file, browser-runnable applications via Pyodide, facilitating the sharing of scientific reports and dashboards.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "visualization",
        "workflow_automation",
        "reporting"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/StructuredLabs/preswald",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "wasm",
        "visualization",
        "dashboard",
        "python",
        "data-science"
      ],
      "id": 11
    },
    {
      "name": "Tablecruncher",
      "one_line_profile": "Lightweight CSV editor and processor",
      "detailed_description": "A desktop tool for opening, editing, and processing large CSV files, supporting JavaScript macros for data cleaning and manipulation tasks common in data science.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_cleaning",
        "data_editing"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/Tablecruncher/tablecruncher",
      "help_website": [
        "https://tablecruncher.com"
      ],
      "license": "GPL-3.0",
      "tags": [
        "csv",
        "editor",
        "data-cleaning",
        "macros"
      ],
      "id": 12
    },
    {
      "name": "RapidJSON",
      "one_line_profile": "Fast JSON parser and generator for C++",
      "detailed_description": "A high-performance C++ library for parsing and generating JSON. It is widely used in scientific computing applications for handling configuration files and data serialization due to its speed and DOM/SAX API support.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "serialization"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/Tencent/rapidjson",
      "help_website": [
        "http://rapidjson.org/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "json",
        "parser",
        "cpp",
        "serialization",
        "high-performance"
      ],
      "id": 13
    },
    {
      "name": "NightConfig",
      "one_line_profile": "Configuration library for TOML, YAML, JSON, and HOCON",
      "detailed_description": "A Java library for reading and writing various configuration formats (TOML, YAML, JSON). It supports scientific software development by providing robust parsing for experiment configurations.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "configuration_management"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/TheElectronWill/night-config",
      "help_website": [],
      "license": "LGPL-3.0",
      "tags": [
        "toml",
        "yaml",
        "json",
        "configuration",
        "java"
      ],
      "id": 14
    },
    {
      "name": "TinyCsvParser",
      "one_line_profile": "High-performance CSV parsing library for .NET",
      "detailed_description": "A library designed for easy and fast parsing of CSV data in .NET applications, suitable for ingesting large scientific datasets stored in CSV format.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "data_ingestion"
      ],
      "application_level": "library",
      "primary_language": "C#",
      "repo_url": "https://github.com/TinyCsvParser/TinyCsvParser",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "csv",
        "parser",
        "dotnet",
        "csharp",
        "data-ingestion"
      ],
      "id": 15
    },
    {
      "name": "dasel",
      "one_line_profile": "Command-line tool for querying and converting data formats (JSON, YAML, TOML, XML, CSV)",
      "detailed_description": "A versatile command-line tool that allows selecting, updating, and deleting data from various structured formats (JSON, TOML, YAML, XML, CSV). It is useful in scientific workflows for data extraction and format conversion.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_conversion",
        "querying",
        "parsing"
      ],
      "application_level": "solver",
      "primary_language": "Go",
      "repo_url": "https://github.com/TomWright/dasel",
      "help_website": [
        "https://daseldocs.tomwright.me/"
      ],
      "license": "MIT",
      "tags": [
        "json",
        "yaml",
        "toml",
        "xml",
        "csv",
        "cli"
      ],
      "id": 16
    },
    {
      "name": "fastexcel",
      "one_line_profile": "Fast Excel reader for Rust and Python",
      "detailed_description": "A high-performance library for reading Excel files (XLSX), enabling efficient ingestion of spreadsheet data into Rust or Python-based scientific analysis pipelines.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "data_ingestion"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/ToucanToco/fastexcel",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "excel",
        "xlsx",
        "parser",
        "rust",
        "python"
      ],
      "id": 17
    },
    {
      "name": "ccorp_yaml_include",
      "one_line_profile": "YAML parser plugin for file inclusion",
      "detailed_description": "A plugin for the Ruamel.YAML parser that adds support for the `!include` tag, allowing modular composition of YAML files, which is useful for managing complex scientific configurations.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "configuration_management"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Tristan-Sweeney-CambridgeConsultants/ccorp_yaml_include",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "yaml",
        "parser-extension",
        "python",
        "configuration"
      ],
      "id": 18
    },
    {
      "name": "VBA-JSON",
      "one_line_profile": "JSON parsing and conversion library for VBA",
      "detailed_description": "A library for parsing and generating JSON within Visual Basic for Applications (VBA), enabling Excel-based scientific workflows to interact with JSON data sources and APIs.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "serialization"
      ],
      "application_level": "library",
      "primary_language": "Visual Basic",
      "repo_url": "https://github.com/VBA-tools/VBA-JSON",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "json",
        "vba",
        "excel",
        "parsing"
      ],
      "id": 19
    },
    {
      "name": "jtoml",
      "one_line_profile": "TOML parser library for Java",
      "detailed_description": "A fully compliant TOML parser for Java, facilitating the use of TOML configuration files in Java-based scientific applications.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "configuration_management"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/WasabiThumb/jtoml",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "toml",
        "parser",
        "java",
        "configuration"
      ],
      "id": 20
    },
    {
      "name": "flatted",
      "one_line_profile": "Circular JSON parser",
      "detailed_description": "A fast and minimal JavaScript parser for JSON structures with circular references, useful for serializing complex data graphs in scientific web applications.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "serialization"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/WebReflection/flatted",
      "help_website": [],
      "license": "ISC",
      "tags": [
        "json",
        "circular-reference",
        "serialization",
        "javascript"
      ],
      "id": 21
    },
    {
      "name": "pxi",
      "one_line_profile": "Command-line data processor for JSON and other formats",
      "detailed_description": "A command-line tool for processing and transforming data, similar to jq and awk, supporting efficient data manipulation in scientific pipelines.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_processing",
        "transformation"
      ],
      "application_level": "solver",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/Yord/pxi",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "cli",
        "data-processing",
        "json",
        "transformation"
      ],
      "id": 22
    },
    {
      "name": "PolarCodeDecodersInMatlab",
      "one_line_profile": "Matlab implementation of Polar Code decoders",
      "detailed_description": "A Matlab library implementing various Polar Code decoding algorithms (CA-SCL, BP), serving as a simulation tool for information theory and telecommunications research.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "simulation",
        "decoding",
        "algorithm_implementation"
      ],
      "application_level": "library",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/YuYongRun/PolarCodeDecodersInMatlab",
      "help_website": [],
      "license": null,
      "tags": [
        "polar-codes",
        "decoding",
        "matlab",
        "information-theory"
      ],
      "id": 23
    },
    {
      "name": "construct (Java)",
      "one_line_profile": "Binary and textual data structure parsing library for Java",
      "detailed_description": "A Java port of the Python 'construct' library, used for declarative parsing and building of binary data structures, essential for handling custom binary formats in scientific data.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "binary_processing"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/ZiglioUK/construct",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "binary-parsing",
        "java",
        "data-structures"
      ],
      "id": 24
    },
    {
      "name": "YamlDotNet",
      "one_line_profile": "YAML library for .NET",
      "detailed_description": "A comprehensive .NET library for parsing and serializing YAML. It is a foundational tool for handling configuration and data serialization in scientific software developed in the .NET ecosystem.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "serialization"
      ],
      "application_level": "library",
      "primary_language": "C#",
      "repo_url": "https://github.com/aaubry/YamlDotNet",
      "help_website": [
        "https://github.com/aaubry/YamlDotNet/wiki"
      ],
      "license": "MIT",
      "tags": [
        "yaml",
        "dotnet",
        "serialization",
        "parsing"
      ],
      "id": 25
    },
    {
      "name": "saneyaml",
      "one_line_profile": "Safer and simpler YAML parsing for Python",
      "detailed_description": "A Python library built on top of PyYAML that provides a safer and cleaner interface for parsing and serializing YAML, reducing risks and complexity in scientific configuration management.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "serialization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aboutcode-org/saneyaml",
      "help_website": [],
      "license": null,
      "tags": [
        "yaml",
        "python",
        "safety",
        "parsing"
      ],
      "id": 26
    },
    {
      "name": "polars_ds_extension",
      "one_line_profile": "Polars extension for data science utilities",
      "detailed_description": "An extension for the Polars dataframe library that adds general data science functionalities, enhancing data processing workflows in Rust and Python.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "data_analysis",
        "statistics"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/abstractqqq/polars_ds_extension",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "polars",
        "data-science",
        "rust",
        "python",
        "extension"
      ],
      "id": 27
    },
    {
      "name": "node-csv",
      "one_line_profile": "Full-featured CSV parser and generator for Node.js",
      "detailed_description": "A comprehensive CSV parsing and generation library for Node.js, widely used for processing tabular data in JavaScript-based scientific applications and data pipelines.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "serialization"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/adaltas/node-csv",
      "help_website": [
        "https://csv.js.org/"
      ],
      "license": "MIT",
      "tags": [
        "csv",
        "parser",
        "nodejs",
        "data-processing"
      ],
      "id": 28
    },
    {
      "name": "node-csv-parse",
      "one_line_profile": "Stream-based CSV parser for Node.js",
      "detailed_description": "A CSV parsing module implementing the Node.js stream API, allowing efficient processing of large scientific datasets row-by-row.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "stream_processing"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/adaltas/node-csv-parse",
      "help_website": [
        "https://csv.js.org/parse/"
      ],
      "license": null,
      "tags": [
        "csv",
        "parser",
        "stream",
        "nodejs"
      ],
      "id": 29
    },
    {
      "name": "node-csv-stringify",
      "one_line_profile": "Stream-based CSV stringifier for Node.js",
      "detailed_description": "A CSV generation module implementing the Node.js stream API, used for exporting scientific data to CSV format efficiently.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "serialization",
        "export"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/adaltas/node-csv-stringify",
      "help_website": [
        "https://csv.js.org/stringify/"
      ],
      "license": null,
      "tags": [
        "csv",
        "generator",
        "stream",
        "nodejs"
      ],
      "id": 30
    },
    {
      "name": "PandasGUI",
      "one_line_profile": "GUI for analyzing Pandas DataFrames",
      "detailed_description": "A graphical user interface tool for viewing, plotting, and analyzing Pandas DataFrames, facilitating interactive data exploration for scientists using Python.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "visualization",
        "data_exploration"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/adamerose/PandasGUI",
      "help_website": [],
      "license": "MIT-0",
      "tags": [
        "pandas",
        "gui",
        "visualization",
        "data-analysis"
      ],
      "id": 31
    },
    {
      "name": "zaml",
      "one_line_profile": "Fast YAML 1.2 parsing library",
      "detailed_description": "A high-performance YAML 1.2 parsing library, providing fast configuration loading for performance-critical scientific applications.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing"
      ],
      "application_level": "library",
      "primary_language": "Zig",
      "repo_url": "https://github.com/adamserafini/zaml",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "yaml",
        "parser",
        "zig",
        "performance"
      ],
      "id": 32
    },
    {
      "name": "Trafilatura",
      "one_line_profile": "Web scraping and text extraction tool",
      "detailed_description": "A tool and library for gathering text and metadata from the web, outputting to formats like CSV, JSON, and XML. It is valuable for creating datasets for NLP and social science research.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "data_collection",
        "text_extraction",
        "scraping"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/adbar/trafilatura",
      "help_website": [
        "https://trafilatura.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "scraping",
        "text-extraction",
        "nlp",
        "dataset-creation"
      ],
      "id": 33
    },
    {
      "name": "frontmatter",
      "one_line_profile": "Go library for parsing content front matter",
      "detailed_description": "A Go library for detecting and decoding front matter (YAML/JSON/TOML metadata) from content files, useful for managing metadata in scientific data repositories and static site generators.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "metadata_extraction"
      ],
      "application_level": "library",
      "primary_language": "Go",
      "repo_url": "https://github.com/adrg/frontmatter",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "frontmatter",
        "metadata",
        "parsing",
        "go"
      ],
      "id": 34
    },
    {
      "name": "Plotlars",
      "one_line_profile": "Integration library for Polars dataframes and Plotly visualization",
      "detailed_description": "A Rust library that facilitates the integration between the Polars data analysis library and the Plotly plotting library, enabling efficient visualization of scientific dataframes.",
      "domains": [
        "D1",
        "D4"
      ],
      "subtask_category": [
        "visualization",
        "data_analysis"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/alceal/plotlars",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "polars",
        "plotly",
        "visualization",
        "rust"
      ],
      "id": 35
    },
    {
      "name": "Ruby Polars",
      "one_line_profile": "High-performance DataFrame library for Ruby based on Polars",
      "detailed_description": "A Ruby binding for the Polars DataFrame library, providing blazingly fast data processing and analysis capabilities suitable for scientific datasets.",
      "domains": [
        "D1",
        "D1-0X"
      ],
      "subtask_category": [
        "data_processing",
        "data_analysis"
      ],
      "application_level": "library",
      "primary_language": "Ruby",
      "repo_url": "https://github.com/ankane/ruby-polars",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "dataframe",
        "polars",
        "ruby",
        "data-science"
      ],
      "id": 36
    },
    {
      "name": "nvParse",
      "one_line_profile": "GPU-accelerated CSV parser",
      "detailed_description": "A fast, GPU-based CSV parser designed to leverage CUDA for high-performance data loading and parsing, suitable for large-scale scientific datasets.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_loading",
        "parsing"
      ],
      "application_level": "library",
      "primary_language": "Cuda",
      "repo_url": "https://github.com/antonmks/nvParse",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "csv",
        "gpu",
        "cuda",
        "hpc"
      ],
      "id": 37
    },
    {
      "name": "Apache Avro",
      "one_line_profile": "Data serialization system for compact binary data exchange",
      "detailed_description": "A data serialization system widely used in big data and scientific computing for efficient data storage and exchange, supporting rich data structures and schema evolution.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "serialization",
        "data_storage"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/apache/avro",
      "help_website": [
        "https://avro.apache.org/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "serialization",
        "big-data",
        "format",
        "schema"
      ],
      "id": 38
    },
    {
      "name": "Apache Fesod",
      "one_line_profile": "Efficient spreadsheet processing library",
      "detailed_description": "A library designed for processing large spreadsheet files (Excel) efficiently without memory overflow (OOM), facilitating the ingestion of scientific data stored in spreadsheets.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_loading",
        "spreadsheet_processing"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/apache/fesod",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "excel",
        "spreadsheet",
        "data-processing",
        "java"
      ],
      "id": 39
    },
    {
      "name": "Apache XTable",
      "one_line_profile": "Cross-table converter for lakehouse table formats",
      "detailed_description": "A cross-table converter for lakehouse table formats (Hudi, Delta, Iceberg) that facilitates interoperability across data processing systems used in large-scale scientific data platforms.",
      "domains": [
        "D1",
        "D1-0X"
      ],
      "subtask_category": [
        "data_conversion",
        "interoperability"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/apache/incubator-xtable",
      "help_website": [
        "https://xtable.apache.org/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "lakehouse",
        "hudi",
        "delta-lake",
        "iceberg",
        "interoperability"
      ],
      "id": 40
    },
    {
      "name": "MyDuckServer",
      "one_line_profile": "DuckDB-powered SQL server for analytics",
      "detailed_description": "A unified server powered by DuckDB that provides MySQL, Postgres, and FlightSQL interfaces, enabling efficient OLAP and data analysis on scientific datasets.",
      "domains": [
        "D1",
        "D1-0X"
      ],
      "subtask_category": [
        "data_analysis",
        "database_service"
      ],
      "application_level": "service",
      "primary_language": "Go",
      "repo_url": "https://github.com/apecloud/myduckserver",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "duckdb",
        "olap",
        "sql",
        "analytics"
      ],
      "id": 41
    },
    {
      "name": "jsonv.sh",
      "one_line_profile": "Bash CLI tool for JSON to CSV conversion",
      "detailed_description": "A Bash command line tool for converting JSON data to CSV format, useful for lightweight data wrangling and pipeline integration in scientific workflows.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_conversion",
        "format_conversion"
      ],
      "application_level": "solver",
      "primary_language": "Awk",
      "repo_url": "https://github.com/archan937/jsonv.sh",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "json",
        "csv",
        "bash",
        "cli"
      ],
      "id": 42
    },
    {
      "name": "Airflow AI SDK",
      "one_line_profile": "SDK for AI/LLM integration in Airflow workflows",
      "detailed_description": "An SDK for integrating Large Language Models (LLMs) and AI Agents into Apache Airflow pipelines, enabling AI-driven scientific workflows and automation.",
      "domains": [
        "D1",
        "D1-0X"
      ],
      "subtask_category": [
        "workflow_orchestration",
        "ai_integration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/astronomer/airflow-ai-sdk",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "airflow",
        "llm",
        "workflow",
        "ai-agent"
      ],
      "id": 43
    },
    {
      "name": "csvdiff",
      "one_line_profile": "Fast diff tool for comparing CSV files",
      "detailed_description": "A fast command-line tool for comparing CSV files, useful for quality control, regression testing, and verifying data processing results in scientific pipelines.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "quality_control",
        "data_comparison"
      ],
      "application_level": "solver",
      "primary_language": "Go",
      "repo_url": "https://github.com/aswinkarthik/csvdiff",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "csv",
        "diff",
        "cli",
        "data-qc"
      ],
      "id": 44
    },
    {
      "name": "AWS SDK for pandas",
      "one_line_profile": "Pandas integration for AWS data services (Athena, Glue, S3)",
      "detailed_description": "An open-source Python library that extends Pandas to easily connect with AWS data services. It simplifies the reading and writing of scientific datasets (Parquet, CSV, JSON) stored in AWS S3, Athena, and Redshift, acting as a critical data engineering tool for cloud-based scientific workflows.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_integration",
        "cloud_io",
        "etl"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aws/aws-sdk-pandas",
      "help_website": [
        "https://aws-sdk-pandas.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "pandas",
        "aws",
        "etl",
        "parquet",
        "data-engineering"
      ],
      "id": 45
    },
    {
      "name": "polars_ols",
      "one_line_profile": "Least squares linear regression extension for Polars",
      "detailed_description": "A plugin for the Polars DataFrame library that enables efficient ordinary least squares (OLS) linear regression directly within Polars expressions. It facilitates fast statistical modeling and inference on large datasets without leaving the Polars ecosystem.",
      "domains": [
        "D1",
        "D4"
      ],
      "subtask_category": [
        "statistical_analysis",
        "linear_modeling",
        "regression"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/azmyrajab/polars_ols",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "polars",
        "statistics",
        "linear-regression",
        "least-squares"
      ],
      "id": 46
    },
    {
      "name": "Orange3",
      "one_line_profile": "Interactive data mining and machine learning toolkit",
      "detailed_description": "An open-source data visualization, machine learning and data mining toolkit. It features a visual programming front-end for explorative data analysis and interactive data visualization, widely used in bioinformatics and social sciences.",
      "domains": [
        "D1",
        "D4"
      ],
      "subtask_category": [
        "data_mining",
        "visualization",
        "machine_learning"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/biolab/orange3",
      "help_website": [
        "https://orangedatamining.com/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "data-mining",
        "visualization",
        "bioinformatics",
        "machine-learning"
      ],
      "id": 47
    },
    {
      "name": "Blaze",
      "one_line_profile": "Interface for querying big data using NumPy/Pandas syntax",
      "detailed_description": "Blaze provides a Python interface to query data on various storage systems (SQL, NoSQL, Spark) using a subset of the NumPy and Pandas API. It abstracts computation and storage, allowing scientific users to process large datasets that exceed memory limits.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "data_processing",
        "big_data_interface",
        "computation_abstraction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/blaze/blaze",
      "help_website": [
        "http://blaze.pydata.org/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "numpy",
        "pandas",
        "big-data",
        "interface"
      ],
      "id": 48
    },
    {
      "name": "pytimetk",
      "one_line_profile": "Time series analysis and forecasting toolkit",
      "detailed_description": "A Python library designed to simplify time series analysis, feature engineering, and forecasting. It integrates with the Pandas and Polars ecosystems to provide fast, functional tools for processing temporal data in scientific and analytical contexts.",
      "domains": [
        "D1",
        "D4"
      ],
      "subtask_category": [
        "time_series_analysis",
        "forecasting",
        "feature_engineering"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/business-science/pytimetk",
      "help_website": [
        "https://business-science.github.io/pytimetk/"
      ],
      "license": "MIT",
      "tags": [
        "time-series",
        "forecasting",
        "pandas",
        "polars"
      ],
      "id": 49
    },
    {
      "name": "datacompy",
      "one_line_profile": "DataFrame comparison tool for Pandas, Polars, and Spark",
      "detailed_description": "A library for comparing two DataFrames (Pandas, Polars, Spark, or Snowpark) to identify differences. It is widely used in data quality control (QC) and validation steps within scientific data processing pipelines to ensure data integrity.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "quality_control",
        "data_comparison",
        "validation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/capitalone/datacompy",
      "help_website": [
        "https://capitalone.github.io/datacompy/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "data-quality",
        "comparison",
        "pandas",
        "polars",
        "spark"
      ],
      "id": 50
    },
    {
      "name": "FastTableViewer",
      "one_line_profile": "High-performance command-line viewer for CSV/TSV and delimited data files",
      "detailed_description": "A fast, feature-rich command-line tool designed for viewing and inspecting large CSV, TSV, and other delimited data files. It supports key navigation, searching, and handling of large datasets, making it useful for quick inspection of tabular scientific data without loading into heavy GUI applications.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_inspection",
        "data_visualization"
      ],
      "application_level": "solver",
      "primary_language": "Go",
      "repo_url": "https://github.com/codechenx/FastTableViewer",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "csv",
        "tsv",
        "cli",
        "data-viewer",
        "tabular-data"
      ],
      "id": 51
    },
    {
      "name": "csv-to-json",
      "one_line_profile": "Command-line utility for converting CSV files to JSON format",
      "detailed_description": "A lightweight Node.js command-line tool for converting CSV data into JSON format. It supports standard CSV parsing and JSON output, facilitating data format conversion in scientific workflows where JSON is required for downstream analysis or web visualization.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "format_conversion",
        "data_processing"
      ],
      "application_level": "workflow",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/cparker15/csv-to-json",
      "help_website": [],
      "license": "LGPL-3.0",
      "tags": [
        "csv",
        "json",
        "converter",
        "cli"
      ],
      "id": 52
    },
    {
      "name": "OctoSQL",
      "one_line_profile": "SQL query tool for analyzing data from multiple file formats and databases",
      "detailed_description": "OctoSQL is a query tool that allows users to join, analyze, and transform data from multiple sources, including CSV, JSON, Parquet, and various databases, using standard SQL. It is highly applicable for scientific data analysis where data resides in heterogeneous file formats.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_analysis",
        "data_query",
        "format_conversion"
      ],
      "application_level": "solver",
      "primary_language": "Go",
      "repo_url": "https://github.com/cube2222/octosql",
      "help_website": [],
      "license": "MPL-2.0",
      "tags": [
        "sql",
        "csv",
        "json",
        "parquet",
        "data-analysis",
        "cli"
      ],
      "id": 53
    },
    {
      "name": "csv2json",
      "one_line_profile": "Ruby command-line tool for converting CSV to JSON",
      "detailed_description": "A Ruby gem providing a command-line interface to convert CSV files into JSON. It serves as a simple utility for data format transformation in data processing pipelines.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "format_conversion",
        "data_processing"
      ],
      "application_level": "workflow",
      "primary_language": "Ruby",
      "repo_url": "https://github.com/darwin/csv2json",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "csv",
        "json",
        "converter",
        "cli",
        "ruby"
      ],
      "id": 54
    },
    {
      "name": "Dask",
      "one_line_profile": "Flexible parallel computing library for analytic computing",
      "detailed_description": "Dask provides advanced parallelism for analytics, enabling performance at scale for the tools of the PyData ecosystem (NumPy, Pandas, and Scikit-Learn). It is fundamental for processing large scientific datasets (e.g., in geoscience, physics) that exceed memory limits.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_processing",
        "parallel_computing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/dask/dask",
      "help_website": [
        "https://dask.org/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "parallel-computing",
        "distributed-systems",
        "numpy",
        "pandas",
        "scaling"
      ],
      "id": 55
    },
    {
      "name": "Yacman",
      "one_line_profile": "YAML configuration manager for scientific workflows",
      "detailed_description": "Developed by the Databio lab, Yacman is a configuration management tool designed to standardize how bioinformatics and scientific tools handle YAML configurations, often used in conjunction with tools like Looper and Peppy.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "configuration_management",
        "workflow_utility"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/databio/yacman",
      "help_website": [
        "http://yacman.databio.org/"
      ],
      "license": "BSD-2-Clause",
      "tags": [
        "yaml",
        "configuration",
        "bioinformatics",
        "workflow"
      ],
      "id": 56
    },
    {
      "name": "Koalas",
      "one_line_profile": "Pandas API on Apache Spark for scalable data science",
      "detailed_description": "Koalas (now integrated into PySpark) allows data scientists to use the familiar pandas API while leveraging the distributed processing power of Apache Spark, enabling the processing of massive scientific datasets.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_processing",
        "big_data_analytics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/databricks/koalas",
      "help_website": [
        "https://koalas.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "pandas",
        "spark",
        "big-data",
        "data-science"
      ],
      "id": 57
    },
    {
      "name": "cad2data",
      "one_line_profile": "Automated conversion pipeline for CAD/BIM data formats",
      "detailed_description": "A workflow tool for converting engineering and architectural CAD files (Revit .rvt, IFC, DWG) into data-accessible formats, facilitating the analysis of construction and structural data.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "format_conversion",
        "data_extraction"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/datadrivenconstruction/cad2data-Revit-IFC-DWG-DGN-pipeline-with-conversion-validation-qto",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "cad",
        "bim",
        "revit",
        "ifc",
        "data-conversion"
      ],
      "id": 58
    },
    {
      "name": "qsv",
      "one_line_profile": "High-performance CSV data wrangling toolkit",
      "detailed_description": "A command-line tool for indexing, slicing, analyzing, and manipulating CSV files. It is widely used in data science pipelines to handle large tabular datasets efficiently without loading them entirely into memory.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_processing",
        "data_wrangling"
      ],
      "application_level": "solver",
      "primary_language": "Rust",
      "repo_url": "https://github.com/dathere/qsv",
      "help_website": [
        "https://qsv.dathere.com"
      ],
      "license": "Unlicense",
      "tags": [
        "csv",
        "cli",
        "rust",
        "data-engineering",
        "etl"
      ],
      "id": 59
    },
    {
      "name": "xlsx2csv",
      "one_line_profile": "Fast converter from XLSX to CSV for data pipelines",
      "detailed_description": "A lightweight tool to convert Microsoft Excel (XLSX) files to CSV format. It is optimized for speed and handling large files, making it useful for ingesting scientific data stored in spreadsheets into analysis pipelines.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "format_conversion",
        "data_ingestion"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/dilshod/xlsx2csv",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "xlsx",
        "csv",
        "excel",
        "conversion"
      ],
      "id": 60
    },
    {
      "name": "DocArray",
      "one_line_profile": "Data structure for multimodal scientific data",
      "detailed_description": "A library for representing, sending, storing, and searching multimodal data (text, image, audio, 3D meshes, tensors). It is used in AI pipelines to handle unstructured data structures common in deep learning applications.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_representation",
        "multimodal_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/docarray/docarray",
      "help_website": [
        "https://docs.docarray.org/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "multimodal",
        "tensor",
        "data-structure",
        "ai",
        "deep-learning"
      ],
      "id": 61
    },
    {
      "name": "fit2gpx",
      "one_line_profile": "Converter for FIT sensor data to GPX format",
      "detailed_description": "A library and tool to convert FIT files (commonly used by GPS devices and sensors) to GPX format. Useful for geospatial analysis and processing of physiological/tracking data in sports science or geography.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "format_conversion",
        "sensor_data_processing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/dodo-saba/fit2gpx",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "fit",
        "gpx",
        "geospatial",
        "sensor-data",
        "strava"
      ],
      "id": 62
    },
    {
      "name": "arrow-tools",
      "one_line_profile": "CLI tools for Apache Arrow and Parquet conversion",
      "detailed_description": "A collection of command-line tools to convert CSV and JSON data into Apache Arrow and Parquet formats. These formats are standard for high-performance scientific data analytics and interchange.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "format_conversion",
        "data_interchange"
      ],
      "application_level": "solver",
      "primary_language": "Rust",
      "repo_url": "https://github.com/domoritz/arrow-tools",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "apache-arrow",
        "parquet",
        "csv",
        "json",
        "cli"
      ],
      "id": 63
    },
    {
      "name": "dsgrid-legacy-efs-api",
      "one_line_profile": "HDF5 data marshalling for energy grid models",
      "detailed_description": "A Python package developed by NREL for marshalling dsgrid (Demand-Side Grid) data into a common HDF5 format, facilitating the analysis and exchange of energy system modeling data.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_marshalling",
        "format_conversion"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/dsgrid/dsgrid-legacy-efs-api",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "hdf5",
        "energy-grid",
        "nrel",
        "data-marshalling"
      ],
      "id": 64
    },
    {
      "name": "ScienceBeam Parser",
      "one_line_profile": "Tool to convert PDF scientific articles into structured XML data",
      "detailed_description": "A set of tools designed to convert scientific publications (PDFs) into structured XML documents, facilitating literature mining and scientific knowledge extraction.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "literature_mining",
        "document_parsing"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/elifesciences/sciencebeam-parser",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pdf-parsing",
        "xml",
        "scientific-literature",
        "nlp"
      ],
      "id": 65
    },
    {
      "name": "MCAP",
      "one_line_profile": "Modular container file format and libraries for robotics data recording",
      "detailed_description": "A modular, performant, and serialization-agnostic container file format designed for robotics applications, serving as a modern alternative to ROS bags for sensor data logging and analysis.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_serialization",
        "sensor_logging"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/foxglove/mcap",
      "help_website": [
        "https://mcap.dev"
      ],
      "license": "MIT",
      "tags": [
        "robotics",
        "serialization",
        "ros",
        "data-logging"
      ],
      "id": 66
    },
    {
      "name": "fst",
      "one_line_profile": "High-performance data frame serialization library for R",
      "detailed_description": "A library for extremely fast serialization and deserialization of data frames in R. It supports random access, compression, and multi-threading, making it essential for handling large scientific datasets in R workflows.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_serialization",
        "data_io"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/fstpackage/fst",
      "help_website": [
        "https://www.fstpackage.org/"
      ],
      "license": "AGPL-3.0",
      "tags": [
        "serialization",
        "data-frame",
        "high-performance",
        "R"
      ],
      "id": 67
    },
    {
      "name": "Fugue",
      "one_line_profile": "Unified interface for distributed computing in scientific workflows",
      "detailed_description": "A unified interface that allows users to execute Python, Pandas, and SQL code on distributed computing frameworks like Spark, Dask, and Ray without rewriting code. It facilitates scaling scientific data analysis workflows.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "distributed_computing",
        "workflow_orchestration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/fugue-project/fugue",
      "help_website": [
        "https://fugue-project.github.io/tutorials/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "distributed-computing",
        "pandas",
        "spark",
        "dask",
        "workflow"
      ],
      "id": 68
    },
    {
      "name": "functime",
      "one_line_profile": "Scalable time-series machine learning library",
      "detailed_description": "A library for time-series machine learning at scale, built on Polars. It provides parallel feature extraction and forecasting capabilities, suitable for analyzing large-scale scientific panel data (e.g., sensor data, environmental metrics).",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "time_series_analysis",
        "forecasting",
        "feature_extraction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/functime-org/functime",
      "help_website": [
        "https://docs.functime.ai/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "time-series",
        "machine-learning",
        "polars",
        "forecasting"
      ],
      "id": 69
    },
    {
      "name": "german-nouns",
      "one_line_profile": "Structured dataset and parser for German linguistic analysis",
      "detailed_description": "A dataset containing ~100,000 German nouns with grammatical properties and a Python module for parsing compound words. It serves as a tool for computational linguistics and NLP research.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "linguistic_analysis",
        "dataset",
        "nlp"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/gambolputty/german-nouns",
      "help_website": [],
      "license": "CC-BY-SA-4.0",
      "tags": [
        "linguistics",
        "nlp",
        "german",
        "dataset"
      ],
      "id": 70
    },
    {
      "name": "simdcsv",
      "one_line_profile": "High-performance SIMD-accelerated CSV parser",
      "detailed_description": "A fast CSV parser for C++ that leverages SIMD instructions. It is designed for high-throughput data loading, which is critical for processing large scientific datasets stored in CSV format.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "data_loading"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/geofflangdale/simdcsv",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "csv",
        "simd",
        "high-performance",
        "parsing"
      ],
      "id": 71
    },
    {
      "name": "GeoPandas",
      "one_line_profile": "Python tools for geographic data analysis",
      "detailed_description": "An open source project to make working with geospatial data in python easier. It extends the datatypes used by pandas to allow spatial operations on geometric types, essential for earth sciences and geography.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "geospatial_analysis",
        "data_manipulation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/geopandas/geopandas",
      "help_website": [
        "https://geopandas.org/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "geospatial",
        "gis",
        "pandas",
        "python"
      ],
      "id": 72
    },
    {
      "name": "reflect-cpp",
      "one_line_profile": "Reflection-based serialization library for C++20",
      "detailed_description": "A C++20 library for fast serialization and deserialization using reflection. It supports multiple formats relevant to scientific computing, including JSON, CSV, Parquet, Avro, and YAML, enabling efficient data exchange.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "serialization",
        "data_io"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/getml/reflect-cpp",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "serialization",
        "reflection",
        "parquet",
        "avro",
        "cpp"
      ],
      "id": 73
    },
    {
      "name": "LangExtract",
      "one_line_profile": "LLM-based structured information extraction library",
      "detailed_description": "A library for extracting structured information from unstructured text using LLMs with source grounding. It is applicable for scientific literature mining and knowledge extraction tasks.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "information_extraction",
        "text_mining",
        "nlp"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/google/langextract",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "information-extraction",
        "nlp",
        "structured-data"
      ],
      "id": 74
    },
    {
      "name": "struct2tensor",
      "one_line_profile": "Structured data manipulation for TensorFlow",
      "detailed_description": "A library for parsing and manipulating structured data (like Protocol Buffers) inside TensorFlow. It is essential for preprocessing complex structured data in AI for Science pipelines.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "data_preprocessing",
        "tensor_manipulation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/google/struct2tensor",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "tensorflow",
        "structured-data",
        "preprocessing",
        "protobuf"
      ],
      "id": 75
    },
    {
      "name": "Hugging Face Datasets",
      "one_line_profile": "Library for easily accessing, sharing, and processing datasets for Audio, Computer Vision, and NLP",
      "detailed_description": "A lightweight and extensible library to easily share and access datasets and evaluation metrics. It features memory-mapped data loading for efficiency, interoperability with NumPy/Pandas/PyTorch/TensorFlow, and supports various data formats (JSON, CSV, Parquet, Arrow) essential for scientific data pipelines.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_loading",
        "data_processing",
        "data_conversion"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/huggingface/datasets",
      "help_website": [
        "https://huggingface.co/docs/datasets"
      ],
      "license": "Apache-2.0",
      "tags": [
        "datasets",
        "etl",
        "data-processing",
        "machine-learning"
      ],
      "id": 76
    },
    {
      "name": "Ibis",
      "one_line_profile": "Portable Python dataframe library for data analysis across various backends",
      "detailed_description": "Ibis provides a standard API for data analysis and manipulation, decoupling the analytics from the backend execution. It supports multiple backends including SQL databases, Pandas, and BigQuery, making it a powerful tool for scientific data workflows and large-scale data processing.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_analysis",
        "data_manipulation",
        "query_generation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ibis-project/ibis",
      "help_website": [
        "https://ibis-project.org"
      ],
      "license": "Apache-2.0",
      "tags": [
        "dataframe",
        "analytics",
        "sql",
        "data-science"
      ],
      "id": 77
    },
    {
      "name": "OpenStreetMap H3 Loader",
      "one_line_profile": "High-performance tool to transform OpenStreetMap data into H3 partitioned formats",
      "detailed_description": "A specialized data processing tool that converts OpenStreetMap (OSM) planet dumps into H3 (Hexagonal Hierarchical Spatial Index) partitioned PostGIS or Arrow/Parquet formats. This facilitates large-scale geospatial scientific analysis and modeling.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_conversion",
        "geospatial_analysis",
        "data_partitioning"
      ],
      "application_level": "tool",
      "primary_language": "Java",
      "repo_url": "https://github.com/igor-suhorukov/openstreetmap_h3",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "openstreetmap",
        "h3",
        "geospatial",
        "etl",
        "parquet"
      ],
      "id": 78
    },
    {
      "name": "Danfo.js",
      "one_line_profile": "Pandas-like data analysis library for JavaScript",
      "detailed_description": "Danfo.js is a JavaScript library that provides high-performance, intuitive data structures (DataFrame and Series) for manipulating and processing structured data, bringing data analysis capabilities similar to Pandas to the JavaScript ecosystem.",
      "domains": [
        "D1",
        "D3"
      ],
      "subtask_category": [
        "data_analysis",
        "data_manipulation"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/javascriptdata/danfojs",
      "help_website": [
        "https://danfo.jsdata.org/"
      ],
      "license": "MIT",
      "tags": [
        "dataframe",
        "data-analysis",
        "javascript",
        "pandas"
      ],
      "id": 79
    },
    {
      "name": "json2csv",
      "one_line_profile": "Command-line tool for converting JSON to CSV",
      "detailed_description": "A command-line utility and library for converting JSON data structures into CSV format, facilitating data exchange, integration, and preprocessing in scientific data workflows.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "format_conversion",
        "data_preprocessing"
      ],
      "application_level": "solver",
      "primary_language": "Go",
      "repo_url": "https://github.com/jehiah/json2csv",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "json",
        "csv",
        "conversion",
        "cli"
      ],
      "id": 80
    },
    {
      "name": "jq",
      "one_line_profile": "Command-line JSON processor",
      "detailed_description": "A lightweight and flexible command-line JSON processor that is widely used in scientific data pipelines for filtering, mapping, transforming, and normalizing JSON-formatted data (e.g., metadata, API responses).",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_processing",
        "filtering",
        "transformation"
      ],
      "application_level": "solver",
      "primary_language": "C",
      "repo_url": "https://github.com/jqlang/jq",
      "help_website": [
        "https://jqlang.github.io/jq/"
      ],
      "license": "MIT",
      "tags": [
        "json",
        "cli",
        "data-processing",
        "filter"
      ],
      "id": 81
    },
    {
      "name": "Flatterer",
      "one_line_profile": "Opinionated JSON to CSV/Parquet/SQL converter for data analysis",
      "detailed_description": "A high-performance CLI tool designed to convert nested JSON data into flat formats like CSV, Parquet, and SQLite. It is particularly useful in scientific data engineering pipelines for preparing large-scale JSON datasets (e.g., from APIs or instruments) for analysis in data science tools.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_conversion",
        "data_preparation"
      ],
      "application_level": "solver",
      "primary_language": "Rust",
      "repo_url": "https://github.com/kindly/flatterer",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "json",
        "parquet",
        "csv",
        "data-engineering",
        "conversion"
      ],
      "id": 82
    },
    {
      "name": "Patito",
      "one_line_profile": "Data modelling and validation layer for Polars dataframes",
      "detailed_description": "A library built on top of Polars and Pydantic that provides a data modeling layer for dataframe validation. It allows scientists and data engineers to define schemas for their dataframes, ensuring data quality and consistency in scientific data processing pipelines.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "quality_control",
        "data_validation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/kolonialno/patito",
      "help_website": [
        "https://patito.readthedocs.io"
      ],
      "license": "MIT",
      "tags": [
        "polars",
        "pydantic",
        "validation",
        "dataframe",
        "data-science"
      ],
      "id": 83
    },
    {
      "name": "Duckling",
      "one_line_profile": "Fast viewer for CSV, Parquet, and database files",
      "detailed_description": "A high-performance data viewer built with Tauri that supports opening and inspecting large CSV and Parquet files, as well as connecting to databases like DuckDB and PostgreSQL. Useful for quick inspection of scientific datasets.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_visualization",
        "data_inspection"
      ],
      "application_level": "solver",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/l1xnan/duckling",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "csv",
        "parquet",
        "data-viewer",
        "duckdb"
      ],
      "id": 84
    },
    {
      "name": "Lance",
      "one_line_profile": "Modern columnar data format for AI and multimodal data",
      "detailed_description": "An open source data format designed for high-performance random access and vector search, optimized for ML workflows and multimodal data (images, text, vectors). It serves as a faster alternative to Parquet for AI training and retrieval tasks.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "data_storage",
        "data_format",
        "vector_search"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/lance-format/lance",
      "help_website": [
        "https://lancedb.github.io/lance/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "data-format",
        "vector-search",
        "machine-learning",
        "parquet-alternative"
      ],
      "id": 85
    },
    {
      "name": "LangChain",
      "one_line_profile": "Framework for developing applications powered by language models",
      "detailed_description": "A comprehensive framework for building agents and workflows using Large Language Models (LLMs). In scientific research (AI4S), it is widely used to orchestrate reasoning agents, automate literature reviews, and build autonomous laboratory controllers.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "workflow_orchestration",
        "inference",
        "agent_framework"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/langchain-ai/langchain",
      "help_website": [
        "https://python.langchain.com/"
      ],
      "license": "MIT",
      "tags": [
        "llm",
        "agents",
        "workflow",
        "ai4s"
      ],
      "id": 86
    },
    {
      "name": "Lark",
      "one_line_profile": "Parsing toolkit for Python",
      "detailed_description": "A modern parsing library for Python that can parse any context-free grammar. It is frequently used in scientific software to build parsers for domain-specific file formats (e.g., chemical formulas, biological sequence notations) and custom configuration languages.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "grammar_definition"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/lark-parser/lark",
      "help_website": [
        "https://lark-parser.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "parser",
        "grammar",
        "dsl",
        "python"
      ],
      "id": 87
    },
    {
      "name": "TinyXML2",
      "one_line_profile": "Simple, small, efficient C++ XML parser",
      "detailed_description": "A lightweight C++ XML parser widely integrated into scientific simulation software and high-performance computing applications for handling configuration files and data exchange formats.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "io"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/leethomason/tinyxml2",
      "help_website": [
        "https://leethomason.github.io/tinyxml2/"
      ],
      "license": "Zlib",
      "tags": [
        "xml",
        "parser",
        "cpp",
        "embedded"
      ],
      "id": 88
    },
    {
      "name": "docker-csv",
      "one_line_profile": "Docker container with CSV processing utilities",
      "detailed_description": "A containerized environment pre-packaged with command-line tools like csvkit, csvcut, and csvsql. It provides a reproducible workflow environment for batch processing and cleaning of tabular scientific data.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_processing",
        "data_cleaning"
      ],
      "application_level": "workflow",
      "primary_language": "Dockerfile",
      "repo_url": "https://github.com/leplusorg/docker-csv",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "csv",
        "docker",
        "etl",
        "cli"
      ],
      "id": 89
    },
    {
      "name": "libexpat",
      "one_line_profile": "Fast streaming XML parser library",
      "detailed_description": "A stream-oriented XML parser library written in C. It is a foundational dependency for many scientific computing packages (e.g., in Python, Perl, and system libraries) to handle XML-based data formats efficiently.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "io"
      ],
      "application_level": "library",
      "primary_language": "C",
      "repo_url": "https://github.com/libexpat/libexpat",
      "help_website": [
        "https://libexpat.github.io/"
      ],
      "license": "MIT",
      "tags": [
        "xml",
        "parser",
        "c",
        "streaming"
      ],
      "id": 90
    },
    {
      "name": "dataclasses-json",
      "one_line_profile": "Serialization for Python Data Classes",
      "detailed_description": "A library that provides simple APIs to convert Python Data Classes to and from JSON. It is extensively used in Python-based scientific data pipelines to structure, validate, and serialize experimental parameters and results.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "serialization",
        "data_binding"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/lidatong/dataclasses-json",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "json",
        "serialization",
        "python",
        "dataclasses"
      ],
      "id": 91
    },
    {
      "name": "zsv",
      "one_line_profile": "High-performance CSV parser and CLI toolkit",
      "detailed_description": "A fast, SIMD-accelerated CSV parser library and command-line utility. It is designed for processing large tabular datasets common in scientific research, offering significant speed advantages over standard parsers.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "data_processing"
      ],
      "application_level": "solver",
      "primary_language": "C",
      "repo_url": "https://github.com/liquidaty/zsv",
      "help_website": [
        "https://zsv-lib.github.io/"
      ],
      "license": "MIT",
      "tags": [
        "csv",
        "simd",
        "high-performance",
        "cli"
      ],
      "id": 92
    },
    {
      "name": "YAJL",
      "one_line_profile": "Fast streaming JSON parsing library in C",
      "detailed_description": "A small, event-driven (SAX-style) JSON parser written in C. It is used in high-performance computing environments to parse large JSON data streams with minimal memory overhead.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "io"
      ],
      "application_level": "library",
      "primary_language": "C",
      "repo_url": "https://github.com/lloyd/yajl",
      "help_website": [
        "http://lloyd.github.io/yajl/"
      ],
      "license": "ISC",
      "tags": [
        "json",
        "parser",
        "c",
        "streaming"
      ],
      "id": 93
    },
    {
      "name": "marshmallow_dataclass",
      "one_line_profile": "Automatic marshmallow schemas from dataclasses",
      "detailed_description": "A library that automates the creation of Marshmallow schemas from Python dataclasses. It simplifies data validation and serialization/deserialization pipelines in scientific Python applications.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "serialization",
        "validation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/lovasoa/marshmallow_dataclass",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "python",
        "serialization",
        "schema",
        "validation"
      ],
      "id": 94
    },
    {
      "name": "streamlit-pydantic",
      "one_line_profile": "Auto-generate Streamlit UI from Pydantic Models",
      "detailed_description": "A utility that automatically generates Streamlit user interfaces based on Pydantic data models. It is useful for rapidly building interactive dashboards and parameter configuration forms for scientific models and data analysis scripts.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "visualization",
        "ui_generation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/lukasmasuch/streamlit-pydantic",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "streamlit",
        "pydantic",
        "dashboard",
        "ui"
      ],
      "id": 95
    },
    {
      "name": "Lux",
      "one_line_profile": "Intelligent visual discovery for Pandas dataframes",
      "detailed_description": "A Python library that enhances Pandas dataframes by automatically suggesting and generating visualizations. It helps researchers quickly explore and understand trends and patterns in their data without writing complex plotting code.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "data_visualization",
        "eda"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/lux-org/lux",
      "help_website": [
        "https://lux-api.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "visualization",
        "pandas",
        "eda",
        "data-analysis"
      ],
      "id": 96
    },
    {
      "name": "Toasted Marshmallow",
      "one_line_profile": "JIT compiler for Marshmallow serialization",
      "detailed_description": "A performance optimization library for Marshmallow that generates JIT-compiled code for serialization. It significantly speeds up data processing pipelines that rely on Marshmallow for handling large volumes of scientific data.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "serialization",
        "optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/lyft/toasted-marshmallow",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "python",
        "performance",
        "serialization",
        "marshmallow"
      ],
      "id": 97
    },
    {
      "name": "dblp-parser",
      "one_line_profile": "Parser for DBLP bibliography data",
      "detailed_description": "A Python tool to parse the DBLP computer science bibliography XML dataset into structured formats. It is used in scientometrics and network analysis research to study citation graphs and publication trends.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "data_parsing",
        "scientometrics"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/macks22/dblp",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "dblp",
        "bibliography",
        "xml-parser",
        "scientometrics"
      ],
      "id": 98
    },
    {
      "name": "Arctic",
      "one_line_profile": "High performance datastore for time series and tick data",
      "detailed_description": "A high-performance time-series and tick data store built on top of MongoDB. While developed for finance, it is applicable to any scientific domain requiring efficient storage and retrieval of large-scale numerical time-series data.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "data_storage",
        "time_series"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/man-group/arctic",
      "help_website": [
        "https://arctic.readthedocs.io/"
      ],
      "license": "LGPL-2.1",
      "tags": [
        "time-series",
        "database",
        "mongodb",
        "python"
      ],
      "id": 99
    },
    {
      "name": "D-Tale",
      "one_line_profile": "Visualizer for pandas data structures",
      "detailed_description": "A tool that brings a Flask-based backend and a React frontend to visualize and analyze Pandas dataframes. It provides a GUI for data exploration, cleaning, and analysis, making it easier to interact with scientific datasets.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "data_visualization",
        "eda",
        "data_cleaning"
      ],
      "application_level": "solver",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/man-group/dtale",
      "help_website": [
        "https://github.com/man-group/dtale"
      ],
      "license": "LGPL-2.1",
      "tags": [
        "pandas",
        "visualization",
        "gui",
        "eda"
      ],
      "id": 100
    },
    {
      "name": "json_repair",
      "one_line_profile": "Repair invalid JSON from LLMs",
      "detailed_description": "A Python module designed to fix malformed JSON strings, specifically those generated by Large Language Models (LLMs). This is a critical utility in AI4S workflows where LLMs are used to extract structured data from scientific literature or experiments.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_cleaning",
        "parsing",
        "llm_utility"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/mangiucugna/json_repair",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "json",
        "llm",
        "parsing",
        "repair"
      ],
      "id": 101
    },
    {
      "name": "dataclasses-avroschema",
      "one_line_profile": "Generate Avro schemas from Python dataclasses",
      "detailed_description": "A library to generate Avro schemas from Python dataclasses and Pydantic models, and to serialize/deserialize data. Avro is a common format in large-scale scientific data processing (e.g., in bioinformatics and physics pipelines).",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "serialization",
        "schema_generation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/marcosschroh/dataclasses-avroschema",
      "help_website": [
        "https://marcosschroh.github.io/dataclasses-avroschema/"
      ],
      "license": "MIT",
      "tags": [
        "avro",
        "schema",
        "serialization",
        "python"
      ],
      "id": 102
    },
    {
      "name": "sqlparser",
      "one_line_profile": "SQL parser for querying CSV files",
      "detailed_description": "A Go library that implements a SQL parser specifically for querying CSV files. It enables researchers to use standard SQL syntax to filter and aggregate data stored in flat CSV files without loading them into a full database.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_querying",
        "parsing"
      ],
      "application_level": "library",
      "primary_language": "Go",
      "repo_url": "https://github.com/marianogappa/sqlparser",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "sql",
        "csv",
        "query",
        "parser"
      ],
      "id": 103
    },
    {
      "name": "tidypolars",
      "one_line_profile": "Tidy interface to Polars",
      "detailed_description": "A Python library that provides a syntax similar to R's Tidyverse for the Polars dataframe library. It facilitates high-performance data analysis for researchers familiar with R/dplyr conventions.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "data_analysis",
        "data_manipulation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/markfairbanks/tidypolars",
      "help_website": [
        "https://tidypolars.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "polars",
        "tidyverse",
        "data-analysis",
        "python"
      ],
      "id": 104
    },
    {
      "name": "OCT-Converter",
      "one_line_profile": "Tool for extracting raw data from proprietary Optical Coherence Tomography (OCT) file formats",
      "detailed_description": "A Python library designed to extract raw optical coherence tomography (OCT) and fundus data from proprietary file formats (e.g., .fda, .e2e, .img) used in medical imaging devices, facilitating ophthalmic research and data analysis.",
      "domains": [
        "D1",
        "Medical Imaging"
      ],
      "subtask_category": [
        "data_extraction",
        "format_conversion"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/marksgraham/OCT-Converter",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "oct",
        "medical-imaging",
        "ophthalmology",
        "file-conversion"
      ],
      "id": 105
    },
    {
      "name": "nuclei",
      "one_line_profile": "Parser, viewer, and editor for Evaluated Nuclear Structure Data (ENSDF)",
      "detailed_description": "A C++ tool designed to parse, view, and edit files in the Evaluated Nuclear Structure Data File (ENSDF) format, which is the standard format for nuclear structure and decay data in nuclear physics research.",
      "domains": [
        "D1",
        "Nuclear Physics"
      ],
      "subtask_category": [
        "data_parsing",
        "data_visualization"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/martukas/nuclei",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "ensdf",
        "nuclear-physics",
        "nuclear-structure",
        "parser"
      ],
      "id": 106
    },
    {
      "name": "FHIR-Converter",
      "one_line_profile": "Conversion utility to translate legacy healthcare data formats into FHIR",
      "detailed_description": "An open-source project that provides a conversion utility to translate legacy data formats (such as HL7v2 and C-CDA) into the Fast Healthcare Interoperability Resources (FHIR) standard, supporting medical informatics research and data interoperability.",
      "domains": [
        "D1",
        "Medical Informatics"
      ],
      "subtask_category": [
        "format_conversion",
        "data_standardization"
      ],
      "application_level": "library",
      "primary_language": "Liquid",
      "repo_url": "https://github.com/microsoft/FHIR-Converter",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fhir",
        "hl7",
        "medical-informatics",
        "healthcare-data"
      ],
      "id": 107
    },
    {
      "name": "caltech-pedestrian-dataset-converter",
      "one_line_profile": "Converter for Caltech Pedestrian Dataset to standard image formats",
      "detailed_description": "A Python utility to extract and convert the Caltech Pedestrian Dataset from its proprietary .seq video format into standard image files and labels, facilitating computer vision research and benchmarking.",
      "domains": [
        "D1",
        "Computer Vision"
      ],
      "subtask_category": [
        "data_extraction",
        "format_conversion"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/mitmul/caltech-pedestrian-dataset-converter",
      "help_website": [],
      "license": null,
      "tags": [
        "computer-vision",
        "dataset-tools",
        "pedestrian-detection"
      ],
      "id": 108
    },
    {
      "name": "Modin",
      "one_line_profile": "Scalable Pandas implementation for distributed computing",
      "detailed_description": "Modin is a library that scales Pandas workflows by changing a single line of code, utilizing Ray or Dask to distribute computation across cores or clusters for large-scale scientific data analysis.",
      "domains": [
        "D1",
        "Data/Workflow"
      ],
      "subtask_category": [
        "data_processing",
        "parallel_computing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/modin-project/modin",
      "help_website": [
        "https://modin.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "pandas",
        "distributed-computing",
        "data-science"
      ],
      "id": 109
    },
    {
      "name": "Seaborn",
      "one_line_profile": "Statistical data visualization library",
      "detailed_description": "Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics, essential for scientific data exploration.",
      "domains": [
        "Data/Workflow"
      ],
      "subtask_category": [
        "visualization",
        "statistical_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/mwaskom/seaborn",
      "help_website": [
        "https://seaborn.pydata.org/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "visualization",
        "statistics",
        "plotting"
      ],
      "id": 110
    },
    {
      "name": "itables",
      "one_line_profile": "Interactive DataTables for Python DataFrames",
      "detailed_description": "Itables renders Python DataFrames (Pandas, Polars) as interactive HTML DataTables in Jupyter notebooks, facilitating data exploration and inspection in scientific workflows.",
      "domains": [
        "Data/Workflow"
      ],
      "subtask_category": [
        "visualization",
        "data_exploration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/mwouts/itables",
      "help_website": [
        "https://mwouts.github.io/itables/"
      ],
      "license": "MIT",
      "tags": [
        "jupyter",
        "pandas",
        "visualization"
      ],
      "id": 111
    },
    {
      "name": "Pandarallel",
      "one_line_profile": "Parallel processing tool for Pandas",
      "detailed_description": "Pandarallel provides a simple interface to parallelize Pandas operations on all available CPUs, significantly speeding up data preprocessing and analysis tasks in scientific pipelines.",
      "domains": [
        "Data/Workflow"
      ],
      "subtask_category": [
        "data_processing",
        "parallel_computing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/nalepae/pandarallel",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "pandas",
        "parallelization",
        "performance"
      ],
      "id": 112
    },
    {
      "name": "Narwhals",
      "one_line_profile": "Compatibility layer for dataframe libraries",
      "detailed_description": "Narwhals is a lightweight compatibility layer that allows library maintainers to write dataframe-agnostic code, supporting Pandas, Polars, and others, facilitating the development of interoperable scientific tools.",
      "domains": [
        "Data/Workflow"
      ],
      "subtask_category": [
        "data_interoperability",
        "workflow_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/narwhals-dev/narwhals",
      "help_website": [
        "https://narwhals-dev.github.io/narwhals/"
      ],
      "license": "MIT",
      "tags": [
        "dataframe",
        "interoperability",
        "polars"
      ],
      "id": 113
    },
    {
      "name": "trdsql",
      "one_line_profile": "CLI tool to execute SQL queries on CSV/JSON/YAML",
      "detailed_description": "trdsql is a command-line tool that allows executing SQL queries directly on CSV, LTSV, JSON, and YAML files, enabling efficient data filtering, aggregation, and transformation in scientific data pipelines.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_querying",
        "data_conversion"
      ],
      "application_level": "solver",
      "primary_language": "Go",
      "repo_url": "https://github.com/noborus/trdsql",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "sql",
        "csv",
        "data-processing"
      ],
      "id": 114
    },
    {
      "name": "MinerU",
      "one_line_profile": "High-quality data extraction tool for scientific documents",
      "detailed_description": "A data processing tool designed to transform complex scientific documents (PDFs) into machine-readable formats (Markdown/JSON). It handles formulas, tables, and layout analysis, specifically enabling LLM-based scientific literature mining and agentic workflows.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "document_parsing",
        "data_extraction",
        "layout_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/opendatalab/MinerU",
      "help_website": [
        "https://github.com/opendatalab/MinerU"
      ],
      "license": "AGPL-3.0",
      "tags": [
        "pdf-parsing",
        "scientific-literature",
        "llm-data-prep"
      ],
      "id": 115
    },
    {
      "name": "Granola",
      "one_line_profile": "Serialization library for Apple HealthKit clinical data",
      "detailed_description": "A library designed to serialize Apple HealthKit data into Open mHealth compliant JSON formats. It facilitates the standardization and interoperability of clinical and personal health data for medical informatics research.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_serialization",
        "health_informatics",
        "data_standardization"
      ],
      "application_level": "library",
      "primary_language": "Objective-C",
      "repo_url": "https://github.com/openmhealth/Granola",
      "help_website": [
        "https://www.openmhealth.org/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "healthkit",
        "openmhealth",
        "clinical-data"
      ],
      "id": 116
    },
    {
      "name": "Buckaroo",
      "one_line_profile": "Interactive data exploration UI for scientific dataframes",
      "detailed_description": "A GUI tool integrated into Jupyter Notebooks for exploring, cleaning, and visualizing Pandas and Polars dataframes. It accelerates the exploratory data analysis (EDA) phase of scientific workflows by providing instant summary statistics, histograms, and filtering capabilities.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "exploratory_data_analysis",
        "data_visualization",
        "data_cleaning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/paddymul/buckaroo",
      "help_website": [
        "https://buckaroo-data.readthedocs.io/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "pandas",
        "jupyter",
        "eda",
        "visualization"
      ],
      "id": 117
    },
    {
      "name": "pandas",
      "one_line_profile": "Fundamental library for scientific data analysis and manipulation",
      "detailed_description": "The core library for data manipulation and analysis in Python, providing high-performance, easy-to-use data structures (DataFrames) and data analysis tools. It is the foundation for processing structured scientific data across all disciplines.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_manipulation",
        "statistical_analysis",
        "data_cleaning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pandas-dev/pandas",
      "help_website": [
        "https://pandas.pydata.org/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "dataframe",
        "data-analysis",
        "statistics"
      ],
      "id": 118
    },
    {
      "name": "hAMRonization",
      "one_line_profile": "Parser and harmonizer for antimicrobial resistance analysis reports",
      "detailed_description": "A bioinformatics tool designed to parse outputs from various Antimicrobial Resistance (AMR) prediction tools and harmonize them into a unified data structure. It facilitates the comparison and aggregation of AMR analysis results in microbiology research.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "bioinformatics_parsing",
        "data_harmonization",
        "antimicrobial_resistance"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/pha4ge/hAMRonization",
      "help_website": [
        "https://github.com/pha4ge/hAMRonization"
      ],
      "license": "LGPL-3.0",
      "tags": [
        "bioinformatics",
        "amr",
        "data-standardization"
      ],
      "id": 119
    },
    {
      "name": "Placemark",
      "one_line_profile": "Web-based editor and converter for geospatial data",
      "detailed_description": "A versatile tool for creating, editing, converting, and visualizing geospatial data. It supports a wide range of formats (GeoJSON, KML, CSV, etc.) and is used in Earth Science and GIS workflows for data preparation and visualization.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "geospatial_visualization",
        "data_conversion",
        "map_editing"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/placemark/placemark",
      "help_website": [
        "https://www.placemark.io/"
      ],
      "license": "MIT",
      "tags": [
        "gis",
        "geospatial",
        "geojson",
        "visualization"
      ],
      "id": 120
    },
    {
      "name": "GeoPolars",
      "one_line_profile": "Geospatial extensions for the Polars DataFrame library",
      "detailed_description": "GeoPolars extends the Polars DataFrame library with geospatial data types and operations, enabling fast processing of geographic data.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "scientific_data_processing",
        "geospatial_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pola-rs/geopolars",
      "help_website": [
        "https://github.com/pola-rs/geopolars"
      ],
      "license": "MIT",
      "tags": [
        "geospatial",
        "polars",
        "gis",
        "dataframe"
      ],
      "id": 121
    },
    {
      "name": "Polars",
      "one_line_profile": "High-performance DataFrame library for data manipulation and analysis",
      "detailed_description": "Polars is a blazingly fast DataFrames library implemented in Rust using Apache Arrow Columnar Format as the memory model. It is a key tool for scientific data processing and analysis, serving as a modern alternative to Pandas.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "scientific_data_processing",
        "data_analysis"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/pola-rs/polars",
      "help_website": [
        "https://pola.rs/"
      ],
      "license": "MIT",
      "tags": [
        "dataframe",
        "data-analysis",
        "arrow",
        "rust"
      ],
      "id": 122
    },
    {
      "name": "Polars CLI",
      "one_line_profile": "Command-line interface for running SQL queries on data files using Polars",
      "detailed_description": "A command-line tool that allows users to run SQL queries directly on CSV, Parquet, and JSON files using the Polars engine, facilitating quick scientific data inspection and processing.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "scientific_data_processing",
        "data_query"
      ],
      "application_level": "solver",
      "primary_language": "Rust",
      "repo_url": "https://github.com/pola-rs/polars-cli",
      "help_website": [
        "https://github.com/pola-rs/polars-cli"
      ],
      "license": "MIT",
      "tags": [
        "cli",
        "sql",
        "data-processing",
        "polars"
      ],
      "id": 123
    },
    {
      "name": "polars-xdt",
      "one_line_profile": "DateTime extension library for Polars",
      "detailed_description": "A plugin for Polars that provides additional datetime functionality, such as business day calculations and holiday handling, useful for time-series analysis in scientific and economic research.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "scientific_data_processing",
        "time_series_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pola-rs/polars-xdt",
      "help_website": [
        "https://github.com/pola-rs/polars-xdt"
      ],
      "license": "MIT",
      "tags": [
        "datetime",
        "polars-plugin",
        "time-series"
      ],
      "id": 124
    },
    {
      "name": "r-polars",
      "one_line_profile": "R bindings for the Polars DataFrame library",
      "detailed_description": "Provides R language bindings for Polars, enabling high-performance data manipulation and analysis within the R scientific computing ecosystem.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "scientific_data_processing",
        "data_analysis"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/pola-rs/r-polars",
      "help_website": [
        "https://rpolars.github.io/"
      ],
      "license": "MIT",
      "tags": [
        "r",
        "dataframe",
        "data-analysis",
        "polars"
      ],
      "id": 125
    },
    {
      "name": "esri2open",
      "one_line_profile": "Tool to export ESRI Feature Classes to open data formats",
      "detailed_description": "A Python toolbox that converts proprietary ESRI geospatial data formats into open standards like CSV, JSON, and GeoJSON, facilitating open science and geospatial data interoperability.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "scientific_data_processing",
        "format_conversion"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/project-open-data/esri2open",
      "help_website": [
        "https://github.com/project-open-data/esri2open"
      ],
      "license": "MIT",
      "tags": [
        "gis",
        "geospatial",
        "data-conversion",
        "esri"
      ],
      "id": 126
    },
    {
      "name": "PyNLPl",
      "one_line_profile": "Python library for Natural Language Processing and linguistic data parsing",
      "detailed_description": "PyNLPl (Pineapple) is a library for Natural Language Processing that includes parsers for specific linguistic research formats like FoLiA, Giza, and Moses, supporting computational linguistics research.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "scientific_data_processing",
        "linguistic_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/proycon/pynlpl",
      "help_website": [
        "https://pynlpl.readthedocs.io/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "nlp",
        "computational-linguistics",
        "folia",
        "parser"
      ],
      "id": 127
    },
    {
      "name": "xarray",
      "one_line_profile": "N-D labeled arrays and datasets for physical sciences",
      "detailed_description": "Xarray introduces labels in the form of dimensions, coordinates and attributes on top of raw NumPy-like arrays, making it a fundamental tool for processing multi-dimensional scientific data (e.g., climate, physics, oceanography).",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "scientific_data_processing",
        "scientific_modeling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pydata/xarray",
      "help_website": [
        "https://docs.xarray.dev/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "netcdf",
        "climate-science",
        "multi-dimensional-arrays",
        "physics"
      ],
      "id": 128
    },
    {
      "name": "windrose",
      "one_line_profile": "Python library to manage wind data and draw windroses",
      "detailed_description": "A specialized library for meteorology to analyze wind data, draw polar rose plots (windroses), and fit Weibull probability density functions.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "scientific_visualization",
        "scientific_data_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/python-windrose/windrose",
      "help_website": [
        "https://windrose.readthedocs.io/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "meteorology",
        "wind-data",
        "visualization",
        "weibull"
      ],
      "id": 129
    },
    {
      "name": "cuDF",
      "one_line_profile": "GPU-accelerated DataFrame library",
      "detailed_description": "cuDF is a GPU DataFrame library for loading, joining, aggregating, filtering, and manipulating data. It is a core component of the RAPIDS ecosystem, enabling high-performance scientific data analysis on GPUs.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "scientific_data_processing",
        "data_analysis"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/rapidsai/cudf",
      "help_website": [
        "https://docs.rapids.ai/api/cudf/stable/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "gpu",
        "dataframe",
        "cuda",
        "rapids"
      ],
      "id": 130
    },
    {
      "name": "Cufflinks",
      "one_line_profile": "Productivity tool that binds Plotly to Pandas dataframes for easy scientific visualization",
      "detailed_description": "Cufflinks connects the Pandas data analysis library with Plotly, enabling users to create interactive visualizations directly from Pandas DataFrames. It is widely used in scientific data analysis workflows to quickly generate charts for data exploration and reporting.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "visualization",
        "data_exploration"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/santosjorge/cufflinks",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "visualization",
        "pandas",
        "plotly",
        "data-analysis"
      ],
      "id": 131
    },
    {
      "name": "VisiData",
      "one_line_profile": "Terminal interface for exploring and arranging tabular data",
      "detailed_description": "VisiData is an interactive multitool for exploring, analyzing, and manipulating tabular data (CSV, Excel, JSON, HDF5, etc.) directly in the terminal. It supports filtering, summarization, and basic statistical analysis, making it a powerful tool for scientific data quality control and quick inspection.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_exploration",
        "quality_control",
        "data_processing"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/saulpw/visidata",
      "help_website": [
        "https://www.visidata.org/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "cli",
        "data-exploration",
        "csv",
        "tabular-data",
        "statistics"
      ],
      "id": 132
    },
    {
      "name": "PandasAI",
      "one_line_profile": "Generative AI capability wrapper for pandas to enable conversational data analysis",
      "detailed_description": "A Python library that integrates Large Language Models (LLMs) with pandas, allowing users to perform data analysis, manipulation, and visualization on scientific datasets (CSV, Parquet, SQL) using natural language queries.",
      "domains": [
        "D1",
        "D4"
      ],
      "subtask_category": [
        "data_analysis",
        "data_visualization",
        "natural_language_query"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/sinaptik-ai/pandas-ai",
      "help_website": [
        "https://docs.pandas-ai.com/"
      ],
      "license": "MIT",
      "tags": [
        "pandas",
        "llm",
        "data-analysis",
        "conversational-ai"
      ],
      "id": 133
    },
    {
      "name": "PyTorch Forecasting",
      "one_line_profile": "Deep learning library for time series forecasting built on PyTorch",
      "detailed_description": "A high-level library for time series forecasting with neural networks. It provides state-of-the-art models (like Temporal Fusion Transformers) and facilitates data handling for scientific time-series tasks.",
      "domains": [
        "D4",
        "M1"
      ],
      "subtask_category": [
        "time_series_forecasting",
        "modeling",
        "deep_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/sktime/pytorch-forecasting",
      "help_website": [
        "https://pytorch-forecasting.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "pytorch",
        "time-series",
        "forecasting",
        "deep-learning"
      ],
      "id": 134
    },
    {
      "name": "pdbx",
      "one_line_profile": "Python parser for Protein Data Bank (PDB) mmCIF format files",
      "detailed_description": "A specialized parser module developed by the Soeding Lab for handling macromolecular structure data in the PDBx/mmCIF format, essential for structural biology and bioinformatics workflows.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_parsing",
        "structure_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/soedinglab/pdbx",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "pdb",
        "mmcif",
        "protein-structure",
        "bioinformatics"
      ],
      "id": 135
    },
    {
      "name": "GarminDB",
      "one_line_profile": "Parser and database manager for Garmin/FitBit health and physiological data",
      "detailed_description": "A tool that parses binary FIT files, TCX, and other proprietary formats from health wearables (Garmin, Fitbit) into a SQLite database for physiological data analysis and visualization.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_parsing",
        "physiological_data_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/tcgoetz/GarminDB",
      "help_website": [],
      "license": "GPL-2.0",
      "tags": [
        "health-informatics",
        "wearables",
        "fit-format",
        "sqlite",
        "quantified-self"
      ],
      "id": 136
    },
    {
      "name": "vroom",
      "one_line_profile": "High-performance delimited file parser for R",
      "detailed_description": "A fast data reading library for R that indexes delimited files (CSV, TSV) for rapid access, widely used in bioinformatics and data science workflows for handling large datasets.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_loading",
        "parsing"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/tidyverse/vroom",
      "help_website": [
        "https://vroom.r-lib.org"
      ],
      "license": "NOASSERTION",
      "tags": [
        "csv-parser",
        "r-package",
        "high-performance",
        "data-science"
      ],
      "id": 137
    },
    {
      "name": "pubmed_parser",
      "one_line_profile": "Parser for PubMed Open-Access XML and MEDLINE XML datasets",
      "detailed_description": "A Python library specifically designed to parse PubMed and MEDLINE XML datasets, enabling bibliometric analysis, text mining, and meta-science research on biomedical literature.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "literature_mining",
        "metadata_extraction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/titipata/pubmed_parser",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pubmed",
        "medline",
        "xml-parser",
        "nlp",
        "bibliometrics"
      ],
      "id": 138
    },
    {
      "name": "gpxpy",
      "one_line_profile": "GPX file parser and manipulator",
      "detailed_description": "A Python library for parsing and manipulating GPX (GPS Exchange Format) files, commonly used in geospatial analysis, ecology (tracking), and sports science.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "geospatial_parsing",
        "trajectory_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/tkrajina/gpxpy",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "gpx",
        "gps",
        "geospatial",
        "xml-parser"
      ],
      "id": 139
    },
    {
      "name": "OSM2World",
      "one_line_profile": "Converter creating 3D models from OpenStreetMap data",
      "detailed_description": "A tool that parses OpenStreetMap (OSM) data and converts it into three-dimensional models, used for urban simulation, geospatial visualization, and cartography.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "3d_reconstruction",
        "geospatial_visualization"
      ],
      "application_level": "solver",
      "primary_language": "Java",
      "repo_url": "https://github.com/tordanik/OSM2World",
      "help_website": [
        "http://osm2world.org/"
      ],
      "license": "LGPL-3.0",
      "tags": [
        "openstreetmap",
        "3d-modeling",
        "geospatial",
        "visualization"
      ],
      "id": 140
    },
    {
      "name": "Pandera",
      "one_line_profile": "Statistical data validation and testing library for pandas dataframes",
      "detailed_description": "Pandera provides a flexible and expressive API for performing statistical validation on dataframe-like objects. It is widely used in scientific data pipelines to ensure data quality, verify schema consistency, and perform hypothesis testing on tabular data structures.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "quality_control",
        "data_validation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/unionai-oss/pandera",
      "help_website": [
        "https://pandera.readthedocs.io"
      ],
      "license": "MIT",
      "tags": [
        "data-validation",
        "pandas",
        "quality-control",
        "schema-validation"
      ],
      "id": 141
    },
    {
      "name": "BioBear",
      "one_line_profile": "Bioinformatics file processing using Arrow and Polars",
      "detailed_description": "BioBear is a library that enables reading and processing of standard bioinformatics file formats (FASTA, FASTQ, VCF, BAM, GFF) directly into Polars DataFrames or Arrow tables. It facilitates high-performance data analysis workflows for genomics and biological data.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_parsing",
        "format_conversion"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/wheretrue/biobear",
      "help_website": [
        "https://www.wheretrue.com/biobear"
      ],
      "license": "MIT",
      "tags": [
        "bioinformatics",
        "genomics",
        "polars",
        "arrow",
        "fastq",
        "vcf"
      ],
      "id": 142
    },
    {
      "name": "csvkit",
      "one_line_profile": "Command-line suite for converting and processing CSV data",
      "detailed_description": "csvkit is a suite of command-line tools for converting to and working with CSV, the common tabular file format in scientific research. It allows researchers to inspect, filter, slice, join, and analyze tabular data directly from the terminal, facilitating reproducible data cleaning workflows.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_cleaning",
        "format_conversion",
        "data_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/wireservice/csvkit",
      "help_website": [
        "https://csvkit.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "csv",
        "data-cleaning",
        "cli",
        "tabular-data"
      ],
      "id": 143
    },
    {
      "name": "ydata-profiling",
      "one_line_profile": "Automated exploratory data analysis and quality profiling tool for Pandas and Spark DataFrames",
      "detailed_description": "A primary tool for exploratory data analysis (EDA) in scientific data workflows. It generates comprehensive profile reports from dataframes, providing statistical insights, correlation analysis, missing value assessment, and distribution visualization, which are essential for scientific data quality control.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "quality_control",
        "exploratory_data_analysis",
        "statistical_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ydataai/ydata-profiling",
      "help_website": [
        "https://docs.profiling.ydata.ai/latest/"
      ],
      "license": "MIT",
      "tags": [
        "eda",
        "data-profiling",
        "quality-control",
        "pandas",
        "statistics"
      ],
      "id": 144
    },
    {
      "name": "Dataset_to_VOC_converter",
      "one_line_profile": "Scripts to convert computer vision datasets (Caltech, COCO, HDA) to PASCAL VOC format",
      "detailed_description": "A set of utility scripts designed to convert various standard computer vision datasets (Caltech pedestrian, MS COCO, HDA) into the PASCAL VOC XML format. This facilitates the normalization of data inputs for object detection model training in scientific research.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_conversion",
        "dataset_preparation",
        "normalization"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/zongfan2/Dataset_to_VOC_converter",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "computer-vision",
        "dataset-conversion",
        "pascal-voc",
        "coco",
        "object-detection"
      ],
      "id": 145
    }
  ]
}
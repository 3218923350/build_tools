{
  "generated_at": "2025-12-16T03:02:35.871547+08:00",
  "metadata": {
    "leaf_cluster": {
      "leaf_cluster_id": "D1",
      "leaf_cluster_name": "科研数据格式/解析/转换生态",
      "domain": "Data/Workflow",
      "typical_objects": "domain formats",
      "task_chain": "解析→转换→校验→ETL→版本",
      "tool_form": "解析器 + 验证器 + ETL"
    },
    "unit": {
      "unit_id": "D1-03",
      "unit_name": "元数据/Schema/校验与质量",
      "target_scale": "150–350",
      "coverage_tools": "schema tools、QC"
    },
    "search": {
      "target_candidates": 350,
      "queries": [
        "[GH] CheckM",
        "[GH] jsonschema",
        "[GH] Cerberus",
        "[GH] Frictionless",
        "[GH] LinkML",
        "[GH] MultiQC",
        "[GH] FastQC",
        "[GH] Pydantic",
        "[GH] Pandera",
        "[GH] Great Expectations",
        "[GH] data validation",
        "[GH] quality control",
        "[GH] schema validator",
        "[GH] metadata management",
        "[GH] data integrity",
        "[GH] json schema",
        "[GH] pydantic",
        "[GH] pandera",
        "[GH] great expectations",
        "[GH] fastqc",
        "[GH] multiqc",
        "[GH] cif validation",
        "[GH] smiles validation",
        "[GH] dataset checker",
        "[WEB] scientific data validation tools github",
        "[WEB] data quality control library python github",
        "[WEB] metadata schema management tools github",
        "[WEB] bioinformatics quality control tools github",
        "[WEB] material science data validation github"
      ],
      "total_candidates": 1073,
      "tool_candidates": 798,
      "final_tools": 127
    }
  },
  "tools": [
    {
      "name": "MungeSumstats",
      "one_line_profile": "Rapid standardisation and quality control of GWAS or QTL summary statistics",
      "detailed_description": "A Bioconductor package designed to facilitate the standardization and quality control of Genome-Wide Association Study (GWAS) summary statistics. It handles formatting, filtering, and correcting inconsistencies to ensure data integrity for downstream analysis.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "standardization",
        "gwas_analysis"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/Al-Murphy/MungeSumstats",
      "help_website": [],
      "license": null,
      "tags": [
        "gwas",
        "bioinformatics",
        "quality-control",
        "statistics"
      ],
      "id": 1
    },
    {
      "name": "SOAPnuke",
      "one_line_profile": "Integrated Quality Control and Preprocessing tool for FASTQ or BAM/CRAM sequencing data",
      "detailed_description": "A software tool developed by BGI for integrated quality control and preprocessing of high-throughput sequencing data. It supports filtering low-quality reads, trimming adapters, and generating statistics for FASTQ, BAM, and CRAM files.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "preprocessing",
        "filtering"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/BGI-flexlab/SOAPnuke",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "bioinformatics",
        "ngs",
        "quality-control",
        "fastq"
      ],
      "id": 2
    },
    {
      "name": "SQANTI3",
      "one_line_profile": "Tool for the Quality Control of Long-Read Defined Transcriptomes",
      "detailed_description": "A pipeline for the structural classification and quality control of isoforms defined by long-read sequencing technologies (PacBio, Oxford Nanopore). It characterizes transcripts based on their splice junctions and compares them to a reference annotation.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "isoform_classification",
        "transcriptomics"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ConesaLab/SQANTI3",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "transcriptomics",
        "long-read-sequencing",
        "quality-control",
        "isoforms"
      ],
      "id": 3
    },
    {
      "name": "blobtools",
      "one_line_profile": "Modular command-line solution for visualisation, quality control and taxonomic partitioning of genome datasets",
      "detailed_description": "A toolset for the visualization, quality control, and taxonomic partitioning of genome assemblies. It allows researchers to identify contamination and assess the quality of genomic datasets using GC content, coverage, and taxonomy.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "visualization",
        "taxonomic_partitioning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/DRL/blobtools",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "genomics",
        "visualization",
        "quality-control",
        "taxonomy"
      ],
      "id": 4
    },
    {
      "name": "CheckM",
      "one_line_profile": "Assess the quality of microbial genomes recovered from isolates, single cells, and metagenomes",
      "detailed_description": "A tool for assessing the quality of microbial genomes recovered from isolates, single cells, and metagenomes. It provides robust estimates of genome completeness and contamination by using collocated sets of genes that are ubiquitous and single-copy within a phylogenetic lineage.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_assessment",
        "genome_recovery",
        "metagenomics"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Ecogenomics/CheckM",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "microbiology",
        "genomics",
        "quality-assessment",
        "metagenomics"
      ],
      "id": 5
    },
    {
      "name": "TrimGalore",
      "one_line_profile": "A wrapper around Cutadapt and FastQC for consistent adapter and quality trimming of FastQ files",
      "detailed_description": "Trim Galore is a wrapper script to automate quality and adapter trimming as well as quality control, with some added functionality to remove Galore-specific RRBS sequence diversity bias when processing RRBS libraries.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "adapter_trimming",
        "preprocessing"
      ],
      "application_level": "workflow",
      "primary_language": "Perl",
      "repo_url": "https://github.com/FelixKrueger/TrimGalore",
      "help_website": [
        "https://github.com/FelixKrueger/TrimGalore/blob/master/Docs/Trim_Galore_User_Guide.md"
      ],
      "license": "GPL-3.0",
      "tags": [
        "bioinformatics",
        "fastq",
        "quality-control",
        "ngs"
      ],
      "id": 6
    },
    {
      "name": "dropSeqPipe",
      "one_line_profile": "A Snakemake workflow for SingleCell RNASeq pre-processing",
      "detailed_description": "dropSeqPipe is a comprehensive pipeline for processing Drop-Seq data, handling steps from raw reads to expression matrices, including quality control and alignment.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "preprocessing",
        "quality_control",
        "alignment"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/Hoohm/dropSeqPipe",
      "help_website": [
        "https://github.com/Hoohm/dropSeqPipe"
      ],
      "license": "CC-BY-SA-4.0",
      "tags": [
        "bioinformatics",
        "rnaseq",
        "single-cell",
        "snakemake"
      ],
      "id": 7
    },
    {
      "name": "kgcl",
      "one_line_profile": "Data model library for the Knowledge Graph Change Language (KGCL)",
      "detailed_description": "Provides the data model and implementation for KGCL, a language designed for defining and executing changes in ontologies and knowledge graphs, widely used in biomedical informatics.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "ontology_management",
        "data_modeling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/INCATools/kgcl",
      "help_website": [
        "https://github.com/INCATools/kgcl"
      ],
      "license": "MIT",
      "tags": [
        "ontology",
        "knowledge-graph",
        "bioinformatics",
        "metadata"
      ],
      "id": 8
    },
    {
      "name": "kgcl-rdflib",
      "one_line_profile": "RDFLib-based tools for manipulating ontologies using KGCL",
      "detailed_description": "A Python library that provides functionality to apply Knowledge Graph Change Language (KGCL) operations on RDF graphs using RDFLib, facilitating ontology curation and evolution.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "ontology_manipulation",
        "graph_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/INCATools/kgcl-rdflib",
      "help_website": [
        "https://github.com/INCATools/kgcl-rdflib"
      ],
      "license": "MIT",
      "tags": [
        "rdflib",
        "ontology",
        "kgcl",
        "bioinformatics"
      ],
      "id": 9
    },
    {
      "name": "semantic-sql",
      "one_line_profile": "Library to create SQL and SQLite builds from OWL ontologies",
      "detailed_description": "A tool designed to convert OWL ontologies into SQL/SQLite formats, enabling efficient querying and integration of semantic data within relational database systems, commonly used in biomedical ontology pipelines.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "format_conversion",
        "ontology_querying"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/INCATools/semantic-sql",
      "help_website": [
        "https://github.com/INCATools/semantic-sql"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "ontology",
        "sql",
        "owl",
        "bioinformatics"
      ],
      "id": 10
    },
    {
      "name": "bombcell",
      "one_line_profile": "Automated quality control and curation tool for spike-sorted electrophysiology data",
      "detailed_description": "A toolbox for automated quality control and curation of spike-sorted data, allowing researchers to classify neurons and assess recording quality in electrophysiology experiments.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "spike_sorting_curation",
        "classification"
      ],
      "application_level": "solver",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/Julie-Fabre/bombcell",
      "help_website": [
        "https://github.com/Julie-Fabre/bombcell/wiki"
      ],
      "license": "GPL-3.0",
      "tags": [
        "neuroscience",
        "electrophysiology",
        "quality-control",
        "spike-sorting"
      ],
      "id": 11
    },
    {
      "name": "MegaQC",
      "one_line_profile": "Longitudinal quality control monitoring platform for MultiQC reports",
      "detailed_description": "MegaQC is a web application designed to aggregate, store, and visualize data from multiple MultiQC runs over time. It enables sequencing facilities and bioinformatics cores to track quality metrics across projects and monitor trends in sequencing performance.",
      "domains": [
        "D1",
        "D1-03",
        "Bioinformatics"
      ],
      "subtask_category": [
        "quality_control",
        "qc_reporting",
        "longitudinal_analysis"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/MultiQC/MegaQC",
      "help_website": [
        "https://megaqc.info/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "bioinformatics",
        "quality-control",
        "visualization",
        "dashboard"
      ],
      "id": 12
    },
    {
      "name": "MultiQC",
      "one_line_profile": "Aggregate bioinformatics analysis reports across many samples",
      "detailed_description": "MultiQC searches a given directory for analysis logs and compiles a HTML report. It's a general use tool, perfect for summarizing the output from numerous bioinformatics tools.",
      "domains": [
        "D1",
        "D1-03",
        "Bioinformatics"
      ],
      "subtask_category": [
        "quality_control",
        "qc_reporting",
        "data_aggregation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/MultiQC/MultiQC",
      "help_website": [
        "https://multiqc.info/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "bioinformatics",
        "ngs",
        "quality-control",
        "reporting"
      ],
      "id": 13
    },
    {
      "name": "MultiQC_SAV",
      "one_line_profile": "MultiQC plugin for Illumina Sequencing Analysis Viewer (SAV) data",
      "detailed_description": "A plugin for MultiQC that parses and visualizes InterOp data from Illumina sequencers, replicating plots found in the Illumina Sequencing Analysis Viewer (SAV) for quality control purposes.",
      "domains": [
        "D1",
        "D1-03",
        "Bioinformatics"
      ],
      "subtask_category": [
        "quality_control",
        "sequencing_qc"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/MultiQC/MultiQC_SAV",
      "help_website": [
        "https://github.com/MultiQC/MultiQC_SAV"
      ],
      "license": "MIT",
      "tags": [
        "illumina",
        "sequencing",
        "quality-control",
        "multiqc-plugin"
      ],
      "id": 14
    },
    {
      "name": "MultiQC_bcbio",
      "one_line_profile": "MultiQC plugin for bcbio-nextgen pipeline metrics",
      "detailed_description": "A plugin for MultiQC that incorporates metrics and logs specifically generated by the bcbio-nextgen bioinformatics analysis pipeline into the aggregate QC report.",
      "domains": [
        "D1",
        "D1-03",
        "Bioinformatics"
      ],
      "subtask_category": [
        "quality_control",
        "pipeline_reporting"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/MultiQC/MultiQC_bcbio",
      "help_website": [
        "https://github.com/MultiQC/MultiQC_bcbio"
      ],
      "license": "MIT",
      "tags": [
        "bcbio",
        "bioinformatics",
        "quality-control",
        "multiqc-plugin"
      ],
      "id": 15
    },
    {
      "name": "AfterQC",
      "one_line_profile": "Automatic filtering, trimming, error removing and quality control for FASTQ data",
      "detailed_description": "AfterQC is a tool for automatic quality control of NGS data. It performs filtering, trimming, error correction, and quality visualization for FASTQ files, designed to be faster and more automated than traditional tools.",
      "domains": [
        "D1",
        "D1-03",
        "Bioinformatics"
      ],
      "subtask_category": [
        "quality_control",
        "read_trimming",
        "error_correction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenGene/AfterQC",
      "help_website": [
        "https://github.com/OpenGene/AfterQC"
      ],
      "license": "MIT",
      "tags": [
        "fastq",
        "ngs",
        "quality-control",
        "trimming"
      ],
      "id": 16
    },
    {
      "name": "fastplong",
      "one_line_profile": "Ultra-fast preprocessing and quality control for long-read sequencing data",
      "detailed_description": "fastplong is a high-performance tool designed for the preprocessing and quality control of long-read sequencing data (e.g., Nanopore, PacBio). It offers functions for filtering, trimming, and generating QC statistics.",
      "domains": [
        "D1",
        "D1-03",
        "Bioinformatics"
      ],
      "subtask_category": [
        "quality_control",
        "read_preprocessing",
        "long_read_sequencing"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/OpenGene/fastplong",
      "help_website": [
        "https://github.com/OpenGene/fastplong"
      ],
      "license": "MIT",
      "tags": [
        "long-read",
        "nanopore",
        "pacbio",
        "quality-control"
      ],
      "id": 17
    },
    {
      "name": "rnaseq-pipeline",
      "one_line_profile": "RNA-seq pipeline for raw sequence alignment and quantification",
      "detailed_description": "A bioinformatics pipeline for processing RNA-seq data, handling steps from raw sequence alignment to transcript and gene quantification. Developed by the Pavlidis Lab.",
      "domains": [
        "D1",
        "Bioinformatics"
      ],
      "subtask_category": [
        "workflow",
        "alignment",
        "quantification"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/PavlidisLab/rnaseq-pipeline",
      "help_website": [
        "https://github.com/PavlidisLab/rnaseq-pipeline"
      ],
      "license": "Unlicense",
      "tags": [
        "rna-seq",
        "bioinformatics",
        "pipeline",
        "quantification"
      ],
      "id": 18
    },
    {
      "name": "Qoala-T",
      "one_line_profile": "Supervised-learning tool for quality control of FreeSurfer segmented MRI data",
      "detailed_description": "Qoala-T is a supervised learning tool designed to assess the quality of FreeSurfer-segmented MRI data. It helps in automatically identifying poor quality segmentations in neuroimaging datasets.",
      "domains": [
        "D1",
        "D1-03",
        "Neuroscience"
      ],
      "subtask_category": [
        "quality_control",
        "image_segmentation_qc",
        "neuroimaging"
      ],
      "application_level": "solver",
      "primary_language": "R",
      "repo_url": "https://github.com/Qoala-T/QC",
      "help_website": [
        "https://qoala-t.shinyapps.io/qoala-t_app/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "mri",
        "freesurfer",
        "quality-control",
        "neuroscience"
      ],
      "id": 19
    },
    {
      "name": "Schematic",
      "one_line_profile": "Biomedical data model and metadata management ingress tool",
      "detailed_description": "A package developed by Sage Bionetworks for managing biomedical data models and metadata ingress. It facilitates the validation and submission of data according to defined schemas, supporting data curation workflows in biomedical research.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "metadata_management",
        "schema_validation",
        "data_curation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Sage-Bionetworks/schematic",
      "help_website": [
        "https://schematic.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "biomedical",
        "metadata",
        "schema",
        "validation",
        "data-model"
      ],
      "id": 20
    },
    {
      "name": "Ketupa",
      "one_line_profile": "Deep learning framework for RF signal integrity analysis and simulation",
      "detailed_description": "A deep learning project applied to signal integrity and RF analysis. It automates modeling, simulation, and data storage of HFSS for patch antennas, transmission lines, vias, and connectors, training S-parameter models on simulation data.",
      "domains": [
        "Physics",
        "Engineering"
      ],
      "subtask_category": [
        "simulation",
        "modeling",
        "rf_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Shallot-2009/Ketupa",
      "help_website": [],
      "license": "BSD-2-Clause",
      "tags": [
        "deep-learning",
        "rf-engineering",
        "electromagnetics",
        "simulation"
      ],
      "id": 21
    },
    {
      "name": "3D-MedDiffusion",
      "one_line_profile": "3D medical diffusion model for controllable image generation",
      "detailed_description": "A 3D Medical Diffusion Model designed for controllable and high-quality medical image generation. It addresses the challenges of generating realistic 3D volumetric medical data for research and analysis.",
      "domains": [
        "Medical Imaging",
        "AI4S"
      ],
      "subtask_category": [
        "image_generation",
        "generative_modeling"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ShanghaiTech-IMPACT/3D-MedDiffusion",
      "help_website": [],
      "license": null,
      "tags": [
        "medical-imaging",
        "diffusion-models",
        "3d-generation",
        "deep-learning"
      ],
      "id": 22
    },
    {
      "name": "Cerberus",
      "one_line_profile": "Visual-Inertial-Leg Odometry (VILO) for legged robots",
      "detailed_description": "A state estimation framework for legged robots that fuses visual, inertial, and leg kinematics data to provide precise odometry. It is used in robotics research for localization and mapping.",
      "domains": [
        "Robotics",
        "Control Systems"
      ],
      "subtask_category": [
        "odometry",
        "state_estimation",
        "localization"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/ShuoYangRobotics/Cerberus",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "robotics",
        "odometry",
        "slam",
        "visual-inertial"
      ],
      "id": 23
    },
    {
      "name": "Cerberus2.0",
      "one_line_profile": "Low-drift Visual-Inertial-Leg Odometry for legged robots",
      "detailed_description": "The second version of the Cerberus VILO framework, offering improved precision and low-drift performance for state estimation in legged robotics research.",
      "domains": [
        "Robotics",
        "Control Systems"
      ],
      "subtask_category": [
        "odometry",
        "state_estimation",
        "localization"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/ShuoYangRobotics/Cerberus2.0",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "robotics",
        "odometry",
        "slam",
        "visual-inertial"
      ],
      "id": 24
    },
    {
      "name": "Cerberus (Histology)",
      "one_line_profile": "Multi-task learning model for histology image segmentation and classification",
      "detailed_description": "A deep learning model that enables simultaneous histology image segmentation and classification. It uses multi-task learning to improve performance on pathology image analysis tasks.",
      "domains": [
        "Bioinformatics",
        "Medical Imaging"
      ],
      "subtask_category": [
        "image_segmentation",
        "classification",
        "histology_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/TissueImageAnalytics/cerberus",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "histology",
        "pathology",
        "deep-learning",
        "segmentation"
      ],
      "id": 25
    },
    {
      "name": "Crowd-Kit",
      "one_line_profile": "Quality control and aggregation for crowdsourced data labeling",
      "detailed_description": "A Python library for computational quality control and aggregation of crowdsourced data. It provides efficient implementations of aggregation algorithms (like Dawid-Skene, GLAD) to ensure high-quality labeled datasets for machine learning research.",
      "domains": [
        "D1",
        "D1-03",
        "Machine Learning"
      ],
      "subtask_category": [
        "quality_control",
        "data_aggregation",
        "annotation_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Toloka/crowd-kit",
      "help_website": [
        "https://crowd-kit.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "crowdsourcing",
        "data-labeling",
        "quality-control",
        "aggregation"
      ],
      "id": 26
    },
    {
      "name": "FluLINE",
      "one_line_profile": "Comprehensive pipeline for influenza virus sequencing analysis",
      "detailed_description": "A bioinformatics pipeline for analyzing influenza sequencing data. It performs filtering, species identification, consensus genome generation, mapping, and SNV identification.",
      "domains": [
        "Bioinformatics",
        "Genomics"
      ],
      "subtask_category": [
        "pipeline",
        "variant_calling",
        "genome_assembly"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/UmaSangumathi/FluLINE",
      "help_website": [],
      "license": null,
      "tags": [
        "influenza",
        "ngs",
        "pipeline",
        "genomics"
      ],
      "id": 27
    },
    {
      "name": "openclean",
      "one_line_profile": "Data cleaning and profiling library for scientific data workflows",
      "detailed_description": "A Python library for data cleaning and profiling, developed by VIDA-NYU. It provides a declarative framework for detecting and fixing data quality issues, suitable for preparing scientific datasets.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_cleaning",
        "data_profiling",
        "quality_control"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/VIDA-NYU/openclean",
      "help_website": [
        "https://github.com/VIDA-NYU/openclean/tree/master/docs"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "data-cleaning",
        "profiling",
        "quality-control",
        "python"
      ],
      "id": 28
    },
    {
      "name": "ModelDB",
      "one_line_profile": "Open source ML model versioning and metadata management",
      "detailed_description": "A system for managing machine learning models, including versioning, metadata, and experiment tracking. Originally developed at MIT CSAIL, it supports scientific reproducibility in ML workflows.",
      "domains": [
        "D1",
        "Machine Learning"
      ],
      "subtask_category": [
        "experiment_tracking",
        "metadata_management",
        "reproducibility"
      ],
      "application_level": "platform",
      "primary_language": "Java",
      "repo_url": "https://github.com/VertaAI/modeldb",
      "help_website": [
        "https://modeldb.ai/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "mlops",
        "metadata",
        "experiment-tracking",
        "reproducibility"
      ],
      "id": 29
    },
    {
      "name": "cfDNApipe",
      "one_line_profile": "Quality control and analysis pipeline for cell-free DNA sequencing data",
      "detailed_description": "A comprehensive pipeline for the quality control and analysis of cell-free DNA (cfDNA) high-throughput sequencing data. It handles tasks specific to liquid biopsy data analysis.",
      "domains": [
        "Bioinformatics",
        "Genomics"
      ],
      "subtask_category": [
        "pipeline",
        "quality_control",
        "data_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/XWangLabTHU/cfDNApipe",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "cfdna",
        "ngs",
        "pipeline",
        "quality-control"
      ],
      "id": 30
    },
    {
      "name": "QuickNAT",
      "one_line_profile": "Fast brain MRI segmentation framework with uncertainty-based quality control",
      "detailed_description": "A PyTorch implementation of QuickNAT and Bayesian QuickNAT for fast and accurate segmentation of neuroanatomy from brain MRI scans. It features a structure-wise uncertainty estimation mechanism that serves as a quality control measure for the segmentation results.",
      "domains": [
        "D3",
        "D1-03"
      ],
      "subtask_category": [
        "segmentation",
        "quality_control"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ai-med/quickNAT_pytorch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mri",
        "neuroimaging",
        "segmentation",
        "quality-control",
        "uncertainty-estimation"
      ],
      "id": 31
    },
    {
      "name": "AGR Curation Schema",
      "one_line_profile": "Data schema and validation specifications for the Alliance of Genome Resources",
      "detailed_description": "The official schema repository for the Alliance of Genome Resources (AGR), defining the data models and validation rules for persistent data storage and curation of genomic data across multiple model organisms.",
      "domains": [
        "D1-03"
      ],
      "subtask_category": [
        "schema_definition",
        "data_validation"
      ],
      "application_level": "library",
      "primary_language": "Makefile",
      "repo_url": "https://github.com/alliance-genome/agr_curation_schema",
      "help_website": [
        "https://www.alliancegenome.org/"
      ],
      "license": "MIT",
      "tags": [
        "genomics",
        "schema",
        "data-curation",
        "model-organisms"
      ],
      "id": 32
    },
    {
      "name": "faster",
      "one_line_profile": "High-performance FASTQ file statistics and quality metrics calculator",
      "detailed_description": "A fast, Rust-based command-line tool for calculating statistics and quality metrics from FASTQ files, designed to provide rapid insights into sequencing data quality.",
      "domains": [
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "statistics"
      ],
      "application_level": "solver",
      "primary_language": "Rust",
      "repo_url": "https://github.com/angelovangel/faster",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fastq",
        "bioinformatics",
        "quality-control",
        "rust"
      ],
      "id": 33
    },
    {
      "name": "partialsmiles",
      "one_line_profile": "Validating SMILES parser with support for incomplete strings",
      "detailed_description": "A Python library for parsing and validating SMILES strings (Simplified Molecular Input Line Entry System), specifically designed to handle and validate incomplete or partial SMILES strings, useful for interactive chemistry applications and data cleaning.",
      "domains": [
        "D1-03"
      ],
      "subtask_category": [
        "parsing",
        "data_validation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/baoilleach/partialsmiles",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "cheminformatics",
        "smiles",
        "validation",
        "chemistry"
      ],
      "id": 34
    },
    {
      "name": "fastqc-viz",
      "one_line_profile": "Enhanced visualization for FastQC reports",
      "detailed_description": "A tool to generate improved, modernized visualizations from FastQC reports, aiding in the interpretation of quality control metrics for high-throughput sequencing data.",
      "domains": [
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "visualization"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/barreiro-r/fastqc-viz",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "fastqc",
        "visualization",
        "ngs",
        "bioinformatics"
      ],
      "id": 35
    },
    {
      "name": "pmultiqc",
      "one_line_profile": "Proteomics quality control reporting library based on MultiQC",
      "detailed_description": "A library designed to generate quality control reports for proteomics data by extending the MultiQC framework, enabling aggregated visualization of metrics from various proteomics analysis tools.",
      "domains": [
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "qc_report"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/bigbio/pmultiqc",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "proteomics",
        "quality-control",
        "multiqc",
        "bioinformatics"
      ],
      "id": 36
    },
    {
      "name": "KneadData",
      "one_line_profile": "Quality control and decontamination tool for metagenomic data",
      "detailed_description": "A tool designed to perform quality control on metagenomic and metatranscriptomic sequencing data. It separates host reads from bacterial reads and performs trimming/filtering to ensure high-quality data for downstream analysis.",
      "domains": [
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "filtering"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/biobakery/kneaddata",
      "help_website": [
        "http://huttenhower.sph.harvard.edu/kneaddata"
      ],
      "license": "NOASSERTION",
      "tags": [
        "metagenomics",
        "quality-control",
        "decontamination",
        "bioinformatics"
      ],
      "id": 37
    },
    {
      "name": "bioio",
      "one_line_profile": "Standardized image reading and metadata management for microscopy",
      "detailed_description": "A Python library providing a standardized interface for reading, writing, and managing metadata of microscopy images, supporting various bioimaging formats and facilitating seamless integration into analysis workflows.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "parsing",
        "metadata_extraction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/bioio-devs/bioio",
      "help_website": [
        "https://github.com/bioio-devs/bioio"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "microscopy",
        "bioimaging",
        "metadata",
        "io"
      ],
      "id": 38
    },
    {
      "name": "Biolink Model",
      "one_line_profile": "Data model and schema for biological entities and relationships",
      "detailed_description": "The Biolink Model is a high-level data model that defines a set of classes, slots, and relationships for representing biological knowledge. It serves as a schema for standardizing data exchange in knowledge graphs and translational science.",
      "domains": [
        "D1-03"
      ],
      "subtask_category": [
        "schema_definition",
        "data_modeling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/biolink/biolink-model",
      "help_website": [
        "https://biolink.github.io/biolink-model/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "knowledge-graph",
        "schema",
        "biology",
        "standardization"
      ],
      "id": 39
    },
    {
      "name": "biolinkml",
      "one_line_profile": "Modeling language and framework for biological entities (predecessor to LinkML)",
      "detailed_description": "A framework for defining data models and schemas for biological entities, facilitating the generation of JSON-Schema, SHACL, and other artifacts. It has been largely superseded by LinkML but remains relevant for legacy biological data models.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "schema_definition",
        "metadata_management"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/biolink/biolinkml",
      "help_website": [
        "https://biolink.github.io/biolinkml/"
      ],
      "license": "CC0-1.0",
      "tags": [
        "biolink",
        "schema",
        "metadata",
        "biology"
      ],
      "id": 40
    },
    {
      "name": "ccdhmodel",
      "one_line_profile": "LinkML schema definitions for Cancer Data Harmonization",
      "detailed_description": "Contains the LinkML model definitions for the Center for Cancer Data Harmonization (CCDH), used to harmonize and validate data across various cancer research programs.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "schema_definition",
        "data_harmonization"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/cancerDHC/ccdhmodel",
      "help_website": [
        "https://cancerdhc.github.io/ccdhmodel/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "cancer",
        "data-harmonization",
        "linkml",
        "schema"
      ],
      "id": 41
    },
    {
      "name": "chemrof",
      "one_line_profile": "Schema definitions for chemistry ontology classes",
      "detailed_description": "Provides schema definitions and structures for chemistry ontology classes, facilitating the organization and validation of chemical knowledge graph data.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "schema_definition",
        "ontology_management"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/chemkg/chemrof",
      "help_website": [],
      "license": "CC0-1.0",
      "tags": [
        "chemistry",
        "ontology",
        "schema",
        "knowledge-graph"
      ],
      "id": 42
    },
    {
      "name": "CheckM2",
      "one_line_profile": "Machine learning tool for assessing metagenome bin quality",
      "detailed_description": "A tool that uses machine learning to assess the quality (completeness and contamination) of metagenome-derived genome bins, improving upon previous lineage-specific marker set methods.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "metagenomics"
      ],
      "application_level": "solver",
      "primary_language": "Scilab",
      "repo_url": "https://github.com/chklovski/CheckM2",
      "help_website": [
        "https://github.com/chklovski/CheckM2"
      ],
      "license": "GPL-3.0",
      "tags": [
        "metagenomics",
        "quality-control",
        "genome-binning",
        "machine-learning"
      ],
      "id": 43
    },
    {
      "name": "HistoQC",
      "one_line_profile": "Quality control tool for digital pathology slides",
      "detailed_description": "An open-source quality control tool designed for digital pathology slides. It automatically identifies artifacts and assesses the suitability of slides for computational analysis.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "image_processing"
      ],
      "application_level": "solver",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/choosehappy/HistoQC",
      "help_website": [
        "https://github.com/choosehappy/HistoQC"
      ],
      "license": "BSD-3-Clause-Clear",
      "tags": [
        "pathology",
        "quality-control",
        "histology",
        "medical-imaging"
      ],
      "id": 44
    },
    {
      "name": "DataHarmonizer",
      "one_line_profile": "Standardized spreadsheet editor and validator for public health data",
      "detailed_description": "A browser-based spreadsheet editor and validator that facilitates the harmonization of data according to defined schemas (e.g., LinkML). It includes templates for SARS-CoV-2 and Monkeypox sampling data.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_harmonization",
        "data_validation",
        "data_entry"
      ],
      "application_level": "platform",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/cidgoh/DataHarmonizer",
      "help_website": [
        "https://github.com/cidgoh/DataHarmonizer"
      ],
      "license": "MIT",
      "tags": [
        "epidemiology",
        "data-harmonization",
        "validation",
        "genomics"
      ],
      "id": 45
    },
    {
      "name": "Epsilon",
      "one_line_profile": "GPS data integrity verification algorithm suite",
      "detailed_description": "A suite of algorithms designed to verify the integrity of received GPS data and ranging signals, improving resiliency against signal loss and spoofing for navigation and timing applications.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "signal_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/cisagov/Epsilon",
      "help_website": [],
      "license": "CC0-1.0",
      "tags": [
        "gps",
        "signal-integrity",
        "pnt",
        "navigation"
      ],
      "id": 46
    },
    {
      "name": "PNT-Integrity",
      "one_line_profile": "Library for verifying GPS/PNT signal integrity",
      "detailed_description": "Provides methods to verify the integrity of received GPS data and ranging signals, serving as a reference implementation for detecting anomalies in Positioning, Navigation, and Timing (PNT) data.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "signal_processing"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/cisagov/PNT-Integrity",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "gps",
        "pnt",
        "integrity-check",
        "signal-processing"
      ],
      "id": 47
    },
    {
      "name": "Clowder",
      "one_line_profile": "Research data management and analysis platform",
      "detailed_description": "A data management system that enables users to share, annotate, organize, and analyze large collections of datasets. It supports extensible metadata annotation (JSON-LD) and automated extraction pipelines.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_management",
        "metadata_management",
        "workflow_automation"
      ],
      "application_level": "platform",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/clowder-framework/clowder",
      "help_website": [
        "https://clowderframework.org/"
      ],
      "license": "NCSA",
      "tags": [
        "data-management",
        "metadata",
        "curation",
        "research-data"
      ],
      "id": 48
    },
    {
      "name": "json-flattener",
      "one_line_profile": "Utility for denormalizing nested JSON objects to tables",
      "detailed_description": "A Python library used in bioinformatics pipelines to denormalize nested dictionaries or JSON objects (such as ontology terms) into tabular formats and vice versa.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_conversion",
        "data_wrangling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/cmungall/json-flattener",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "json",
        "conversion",
        "bioinformatics",
        "flattening"
      ],
      "id": 49
    },
    {
      "name": "linkml-phenopackets",
      "one_line_profile": "LinkML rendering and tools for Phenopackets",
      "detailed_description": "Provides tools and schema definitions for working with the Phenopackets standard using the LinkML modeling framework, facilitating phenotypic data exchange.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "schema_definition",
        "data_conversion"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/cmungall/linkml-phenopackets",
      "help_website": [],
      "license": null,
      "tags": [
        "phenopackets",
        "linkml",
        "phenotype",
        "schema"
      ],
      "id": 50
    },
    {
      "name": "semantic-llama",
      "one_line_profile": "LLM-based scientific knowledge extraction tool",
      "detailed_description": "A tool that leverages Large Language Models (LLMs) to extract structured semantic information and knowledge from unstructured text, tailored for scientific curation tasks.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "knowledge_extraction",
        "nlp"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/cmungall/semantic-llama",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "llm",
        "knowledge-extraction",
        "semantics",
        "curation"
      ],
      "id": 51
    },
    {
      "name": "GrandQC",
      "one_line_profile": "Quality control and tissue detection tool for digital pathology slides",
      "detailed_description": "GrandQC is a tool designed for the quality control of digital pathology images. It performs tissue detection and identifies artifacts or quality issues in whole slide images (WSIs), facilitating the preprocessing steps required for computational pathology and AI model training in medical research.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "image_processing",
        "tissue_detection"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/cpath-ukk/grandqc",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "pathology",
        "quality-control",
        "wsi",
        "tissue-detection"
      ],
      "id": 52
    },
    {
      "name": "FastQC (CSF Fork)",
      "one_line_profile": "Quality control tool for high throughput sequence data (CSF fork)",
      "detailed_description": "This is a fork of the standard FastQC tool, adapted for usage on selected reads of unaligned BAM files. It provides a modular set of analyses which can be used to give a quick impression of whether your data has any problems of which you should be aware before doing any further analysis.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "ngs_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Java",
      "repo_url": "https://github.com/csf-ngs/fastqc",
      "help_website": [
        "http://www.bioinformatics.babraham.ac.uk/projects/fastqc/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "fastqc",
        "ngs",
        "quality-control",
        "bam"
      ],
      "id": 53
    },
    {
      "name": "PLUTON",
      "one_line_profile": "Automated shell pipeline for NIPT data preprocessing and aneuploidy prediction",
      "detailed_description": "An automated pipeline wrapping tools like FastQC, SAMtools, and WisecondorX to execute pre-processing of aligned reads and predict fetal gender and chromosomal aneuploidies.",
      "domains": [
        "D1-03",
        "D2"
      ],
      "subtask_category": [
        "quality_control",
        "variant_calling",
        "workflow"
      ],
      "application_level": "workflow",
      "primary_language": "Shell",
      "repo_url": "https://github.com/epigen-bioinfolab/PLUTON",
      "help_website": [],
      "license": null,
      "tags": [
        "nipt",
        "bioinformatics",
        "pipeline",
        "aneuploidy"
      ],
      "id": 54
    },
    {
      "name": "nanoq",
      "one_line_profile": "Ultra-fast quality control and filtering for Nanopore sequencing reads",
      "detailed_description": "A minimal but speedy quality control tool for nanopore reads written in Rust, providing read filtering and summary statistics.",
      "domains": [
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "filtering"
      ],
      "application_level": "solver",
      "primary_language": "Rust",
      "repo_url": "https://github.com/esteinig/nanoq",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nanopore",
        "quality-control",
        "bioinformatics",
        "fastq"
      ],
      "id": 55
    },
    {
      "name": "FAIR4Health Data Curation Tool",
      "one_line_profile": "Desktop tool for cleaning, validating and harmonizing health datasets for FAIR compliance",
      "detailed_description": "A tool developed by the FAIR4Health project to assist researchers in curating and validating health data to ensure it meets FAIR principles.",
      "domains": [
        "D1-03"
      ],
      "subtask_category": [
        "data_curation",
        "validation",
        "harmonization"
      ],
      "application_level": "solver",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/fair4health/data-curation-tool",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "fair-data",
        "health-informatics",
        "data-curation"
      ],
      "id": 56
    },
    {
      "name": "fastqc-rs",
      "one_line_profile": "High-performance Rust implementation of the FastQC quality control tool",
      "detailed_description": "A quality control tool for FASTQ files written in Rust, offering a faster alternative to the original Java-based FastQC.",
      "domains": [
        "D1-03"
      ],
      "subtask_category": [
        "quality_control"
      ],
      "application_level": "solver",
      "primary_language": "Rust",
      "repo_url": "https://github.com/fastqc-rs/fastqc-rs",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fastq",
        "quality-control",
        "bioinformatics",
        "rust"
      ],
      "id": 57
    },
    {
      "name": "frictionless-ci",
      "one_line_profile": "Continuous integration service for validating tabular data packages in repositories",
      "detailed_description": "A data management service that brings continuous data validation to tabular data in your repository via Github Action, ensuring data quality in scientific workflows.",
      "domains": [
        "D1-03"
      ],
      "subtask_category": [
        "validation",
        "quality_control"
      ],
      "application_level": "service",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/frictionlessdata/frictionless-ci",
      "help_website": [
        "https://frictionlessdata.io"
      ],
      "license": "MIT",
      "tags": [
        "data-validation",
        "continuous-integration",
        "frictionless-data"
      ],
      "id": 58
    },
    {
      "name": "frictionless-darwin-core",
      "one_line_profile": "Converter and validator for Darwin Core Archive biodiversity data using Frictionless standards",
      "detailed_description": "A tool to handle DarwinCore Archives as Frictionless Data Packages, facilitating the validation and management of biodiversity data.",
      "domains": [
        "D1-03",
        "D1"
      ],
      "subtask_category": [
        "data_conversion",
        "validation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/frictionlessdata/frictionless-darwin-core",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "biodiversity",
        "darwin-core",
        "frictionless-data",
        "metadata"
      ],
      "id": 59
    },
    {
      "name": "Frictionless Framework (Python)",
      "one_line_profile": "Data management framework to describe, extract, validate, and transform tabular data",
      "detailed_description": "A comprehensive framework for Python that provides functionality to describe, extract, validate, and transform tabular data, widely used in research data management.",
      "domains": [
        "D1-03",
        "D1"
      ],
      "subtask_category": [
        "validation",
        "metadata_management",
        "data_cleaning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/frictionlessdata/frictionless-py",
      "help_website": [
        "https://framework.frictionlessdata.io"
      ],
      "license": "MIT",
      "tags": [
        "data-validation",
        "metadata",
        "tabular-data",
        "frictionless-data"
      ],
      "id": 60
    },
    {
      "name": "frictionless-r",
      "one_line_profile": "R interface for reading, writing, and validating Frictionless Data Packages",
      "detailed_description": "An R package to read and write Frictionless Data Packages, allowing R users to integrate Frictionless validation and metadata standards into their analysis pipelines.",
      "domains": [
        "D1-03",
        "D1"
      ],
      "subtask_category": [
        "validation",
        "data_io"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/frictionlessdata/frictionless-r",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "r-package",
        "data-validation",
        "frictionless-data"
      ],
      "id": 61
    },
    {
      "name": "RNA-SeQC",
      "one_line_profile": "Efficient tool for computing quality control metrics for RNA-seq data",
      "detailed_description": "Fast, efficient RNA-Seq metrics for quality control and process optimization, providing essential metrics for transcriptomics data analysis.",
      "domains": [
        "D1-03"
      ],
      "subtask_category": [
        "quality_control"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/getzlab/rnaseqc",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "rna-seq",
        "quality-control",
        "bioinformatics",
        "genomics"
      ],
      "id": 62
    },
    {
      "name": "tableschema-to-template",
      "one_line_profile": "Tool to generate Excel data entry templates from Frictionless Table Schemas for HuBMAP data submission",
      "detailed_description": "Developed by the HuBMAP Consortium, this tool converts Frictionless Table Schema definitions into Excel spreadsheets with built-in validation, facilitating standardized metadata collection and data submission in biomedical research.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "metadata_management",
        "schema_validation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/hubmapconsortium/tableschema-to-template",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "metadata",
        "hubmap",
        "frictionless-data",
        "biomedical-data"
      ],
      "id": 63
    },
    {
      "name": "compliance-checker",
      "one_line_profile": "Tool to check oceanographic datasets against metadata compliance standards",
      "detailed_description": "A Python tool developed by IOOS to check local or remote datasets (NetCDF, OPeNDAP) against various compliance standards such as CF (Climate and Forecast), ACDD, and ISO metadata standards, ensuring data interoperability in earth sciences.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "metadata_validation",
        "standard_compliance"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ioos/compliance-checker",
      "help_website": [
        "https://ioos.github.io/compliance-checker/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "oceanography",
        "netcdf",
        "metadata-validation",
        "cf-conventions"
      ],
      "id": 64
    },
    {
      "name": "iRODS",
      "one_line_profile": "Open source data management software for data virtualization and workflow automation",
      "detailed_description": "The Integrated Rule-Oriented Data System (iRODS) is a data management software used in high-performance computing and scientific research (genomics, physics, etc.) to manage data lifecycle, metadata, and provenance across distributed storage resources.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_management",
        "workflow_automation",
        "metadata_management"
      ],
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/irods/irods",
      "help_website": [
        "https://irods.org/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "data-management",
        "hpc",
        "metadata",
        "workflow"
      ],
      "id": 65
    },
    {
      "name": "TaxTriage",
      "one_line_profile": "Nextflow workflow for taxonomic classification and triage of metagenomic NGS data",
      "detailed_description": "A comprehensive bioinformatics workflow designed to identify and classify microbial organisms within short- or long-read metagenomic Next-Generation Sequencing (NGS) data. It performs quality control, host removal, and taxonomic classification to triage samples for further analysis.",
      "domains": [
        "D1",
        "D6"
      ],
      "subtask_category": "taxonomic_classification",
      "application_level": "workflow",
      "primary_language": "Java",
      "repo_url": "https://github.com/jhuapl-bio/taxtriage",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "metagenomics",
        "ngs",
        "microbiome",
        "nextflow",
        "bioinformatics"
      ],
      "id": 66
    },
    {
      "name": "fastqcr",
      "one_line_profile": "Quality control and reporting for sequencing data",
      "detailed_description": "An R package designed for quality control (QC) of high-throughput sequencing data. It facilitates running FastQC from R, aggregating reports, and generating summaries to assess the quality of raw sequencing reads (FASTQ files).",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": "quality_control",
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/kassambara/fastqcr",
      "help_website": [
        "http://www.sthda.com/english/wiki/fastqcr-an-r-package-facilitating-quality-controls-of-sequencing-data"
      ],
      "license": "GPL-3.0",
      "tags": [
        "fastq",
        "quality-control",
        "ngs",
        "bioinformatics",
        "r-package"
      ],
      "id": 67
    },
    {
      "name": "FastQt",
      "one_line_profile": "Qt5 port of FastQC for high throughput sequence data quality control",
      "detailed_description": "A quality control tool for high throughput sequence data, serving as a C++/Qt5 port of the popular FastQC tool. It provides interactive visualization and analysis of FASTQ files to ensure data quality before downstream analysis.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "sequence_analysis"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/labsquare/FastQt",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "fastqc",
        "ngs",
        "quality-control",
        "bioinformatics"
      ],
      "id": 68
    },
    {
      "name": "FastqCleaner",
      "one_line_profile": "Shiny application for FASTQ quality control and filtering",
      "detailed_description": "A Shiny application and R package designed for quality control, filtering, and trimming of FASTQ files, enabling interactive data cleaning for NGS workflows.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "data_filtering"
      ],
      "application_level": "solver",
      "primary_language": "HTML",
      "repo_url": "https://github.com/leandroroser/FastqCleaner",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "fastq",
        "quality-control",
        "shiny",
        "bioinformatics"
      ],
      "id": 69
    },
    {
      "name": "LinkML",
      "one_line_profile": "Linked Open Data Modeling Language and toolkit",
      "detailed_description": "A general-purpose modeling language and toolkit for defining data schemas (ontologies, data models) that can be compiled into various artifacts (JSON-Schema, SHACL, SQL DDL, Python dataclasses), widely used in scientific data standardization.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "schema_definition",
        "data_modeling",
        "validation"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/linkml/linkml",
      "help_website": [
        "https://linkml.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "schema",
        "metadata",
        "linked-data",
        "modeling"
      ],
      "id": 70
    },
    {
      "name": "linkml-arrays",
      "one_line_profile": "N-dimensional array support for LinkML",
      "detailed_description": "An extension for LinkML to support loading, dumping, and validating N-dimensional arrays, facilitating the integration of complex scientific data structures into LinkML models.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_modeling",
        "serialization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/linkml/linkml-arrays",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "arrays",
        "linkml",
        "scientific-data"
      ],
      "id": 71
    },
    {
      "name": "linkml-datalog",
      "one_line_profile": "Datalog translation and inference for LinkML schemas",
      "detailed_description": "A tool that translates LinkML schemas into Datalog programs and executes them using Souffle, enabling advanced validation and logical inference over scientific instance data.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "validation",
        "inference"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/linkml/linkml-datalog",
      "help_website": [],
      "license": null,
      "tags": [
        "datalog",
        "inference",
        "validation",
        "linkml"
      ],
      "id": 72
    },
    {
      "name": "linkml-dataops",
      "one_line_profile": "Data manipulation API for LinkML instances",
      "detailed_description": "A library providing a data API for manipulating, querying, and validating LinkML instance data, supporting operations like patching and diffing.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_processing",
        "data_manipulation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/linkml/linkml-dataops",
      "help_website": [],
      "license": null,
      "tags": [
        "data-ops",
        "linkml",
        "api"
      ],
      "id": 73
    },
    {
      "name": "linkml-map",
      "one_line_profile": "Schema mapping tool for LinkML",
      "detailed_description": "A tool for defining and executing mappings between different LinkML schemas, facilitating data transformation and interoperability between scientific data standards.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_transformation",
        "schema_mapping"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/linkml/linkml-map",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "mapping",
        "transformation",
        "linkml"
      ],
      "id": 74
    },
    {
      "name": "linkml-owl",
      "one_line_profile": "LinkML to OWL converter",
      "detailed_description": "An extension of the LinkML runtime for converting instances of LinkML classes to OWL (Web Ontology Language) classes or instances, bridging data modeling with semantic web ontologies.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_conversion",
        "ontology_mapping"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/linkml/linkml-owl",
      "help_website": [],
      "license": null,
      "tags": [
        "owl",
        "ontology",
        "semantic-web",
        "linkml"
      ],
      "id": 75
    },
    {
      "name": "linkml-reference-validator",
      "one_line_profile": "Validator for text quotes in scientific data",
      "detailed_description": "A tool to validate that supporting text quotes in data actually appear in their cited references, useful for data curation and quality control in scientific databases.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "validation",
        "quality_control"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/linkml/linkml-reference-validator",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "validation",
        "curation",
        "text-mining"
      ],
      "id": 76
    },
    {
      "name": "linkml-renderer",
      "one_line_profile": "Renderer for LinkML instance data",
      "detailed_description": "A library and tool for rendering LinkML instance data into various formats such as HTML, Markdown, and Mermaid diagrams, aiding in data visualization and documentation.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "visualization",
        "reporting"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/linkml/linkml-renderer",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rendering",
        "visualization",
        "documentation"
      ],
      "id": 77
    },
    {
      "name": "linkml-runtime",
      "one_line_profile": "Runtime library for LinkML models",
      "detailed_description": "The core runtime support library for working with LinkML generated models in Python, providing utilities for data loading, dumping, and manipulation.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_processing",
        "serialization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/linkml/linkml-runtime",
      "help_website": [],
      "license": "CC0-1.0",
      "tags": [
        "runtime",
        "linkml",
        "serialization"
      ],
      "id": 78
    },
    {
      "name": "linkml-runtime.js",
      "one_line_profile": "JavaScript runtime for LinkML",
      "detailed_description": "The JavaScript implementation of the LinkML runtime, enabling the use of LinkML models and validation in web and Node.js environments.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_processing",
        "validation"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/linkml/linkml-runtime.js",
      "help_website": [],
      "license": null,
      "tags": [
        "javascript",
        "linkml",
        "runtime"
      ],
      "id": 79
    },
    {
      "name": "linkml-solr",
      "one_line_profile": "Solr integration for LinkML",
      "detailed_description": "A wrapper and utility for using Apache Solr with LinkML schemas, facilitating the indexing and querying of scientific data modeled with LinkML.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_indexing",
        "data_storage"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/linkml/linkml-solr",
      "help_website": [],
      "license": null,
      "tags": [
        "solr",
        "search",
        "indexing",
        "linkml"
      ],
      "id": 80
    },
    {
      "name": "linkml-sparql",
      "one_line_profile": "LinkML to SPARQL mapper",
      "detailed_description": "A library that maps LinkML queries to SPARQL and back, enabling semantic queries over LinkML-compliant data stored in RDF stores.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_querying",
        "semantic_web"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/linkml/linkml-sparql",
      "help_website": [],
      "license": null,
      "tags": [
        "sparql",
        "rdf",
        "querying"
      ],
      "id": 81
    },
    {
      "name": "linkml-store",
      "one_line_profile": "Storage abstraction for LinkML",
      "detailed_description": "A wrapper library providing a unified interface for multiple storage engines (e.g., SQL, MongoDB, Solr) for LinkML data.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_storage",
        "database_interface"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/linkml/linkml-store",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "storage",
        "database",
        "abstraction"
      ],
      "id": 82
    },
    {
      "name": "prefixmaps",
      "one_line_profile": "Semantic prefix map registry and library",
      "detailed_description": "A Python library and registry for managing semantic prefix maps (CURIEs), essential for resolving identifiers in scientific data and semantic web applications.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "identifier_resolution",
        "metadata_management"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/linkml/prefixmaps",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "curie",
        "prefixes",
        "semantic-web"
      ],
      "id": 83
    },
    {
      "name": "schema-automator",
      "one_line_profile": "Automated schema induction tool",
      "detailed_description": "A toolkit for automating the schema development lifecycle, including inducing LinkML schemas from structured data sources like TSV, JSON, or other schema formats.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "schema_induction",
        "data_modeling"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/linkml/schema-automator",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "automation",
        "schema-inference",
        "modeling"
      ],
      "id": 84
    },
    {
      "name": "schemasheets",
      "one_line_profile": "Schema management via spreadsheets",
      "detailed_description": "A tool that allows users to structure and define data schemas using Google Sheets or TSVs, which are then converted to LinkML and other formats, promoting FAIR data practices.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "schema_definition",
        "data_curation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/linkml/schemasheets",
      "help_website": [],
      "license": null,
      "tags": [
        "spreadsheets",
        "fair-data",
        "schema-conversion"
      ],
      "id": 85
    },
    {
      "name": "semantic-dsl",
      "one_line_profile": "Domain Specific Language creator for schemas",
      "detailed_description": "A library for creating easy-to-use Domain Specific Languages (DSLs) for scientific schemas, simplifying the process of defining complex data models.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "schema_definition",
        "dsl"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/linkml/semantic-dsl",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "dsl",
        "modeling",
        "semantics"
      ],
      "id": 86
    },
    {
      "name": "sparqlfun",
      "one_line_profile": "SPARQL templating library",
      "detailed_description": "A library for managing and executing SPARQL templates, providing a Python wrapper to simplify querying RDF data in scientific workflows.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_querying",
        "sparql"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/linkml/sparqlfun",
      "help_website": [],
      "license": "CC0-1.0",
      "tags": [
        "sparql",
        "templates",
        "rdf"
      ],
      "id": 87
    },
    {
      "name": "sssom-py",
      "one_line_profile": "Python toolkit for working with SSSOM (Simple Standard for Sharing Ontology Mappings) metadata",
      "detailed_description": "A Python library and command-line toolkit for manipulating, validating, and converting ontology mapping sets in the SSSOM format, facilitating interoperability between scientific ontologies.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "metadata_management",
        "ontology_mapping",
        "data_conversion"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/mapping-commons/sssom-py",
      "help_website": [
        "https://mapping-commons.github.io/sssom-py/"
      ],
      "license": "MIT",
      "tags": [
        "ontology",
        "metadata",
        "mapping",
        "sssom",
        "bioinformatics"
      ],
      "id": 88
    },
    {
      "name": "nmdc-schema",
      "one_line_profile": "Unified data model and validation schema for the National Microbiome Data Collaborative",
      "detailed_description": "Provides the schema definitions and generated Python libraries (Pydantic models) for validating and structuring microbiome data within the NMDC ecosystem, ensuring data interoperability and quality.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "metadata_schema",
        "data_validation",
        "data_modeling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/microbiomedata/nmdc-schema",
      "help_website": [
        "https://microbiomedata.github.io/nmdc-schema/"
      ],
      "license": "CC0-1.0",
      "tags": [
        "microbiome",
        "schema",
        "metadata",
        "linkml",
        "validation"
      ],
      "id": 89
    },
    {
      "name": "NMDC Sample Annotator",
      "one_line_profile": "Tool for annotating microbiome samples with NMDC-compliant metadata",
      "detailed_description": "A tool designed to assist researchers in annotating biosamples with standardized metadata according to the National Microbiome Data Collaborative (NMDC) schema.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "metadata_curation",
        "sample_annotation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/microbiomedata/sample-annotator",
      "help_website": [],
      "license": null,
      "tags": [
        "microbiome",
        "metadata",
        "annotation",
        "nmdc"
      ],
      "id": 90
    },
    {
      "name": "Koza",
      "one_line_profile": "Data transformation framework for ingesting data into LinkML-compliant knowledge graphs",
      "detailed_description": "A data transformation framework used to transform source data (CSV, JSON, etc.) into a target data model (LinkML), primarily used for building biological knowledge graphs.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_transformation",
        "knowledge_graph_construction",
        "etl"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/monarch-initiative/koza",
      "help_website": [
        "https://koza.monarchinitiative.org/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "etl",
        "knowledge-graph",
        "linkml",
        "data-transformation"
      ],
      "id": 91
    },
    {
      "name": "OntoGPT",
      "one_line_profile": "LLM-based tool for extracting structured ontological information from unstructured text",
      "detailed_description": "A Python package that uses Large Language Models (LLMs) to extract structured information from text, conforming to LinkML schemas, useful for knowledge base curation and ontology population.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "knowledge_extraction",
        "ontology_population",
        "text_mining"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/monarch-initiative/ontogpt",
      "help_website": [
        "https://monarch-initiative.github.io/ontogpt/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "llm",
        "ontology",
        "knowledge-extraction",
        "nlp",
        "bioinformatics"
      ],
      "id": 92
    },
    {
      "name": "TidyMultiqc",
      "one_line_profile": "R package to convert MultiQC reports into tidy data frames for downstream analysis",
      "detailed_description": "Provides functions to parse the output of MultiQC (a common bioinformatics QC tool) into tidy data frames, enabling easy statistical analysis and visualization in R.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "qc_reporting",
        "data_parsing",
        "data_conversion"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/multimeric/TidyMultiqc",
      "help_website": [
        "https://multimeric.github.io/TidyMultiqc/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "multiqc",
        "bioinformatics",
        "r",
        "tidy-data",
        "quality-control"
      ],
      "id": 93
    },
    {
      "name": "NASA MMT",
      "one_line_profile": "Web-based tool for managing metadata in the NASA Common Metadata Repository (CMR)",
      "detailed_description": "The Metadata Management Tool (MMT) allows users to create, update, and manage metadata records within NASA's Common Metadata Repository, supporting Earth Science data stewardship.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "metadata_management",
        "data_curation"
      ],
      "application_level": "platform",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/nasa/mmt",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "nasa",
        "metadata",
        "cmr",
        "earth-science",
        "data-management"
      ],
      "id": 94
    },
    {
      "name": "nf-core/hgtseq",
      "one_line_profile": "Bioinformatics pipeline for investigating horizontal gene transfer from NGS data",
      "detailed_description": "A Nextflow pipeline designed to detect and analyze horizontal gene transfer events using Next Generation Sequencing data, facilitating evolutionary biology research.",
      "domains": [
        "D1",
        "Bioinformatics"
      ],
      "subtask_category": [
        "sequence_analysis",
        "horizontal_gene_transfer_detection"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/nf-core/hgtseq",
      "help_website": [
        "https://nf-co.re/hgtseq"
      ],
      "license": "MIT",
      "tags": [
        "bioinformatics",
        "ngs",
        "hgt",
        "nextflow"
      ],
      "id": 95
    },
    {
      "name": "nf-core/rnaseq",
      "one_line_profile": "Comprehensive RNA sequencing analysis pipeline",
      "detailed_description": "A standard bioinformatics pipeline for RNA-seq data analysis, integrating aligners (STAR, HISAT2) and quantification tools (Salmon) with extensive quality control reporting.",
      "domains": [
        "D1",
        "Bioinformatics"
      ],
      "subtask_category": [
        "rna_seq_analysis",
        "quality_control",
        "quantification"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/nf-core/rnaseq",
      "help_website": [
        "https://nf-co.re/rnaseq"
      ],
      "license": "MIT",
      "tags": [
        "bioinformatics",
        "rna-seq",
        "gene-expression",
        "nextflow"
      ],
      "id": 96
    },
    {
      "name": "nf-core/seqinspector",
      "one_line_profile": "Dedicated quality control pipeline for sequencing data",
      "detailed_description": "A pipeline focused solely on generating comprehensive quality control reports for large-scale sequencing datasets, suitable for core facilities and high-throughput research.",
      "domains": [
        "D1",
        "D1-03",
        "Bioinformatics"
      ],
      "subtask_category": [
        "quality_control",
        "qc_report"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/nf-core/seqinspector",
      "help_website": [
        "https://nf-co.re/seqinspector"
      ],
      "license": "MIT",
      "tags": [
        "bioinformatics",
        "qc",
        "ngs",
        "multiqc"
      ],
      "id": 97
    },
    {
      "name": "datapackage-m",
      "one_line_profile": "Power Query M functions for Frictionless Data Packages",
      "detailed_description": "A library enabling Power BI and Excel to directly ingest and parse Tabular Data Packages, a standard format in Open Science and research data management.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_parsing",
        "data_import"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/nimblelearn/datapackage-m",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "frictionless-data",
        "power-bi",
        "data-package",
        "open-data"
      ],
      "id": 98
    },
    {
      "name": "MRIQC",
      "one_line_profile": "Automated Quality Control for structural and functional MRI",
      "detailed_description": "A tool for extracting quality measures from structural (T1w, T2w) and functional MRI data, generating visual reports to assess data integrity in neuroimaging studies.",
      "domains": [
        "D1",
        "D1-03",
        "Neuroscience"
      ],
      "subtask_category": [
        "quality_control",
        "image_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/nipreps/mriqc",
      "help_website": [
        "https://mriqc.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "neuroimaging",
        "mri",
        "quality-control",
        "bids"
      ],
      "id": 99
    },
    {
      "name": "Cosmos-Transfer2.5",
      "one_line_profile": "World simulation model for physical AI applications",
      "detailed_description": "A foundation model for generating high-quality physical world simulations conditioned on spatial inputs, used for robotics and physical AI research.",
      "domains": [
        "Physics",
        "Robotics",
        "Simulation"
      ],
      "subtask_category": [
        "simulation",
        "generative_modeling"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/nvidia-cosmos/cosmos-transfer2.5",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "simulation",
        "physical-ai",
        "world-model",
        "generative-ai"
      ],
      "id": 100
    },
    {
      "name": "Open Data Editor",
      "one_line_profile": "No-code application to explore and validate tabular research data",
      "detailed_description": "A desktop application powered by the Frictionless Framework for validating, cleaning, and exploring tabular data, designed to support Open Data standards in research.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_validation",
        "data_cleaning"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/okfn/opendataeditor",
      "help_website": [
        "https://opendataeditor.okfn.org/"
      ],
      "license": "MIT",
      "tags": [
        "frictionless-data",
        "data-validation",
        "open-science",
        "csv"
      ],
      "id": 101
    },
    {
      "name": "Onedata",
      "one_line_profile": "Distributed scientific data management platform",
      "detailed_description": "A high-performance, distributed global data management system that provides transparent data access and POSIX-compliant virtual filesystems for research infrastructures.",
      "domains": [
        "D1",
        "Data_Infrastructure"
      ],
      "subtask_category": [
        "data_management",
        "storage_virtualization"
      ],
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/onedata/onedata",
      "help_website": [
        "https://onedata.org/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "distributed-storage",
        "research-data-management",
        "posix",
        "grid-computing"
      ],
      "id": 102
    },
    {
      "name": "Open Semantic Search",
      "one_line_profile": "Research tool for analyzing and searching large document collections",
      "detailed_description": "An integrated search and analysis platform for research document collections, featuring text mining, named entity recognition, and metadata management.",
      "domains": [
        "D1",
        "Text_Mining"
      ],
      "subtask_category": [
        "text_mining",
        "information_retrieval",
        "document_analysis"
      ],
      "application_level": "platform",
      "primary_language": "Shell",
      "repo_url": "https://github.com/opensemanticsearch/open-semantic-search",
      "help_website": [
        "https://www.opensemanticsearch.org/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "semantic-search",
        "text-mining",
        "ner",
        "research-tool"
      ],
      "id": 103
    },
    {
      "name": "Atlas Checks",
      "one_line_profile": "Integrity checks for geospatial data (OpenStreetMap)",
      "detailed_description": "A framework for running data integrity checks on geospatial map data, utilizing the Atlas library to identify quality issues in OpenStreetMap datasets.",
      "domains": [
        "D1",
        "D1-03",
        "Geospatial"
      ],
      "subtask_category": [
        "data_validation",
        "quality_control"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/osmlab/atlas-checks",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "geospatial",
        "osm",
        "quality-control",
        "map-data"
      ],
      "id": 104
    },
    {
      "name": "pytest-pandera",
      "one_line_profile": "Data testing plugin for scientific dataframes",
      "detailed_description": "A pytest plugin for Pandera, enabling statistical data validation and schema enforcement for pandas dataframes, widely used in scientific data processing pipelines.",
      "domains": [
        "D1",
        "D1-03",
        "Data_Science"
      ],
      "subtask_category": [
        "data_validation",
        "schema_enforcement"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pandera-dev/pytest-pandera",
      "help_website": [
        "https://pandera.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "data-validation",
        "pandas",
        "testing",
        "quality-control"
      ],
      "id": 105
    },
    {
      "name": "Nabu",
      "one_line_profile": "Digital media management system for ethnographic research",
      "detailed_description": "A catalog and workflow management system for audio/video items and metadata, developed by PARADISEC for archiving endangered cultures' linguistic data.",
      "domains": [
        "D1",
        "Humanities"
      ],
      "subtask_category": [
        "data_management",
        "archiving",
        "metadata_management"
      ],
      "application_level": "platform",
      "primary_language": "Ruby",
      "repo_url": "https://github.com/paradisec-archive/nabu",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "archival-science",
        "linguistics",
        "metadata",
        "workflow"
      ],
      "id": 106
    },
    {
      "name": "pointblank",
      "one_line_profile": "Data validation toolkit for R",
      "detailed_description": "A comprehensive toolkit for data validation and quality monitoring in R, allowing scientists to define and verify data quality rules within analysis pipelines.",
      "domains": [
        "D1",
        "D1-03",
        "Statistics"
      ],
      "subtask_category": [
        "data_validation",
        "quality_assessment"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/posit-dev/pointblank",
      "help_website": [
        "https://pointblank.posit.co/"
      ],
      "license": "MIT",
      "tags": [
        "r-stats",
        "data-validation",
        "quality-control",
        "data-science"
      ],
      "id": 107
    },
    {
      "name": "pvanalytics",
      "one_line_profile": "Quality control and feature labeling tools for photovoltaic system data",
      "detailed_description": "A Python library providing functions for quality control, filtering, and feature labeling of data from photovoltaic energy systems. It supports tasks like clipping detection, daytime identification, and sensor data validation.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "data_filtering",
        "feature_labeling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pvlib/pvanalytics",
      "help_website": [
        "https://pvanalytics.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "photovoltaic",
        "energy",
        "quality-control",
        "time-series"
      ],
      "id": 108
    },
    {
      "name": "geoflow",
      "one_line_profile": "Orchestrator for geospatial metadata management and FAIR services",
      "detailed_description": "An R package to orchestrate geospatial metadata management workflows. It facilitates the management of FAIR services and metadata for geospatial data, supporting various standards and formats.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "metadata_management",
        "workflow_orchestration"
      ],
      "application_level": "workflow",
      "primary_language": "R",
      "repo_url": "https://github.com/r-geoflow/geoflow",
      "help_website": [
        "https://github.com/r-geoflow/geoflow"
      ],
      "license": "NOASSERTION",
      "tags": [
        "geospatial",
        "metadata",
        "fair-data",
        "workflow"
      ],
      "id": 109
    },
    {
      "name": "MetaCerberus",
      "one_line_profile": "Functional ontology assignments for metagenomes using HMM",
      "detailed_description": "A Python tool for versatile functional ontology assignments for metagenomes via Hidden Markov Models (HMM), focusing on environmental shotgun metaomics data.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "annotation",
        "ontology_mapping",
        "metagenomics_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/raw-lab/MetaCerberus",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "metagenomics",
        "hmm",
        "ontology",
        "bioinformatics"
      ],
      "id": 110
    },
    {
      "name": "Refinery Platform",
      "one_line_profile": "Data management, analysis, and visualization system for bioinformatics",
      "detailed_description": "A platform for bioinformatics and computational biology applications consisting of a data repository with rich metadata capabilities, a Galaxy-based workflow engine, and visualization tools.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_management",
        "workflow_orchestration",
        "visualization"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/refinery-platform/refinery-platform",
      "help_website": [
        "http://refinery-platform.org/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "bioinformatics",
        "data-management",
        "galaxy",
        "metadata"
      ],
      "id": 111
    },
    {
      "name": "fastq.bio",
      "one_line_profile": "Interactive web tool for quality control of DNA sequencing data",
      "detailed_description": "A web-based interactive tool designed for performing quality control on DNA sequencing data (FASTQ files), running entirely in the browser.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "visualization"
      ],
      "application_level": "solver",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/robertaboukhalil/fastq.bio",
      "help_website": [
        "http://fastq.bio/"
      ],
      "license": "MIT",
      "tags": [
        "fastq",
        "quality-control",
        "dna-sequencing",
        "web-tool"
      ],
      "id": 112
    },
    {
      "name": "minion_multiQC",
      "one_line_profile": "Multi-tool quality control wrapper for MinION sequencing data",
      "detailed_description": "A shell-based wrapper tool to perform quality control on MinION sequencing data using multiple underlying QC tools.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control"
      ],
      "application_level": "workflow",
      "primary_language": "Shell",
      "repo_url": "https://github.com/roblanf/minion_multiQC",
      "help_website": [],
      "license": null,
      "tags": [
        "minion",
        "nanopore",
        "quality-control"
      ],
      "id": 113
    },
    {
      "name": "minion_qc",
      "one_line_profile": "Quality control tool for MinION sequencing data",
      "detailed_description": "An R script/tool for generating quality control plots and statistics from MinION sequencing data.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "visualization"
      ],
      "application_level": "solver",
      "primary_language": "R",
      "repo_url": "https://github.com/roblanf/minion_qc",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "minion",
        "nanopore",
        "quality-control",
        "r"
      ],
      "id": 114
    },
    {
      "name": "skimr",
      "one_line_profile": "Compact and flexible summary statistics for data analysis",
      "detailed_description": "An R package from rOpenSci that provides a frictionless approach to dealing with summary statistics, commonly used for data profiling and initial quality assessment in scientific workflows.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_profiling",
        "statistics"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/ropensci/skimr",
      "help_website": [
        "https://docs.ropensci.org/skimr/"
      ],
      "license": null,
      "tags": [
        "statistics",
        "data-summary",
        "r-package",
        "ropensci"
      ],
      "id": 115
    },
    {
      "name": "FastQC",
      "one_line_profile": "Standard quality control analysis tool for high throughput sequencing data",
      "detailed_description": "A quality control tool for high throughput sequence data. It provides a modular set of analyses which you can use to give a quick impression of whether your data has any problems of which you should be aware before doing any further analysis.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "qc_report"
      ],
      "application_level": "solver",
      "primary_language": "Java",
      "repo_url": "https://github.com/s-andrews/FastQC",
      "help_website": [
        "http://www.bioinformatics.babraham.ac.uk/projects/fastqc/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "ngs",
        "quality-control",
        "fastq",
        "bioinformatics"
      ],
      "id": 116
    },
    {
      "name": "sequana_fastqc",
      "one_line_profile": "Sequana pipeline wrapper for parallel FastQC execution",
      "detailed_description": "A pipeline based on Sequana to perform FastQC analysis in parallel and summarize results using MultiQC, designed for efficient quality control of sequencing data.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "workflow"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/sequana/fastqc",
      "help_website": [
        "https://sequana.readthedocs.io"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "pipeline",
        "fastqc",
        "sequana",
        "bioinformatics"
      ],
      "id": 117
    },
    {
      "name": "Falco",
      "one_line_profile": "High-performance C++ quality control tool for sequencing data, compatible with FastQC",
      "detailed_description": "Falco is a C++ implementation of FastQC, designed to assess the quality of high-throughput sequencing data (NGS). It provides faster processing speeds while maintaining compatibility with FastQC reports, enabling efficient quality control in large-scale bioinformatics pipelines.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "qc_report"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/smithlabcode/falco",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "bioinformatics",
        "ngs",
        "quality-control",
        "fastq"
      ],
      "id": 118
    },
    {
      "name": "iterative-stratification",
      "one_line_profile": "Cross-validators for iterative stratification of multilabel data in scikit-learn",
      "detailed_description": "A Python library providing scikit-learn compatible cross-validators for multilabel data, ensuring balanced label distribution across splits, which is essential for rigorous scientific machine learning experiments.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "data_splitting",
        "model_validation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/trent-b/iterative-stratification",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "scikit-learn",
        "cross-validation",
        "multilabel",
        "machine-learning"
      ],
      "id": 119
    },
    {
      "name": "Tropy",
      "one_line_profile": "Research photo and metadata management tool for scientists and archivists",
      "detailed_description": "A desktop application designed for researchers to organize, describe, and annotate photos of research material (e.g., from archives), managing metadata and facilitating the research workflow.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "metadata_management",
        "data_organization"
      ],
      "application_level": "platform",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/tropy/tropy",
      "help_website": [
        "https://tropy.org"
      ],
      "license": "NOASSERTION",
      "tags": [
        "metadata",
        "research-data-management",
        "archival-science"
      ],
      "id": 120
    },
    {
      "name": "SVI-Quality-Checker",
      "one_line_profile": "Quality control tool for Street View Imagery datasets",
      "detailed_description": "A tool developed by the Urban Analytics Lab to examine and validate the quality of street view imagery (SVI) datasets, supporting urban science and geospatial research.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "image_validation"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/ualsg/SVI-Quality-Checker",
      "help_website": [],
      "license": null,
      "tags": [
        "urban-science",
        "geospatial",
        "quality-control",
        "street-view"
      ],
      "id": 121
    },
    {
      "name": "Pandera",
      "one_line_profile": "Statistical data validation and testing library for pandas dataframes",
      "detailed_description": "A Python library for validating pandas DataFrames, providing a flexible and expressive API for defining data schemas and statistical checks, widely used in scientific data processing pipelines.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_validation",
        "schema_enforcement"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/unionai-oss/pandera",
      "help_website": [
        "https://pandera.readthedocs.io"
      ],
      "license": "MIT",
      "tags": [
        "pandas",
        "data-validation",
        "data-science",
        "quality-assurance"
      ],
      "id": 122
    },
    {
      "name": "Checkmate",
      "one_line_profile": "Checkpoint management tool for TensorFlow models",
      "detailed_description": "A lightweight utility for managing and saving the best TensorFlow checkpoints during model training, ensuring model integrity and preventing data loss in scientific ML workflows.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "workflow_management",
        "artifact_management"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/vonclites/checkmate",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "tensorflow",
        "checkpointing",
        "ml-ops"
      ],
      "id": 123
    },
    {
      "name": "nanoQC",
      "one_line_profile": "Quality control tool for nanopore sequencing data",
      "detailed_description": "A bioinformatics tool specifically designed for quality control of Oxford Nanopore sequencing data, generating plots and statistics to assess read quality.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "sequencing_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/wdecoster/nanoQC",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "nanopore",
        "bioinformatics",
        "quality-control",
        "long-read"
      ],
      "id": 124
    },
    {
      "name": "ydata-quality",
      "one_line_profile": "Data quality assessment library for data science",
      "detailed_description": "A Python library for comprehensive data quality assessment, providing metrics and visualizations to identify issues in datasets prior to analysis or modeling.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_assessment",
        "data_profiling"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/ydataai/ydata-quality",
      "help_website": [
        "https://github.com/ydataai/ydata-quality"
      ],
      "license": "MIT",
      "tags": [
        "data-quality",
        "data-science",
        "profiling"
      ],
      "id": 125
    },
    {
      "name": "ZotaData",
      "one_line_profile": "Metadata management plugin for Zotero",
      "detailed_description": "A tool (likely a plugin or extension) for Zotero that enhances metadata management capabilities, facilitating the organization of scientific literature and citations.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "metadata_management",
        "literature_management"
      ],
      "application_level": "solver",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/ydeng11/zotero-zotadata",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "zotero",
        "metadata",
        "bibliography"
      ],
      "id": 126
    },
    {
      "name": "LongQC",
      "one_line_profile": "Quality control tool for PacBio and ONT long read data",
      "detailed_description": "A tool designed for the quality control of long-read sequencing data (PacBio and Oxford Nanopore), addressing specific error profiles and artifacts associated with these technologies.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "sequencing_analysis"
      ],
      "application_level": "solver",
      "primary_language": "C",
      "repo_url": "https://github.com/yfukasawa/LongQC",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "long-read",
        "pacbio",
        "ont",
        "bioinformatics",
        "quality-control"
      ],
      "id": 127
    }
  ]
}
{
  "generated_at": "2025-12-17T05:03:26.857852+08:00",
  "metadata": {
    "leaf_cluster": {
      "leaf_cluster_id": "AI6",
      "leaf_cluster_name": "可复现运行时与加速生态（容器/HPC）",
      "domain": "Infra/HPC",
      "typical_objects": "jobs/artifacts",
      "task_chain": "环境→调度→缓存→追踪→加速",
      "tool_form": "容器/调度 + 追踪/成本"
    },
    "unit": {
      "unit_id": "AI6-04",
      "unit_name": "推理/训练加速与量化",
      "target_scale": "200–450",
      "coverage_tools": "compilers、quantization"
    },
    "search": {
      "target_candidates": 450,
      "queries": [
        "[GH] Horovod",
        "[GH] AutoGPTQ",
        "[GH] bitsandbytes",
        "[GH] XLA",
        "[GH] vLLM",
        "[GH] DeepSpeed",
        "[GH] Triton",
        "[GH] ONNX Runtime",
        "[GH] TensorRT",
        "[GH] Apache TVM",
        "[GH] deep learning compiler",
        "[GH] tensor compiler",
        "[GH] model quantization",
        "[GH] inference acceleration",
        "[GH] training acceleration",
        "[GH] post-training quantization",
        "[GH] quantization aware training",
        "[GH] gpu kernel optimization",
        "[GH] mixed precision training",
        "[GH] model compression",
        "[GH] kernel fusion",
        "[GH] distributed training framework",
        "[GH] llm acceleration",
        "[WEB] deep learning compiler framework github",
        "[WEB] model quantization tools github",
        "[WEB] high performance inference engine github",
        "[WEB] distributed training acceleration github",
        "[WEB] llm quantization and acceleration github",
        "[WEB] gpu kernel optimization library github"
      ],
      "total_candidates": 1269,
      "tool_candidates": 954,
      "final_tools": 450
    }
  },
  "tools": [
    {
      "name": "tensorscript",
      "one_line_profile": "Shape-checking neural network DSL compiling to PyTorch",
      "detailed_description": "A domain-specific language (DSL) for defining neural networks with a Hindley-Milner type system for shape checking, compiling to PyTorch for execution.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "compiler",
        "model_definition"
      ],
      "application_level": "solver",
      "primary_language": "Rust",
      "repo_url": "https://github.com/0b01/tensorscript",
      "help_website": [],
      "license": null,
      "tags": [
        "dsl",
        "compiler",
        "pytorch",
        "shape-checking"
      ],
      "id": 1
    },
    {
      "name": "YOLO_RKNN_Acceleration_Program",
      "one_line_profile": "Multi-threaded hardware-accelerated inference framework for YOLO on RKNN",
      "detailed_description": "A framework for accelerating YOLO object detection models using Rockchip NPU (RKNN), featuring multi-threading support for efficient inference.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "edge_computing"
      ],
      "application_level": "solver",
      "primary_language": "C",
      "repo_url": "https://github.com/1125962926/YOLO_RKNN_Acceleration_Program",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "yolo",
        "rknn",
        "npu",
        "inference"
      ],
      "id": 2
    },
    {
      "name": "PTQ4DM",
      "one_line_profile": "Post-training quantization framework for diffusion models",
      "detailed_description": "Implementation of post-training quantization (PTQ) techniques specifically optimized for diffusion models to reduce model size and accelerate inference.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "model_compression"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/42Shawn/PTQ4DM",
      "help_website": [],
      "license": null,
      "tags": [
        "diffusion-models",
        "ptq",
        "quantization"
      ],
      "id": 3
    },
    {
      "name": "OpenEmbedding",
      "one_line_profile": "Distributed training acceleration framework for TensorFlow embedding layers",
      "detailed_description": "An open-source framework designed to accelerate distributed training of large-scale embedding layers in TensorFlow models.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "training_acceleration",
        "distributed_training"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/4paradigm/OpenEmbedding",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "tensorflow",
        "embedding",
        "distributed-training"
      ],
      "id": 4
    },
    {
      "name": "micronet",
      "one_line_profile": "Comprehensive model compression and deployment library",
      "detailed_description": "A library for neural network compression and deployment, supporting quantization (QAT, PTQ), pruning, and TensorRT deployment integration.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_compression",
        "quantization",
        "pruning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/666DZY666/micronet",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "model-compression",
        "quantization",
        "pruning",
        "tensorrt"
      ],
      "id": 5
    },
    {
      "name": "JetStream",
      "one_line_profile": "High-throughput LLM inference engine for XLA devices",
      "detailed_description": "A throughput and memory optimized inference engine for Large Language Models (LLMs) designed for XLA devices like TPUs.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "llm_serving"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/AI-Hypercomputer/JetStream",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "inference",
        "tpu",
        "xla"
      ],
      "id": 6
    },
    {
      "name": "jetstream-pytorch",
      "one_line_profile": "PyTorch/XLA integration for JetStream inference engine",
      "detailed_description": "Provides PyTorch/XLA integration with the JetStream engine to enable high-performance LLM inference using PyTorch models on XLA devices.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "framework_integration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/AI-Hypercomputer/jetstream-pytorch",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "pytorch",
        "xla",
        "jetstream",
        "inference"
      ],
      "id": 7
    },
    {
      "name": "BiLLM",
      "one_line_profile": "Ultra-low bit post-training quantization for LLMs",
      "detailed_description": "Implementation of BiLLM, a method for pushing the limits of post-training quantization for Large Language Models to extremely low bit-widths.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "model_compression"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Aaronhuang-778/BiLLM",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "quantization",
        "ptq"
      ],
      "id": 8
    },
    {
      "name": "Hash3D",
      "one_line_profile": "Training-free acceleration tool for 3D generation",
      "detailed_description": "A tool providing training-free acceleration techniques for 3D generative models, optimizing the generation process.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "3d_generation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Adamdad/hash3D",
      "help_website": [],
      "license": null,
      "tags": [
        "3d-generation",
        "acceleration",
        "training-free"
      ],
      "id": 9
    },
    {
      "name": "Adlik",
      "one_line_profile": "End-to-end deep learning inference acceleration toolkit",
      "detailed_description": "A toolkit for accelerating deep learning inference, providing model optimization and deployment capabilities across various hardware platforms.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "model_optimization"
      ],
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/Adlik/Adlik",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "inference",
        "acceleration",
        "deep-learning"
      ],
      "id": 10
    },
    {
      "name": "LLM-distributed-finetune",
      "one_line_profile": "Distributed fine-tuning workflow for LLMs using DeepSpeed and Ray",
      "detailed_description": "A workflow tool for efficiently fine-tuning Large Language Models (LLMs) using distributed training techniques with DeepSpeed and Ray orchestration.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "distributed_training",
        "fine_tuning"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/AdrianBZG/LLM-distributed-finetune",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "distributed-training",
        "deepspeed",
        "ray"
      ],
      "id": 11
    },
    {
      "name": "optimum-transformers",
      "one_line_profile": "Accelerated NLP pipelines using ONNX Runtime",
      "detailed_description": "A library providing accelerated NLP pipelines for fast inference on CPU and GPU, integrating Transformers with Optimum and ONNX Runtime.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "nlp_pipeline"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/AlekseyKorshuk/optimum-transformers",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "nlp",
        "onnx",
        "inference",
        "acceleration"
      ],
      "id": 12
    },
    {
      "name": "torchacc",
      "one_line_profile": "PyTorch distributed training acceleration framework",
      "detailed_description": "A framework designed to accelerate distributed training workloads in PyTorch, optimizing performance for large-scale models.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "training_acceleration",
        "distributed_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/AlibabaPAI/torchacc",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "pytorch",
        "distributed-training",
        "acceleration"
      ],
      "id": 13
    },
    {
      "name": "Compass_Apache_TVM",
      "one_line_profile": "Enhanced Apache TVM compiler for heterogeneous execution",
      "detailed_description": "An enhanced version of the Apache TVM compiler stack, optimized for wide neural network support and heterogeneous execution on specific hardware.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "compiler",
        "inference_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Arm-China/Compass_Apache_TVM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "tvm",
        "compiler",
        "heterogeneous-computing"
      ],
      "id": 14
    },
    {
      "name": "AutoGPTQ",
      "one_line_profile": "Easy-to-use LLM quantization package based on GPTQ",
      "detailed_description": "A user-friendly library for quantizing Large Language Models (LLMs) using the GPTQ algorithm, facilitating efficient inference.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "model_compression"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/AutoGPTQ/AutoGPTQ",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gptq",
        "quantization",
        "llm"
      ],
      "id": 15
    },
    {
      "name": "distribuuuu",
      "one_line_profile": "Lightweight PyTorch distributed training framework",
      "detailed_description": "A minimalist and clear framework for PyTorch distributed training, designed to simplify the setup and execution of distributed experiments.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "distributed_training",
        "training_acceleration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/BIGBALLON/distribuuuu",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pytorch",
        "distributed-training"
      ],
      "id": 16
    },
    {
      "name": "BitNet-Transformers",
      "one_line_profile": "1-bit Transformer implementation for LLMs",
      "detailed_description": "A PyTorch implementation of BitNet (Scaling 1-bit Transformers for Large Language Models) integrated with Huggingface Transformers.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "model_architecture"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Beomi/BitNet-Transformers",
      "help_website": [],
      "license": null,
      "tags": [
        "bitnet",
        "1-bit",
        "quantization",
        "llm"
      ],
      "id": 17
    },
    {
      "name": "bluefog",
      "one_line_profile": "Decentralized distributed training framework for PyTorch",
      "detailed_description": "A distributed and decentralized training framework for PyTorch that operates over graphs, optimizing communication for large-scale training.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "distributed_training",
        "decentralized_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Bluefog-Lib/bluefog",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "distributed-training",
        "decentralized",
        "pytorch"
      ],
      "id": 18
    },
    {
      "name": "attorch",
      "one_line_profile": "PyTorch modules accelerated with OpenAI Triton",
      "detailed_description": "A collection of PyTorch neural network modules re-implemented using OpenAI's Triton language for high-performance GPU acceleration.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "acceleration",
        "kernel_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/BobMcDear/attorch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "triton",
        "pytorch",
        "acceleration"
      ],
      "id": 19
    },
    {
      "name": "metalQwen3",
      "one_line_profile": "Metal GPU accelerated inference for Qwen3 on macOS",
      "detailed_description": "A C++ implementation of the Qwen3 transformer model optimized for Apple Silicon using Metal compute shaders for acceleration.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "edge_computing"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/BoltzmannEntropy/metalQwen3",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "metal",
        "apple-silicon",
        "qwen",
        "inference"
      ],
      "id": 20
    },
    {
      "name": "Triton-distributed",
      "one_line_profile": "Distributed compiler based on Triton",
      "detailed_description": "A distributed compiler infrastructure built on top of Triton, designed to optimize parallel execution across distributed systems.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "compiler",
        "distributed_computing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ByteDance-Seed/Triton-distributed",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "triton",
        "compiler",
        "distributed-systems"
      ],
      "id": 21
    },
    {
      "name": "Super-Fast-Adversarial-Training",
      "one_line_profile": "High-performance adversarial training framework",
      "detailed_description": "A PyTorch implementation for super fast adversarial training, incorporating distributed data parallel, mixed precision, and efficient data loading techniques.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "training_acceleration",
        "adversarial_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ByungKwanLee/Super-Fast-Adversarial-Training",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "adversarial-training",
        "acceleration",
        "pytorch"
      ],
      "id": 22
    },
    {
      "name": "XB-Sim",
      "one_line_profile": "Simulation framework for ReRAM-based CNN acceleration",
      "detailed_description": "A unified framework for training, mapping, and simulating Convolutional Neural Networks (CNNs) on ReRAM-based hardware accelerators.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "hardware_simulation",
        "neuromorphic_computing"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/CRAFT-THU/XB-Sim",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "reram",
        "simulation",
        "hardware-acceleration"
      ],
      "id": 23
    },
    {
      "name": "ChatGLM_mutli_gpu_tuning",
      "one_line_profile": "Multi-GPU fine-tuning tool for ChatGLM",
      "detailed_description": "A streamlined implementation for multi-GPU fine-tuning of ChatGLM models using DeepSpeed and HuggingFace Trainer.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "fine_tuning",
        "distributed_training"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/CSHaitao/ChatGLM_mutli_gpu_tuning",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "chatglm",
        "fine-tuning",
        "deepspeed"
      ],
      "id": 24
    },
    {
      "name": "triton-linalg",
      "one_line_profile": "Triton to Linalg dialect conversion tool",
      "detailed_description": "A development repository for converting Triton IR to MLIR Linalg dialect, facilitating compiler interoperability and optimization.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "compiler",
        "ir_conversion"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/Cambricon/triton-linalg",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "triton",
        "mlir",
        "linalg",
        "compiler"
      ],
      "id": 25
    },
    {
      "name": "APQ-DM",
      "one_line_profile": "Accurate post-training quantization for diffusion models",
      "detailed_description": "Implementation of APQ-DM, a method for accurate post-training quantization of diffusion models, preserving generation quality.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "model_compression"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ChangyuanWang17/APQ-DM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "diffusion-models",
        "quantization",
        "ptq"
      ],
      "id": 26
    },
    {
      "name": "vllm-cli",
      "one_line_profile": "CLI tool for serving LLMs with vLLM",
      "detailed_description": "A command-line interface wrapper for vLLM, simplifying the deployment and serving of Large Language Models.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_serving",
        "deployment"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/Chen-zexi/vllm-cli",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "vllm",
        "cli",
        "serving",
        "llm"
      ],
      "id": 27
    },
    {
      "name": "ncnnqat",
      "one_line_profile": "Quantization-aware training package for NCNN on PyTorch",
      "detailed_description": "A Python package designed to facilitate quantization-aware training (QAT) for models intended to be deployed with the NCNN inference framework, helping to maintain accuracy while optimizing for edge devices.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "model_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ChenShisen/ncnnqat",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ncnn",
        "quantization-aware-training",
        "pytorch"
      ],
      "id": 28
    },
    {
      "name": "llm-rk3588",
      "one_line_profile": "GPU-accelerated LLM inference on Rockchip RK3588",
      "detailed_description": "A deployment tool enabling the execution of Large Language Models on Rockchip RK3588 hardware with GPU acceleration, optimizing inference for edge computing scenarios.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "edge_computing"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/Chrisz236/llm-rk3588",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rk3588",
        "llm",
        "edge-inference"
      ],
      "id": 29
    },
    {
      "name": "BiSeNet",
      "one_line_profile": "Implementation of BiSeNet V1 and V2 for real-time semantic segmentation",
      "detailed_description": "A PyTorch implementation of the BiSeNet (Bilateral Segmentation Network) architecture, designed for efficient real-time semantic segmentation tasks in computer vision.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "image_segmentation",
        "model_implementation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/CoinCheung/BiSeNet",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "semantic-segmentation",
        "bisenet",
        "computer-vision"
      ],
      "id": 30
    },
    {
      "name": "gdGPT",
      "one_line_profile": "Accelerated LLM training using DeepSpeed pipeline mode",
      "detailed_description": "A training framework for Large Language Models (Bloom, Llama, Baichuan, ChatGLM) utilizing DeepSpeed's pipeline parallelism to achieve faster training speeds compared to standard Zero/FSDP methods.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "training_acceleration",
        "distributed_training"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/CoinCheung/gdGPT",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-training",
        "deepspeed",
        "pipeline-parallelism"
      ],
      "id": 31
    },
    {
      "name": "MPP-LLaVA",
      "one_line_profile": "Multimodal Pipeline Parallel training for LLaVA-like models",
      "detailed_description": "A project enabling the training of large multimodal models (like Qwen-VL) on consumer-grade hardware (e.g., RTX3090/4090) using pipeline parallelism techniques.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "training_acceleration",
        "multimodal_learning"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/Coobiw/MPP-LLaVA",
      "help_website": [],
      "license": null,
      "tags": [
        "mllm",
        "pipeline-parallelism",
        "consumer-gpu"
      ],
      "id": 32
    },
    {
      "name": "QuIP",
      "one_line_profile": "2-Bit Quantization of Large Language Models with guarantees",
      "detailed_description": "Implementation of the QuIP algorithm for extreme quantization (2-bit) of Large Language Models, providing theoretical guarantees and practical code for model compression.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "model_compression"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Cornell-RelaxML/QuIP",
      "help_website": [],
      "license": null,
      "tags": [
        "llm-quantization",
        "2-bit",
        "model-compression"
      ],
      "id": 33
    },
    {
      "name": "dfq-toolkit",
      "one_line_profile": "Task-Specific Zero-shot Quantization-Aware Training toolkit",
      "detailed_description": "A toolkit for performing zero-shot quantization-aware training specifically optimized for object detection tasks, as presented at ICCV 2025.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "object_detection"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/DFQ-Dojo/dfq-toolkit",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "quantization-aware-training",
        "zero-shot",
        "object-detection"
      ],
      "id": 34
    },
    {
      "name": "BerryNet",
      "one_line_profile": "Deep learning gateway for Raspberry Pi and edge devices",
      "detailed_description": "An open-source deep learning gateway designed to turn edge devices like Raspberry Pi into intelligent AI nodes, managing inference tasks and data flow.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "edge_inference",
        "iot_gateway"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/DT42/BerryNet",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "edge-ai",
        "raspberry-pi",
        "inference-gateway"
      ],
      "id": 35
    },
    {
      "name": "Audio-Denoiser-ONNX",
      "one_line_profile": "Audio denoising using ONNX Runtime",
      "detailed_description": "A tool leveraging ONNX Runtime to perform efficient audio denoising, suitable for cleaning up audio data in scientific or media processing pipelines.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "audio_processing",
        "denoising"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/DakeQQ/Audio-Denoiser-ONNX",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "audio-denoising",
        "onnx",
        "signal-processing"
      ],
      "id": 36
    },
    {
      "name": "F5-TTS-ONNX",
      "one_line_profile": "F5-TTS implementation using ONNX Runtime",
      "detailed_description": "A runtime implementation for the F5 Text-to-Speech model using ONNX, enabling efficient speech synthesis inference.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "speech_synthesis",
        "inference_acceleration"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/DakeQQ/F5-TTS-ONNX",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "tts",
        "onnx",
        "inference"
      ],
      "id": 37
    },
    {
      "name": "HiPrune",
      "one_line_profile": "Training-free visual token pruning for VLM acceleration",
      "detailed_description": "Implementation of a method for pruning visual tokens in Vision-Language Models (VLMs) without retraining, aiming to accelerate inference.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_pruning",
        "vlm_acceleration"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/Danielement321/HiPrune",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pruning",
        "vlm",
        "acceleration"
      ],
      "id": 38
    },
    {
      "name": "CVFusion",
      "one_line_profile": "Deep learning compiler to fuse OpenCV operators",
      "detailed_description": "An open-source deep learning compiler designed to optimize computer vision pipelines by fusing OpenCV operators, improving execution efficiency.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "compiler",
        "operator_fusion"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/DeepLink-org/CVFusion",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "compiler",
        "opencv",
        "optimization"
      ],
      "id": 39
    },
    {
      "name": "jaxDecomp",
      "one_line_profile": "JAX bindings for NVIDIA cuDecomp library",
      "detailed_description": "Provides JAX bindings for the NVIDIA cuDecomp library, enabling efficient domain decomposition for high-performance scientific computing and simulations on GPUs.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "hpc",
        "domain_decomposition"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/DifferentiableUniverseInitiative/jaxDecomp",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "jax",
        "hpc",
        "cudecomp"
      ],
      "id": 40
    },
    {
      "name": "keras_compressor",
      "one_line_profile": "Model Compression CLI Tool for Keras",
      "detailed_description": "A command-line interface tool for compressing Keras models, facilitating optimization for deployment.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_compression",
        "optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/DwangoMediaVillage/keras_compressor",
      "help_website": [],
      "license": null,
      "tags": [
        "keras",
        "compression",
        "cli"
      ],
      "id": 41
    },
    {
      "name": "EduChat",
      "one_line_profile": "Open-source educational chat model",
      "detailed_description": "A large language model specifically tuned for educational contexts, including tools for data cleaning and GPU deployment.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "llm",
        "education_domain"
      ],
      "application_level": "platform",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/ECNU-ICALK/EduChat",
      "help_website": [],
      "license": null,
      "tags": [
        "llm",
        "education",
        "deployment"
      ],
      "id": 42
    },
    {
      "name": "kernl",
      "one_line_profile": "Accelerated PyTorch transformer inference on GPU",
      "detailed_description": "A library that optimizes PyTorch transformer models for faster GPU inference using custom kernels, designed to be easily integrated and modified.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "kernel_optimization"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/ELS-RD/kernl",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "pytorch",
        "transformer",
        "gpu-acceleration"
      ],
      "id": 43
    },
    {
      "name": "evogp",
      "one_line_profile": "GPU-accelerated library for Tree-based Genetic Programming",
      "detailed_description": "A library leveraging PyTorch and CUDA for high-performance evolutionary computation, specifically tree-based genetic programming for symbolic regression and classification.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "genetic_programming",
        "symbolic_regression"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/EMI-Group/evogp",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "genetic-programming",
        "gpu",
        "symbolic-regression"
      ],
      "id": 44
    },
    {
      "name": "Stable-Diffusion-NCNN",
      "one_line_profile": "Stable Diffusion implementation in NCNN with C++",
      "detailed_description": "A C++ implementation of Stable Diffusion using the NCNN framework, enabling image generation on edge devices and mobile platforms.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "generative_ai",
        "edge_inference"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/EdVince/Stable-Diffusion-NCNN",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "stable-diffusion",
        "ncnn",
        "edge-ai"
      ],
      "id": 45
    },
    {
      "name": "mera",
      "one_line_profile": "Heterogeneous Platform Deep Learning Compiler Framework",
      "detailed_description": "A compiler framework from EdgeCortix designed to optimize deep learning models for heterogeneous computing platforms.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "compiler",
        "heterogeneous_computing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Edgecortix-Inc/mera",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "compiler",
        "edge-ai",
        "optimization"
      ],
      "id": 46
    },
    {
      "name": "Einsums",
      "one_line_profile": "Compile-time tensor contraction analysis and optimization",
      "detailed_description": "A C++ library that performs compile-time analysis of tensor contraction patterns to determine and execute optimal tensor operations.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "tensor_computation",
        "optimization"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/Einsums/Einsums",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "tensor",
        "c++",
        "optimization"
      ],
      "id": 47
    },
    {
      "name": "gpt-neox",
      "one_line_profile": "Model parallel autoregressive transformer implementation on GPUs",
      "detailed_description": "A library for training large-scale language models on GPUs using model parallelism, built on top of Megatron-LM and DeepSpeed.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "llm_training",
        "distributed_computing"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/EleutherAI/gpt-neox",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "distributed-training",
        "gpu"
      ],
      "id": 48
    },
    {
      "name": "Reactant.jl",
      "one_line_profile": "Optimize Julia functions with MLIR and XLA",
      "detailed_description": "A tool to optimize Julia code execution on high-performance hardware (CPU, GPU, TPU) by leveraging MLIR and XLA compilation stacks.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "compiler",
        "hpc"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/EnzymeAD/Reactant.jl",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "julia",
        "xla",
        "mlir"
      ],
      "id": 49
    },
    {
      "name": "candle-vllm",
      "one_line_profile": "Efficient inference and serving platform for local LLMs",
      "detailed_description": "A Rust-based platform for efficient inference and serving of Large Language Models, compatible with OpenAI API standards.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_serving",
        "llm"
      ],
      "application_level": "service",
      "primary_language": "Rust",
      "repo_url": "https://github.com/EricLBuehler/candle-vllm",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm-serving",
        "rust",
        "inference"
      ],
      "id": 50
    },
    {
      "name": "FaceONNX",
      "one_line_profile": "Face recognition library based on ONNX Runtime",
      "detailed_description": "A C# library for face recognition and analytics utilizing deep neural networks and the ONNX Runtime for inference.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "face_recognition",
        "biometrics"
      ],
      "application_level": "library",
      "primary_language": "C#",
      "repo_url": "https://github.com/FaceONNX/FaceONNX",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "face-recognition",
        "onnx",
        "c#"
      ],
      "id": 51
    },
    {
      "name": "fasterai",
      "one_line_profile": "Model pruning and distillation library for FastAI/PyTorch",
      "detailed_description": "A library designed to facilitate neural network compression techniques such as pruning and knowledge distillation within the FastAI and PyTorch ecosystems.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_compression",
        "pruning"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/FasterAI-Labs/fasterai",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "pruning",
        "distillation",
        "fastai"
      ],
      "id": 52
    },
    {
      "name": "TensorRT-Alpha",
      "one_line_profile": "TensorRT implementations for YOLO series and other models",
      "detailed_description": "A comprehensive collection of TensorRT implementations for accelerating various YOLO models (v5-v8) and other architectures on NVIDIA GPUs.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "tensorrt"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/FeiYull/TensorRT-Alpha",
      "help_website": [],
      "license": "GPL-2.0",
      "tags": [
        "tensorrt",
        "yolo",
        "inference"
      ],
      "id": 53
    },
    {
      "name": "QDLM",
      "one_line_profile": "Post-training quantization for Diffusion LLMs",
      "detailed_description": "A tool for applying post-training quantization techniques specifically to Diffusion Large Language Models (dLLMs) to improve efficiency.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "diffusion_models"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/FelixMessi/QDLM",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "quantization",
        "diffusion-llm",
        "ptq"
      ],
      "id": 54
    },
    {
      "name": "CMUNeXt",
      "one_line_profile": "Efficient Medical Image Segmentation Network",
      "detailed_description": "An efficient deep learning network architecture designed for medical image segmentation, featuring large kernels and skip fusion mechanisms.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "medical_image_segmentation",
        "model_architecture"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/FengheTan9/CMUNeXt",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "medical-imaging",
        "segmentation",
        "efficient-network"
      ],
      "id": 55
    },
    {
      "name": "RoboBrain",
      "one_line_profile": "Unified Brain Model for Robotic Manipulation",
      "detailed_description": "A unified foundation model for robotic manipulation tasks, bridging abstract reasoning with concrete control actions.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "robotics",
        "foundation_model"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/FlagOpen/RoboBrain",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "robotics",
        "manipulation",
        "foundation-model"
      ],
      "id": 56
    },
    {
      "name": "XLA.jl",
      "one_line_profile": "XLA compiler bindings for Julia",
      "detailed_description": "A package providing bindings to the XLA (Accelerated Linear Algebra) compiler for the Julia language, enabling hardware acceleration for machine learning workloads.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "compiler",
        "acceleration"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/FluxML/XLA.jl",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "julia",
        "xla",
        "compiler"
      ],
      "id": 57
    },
    {
      "name": "compressonator",
      "one_line_profile": "Texture and 3D Model Compression Tool Suite",
      "detailed_description": "A suite of tools for compressing, optimizing, and analyzing textures and 3D models, supporting various hardware accelerations (CPU, GPU, APU).",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "data_compression",
        "3d_visualization"
      ],
      "application_level": "workflow",
      "primary_language": "C++",
      "repo_url": "https://github.com/GPUOpen-Tools/compressonator",
      "help_website": [],
      "license": null,
      "tags": [
        "compression",
        "texture",
        "3d-model"
      ],
      "id": 58
    },
    {
      "name": "nano-vllm",
      "one_line_profile": "Lightweight implementation of vLLM",
      "detailed_description": "A simplified and lightweight version of the vLLM library, designed for efficient LLM inference with lower overhead.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "llm"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/GeeeekExplorer/nano-vllm",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "vllm",
        "inference",
        "lightweight"
      ],
      "id": 59
    },
    {
      "name": "FastMOT",
      "one_line_profile": "High-performance multiple object tracking",
      "detailed_description": "A high-performance multiple object tracking library integrating YOLO, Deep SORT, and KLT optical flow for efficient video analysis.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "object_tracking",
        "video_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/GeekAlexis/FastMOT",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "tracking",
        "yolo",
        "computer-vision"
      ],
      "id": 60
    },
    {
      "name": "Depths-CPP",
      "one_line_profile": "High-performance C++ depth estimation using ONNX Runtime",
      "detailed_description": "A C++ application and header library for real-time metric depth estimation using Depth-Anything-V2 models, optimized with ONNX Runtime and OpenCV.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "depth_estimation",
        "computer_vision"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/Geekgineer/Depths-CPP",
      "help_website": [],
      "license": null,
      "tags": [
        "depth-estimation",
        "onnx",
        "c++"
      ],
      "id": 61
    },
    {
      "name": "YOLOs-CPP",
      "one_line_profile": "High-performance C++ YOLO inference library",
      "detailed_description": "A C++ library for real-time object detection and segmentation using various YOLO models (v5-v12), leveraging ONNX Runtime for optimized CPU/GPU inference.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "object_detection",
        "inference_acceleration"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/Geekgineer/YOLOs-CPP",
      "help_website": [],
      "license": null,
      "tags": [
        "yolo",
        "onnx",
        "c++"
      ],
      "id": 62
    },
    {
      "name": "PiSSA",
      "one_line_profile": "Parameter-efficient fine-tuning method for Large Language Models",
      "detailed_description": "A library implementing Principal Singular Values and Singular Vectors Adaptation (PiSSA) for efficient fine-tuning of LLMs, optimizing parameter adaptation compared to LoRA.",
      "domains": [
        "AI6-04",
        "AI6"
      ],
      "subtask_category": [
        "model_adaptation",
        "fine_tuning"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/GraphPKU/PiSSA",
      "help_website": [],
      "license": null,
      "tags": [
        "peft",
        "llm-finetuning",
        "parameter-efficiency"
      ],
      "id": 63
    },
    {
      "name": "YOLOv5-Multibackbone-Compression",
      "one_line_profile": "Comprehensive compression toolbox for YOLOv5 models",
      "detailed_description": "A toolbox integrating multi-backbone support, pruning (EagleEye, Network Slimming), quantization (MQBench), and deployment (TensorRT, ncnn) for YOLOv5 series models.",
      "domains": [
        "AI6-04",
        "AI6"
      ],
      "subtask_category": [
        "model_compression",
        "quantization",
        "pruning"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/Gumpest/YOLOv5-Multibackbone-Compression",
      "help_website": [],
      "license": null,
      "tags": [
        "yolov5",
        "model-compression",
        "pruning",
        "quantization"
      ],
      "id": 64
    },
    {
      "name": "EasyCache",
      "one_line_profile": "Training-free acceleration for video diffusion models",
      "detailed_description": "A library for accelerating video diffusion models using runtime-adaptive caching mechanisms, enabling faster inference without retraining.",
      "domains": [
        "AI6-04",
        "AI6"
      ],
      "subtask_category": [
        "inference_acceleration",
        "caching"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/H-EmbodVis/EasyCache",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "diffusion-models",
        "video-generation",
        "acceleration"
      ],
      "id": 65
    },
    {
      "name": "TTC",
      "one_line_profile": "High-performance compiler for tensor transpositions",
      "detailed_description": "A specialized compiler designed to generate high-performance code for tensor transpositions, optimizing data layout transformations in scientific computing and deep learning.",
      "domains": [
        "AI6-04",
        "AI6"
      ],
      "subtask_category": [
        "compilation",
        "tensor_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/HPAC/TTC",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "compiler",
        "tensor-transposition",
        "hpc"
      ],
      "id": 66
    },
    {
      "name": "tpc_llvm",
      "one_line_profile": "LLVM-based compiler for HabanaLabs TPC accelerators",
      "detailed_description": "The TPC-CLANG compiler based on LLVM, designed to compile TPC C programming language for HabanaLabs Deep-Learning Accelerators.",
      "domains": [
        "AI6-04",
        "AI6"
      ],
      "subtask_category": [
        "compilation",
        "hardware_acceleration"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/HabanaAI/tpc_llvm",
      "help_website": [],
      "license": null,
      "tags": [
        "llvm",
        "compiler",
        "habana",
        "accelerator"
      ],
      "id": 67
    },
    {
      "name": "YOLO-Multi-Backbones-Attention",
      "one_line_profile": "Model compression toolkit for YOLOv3",
      "detailed_description": "A model compression toolkit for YOLOv3 incorporating lightweight backbones (ShuffleNet, GhostNet), attention mechanisms, pruning, and quantization.",
      "domains": [
        "AI6-04",
        "AI6"
      ],
      "subtask_category": [
        "model_compression",
        "pruning",
        "quantization"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/HaloTrouvaille/YOLO-Multi-Backbones-Attention",
      "help_website": [],
      "license": null,
      "tags": [
        "yolo",
        "model-compression",
        "pruning"
      ],
      "id": 68
    },
    {
      "name": "revlib",
      "one_line_profile": "Memory-efficient Reversible Network library for PyTorch",
      "detailed_description": "A library implementing Reversible Networks (RevNet) for PyTorch, supporting XLA and DeepSpeed to significantly reduce memory usage during training.",
      "domains": [
        "AI6-04",
        "AI6"
      ],
      "subtask_category": [
        "training_optimization",
        "memory_management"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/HomebrewML/revlib",
      "help_website": [],
      "license": "BSD-2-Clause",
      "tags": [
        "memory-optimization",
        "reversible-networks",
        "pytorch"
      ],
      "id": 69
    },
    {
      "name": "transpeeder",
      "one_line_profile": "Tool for training Llama on single A100 using Pipeline Parallelism",
      "detailed_description": "A utility enabling the training of large language models (like Llama) on limited hardware (single A100 node) by leveraging DeepSpeed Pipeline Parallelism.",
      "domains": [
        "AI6-04",
        "AI6"
      ],
      "subtask_category": [
        "distributed_training",
        "pipeline_parallelism"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/HuangLK/transpeeder",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-training",
        "deepspeed",
        "pipeline-parallelism"
      ],
      "id": 70
    },
    {
      "name": "tfbert",
      "one_line_profile": "TensorFlow 1.x based BERT pre-training framework",
      "detailed_description": "A pre-training framework for BERT models based on TensorFlow 1.x, supporting multi-GPU training, gradient accumulation, XLA acceleration, and mixed precision.",
      "domains": [
        "AI6-04",
        "AI6"
      ],
      "subtask_category": [
        "model_training",
        "pretraining"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/HuiResearch/tfbert",
      "help_website": [],
      "license": null,
      "tags": [
        "bert",
        "pretraining",
        "tensorflow",
        "xla"
      ],
      "id": 71
    },
    {
      "name": "zDLC",
      "one_line_profile": "Deep Learning Compiler for IBM Z systems",
      "detailed_description": "A deep learning compiler specifically optimized for IBM Z mainframes, enabling efficient execution of neural networks on this architecture.",
      "domains": [
        "AI6-04",
        "AI6"
      ],
      "subtask_category": [
        "compilation",
        "hardware_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/IBM/zDLC",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "compiler",
        "ibm-z",
        "deep-learning"
      ],
      "id": 72
    },
    {
      "name": "OBC",
      "one_line_profile": "Optimal Brain Compression framework for quantization and pruning",
      "detailed_description": "A framework for accurate post-training quantization and pruning of neural networks, implementing the Optimal Brain Compression method.",
      "domains": [
        "AI6-04",
        "AI6"
      ],
      "subtask_category": [
        "model_compression",
        "quantization",
        "pruning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/IST-DASLab/OBC",
      "help_website": [],
      "license": null,
      "tags": [
        "quantization",
        "pruning",
        "model-compression"
      ],
      "id": 73
    },
    {
      "name": "GPTQ",
      "one_line_profile": "Accurate post-training quantization for GPT models",
      "detailed_description": "A widely used library for post-training quantization of generative pretrained transformers (GPT), enabling efficient inference on consumer hardware.",
      "domains": [
        "AI6-04",
        "AI6"
      ],
      "subtask_category": [
        "quantization",
        "model_compression"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/IST-DASLab/gptq",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "quantization",
        "llm",
        "gpt",
        "inference-acceleration"
      ],
      "id": 74
    },
    {
      "name": "QMoE",
      "one_line_profile": "Sub-1-bit compression tool for Mixture-of-Experts models",
      "detailed_description": "A library for compressing trillion-parameter Mixture-of-Experts (MoE) models to sub-1-bit precision while maintaining accuracy.",
      "domains": [
        "AI6-04",
        "AI6"
      ],
      "subtask_category": [
        "quantization",
        "model_compression"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/IST-DASLab/qmoe",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "moe",
        "quantization",
        "model-compression"
      ],
      "id": 75
    },
    {
      "name": "InferenceMAX",
      "one_line_profile": "Continuous inference benchmarking tool for LLM hardware",
      "detailed_description": "A benchmarking tool for evaluating large language model inference performance across various hardware accelerators (NVIDIA, AMD, TPU).",
      "domains": [
        "AI6-04",
        "AI6"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/InferenceMAX/InferenceMAX",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "benchmarking",
        "llm-inference",
        "hardware-evaluation"
      ],
      "id": 76
    },
    {
      "name": "TriForce",
      "one_line_profile": "Lossless acceleration for long sequence generation",
      "detailed_description": "A system for accelerating long sequence generation in LLMs using Hierarchical Speculative Decoding, maintaining lossless quality.",
      "domains": [
        "AI6-04",
        "AI6"
      ],
      "subtask_category": [
        "inference_acceleration",
        "speculative_decoding"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Infini-AI-Lab/TriForce",
      "help_website": [],
      "license": null,
      "tags": [
        "llm-acceleration",
        "speculative-decoding",
        "long-sequence"
      ],
      "id": 77
    },
    {
      "name": "NLP Architect",
      "one_line_profile": "Intel's library for NLP model optimization and exploration",
      "detailed_description": "A library by Intel Labs for exploring state-of-the-art deep learning topologies and optimization techniques specifically for Natural Language Processing.",
      "domains": [
        "AI6-04",
        "AI6"
      ],
      "subtask_category": [
        "model_optimization",
        "nlp_modeling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/IntelLabs/nlp-architect",
      "help_website": [
        "http://nlp_architect.nervanasys.com/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "nlp",
        "optimization",
        "intel",
        "deep-learning"
      ],
      "id": 78
    },
    {
      "name": "LMDeploy",
      "one_line_profile": "Toolkit for compressing, deploying, and serving LLMs",
      "detailed_description": "A comprehensive toolkit for the efficient compression, deployment, and serving of Large Language Models, supporting high-throughput inference.",
      "domains": [
        "AI6-04",
        "AI6"
      ],
      "subtask_category": [
        "model_deployment",
        "inference_serving",
        "compression"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/InternLM/lmdeploy",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-serving",
        "deployment",
        "compression",
        "inference"
      ],
      "id": 79
    },
    {
      "name": "Neural-Net-LabView-DLL",
      "one_line_profile": "Deep Learning library for LabView integration",
      "detailed_description": "A C++-based library enabling the execution of feed-forward neural networks within the LabView environment, facilitating deep learning integration in experimental setups.",
      "domains": [
        "AI6-04",
        "AI6"
      ],
      "subtask_category": [
        "inference_integration",
        "data_acquisition"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/JamesGlare/Neural-Net-LabView-DLL",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "labview",
        "neural-network",
        "integration",
        "experimental-physics"
      ],
      "id": 80
    },
    {
      "name": "Jittor",
      "one_line_profile": "High-performance JIT-based deep learning framework",
      "detailed_description": "A deep learning framework based on JIT compiling and meta-operators, designed for high performance and easy optimization.",
      "domains": [
        "AI6-04",
        "AI6"
      ],
      "subtask_category": [
        "deep_learning_framework",
        "jit_compilation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/Jittor/jittor",
      "help_website": [
        "https://jittor.org/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "deep-learning-framework",
        "jit",
        "compiler"
      ],
      "id": 81
    },
    {
      "name": "XLA.jl",
      "one_line_profile": "Julia interface for XLA (TPU) compilation",
      "detailed_description": "A library enabling the compilation of Julia code to XLA, allowing Julia programs to run on Google TPUs and other XLA-supported hardware.",
      "domains": [
        "AI6-04",
        "AI6"
      ],
      "subtask_category": [
        "compilation",
        "hpc_integration"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/JuliaGPU/XLA.jl",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "julia",
        "xla",
        "tpu",
        "compiler"
      ],
      "id": 82
    },
    {
      "name": "Kernel Tuner",
      "one_line_profile": "Auto-tuning tool for GPU kernels",
      "detailed_description": "A tool for automatically tuning and optimizing CUDA, OpenCL, and C code kernels for GPUs, essential for high-performance scientific computing.",
      "domains": [
        "AI6-04",
        "AI6"
      ],
      "subtask_category": [
        "kernel_tuning",
        "performance_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/KernelTuner/kernel_tuner",
      "help_website": [
        "http://kerneltuner.github.io/kernel_tuner/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "auto-tuning",
        "gpu",
        "cuda",
        "opencl"
      ],
      "id": 83
    },
    {
      "name": "fastT5",
      "one_line_profile": "Inference acceleration library for T5 models",
      "detailed_description": "A library to boost the inference speed of T5 models by converting them to ONNX Runtime and quantizing them, reducing model size and latency.",
      "domains": [
        "AI6-04",
        "AI6"
      ],
      "subtask_category": [
        "inference_acceleration",
        "model_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Ki6an/fastT5",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "t5",
        "onnx",
        "quantization",
        "inference"
      ],
      "id": 84
    },
    {
      "name": "LMCache",
      "one_line_profile": "High-performance KV cache storage backend for LLM inference acceleration",
      "detailed_description": "LMCache is a specialized cache layer designed to accelerate Large Language Model (LLM) inference by optimizing Key-Value (KV) cache management. It supports sharing KV caches across different inference engines and instances, significantly reducing latency and improving throughput for distributed LLM serving.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "memory_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/LMCache/LMCache",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "kv-cache",
        "inference-optimization"
      ],
      "id": 85
    },
    {
      "name": "ocaml-xla",
      "one_line_profile": "OCaml bindings for the XLA (Accelerated Linear Algebra) compiler",
      "detailed_description": "This library provides OCaml bindings for Google's XLA (Accelerated Linear Algebra) compiler, enabling OCaml developers to leverage high-performance machine learning compilation and hardware acceleration (GPU/TPU).",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "compiler_binding",
        "model_compilation"
      ],
      "application_level": "library",
      "primary_language": "OCaml",
      "repo_url": "https://github.com/LaurentMazare/ocaml-xla",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "xla",
        "ocaml",
        "compiler",
        "acceleration"
      ],
      "id": 86
    },
    {
      "name": "ug",
      "one_line_profile": "Experimental deep learning compiler written in Rust",
      "detailed_description": "ug is an experimental compiler for deep learning models, written in Rust. It aims to provide a testbed for exploring compilation techniques and optimizations for neural network inference and training.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_compilation"
      ],
      "application_level": "solver",
      "primary_language": "Rust",
      "repo_url": "https://github.com/LaurentMazare/ug",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "compiler",
        "rust",
        "deep-learning"
      ],
      "id": 87
    },
    {
      "name": "xla-rs",
      "one_line_profile": "Rust bindings for the XLA compiler",
      "detailed_description": "xla-rs provides Rust bindings for the XLA compiler, allowing Rust programs to construct and execute XLA computations for accelerated linear algebra and machine learning tasks.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "compiler_binding",
        "model_compilation"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/LaurentMazare/xla-rs",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rust",
        "xla",
        "compiler"
      ],
      "id": 88
    },
    {
      "name": "AutoGPTQ.tvm",
      "one_line_profile": "TVM kernel implementation for GPTQ quantization inference",
      "detailed_description": "This project provides a TVM (Tensor Virtual Machine) kernel implementation for GPTQ (Generative Pre-trained Transformer Quantization), enabling efficient inference of quantized LLMs using the TVM compiler stack.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization_inference",
        "kernel_optimization"
      ],
      "application_level": "library",
      "primary_language": "Cuda",
      "repo_url": "https://github.com/LeiWang1999/AutoGPTQ.tvm",
      "help_website": [],
      "license": null,
      "tags": [
        "tvm",
        "gptq",
        "quantization",
        "cuda"
      ],
      "id": 89
    },
    {
      "name": "FusedKernelLibrary",
      "one_line_profile": "Library for user-defined GPU kernel fusion",
      "detailed_description": "FusedKernelLibrary implements a methodology allowing users to define and execute fused GPU kernels without writing raw CUDA code, optimizing memory bandwidth and execution speed for custom operations.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "kernel_optimization",
        "gpu_acceleration"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/Libraries-Openly-Fused/FusedKernelLibrary",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "kernel-fusion",
        "gpu",
        "cuda",
        "optimization"
      ],
      "id": 90
    },
    {
      "name": "lit-llama",
      "one_line_profile": "Optimized implementation of LLaMA for training and fine-tuning",
      "detailed_description": "lit-llama is a clean, optimized implementation of the LLaMA language model based on nanoGPT. It supports advanced features like Flash Attention, Int8/GPTQ quantization, LoRA, and LLaMA-Adapter for efficient pre-training and fine-tuning.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_training",
        "model_finetuning",
        "quantization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Lightning-AI/lit-llama",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llama",
        "finetuning",
        "quantization",
        "lora"
      ],
      "id": 91
    },
    {
      "name": "TensorRT-For-YOLO-Series",
      "one_line_profile": "TensorRT deployment toolkit for YOLO object detection models",
      "detailed_description": "A comprehensive toolkit for deploying various versions of YOLO (v5-v11, X) using NVIDIA TensorRT. It includes C++ and Python implementations for efficient inference acceleration and NMS plugin support.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "object_detection"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Linaom1214/TensorRT-For-YOLO-Series",
      "help_website": [],
      "license": null,
      "tags": [
        "tensorrt",
        "yolo",
        "inference",
        "object-detection"
      ],
      "id": 92
    },
    {
      "name": "Lux.jl",
      "one_line_profile": "Explicit parameter deep learning framework for Julia",
      "detailed_description": "Lux.jl is a Julia deep learning framework that emphasizes explicit parameter handling and functional design, making it highly suitable for scientific machine learning (SciML) and integration with differential equation solvers.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "deep_learning_framework",
        "scientific_modeling"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/LuxDL/Lux.jl",
      "help_website": [
        "https://lux.csail.mit.edu/"
      ],
      "license": "MIT",
      "tags": [
        "julia",
        "deep-learning",
        "sciml"
      ],
      "id": 93
    },
    {
      "name": "mpeg-pcc-tmc13",
      "one_line_profile": "Reference software for Geometry-based Point Cloud Compression (G-PCC)",
      "detailed_description": "The official reference software implementation for the MPEG Geometry-based Point Cloud Compression (G-PCC) standard, providing encoder and decoder tools for scientific and industrial point cloud data.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "data_compression",
        "point_cloud_processing"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/MPEGGroup/mpeg-pcc-tmc13",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "point-cloud",
        "compression",
        "mpeg",
        "g-pcc"
      ],
      "id": 94
    },
    {
      "name": "mpeg-pcc-tmc2",
      "one_line_profile": "Reference software for Video-based Point Cloud Compression (V-PCC)",
      "detailed_description": "The official reference software implementation for the MPEG Video-based Point Cloud Compression (V-PCC) standard, enabling compression of dynamic point clouds by projecting them onto 2D video frames.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "data_compression",
        "point_cloud_processing"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/MPEGGroup/mpeg-pcc-tmc2",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "point-cloud",
        "compression",
        "mpeg",
        "v-pcc"
      ],
      "id": 95
    },
    {
      "name": "NNPACK",
      "one_line_profile": "High-performance neural network inference acceleration package for multi-core CPUs",
      "detailed_description": "NNPACK is an acceleration package for neural network computations, optimized for multi-core CPUs. It provides high-performance implementations of convolution, pooling, and matrix multiplication operations.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "cpu_optimization"
      ],
      "application_level": "library",
      "primary_language": "C",
      "repo_url": "https://github.com/Maratyszcza/NNPACK",
      "help_website": [],
      "license": "BSD-2-Clause",
      "tags": [
        "acceleration",
        "cpu",
        "neural-network",
        "optimization"
      ],
      "id": 96
    },
    {
      "name": "volksdep",
      "one_line_profile": "Toolbox for deploying and accelerating models with TensorRT",
      "detailed_description": "volksdep is an open-source toolbox designed to simplify the deployment and acceleration of PyTorch, ONNX, and TensorFlow models using TensorRT, providing a unified interface for inference optimization.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_deployment",
        "model_conversion"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Media-Smart/volksdep",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "tensorrt",
        "deployment",
        "inference",
        "acceleration"
      ],
      "id": 97
    },
    {
      "name": "YOLOX",
      "one_line_profile": "High-performance anchor-free YOLO object detection library",
      "detailed_description": "YOLOX is a high-performance, anchor-free object detection model family. It supports multiple deployment backends including ONNX, TensorRT, ncnn, and OpenVINO, making it suitable for scientific image analysis and real-time detection tasks.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "object_detection",
        "inference_acceleration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Megvii-BaseDetection/YOLOX",
      "help_website": [
        "https://yolox.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "object-detection",
        "yolo",
        "computer-vision"
      ],
      "id": 98
    },
    {
      "name": "TensorFrost",
      "one_line_profile": "Static optimizing tensor compiler with Python frontend",
      "detailed_description": "TensorFrost is a static optimizing tensor compiler that provides a Python frontend and autodifferentiation capabilities. It aims to offer a shader-like syntax for high-performance tensor computations.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "tensor_compiler",
        "autodifferentiation"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/MichaelMoroz/TensorFrost",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "compiler",
        "tensor",
        "optimization"
      ],
      "id": 99
    },
    {
      "name": "modalities",
      "one_line_profile": "PyTorch-native framework for distributed foundation model training",
      "detailed_description": "Modalities is a framework designed for the distributed and reproducible training of foundation models. It leverages PyTorch native components to provide a modular and scalable training infrastructure.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "distributed_training",
        "model_training"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/Modalities/modalities",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "distributed-training",
        "pytorch",
        "foundation-models"
      ],
      "id": 100
    },
    {
      "name": "GPTQModel",
      "one_line_profile": "LLM quantization toolkit with multi-backend hardware acceleration",
      "detailed_description": "GPTQModel is a toolkit for quantizing Large Language Models (LLMs) using GPTQ. It supports hardware acceleration for NVIDIA CUDA, AMD ROCm, Intel XPU, and CPUs, integrating with HF, vLLM, and SGLang.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_quantization",
        "inference_acceleration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ModelCloud/GPTQModel",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "quantization",
        "gptq",
        "llm",
        "acceleration"
      ],
      "id": 101
    },
    {
      "name": "EasyLLM",
      "one_line_profile": "Usability-focused LLM training framework based on Megatron-Deepspeed",
      "detailed_description": "EasyLLM is a training framework built upon Megatron-Deepspeed and HuggingFace Trainer. It simplifies the complexity of distributed training for Large Language Models while maintaining high efficiency.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_training",
        "distributed_training"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/ModelTC/EasyLLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "training",
        "megatron-deepspeed"
      ],
      "id": 102
    },
    {
      "name": "LightCompress",
      "one_line_profile": "Toolkit for compressing large language and vision models",
      "detailed_description": "LightCompress is a toolkit dedicated to the compression of large models, including LLMs, VLMs, and video generation models. It implements techniques to reduce model size and improve inference efficiency.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_compression",
        "quantization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ModelTC/LightCompress",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "compression",
        "llm",
        "vlm"
      ],
      "id": 103
    },
    {
      "name": "MQBench",
      "one_line_profile": "Benchmark framework for model quantization algorithms",
      "detailed_description": "MQBench is a comprehensive benchmark framework for evaluating model quantization techniques. It allows researchers to assess the performance and accuracy of different quantization algorithms on various models.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization_benchmark",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ModelTC/MQBench",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "quantization",
        "benchmark",
        "deep-learning"
      ],
      "id": 104
    },
    {
      "name": "LARS-ImageNet-PyTorch",
      "one_line_profile": "LARS optimizer implementation for large batch training",
      "detailed_description": "This repository provides a PyTorch implementation of the LARS (Layer-wise Adaptive Rate Scaling) optimizer, designed for large batch training of deep learning models (e.g., ResNet on ImageNet) in distributed environments.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "optimization_algorithm",
        "distributed_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NUS-HPC-AI-Lab/LARS-ImageNet-PyTorch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "optimizer",
        "lars",
        "distributed-training"
      ],
      "id": 105
    },
    {
      "name": "torch2trt",
      "one_line_profile": "Easy-to-use PyTorch to TensorRT converter",
      "detailed_description": "torch2trt is a Python library that simplifies the conversion of PyTorch models to NVIDIA TensorRT engines, enabling easy acceleration of inference on NVIDIA GPUs.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_conversion",
        "inference_acceleration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NVIDIA-AI-IOT/torch2trt",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pytorch",
        "tensorrt",
        "converter"
      ],
      "id": 106
    },
    {
      "name": "trt_pose",
      "one_line_profile": "Real-time pose estimation accelerated with TensorRT",
      "detailed_description": "trt_pose provides tools for training and deploying real-time human pose estimation models accelerated by NVIDIA TensorRT, suitable for computer vision and behavioral analysis tasks.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "pose_estimation",
        "inference_acceleration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NVIDIA-AI-IOT/trt_pose",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pose-estimation",
        "tensorrt",
        "computer-vision"
      ],
      "id": 107
    },
    {
      "name": "isaac_ros_object_detection",
      "one_line_profile": "NVIDIA-accelerated object detection package for ROS",
      "detailed_description": "This package provides NVIDIA-accelerated deep learning model support for object detection within the ROS (Robot Operating System) ecosystem, enabling high-performance vision for robotics applications.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "object_detection",
        "robotics_inference"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_object_detection",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ros",
        "object-detection",
        "isaac",
        "robotics"
      ],
      "id": 108
    },
    {
      "name": "Model-Optimizer",
      "one_line_profile": "Unified library for SOTA model optimization techniques",
      "detailed_description": "NVIDIA Model-Optimizer is a library containing state-of-the-art techniques for model optimization, including quantization, pruning, distillation, and speculative decoding, to compress models for efficient deployment.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_optimization",
        "quantization",
        "pruning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NVIDIA/Model-Optimizer",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "optimization",
        "quantization",
        "pruning",
        "distillation"
      ],
      "id": 109
    },
    {
      "name": "Stable-Diffusion-WebUI-TensorRT",
      "one_line_profile": "TensorRT acceleration extension for Stable Diffusion Web UI",
      "detailed_description": "This extension integrates NVIDIA TensorRT into the Stable Diffusion Web UI, providing significant inference speedups for generative AI image synthesis tasks.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "image_synthesis"
      ],
      "application_level": "plugin",
      "primary_language": "Python",
      "repo_url": "https://github.com/NVIDIA/Stable-Diffusion-WebUI-TensorRT",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "stable-diffusion",
        "tensorrt",
        "acceleration",
        "generative-ai"
      ],
      "id": 110
    },
    {
      "name": "TensorRT",
      "one_line_profile": "SDK for high-performance deep learning inference on NVIDIA GPUs",
      "detailed_description": "NVIDIA TensorRT is a high-performance deep learning inference SDK. It includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for deep learning inference applications.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "model_optimization"
      ],
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/NVIDIA/TensorRT",
      "help_website": [
        "https://developer.nvidia.com/tensorrt"
      ],
      "license": "Apache-2.0",
      "tags": [
        "inference",
        "gpu",
        "optimization",
        "sdk"
      ],
      "id": 111
    },
    {
      "name": "TensorRT-LLM",
      "one_line_profile": "Library for optimizing and executing Large Language Models on NVIDIA GPUs",
      "detailed_description": "TensorRT-LLM provides a comprehensive Python API and C++ runtime for defining, optimizing, and executing Large Language Models (LLMs) on NVIDIA GPUs, incorporating state-of-the-art techniques for efficient inference.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "llm_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NVIDIA/TensorRT-LLM",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "llm",
        "tensorrt",
        "inference",
        "gpu"
      ],
      "id": 112
    },
    {
      "name": "apex",
      "one_line_profile": "PyTorch extension for mixed precision and distributed training",
      "detailed_description": "A PyTorch extension that provides tools for easy mixed precision and distributed training, enabling faster training times and lower memory usage for deep learning models.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "training_acceleration",
        "mixed_precision"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NVIDIA/apex",
      "help_website": [
        "https://nvidia.github.io/apex/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "pytorch",
        "mixed-precision",
        "distributed-training"
      ],
      "id": 113
    },
    {
      "name": "nvvl",
      "one_line_profile": "Hardware-accelerated video loading library for ML training",
      "detailed_description": "A library that leverages hardware acceleration to load sequences of video frames, facilitating efficient machine learning training by offloading video decoding to the GPU.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "data_loading",
        "video_processing"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/NVIDIA/nvvl",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "video-loading",
        "gpu-acceleration",
        "machine-learning"
      ],
      "id": 114
    },
    {
      "name": "Fast-dLLM",
      "one_line_profile": "Training-free acceleration framework for Diffusion LLMs",
      "detailed_description": "An acceleration framework for Diffusion Language Models that enables KV Cache and Parallel Decoding without the need for retraining, improving inference speed.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "diffusion_models"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NVlabs/Fast-dLLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "diffusion-llm",
        "acceleration",
        "kv-cache"
      ],
      "id": 115
    },
    {
      "name": "he-transformer",
      "one_line_profile": "Homomorphic Encryption backend for Intel nGraph",
      "detailed_description": "A tool enabling deep learning with Homomorphic Encryption (HE) through the Intel nGraph compiler, allowing computation on encrypted data.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "privacy_preserving_computation",
        "compiler_backend"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/NervanaSystems/he-transformer",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "homomorphic-encryption",
        "ngraph",
        "privacy"
      ],
      "id": 116
    },
    {
      "name": "Tengine",
      "one_line_profile": "Lite, high-performance modular inference engine for embedded devices",
      "detailed_description": "A modular and high-performance inference engine designed for embedded devices, supporting various AI models and hardware backends.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_engine",
        "embedded_ai"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/OAID/Tengine",
      "help_website": [
        "http://tengine.openailab.com/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "inference-engine",
        "embedded",
        "edge-ai"
      ],
      "id": 117
    },
    {
      "name": "GRAT",
      "one_line_profile": "Training-free acceleration for Diffusion Transformers",
      "detailed_description": "A tool implementing 'Grouping First, Attending Smartly' to accelerate Diffusion Transformers without retraining, optimizing the attention mechanism.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "diffusion_transformers"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/OliverRensu/GRAT",
      "help_website": [],
      "license": null,
      "tags": [
        "acceleration",
        "diffusion-models",
        "transformer"
      ],
      "id": 118
    },
    {
      "name": "BMCook",
      "one_line_profile": "Model compression toolkit for large language models",
      "detailed_description": "A toolkit designed for compressing big models through techniques like quantization, pruning, and distillation to reduce resource consumption.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_compression",
        "quantization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenBMB/BMCook",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "model-compression",
        "llm",
        "quantization"
      ],
      "id": 119
    },
    {
      "name": "UltraRAG",
      "one_line_profile": "Low-code framework for building RAG pipelines",
      "detailed_description": "A framework for constructing complex Retrieval-Augmented Generation (RAG) pipelines, facilitating the integration of external knowledge into LLMs.",
      "domains": [
        "AI6"
      ],
      "subtask_category": [
        "rag_pipeline",
        "knowledge_retrieval"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenBMB/UltraRAG",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "llm",
        "pipeline"
      ],
      "id": 120
    },
    {
      "name": "llm-inference",
      "one_line_profile": "Platform for managing and deploying LLM inference",
      "detailed_description": "A platform providing out-of-the-box features for LLM model deployment, auto-scaling, and computing resource management.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_serving",
        "resource_management"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenCSGs/llm-inference",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-serving",
        "inference",
        "deployment"
      ],
      "id": 121
    },
    {
      "name": "EfficientQAT",
      "one_line_profile": "Efficient Quantization-Aware Training for LLMs",
      "detailed_description": "A tool for performing efficient Quantization-Aware Training (QAT) on Large Language Models, enabling high-performance low-bit inference.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "model_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenGVLab/EfficientQAT",
      "help_website": [],
      "license": null,
      "tags": [
        "qat",
        "quantization",
        "llm"
      ],
      "id": 122
    },
    {
      "name": "VideoChat-Flash",
      "one_line_profile": "Hierarchical compression tool for long-context video modeling",
      "detailed_description": "A tool implementing hierarchical compression techniques to enable efficient modeling of long-context videos.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "video_modeling",
        "data_compression"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenGVLab/VideoChat-Flash",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "video-understanding",
        "compression",
        "long-context"
      ],
      "id": 123
    },
    {
      "name": "CoLLiE",
      "one_line_profile": "Framework for collaborative training of Large Language Models",
      "detailed_description": "A library designed to facilitate efficient and collaborative training of Large Language Models, optimizing resource usage and training speed.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "distributed_training",
        "llm_training"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenMOSS/CoLLiE",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "distributed-training",
        "optimization"
      ],
      "id": 124
    },
    {
      "name": "CTranslate2",
      "one_line_profile": "Fast inference engine for Transformer models",
      "detailed_description": "A C++ and Python library for efficient inference with Transformer models, supporting weights quantization and hardware acceleration.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_engine",
        "transformer_acceleration"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/OpenNMT/CTranslate2",
      "help_website": [
        "https://opennmt.net/CTranslate2/"
      ],
      "license": "MIT",
      "tags": [
        "inference",
        "transformer",
        "quantization"
      ],
      "id": 125
    },
    {
      "name": "OpenRLHF",
      "one_line_profile": "High-performance RLHF framework based on Ray",
      "detailed_description": "A scalable and high-performance framework for Reinforcement Learning from Human Feedback (RLHF), supporting various algorithms like PPO and GRPO.",
      "domains": [
        "AI6"
      ],
      "subtask_category": [
        "rlhf",
        "model_alignment"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenRLHF/OpenRLHF",
      "help_website": [
        "https://openrlhf.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "rlhf",
        "distributed-training",
        "ray"
      ],
      "id": 126
    },
    {
      "name": "simple-onnx-processing-tools",
      "one_line_profile": "Tools for ONNX model manipulation and optimization",
      "detailed_description": "A collection of utilities for splitting, merging, compressing, and modifying ONNX models to facilitate deployment and inference optimization.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_optimization",
        "onnx_manipulation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/PINTO0309/simple-onnx-processing-tools",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "onnx",
        "model-optimization",
        "tools"
      ],
      "id": 127
    },
    {
      "name": "tflite2tensorflow",
      "one_line_profile": "Model converter for TFLite to various formats",
      "detailed_description": "A tool to generate saved_model, ONNX, OpenVINO, and other formats from .tflite files, supporting quantization and inverse quantization.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_conversion",
        "interoperability"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/PINTO0309/tflite2tensorflow",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "model-conversion",
        "tflite",
        "onnx"
      ],
      "id": 128
    },
    {
      "name": "safe-rlhf",
      "one_line_profile": "Framework for constrained value alignment via Safe RLHF",
      "detailed_description": "A framework implementing Safe Reinforcement Learning from Human Feedback to ensure AI model alignment with safety constraints.",
      "domains": [
        "AI6"
      ],
      "subtask_category": [
        "safety_alignment",
        "rlhf"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/PKU-Alignment/safe-rlhf",
      "help_website": [
        "https://safe-rlhf.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "safety",
        "alignment",
        "rlhf"
      ],
      "id": 129
    },
    {
      "name": "LLM-boost-recognition",
      "one_line_profile": "OCR and Voice Recognition module with LLM correction",
      "detailed_description": "A module for converting documents and audio into text using OCR and voice recognition, enhanced by LLM-based correction and GPU acceleration.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "data_ingestion",
        "ocr",
        "speech_recognition"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/PStarH/LLM-boost-recognition",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "ocr",
        "voice-recognition",
        "data-processing"
      ],
      "id": 130
    },
    {
      "name": "FastDeploy",
      "one_line_profile": "High-performance inference and deployment toolkit",
      "detailed_description": "A toolkit for the inference and deployment of Large Language Models (LLMs) and Vision-Language Models (VLMs), supporting multiple backends and hardware.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_deployment",
        "model_serving"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/PaddlePaddle/FastDeploy",
      "help_website": [
        "https://github.com/PaddlePaddle/FastDeploy"
      ],
      "license": "Apache-2.0",
      "tags": [
        "deployment",
        "inference",
        "paddlepaddle"
      ],
      "id": 131
    },
    {
      "name": "PARL",
      "one_line_profile": "High-performance distributed reinforcement learning framework",
      "detailed_description": "A flexible and high-performance framework for distributed Reinforcement Learning, built on PaddlePaddle.",
      "domains": [
        "AI6"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "distributed_training"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/PaddlePaddle/PARL",
      "help_website": [
        "https://parl.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "reinforcement-learning",
        "distributed-system",
        "paddlepaddle"
      ],
      "id": 132
    },
    {
      "name": "PaddleNLP",
      "one_line_profile": "NLP library for LLMs and SLMs",
      "detailed_description": "A comprehensive Natural Language Processing library providing easy-to-use interfaces for Large Language Models and Small Language Models.",
      "domains": [
        "AI6"
      ],
      "subtask_category": [
        "nlp_modeling",
        "text_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/PaddlePaddle/PaddleNLP",
      "help_website": [
        "https://paddlenlp.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "nlp",
        "llm",
        "paddlepaddle"
      ],
      "id": 133
    },
    {
      "name": "PaddleSlim",
      "one_line_profile": "Deep model compression and architecture search library",
      "detailed_description": "A library for deep learning model compression, including quantization, pruning, distillation, and neural architecture search.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_compression",
        "nas"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/PaddlePaddle/PaddleSlim",
      "help_website": [
        "https://paddleslim.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "compression",
        "quantization",
        "pruning"
      ],
      "id": 134
    },
    {
      "name": "PERSIA",
      "one_line_profile": "Distributed framework for training recommendation models",
      "detailed_description": "A high-performance distributed training framework specifically optimized for deep learning recommendation models, based on PyTorch.",
      "domains": [
        "AI6"
      ],
      "subtask_category": [
        "recommendation_systems",
        "distributed_training"
      ],
      "application_level": "framework",
      "primary_language": "Rust",
      "repo_url": "https://github.com/PersiaML/PERSIA",
      "help_website": [
        "https://persiaml-tutorials.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "recommendation",
        "distributed-training",
        "pytorch"
      ],
      "id": 135
    },
    {
      "name": "NeuralSolvers",
      "one_line_profile": "Neural network based solvers for PDEs",
      "detailed_description": "A library implementing physics-informed neural networks (PINNs) and other neural solvers for partial differential equations and inverse problems.",
      "domains": [
        "AI6"
      ],
      "subtask_category": [
        "pde_solver",
        "scientific_modeling"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Photon-AI-Research/NeuralSolvers",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pinn",
        "pde",
        "scientific-computing"
      ],
      "id": 136
    },
    {
      "name": "OpenDiloco",
      "one_line_profile": "Framework for globally distributed low-communication training",
      "detailed_description": "An open-source framework enabling efficient, globally distributed training of AI models with low communication overhead.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "distributed_training",
        "communication_optimization"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/PrimeIntellect-ai/OpenDiloco",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "distributed-training",
        "decentralized-ai",
        "optimization"
      ],
      "id": 137
    },
    {
      "name": "prime-diloco",
      "one_line_profile": "Framework for globally distributed AI model training",
      "detailed_description": "A framework designed for efficient training of AI models across globally distributed compute resources over the internet.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "distributed_training",
        "decentralized_computing"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/PrimeIntellect-ai/prime-diloco",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "distributed-training",
        "infrastructure",
        "ai"
      ],
      "id": 138
    },
    {
      "name": "tensornet",
      "one_line_profile": "Distributed training framework for sparse data",
      "detailed_description": "A TensorFlow-based distributed training framework optimized for large-scale sparse data, often used in recommendation and search ranking.",
      "domains": [
        "AI6"
      ],
      "subtask_category": [
        "distributed_training",
        "sparse_data_processing"
      ],
      "application_level": "framework",
      "primary_language": "C++",
      "repo_url": "https://github.com/Qihoo360/tensornet",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "distributed-training",
        "tensorflow",
        "sparse-data"
      ],
      "id": 139
    },
    {
      "name": "QizNLP",
      "one_line_profile": "Tensorflow-based NLP task framework",
      "detailed_description": "A framework for quickly running various NLP tasks such as classification, sequence labeling, and matching, with support for distributed training.",
      "domains": [
        "AI6"
      ],
      "subtask_category": [
        "nlp_workflow",
        "model_training"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/Qznan/QizNLP",
      "help_website": [],
      "license": "MPL-2.0",
      "tags": [
        "nlp",
        "tensorflow",
        "distributed-training"
      ],
      "id": 140
    },
    {
      "name": "TritonForge",
      "one_line_profile": "LLM-powered GPU kernel synthesis tool",
      "detailed_description": "A tool that uses LLMs to synthesize optimized Triton kernels from PyTorch operations, facilitating custom operator acceleration.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "kernel_synthesis",
        "code_generation"
      ],
      "application_level": "tool",
      "primary_language": "Python",
      "repo_url": "https://github.com/RLsys-Foundation/TritonForge",
      "help_website": [],
      "license": null,
      "tags": [
        "triton",
        "gpu-kernel",
        "compiler"
      ],
      "id": 141
    },
    {
      "name": "rwkv.cpp",
      "one_line_profile": "CPU inference engine for RWKV models",
      "detailed_description": "A high-performance CPU inference implementation for RWKV language models, supporting various quantization formats (INT4/INT5/INT8).",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_engine",
        "cpu_acceleration"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/RWKV/rwkv.cpp",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "inference",
        "rwkv",
        "quantization"
      ],
      "id": 142
    },
    {
      "name": "Model Compression Toolkit (MCT)",
      "one_line_profile": "Advanced quantization and compression tools for neural network optimization on constrained hardware",
      "detailed_description": "An open-source project by Sony Semiconductor Solutions providing researchers and developers with tools for neural network model optimization, specifically focusing on quantization and compression to enable efficient deployment on hardware with constraints.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "model_compression"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/SonySemiconductorSolutions/mct-model-optimization",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "quantization",
        "model-compression",
        "neural-network",
        "optimization"
      ],
      "id": 143
    },
    {
      "name": "YOLO-ModelCompression",
      "one_line_profile": "Framework for YOLO model compression, multi-dataset training, and multi-backbone support",
      "detailed_description": "A comprehensive framework designed for compressing YOLO series models (v3/v4), supporting advanced features like multi-dataset training, pruning, and quantization to optimize models for deployment.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_compression",
        "training_optimization"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/SpursLipu/YOLOv3v4-ModelCompression-MultidatasetTraining-Multibackbone",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "yolo",
        "model-compression",
        "pruning",
        "quantization"
      ],
      "id": 144
    },
    {
      "name": "Mobile-YOLOv5-Pruning-Distillation",
      "one_line_profile": "Toolkit for pruning and distilling YOLOv5 models for efficient mobile deployment",
      "detailed_description": "A specialized toolkit for optimizing YOLOv5 models through pruning and knowledge distillation, specifically targeting mobile deployment with support for NCNN and TensorRT export.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_compression",
        "pruning",
        "distillation"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/Syencil/mobile-yolov5-pruning-distillation",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "yolov5",
        "pruning",
        "knowledge-distillation",
        "ncnn",
        "tensorrt"
      ],
      "id": 145
    },
    {
      "name": "Oobleck",
      "one_line_profile": "Resilient distributed training framework for large models",
      "detailed_description": "A distributed training framework designed to be resilient to faults, enabling robust and efficient training of large language models by handling node failures and topology changes dynamically.",
      "domains": [
        "AI6"
      ],
      "subtask_category": [
        "distributed_training",
        "fault_tolerance"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/SymbioticLab/Oobleck",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "distributed-training",
        "fault-tolerance",
        "llm",
        "hpc"
      ],
      "id": 146
    },
    {
      "name": "Torch-Model-Compression",
      "one_line_profile": "Automated toolset for analyzing and compressing PyTorch models",
      "detailed_description": "A toolset developed by THU-MIG for automated model structure analysis and modification, providing a library of compression algorithms to optimize PyTorch models for efficiency.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_compression",
        "model_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/THU-MIG/torch-model-compression",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pytorch",
        "model-compression",
        "automation",
        "optimization"
      ],
      "id": 147
    },
    {
      "name": "lyraDiff",
      "one_line_profile": "Inference acceleration engine for Diffusion and DiT models",
      "detailed_description": "An out-of-the-box inference acceleration engine specifically optimized for Diffusion and Diffusion Transformer (DiT) models, aiming to speed up generative AI tasks.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "generative_ai"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/TMElyralab/lyraDiff",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "diffusion-models",
        "inference-acceleration",
        "dit",
        "generative-ai"
      ],
      "id": 148
    },
    {
      "name": "AngelSlim",
      "one_line_profile": "Comprehensive model compression toolkit for enhanced usability and efficiency",
      "detailed_description": "A model compression toolkit from Tencent designed to improve the usability and efficiency of deploying AI models, offering a range of compression techniques.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_compression",
        "deployment"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Tencent/AngelSlim",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "model-compression",
        "tencent",
        "optimization",
        "deployment"
      ],
      "id": 149
    },
    {
      "name": "PocketFlow",
      "one_line_profile": "Automatic Model Compression (AutoMC) framework",
      "detailed_description": "An open-source framework for Automatic Model Compression (AutoMC) developed by Tencent, enabling developers to create smaller and faster AI applications with minimal human effort.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_compression",
        "automl"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/Tencent/PocketFlow",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "automc",
        "model-compression",
        "automl",
        "optimization"
      ],
      "id": 150
    },
    {
      "name": "TNN",
      "one_line_profile": "High-performance deep learning inference framework for mobile and edge",
      "detailed_description": "A uniform deep learning inference framework developed by Tencent Youtu Lab, optimized for mobile, desktop, and server platforms, featuring high performance, cross-platform support, and model compression capabilities.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_engine",
        "edge_computing"
      ],
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/Tencent/TNN",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "inference-framework",
        "mobile-ai",
        "edge-computing",
        "ncnn"
      ],
      "id": 151
    },
    {
      "name": "OnnxStack",
      "one_line_profile": ".NET library for running Stable Diffusion and AI models via ONNX Runtime",
      "detailed_description": "A library that enables the execution of Stable Diffusion and other deep learning models within the .NET ecosystem using ONNX Runtime, facilitating inference integration in Windows applications.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_runtime",
        "generative_ai"
      ],
      "application_level": "library",
      "primary_language": "C#",
      "repo_url": "https://github.com/TensorStack-AI/OnnxStack",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "onnx",
        "stable-diffusion",
        "dotnet",
        "inference"
      ],
      "id": 152
    },
    {
      "name": "PyTorch-YOLOv4",
      "one_line_profile": "PyTorch implementation of YOLOv4 with ONNX and TensorRT support",
      "detailed_description": "A widely used PyTorch implementation of the YOLOv4 object detection model, including tools for training, inference, and conversion to ONNX and TensorRT for accelerated deployment.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "object_detection",
        "model_implementation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Tianxiaomo/pytorch-YOLOv4",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "yolov4",
        "pytorch",
        "tensorrt",
        "onnx",
        "object-detection"
      ],
      "id": 153
    },
    {
      "name": "Frame_Extractor",
      "one_line_profile": "Automated video scene detection and frame extraction tool for datasets",
      "detailed_description": "A utility tool that automatically detects scenes in videos and extracts the sharpest frames, optimized for preparing high-quality image datasets for training AI models (e.g., LoRA fine-tuning).",
      "domains": [
        "AI6"
      ],
      "subtask_category": [
        "data_preparation",
        "dataset_creation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Tranchillo/Frame_Extractor",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "dataset-preparation",
        "video-processing",
        "frame-extraction",
        "lora"
      ],
      "id": 154
    },
    {
      "name": "AQLM",
      "one_line_profile": "Extreme compression of Large Language Models via Additive Quantization",
      "detailed_description": "AQLM (Additive Quantization for Language Models) is a library and method for extreme compression of LLMs, enabling efficient inference on consumer hardware while maintaining high accuracy.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "model_compression"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Vahe1994/AQLM",
      "help_website": [
        "https://arxiv.org/pdf/2401.06118.pdf"
      ],
      "license": "Apache-2.0",
      "tags": [
        "quantization",
        "llm",
        "compression",
        "inference-acceleration"
      ],
      "id": 155
    },
    {
      "name": "FasterCache",
      "one_line_profile": "Training-free acceleration for video diffusion models",
      "detailed_description": "FasterCache is a training-free acceleration tool for video diffusion models that utilizes caching mechanisms to speed up inference while maintaining high generation quality.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "generative_model_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Vchitect/FasterCache",
      "help_website": [],
      "license": null,
      "tags": [
        "diffusion-models",
        "video-generation",
        "acceleration",
        "caching"
      ],
      "id": 156
    },
    {
      "name": "voltaML",
      "one_line_profile": "Lightweight library for accelerating ML/DL models on high-performance runtimes",
      "detailed_description": "VoltaML is a library designed to convert and run machine learning and deep learning models on high-performance inference runtimes such as TensorRT, TorchScript, ONNX, and TVM, optimizing them for speed.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "model_conversion"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/VoltaML/voltaML",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "tensorrt",
        "onnx",
        "inference",
        "acceleration"
      ],
      "id": 157
    },
    {
      "name": "Fast3Dcache",
      "one_line_profile": "Training-free acceleration for 3D geometry synthesis",
      "detailed_description": "Fast3Dcache provides a training-free method to accelerate 3D geometry synthesis, optimizing the inference process for 3D generative models.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "3d_synthesis"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/Westlake-AGI-Lab/Fast3Dcache",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "3d-generation",
        "acceleration",
        "geometry-synthesis"
      ],
      "id": 158
    },
    {
      "name": "Q-Diffusion",
      "one_line_profile": "Quantization framework for diffusion models",
      "detailed_description": "Q-Diffusion is a tool for quantizing diffusion models to reduce their memory footprint and accelerate inference without significant loss in generation quality.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "model_compression"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Xiuyu-Li/q-diffusion",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "diffusion-models",
        "quantization",
        "compression"
      ],
      "id": 159
    },
    {
      "name": "native-sparse-attention-triton",
      "one_line_profile": "Efficient Triton implementation of Native Sparse Attention",
      "detailed_description": "This repository provides an optimized implementation of Native Sparse Attention using OpenAI Triton, serving as a kernel-level acceleration tool for transformer models.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "kernel_optimization",
        "inference_acceleration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/XunhaoLai/native-sparse-attention-triton",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "triton",
        "sparse-attention",
        "cuda",
        "optimization"
      ],
      "id": 160
    },
    {
      "name": "DashGaussian",
      "one_line_profile": "Acceleration method for 3D Gaussian Splatting training",
      "detailed_description": "DashGaussian implements a powerful acceleration method for training 3D Gaussian Splatting (3DGS) models, significantly reducing training time for 3D scene reconstruction.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "training_acceleration",
        "3d_reconstruction"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/YouyuChen0207/DashGaussian",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "3d-gaussian-splatting",
        "acceleration",
        "rendering"
      ],
      "id": 161
    },
    {
      "name": "KVCache-Factory",
      "one_line_profile": "Unified KV Cache compression methods for auto-regressive models",
      "detailed_description": "KVCache-Factory is a unified framework providing various KV cache compression methods to accelerate inference and reduce memory usage for large language models.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "memory_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Zefan-Cai/KVCache-Factory",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "kv-cache",
        "llm",
        "compression",
        "inference"
      ],
      "id": 162
    },
    {
      "name": "R-KV",
      "one_line_profile": "Redundancy-aware KV Cache compression for reasoning models",
      "detailed_description": "R-KV implements a redundancy-aware KV cache compression technique specifically designed for reasoning models, optimizing memory efficiency during long-context inference.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "memory_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Zefan-Cai/R-KV",
      "help_website": [],
      "license": null,
      "tags": [
        "kv-cache",
        "llm",
        "reasoning",
        "compression"
      ],
      "id": 163
    },
    {
      "name": "deepC",
      "one_line_profile": "Vendor-independent TinyML deep learning compiler and inference framework",
      "detailed_description": "deepC is a deep learning compiler and inference framework designed for microcomputers and microcontrollers, enabling vendor-independent TinyML model deployment.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "compiler",
        "inference_deployment"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/ai-techsystems/deepC",
      "help_website": [
        "https://deepc.ai"
      ],
      "license": "Apache-2.0",
      "tags": [
        "tinyml",
        "compiler",
        "embedded-ai",
        "inference"
      ],
      "id": 164
    },
    {
      "name": "AidLearning-FrameWork",
      "one_line_profile": "AIoT development platform for Linux on Android",
      "detailed_description": "AidLearning is a mobile AI development platform that provides a Linux environment on Android with support for GUI, deep learning inference acceleration (CPU/GPU/NPU), and visual IDEs.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_platform",
        "edge_computing"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/aidlearning/AidLearning-FrameWork",
      "help_website": [
        "http://www.aidlearning.net"
      ],
      "license": "NOASSERTION",
      "tags": [
        "android",
        "linux",
        "aiot",
        "inference"
      ],
      "id": 165
    },
    {
      "name": "EasyParallelLibrary",
      "one_line_profile": "General and efficient deep learning framework for distributed model training",
      "detailed_description": "Easy Parallel Library (EPL) is a framework designed to simplify and accelerate distributed model training for deep learning, providing efficient parallelization strategies.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "distributed_training",
        "parallel_computing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/alibaba/EasyParallelLibrary",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "distributed-training",
        "parallelism",
        "deep-learning"
      ],
      "id": 166
    },
    {
      "name": "TePDist",
      "one_line_profile": "HLO-level automatic distributed system for DL models",
      "detailed_description": "TePDist (TEnsor Program DISTributed) is an automatic distributed system that operates at the HLO (High Level Optimizer) level to optimize deep learning models for distributed execution.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "distributed_training",
        "compiler_optimization"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/alibaba/TePDist",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "distributed-systems",
        "compiler",
        "hlo",
        "deep-learning"
      ],
      "id": 167
    },
    {
      "name": "TinyNeuralNetwork",
      "one_line_profile": "Efficient deep learning model compression framework",
      "detailed_description": "TinyNeuralNetwork is a framework focused on deep learning model compression, offering tools for quantization, pruning, and optimization to enable efficient deployment on resource-constrained devices.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_compression",
        "quantization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/alibaba/TinyNeuralNetwork",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "model-compression",
        "quantization",
        "tinyml",
        "optimization"
      ],
      "id": 168
    },
    {
      "name": "PainlessInferenceAcceleration",
      "one_line_profile": "Tool for simplified inference acceleration",
      "detailed_description": "PainlessInferenceAcceleration provides a streamlined workflow and tools to accelerate deep learning model inference with minimal user effort, supporting various backends.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "model_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/alipay/PainlessInferenceAcceleration",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "inference",
        "acceleration",
        "optimization"
      ],
      "id": 169
    },
    {
      "name": "cONNXr",
      "one_line_profile": "Pure C ONNX runtime for embedded devices",
      "detailed_description": "cONNXr is a lightweight, zero-dependency ONNX runtime written in pure C, specifically designed for running inference on embedded devices.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_runtime",
        "embedded_ai"
      ],
      "application_level": "library",
      "primary_language": "C",
      "repo_url": "https://github.com/alrevuelta/cONNXr",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "onnx",
        "embedded",
        "inference",
        "c"
      ],
      "id": 170
    },
    {
      "name": "GLake",
      "one_line_profile": "GPU memory management and IO transmission optimization",
      "detailed_description": "GLake is a system tool for optimizing GPU memory management and I/O transmission efficiency, aiming to improve the performance of large-scale model training and inference.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "memory_optimization",
        "io_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/antgroup/glake",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "gpu-memory",
        "optimization",
        "io-acceleration"
      ],
      "id": 171
    },
    {
      "name": "nnieqat-pytorch",
      "one_line_profile": "Quantization aware training tool for NNIE on PyTorch",
      "detailed_description": "nnieqat-pytorch is a quantization-aware training (QAT) tool designed for the HiSilicon NNIE (Neural Network Inference Engine), enabling users to train models compatible with NNIE quantization requirements.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "model_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aovoc/nnieqat-pytorch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "quantization",
        "nnie",
        "pytorch",
        "qat"
      ],
      "id": 172
    },
    {
      "name": "TVM",
      "one_line_profile": "Open deep learning compiler stack for cpu, gpu and specialized accelerators",
      "detailed_description": "Apache TVM is an open source machine learning compiler framework for CPUs, GPUs, and machine learning accelerators. It aims to enable machine learning engineers to optimize and run computations efficiently on any hardware backend.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "compiler",
        "optimization",
        "inference_acceleration"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/apache/tvm",
      "help_website": [
        "https://tvm.apache.org/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "compiler",
        "deep-learning",
        "optimization",
        "inference"
      ],
      "id": 173
    },
    {
      "name": "flux-fp8-api",
      "one_line_profile": "Flux diffusion model implementation with quantized FP8 acceleration",
      "detailed_description": "A specialized implementation of the Flux diffusion model utilizing quantized FP8 matrix multiplication and half-precision accumulation to achieve acceleration on consumer devices.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "inference_acceleration",
        "image_generation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aredden/flux-fp8-api",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "quantization",
        "fp8",
        "diffusion-model",
        "acceleration"
      ],
      "id": 174
    },
    {
      "name": "torch-bnb-fp4",
      "one_line_profile": "Accelerated 4-bit FP4 linear operations for PyTorch",
      "detailed_description": "A library providing faster PyTorch bitsandbytes 4-bit FP4 nn.Linear operations, enabling efficient low-bit quantization for neural networks.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "inference_acceleration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aredden/torch-bnb-fp4",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "quantization",
        "fp4",
        "pytorch",
        "acceleration"
      ],
      "id": 175
    },
    {
      "name": "LLM-Inference-Bench",
      "one_line_profile": "Benchmark suite for Large Language Model inference performance",
      "detailed_description": "A benchmarking tool developed by Argonne National Laboratory to evaluate and analyze the inference performance of Large Language Models on various hardware configurations.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/argonne-lcf/LLM-Inference-Bench",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "benchmarking",
        "llm",
        "inference",
        "hpc"
      ],
      "id": 176
    },
    {
      "name": "DeepX",
      "one_line_profile": "Unified framework for large-scale auto-distributed training and inference",
      "detailed_description": "A large-scale auto-distributed training and inference unified framework featuring a memory-compute-control decoupled architecture and support for heterogeneous hardware.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "distributed_training",
        "inference",
        "hpc"
      ],
      "application_level": "framework",
      "primary_language": "C++",
      "repo_url": "https://github.com/array2d/deepx",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "distributed-training",
        "inference",
        "framework",
        "hpc"
      ],
      "id": 177
    },
    {
      "name": "Fine-Tune Codebase",
      "one_line_profile": "Tool for fine-tuning LLMs on codebases with LoRA and quantization support",
      "detailed_description": "A scalable and efficient tool designed for fine-tuning large language models on codebases, supporting LoRA, mixed precision training, and quantization.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_finetuning",
        "training"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/ayminovitch/fine-tune-codebase",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "finetuning",
        "llm",
        "lora",
        "quantization"
      ],
      "id": 178
    },
    {
      "name": "TokenSwift",
      "one_line_profile": "Lossless acceleration method for ultra-long sequence generation",
      "detailed_description": "Implementation of TokenSwift, a method for accelerating ultra-long sequence generation in large language models without loss of quality.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "sequence_generation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/bigai-nlco/TokenSwift",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "acceleration",
        "llm",
        "inference",
        "long-sequence"
      ],
      "id": 179
    },
    {
      "name": "Megatron-DeepSpeed",
      "one_line_profile": "Large-scale distributed training framework for transformer models",
      "detailed_description": "A deep learning library that combines Megatron-LM and DeepSpeed to enable efficient large-scale distributed training of transformer language models.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "distributed_training",
        "hpc",
        "model_training"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/bigscience-workshop/Megatron-DeepSpeed",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "distributed-training",
        "transformer",
        "hpc",
        "deepspeed"
      ],
      "id": 180
    },
    {
      "name": "bitsandbytes",
      "one_line_profile": "Lightweight wrapper around CUDA custom functions, in particular 8-bit optimizers and quantization",
      "detailed_description": "A library that provides accessible large language models via k-bit quantization for PyTorch, including 8-bit optimizers and matrix multiplication routines.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "optimization",
        "inference_acceleration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/bitsandbytes-foundation/bitsandbytes",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "quantization",
        "cuda",
        "optimization",
        "pytorch"
      ],
      "id": 181
    },
    {
      "name": "X-LLM",
      "one_line_profile": "Library for cutting-edge and easy LLM fine-tuning",
      "detailed_description": "A library designed to simplify and accelerate the fine-tuning process of Large Language Models (LLMs), providing cutting-edge techniques.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_finetuning",
        "training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/bobazooba/xllm",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "finetuning",
        "llm",
        "training"
      ],
      "id": 182
    },
    {
      "name": "QuantVSR",
      "one_line_profile": "Low-bit post-training quantization for video super-resolution",
      "detailed_description": "PyTorch implementation of QuantVSR, a method for low-bit post-training quantization specifically designed for real-world video super-resolution tasks.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "super_resolution",
        "image_processing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/bowenchai/QuantVSR",
      "help_website": [],
      "license": null,
      "tags": [
        "quantization",
        "super-resolution",
        "video-processing"
      ],
      "id": 183
    },
    {
      "name": "fastai_xla_extensions",
      "one_line_profile": "Extensions to run fastai on TPUs using PyTorch-XLA",
      "detailed_description": "A Python package that enables the fastai library to utilize TPUs for accelerated training and inference via PyTorch-XLA.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "hardware_acceleration",
        "training",
        "tpu_support"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/butchland/fastai_xla_extensions",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "fastai",
        "tpu",
        "xla",
        "acceleration"
      ],
      "id": 184
    },
    {
      "name": "ABQ-LLM",
      "one_line_profile": "Acceleration library for arbitrary bit-width combinatorial quantization",
      "detailed_description": "An acceleration library that supports arbitrary bit-width combinatorial quantization operations for Large Language Models.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "inference_acceleration"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/bytedance/ABQ-LLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "quantization",
        "acceleration",
        "llm"
      ],
      "id": 185
    },
    {
      "name": "BytePS",
      "one_line_profile": "High performance and generic framework for distributed DNN training",
      "detailed_description": "A high-performance, generic framework for distributed Deep Neural Network (DNN) training, designed to run on heterogeneous hardware clusters.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "distributed_training",
        "hpc"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/bytedance/byteps",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "distributed-training",
        "hpc",
        "deep-learning"
      ],
      "id": 186
    },
    {
      "name": "exprgrad",
      "one_line_profile": "Differentiable array programming language and DL framework for Nim",
      "detailed_description": "An experimental deep learning framework for the Nim programming language, based on a differentiable array programming approach.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "modeling",
        "training"
      ],
      "application_level": "framework",
      "primary_language": "Nim",
      "repo_url": "https://github.com/can-lehmann/exprgrad",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "deep-learning",
        "nim",
        "differentiable-programming"
      ],
      "id": 187
    },
    {
      "name": "JEDI",
      "one_line_profile": "Jetson embedded platform deep learning inference acceleration framework",
      "detailed_description": "A deep learning inference acceleration framework specifically targeted for Jetson embedded platforms using TensorRT.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "embedded_ai"
      ],
      "application_level": "framework",
      "primary_language": "C++",
      "repo_url": "https://github.com/cap-lab/jedi",
      "help_website": [],
      "license": "GPL-2.0",
      "tags": [
        "jetson",
        "tensorrt",
        "inference",
        "embedded"
      ],
      "id": 188
    },
    {
      "name": "NeuPIMs",
      "one_line_profile": "NPU-PIM heterogeneous acceleration simulator for batched LLM inferencing",
      "detailed_description": "A framework/simulator for NPU-PIM (Processing-in-Memory) heterogeneous acceleration, specifically optimized for batched Large Language Model inferencing.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "hardware_simulation"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/casys-kaist/NeuPIMs",
      "help_website": [],
      "license": null,
      "tags": [
        "pim",
        "acceleration",
        "llm",
        "simulation"
      ],
      "id": 189
    },
    {
      "name": "petit-kernel",
      "one_line_profile": "Optimized FP16/BF16 x FP4 GPU kernels for AMD GPUs",
      "detailed_description": "A library of optimized GPU kernels for AMD GPUs, supporting FP16/BF16 and FP4 precisions, useful for accelerating low-precision inference.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "kernel_optimization",
        "inference_acceleration"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/causalflow-ai/petit-kernel",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "amd-gpu",
        "kernels",
        "quantization",
        "acceleration"
      ],
      "id": 190
    },
    {
      "name": "flex-nano-vllm",
      "one_line_profile": "Minimal FlexAttention-based inference engine for Gemma 2",
      "detailed_description": "A minimal, vLLM-style inference engine optimized for fast Gemma 2 inference using FlexAttention.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_engine",
        "acceleration"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/changjonathanc/flex-nano-vllm",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "inference",
        "vllm",
        "gemma",
        "acceleration"
      ],
      "id": 191
    },
    {
      "name": "llama-dfdx",
      "one_line_profile": "Rust implementation of LLaMa 7b with CUDA acceleration",
      "detailed_description": "A Rust implementation of the LLaMa 7b model featuring CUDA acceleration and optimized for minimal GPU memory usage.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference",
        "model_implementation"
      ],
      "application_level": "solver",
      "primary_language": "Rust",
      "repo_url": "https://github.com/chelsea0x3b/llama-dfdx",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rust",
        "llama",
        "cuda",
        "inference"
      ],
      "id": 192
    },
    {
      "name": "model_compression",
      "one_line_profile": "Model compression implementation using knowledge distillation",
      "detailed_description": "A repository implementing model compression techniques, specifically focusing on knowledge distillation methods.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_compression",
        "knowledge_distillation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/chengshengchan/model_compression",
      "help_website": [],
      "license": null,
      "tags": [
        "model-compression",
        "knowledge-distillation"
      ],
      "id": 193
    },
    {
      "name": "PTQ4SAM",
      "one_line_profile": "Post-Training Quantization for Segment Anything Model",
      "detailed_description": "Implementation of Post-Training Quantization (PTQ) specifically tailored for the Segment Anything Model (SAM), enabling efficient inference.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "inference_acceleration",
        "computer_vision"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/chengtao-lv/PTQ4SAM",
      "help_website": [],
      "license": null,
      "tags": [
        "quantization",
        "sam",
        "segmentation"
      ],
      "id": 194
    },
    {
      "name": "DDRNet.pytorch",
      "one_line_profile": "Deep Dual-resolution Networks for real-time semantic segmentation",
      "detailed_description": "Implementation of Deep Dual-resolution Networks (DDRNet) for real-time and accurate semantic segmentation of road scenes.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "image_segmentation",
        "model_implementation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/chenjun2hao/DDRNet.pytorch",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "semantic-segmentation",
        "real-time",
        "computer-vision"
      ],
      "id": 195
    },
    {
      "name": "topicGPT",
      "one_line_profile": "Prompt-based framework for topic modeling",
      "detailed_description": "A framework utilizing Large Language Models (LLMs) and prompting techniques to perform topic modeling on text data.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "topic_modeling",
        "text_analysis"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/chtmp223/topicGPT",
      "help_website": [],
      "license": null,
      "tags": [
        "topic-modeling",
        "llm",
        "nlp"
      ],
      "id": 196
    },
    {
      "name": "Cube Studio",
      "one_line_profile": "Cloud-native one-stop machine learning and deep learning platform",
      "detailed_description": "An open-source, cloud-native AI platform that provides a full-stack solution for machine learning and deep learning workflows. It includes features for notebook development, pipeline orchestration, distributed training, hyperparameter search, and inference serving, supporting various frameworks and hardware accelerators.",
      "domains": [
        "AI6",
        "MLOps"
      ],
      "subtask_category": [
        "mlops_platform",
        "distributed_training",
        "inference_serving"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/data-infra/cube-studio",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "mlops",
        "distributed-training",
        "kubernetes",
        "ai-platform"
      ],
      "id": 197
    },
    {
      "name": "EasyQuant",
      "one_line_profile": "Efficient post-training quantization method optimizing weights and activations",
      "detailed_description": "An efficient and simple post-training quantization (PTQ) tool that optimizes the scales of weights and activations to reduce quantization error. It is designed to facilitate the deployment of deep learning models on low-precision hardware.",
      "domains": [
        "AI6-04",
        "Computer Vision"
      ],
      "subtask_category": [
        "quantization",
        "model_compression"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/deepglint/EasyQuant",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "quantization",
        "post-training-quantization",
        "cnn",
        "optimization"
      ],
      "id": 198
    },
    {
      "name": "DeepSpeed",
      "one_line_profile": "Deep learning optimization library for distributed training and inference",
      "detailed_description": "A deep learning optimization library that enables massive-scale distributed training and inference. It provides system innovations like ZeRO (Zero Redundancy Optimizer), 3D parallelism, and sparse attention to improve speed, memory efficiency, and scalability of large models.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "distributed_training",
        "inference_acceleration",
        "memory_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/deepspeedai/DeepSpeed",
      "help_website": [
        "https://www.deepspeed.ai"
      ],
      "license": "Apache-2.0",
      "tags": [
        "distributed-training",
        "optimization",
        "zero-redundancy-optimizer",
        "hpc"
      ],
      "id": 199
    },
    {
      "name": "DeepSpeed-MII",
      "one_line_profile": "High-throughput and low-latency inference library powered by DeepSpeed",
      "detailed_description": "DeepSpeed Model Implementations for Inference (MII) is a library designed to accelerate low-latency and high-throughput inference of large models. It leverages DeepSpeed's inference engine to provide optimized serving solutions.",
      "domains": [
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "model_serving"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/deepspeedai/DeepSpeed-MII",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "inference",
        "latency-optimization",
        "throughput",
        "serving"
      ],
      "id": 200
    },
    {
      "name": "tensorRTIntegrate",
      "one_line_profile": "C++ integration tool for TensorRT inference and ONNX plugin management",
      "detailed_description": "A C++ repository providing tools and examples for integrating TensorRT inference, managing ONNX plugins, and compiling models. It facilitates the deployment of deep learning models using NVIDIA's TensorRT engine.",
      "domains": [
        "AI6-04"
      ],
      "subtask_category": [
        "inference_deployment",
        "model_conversion"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/dlunion/tensorRTIntegrate",
      "help_website": [],
      "license": null,
      "tags": [
        "tensorrt",
        "onnx",
        "inference",
        "cpp"
      ],
      "id": 201
    },
    {
      "name": "nnvm-fusion",
      "one_line_profile": "Kernel fusion and runtime compilation module based on NNVM",
      "detailed_description": "A library implementing kernel fusion and runtime compilation techniques based on the NNVM (Neural Network Virtual Machine) intermediate representation. It aims to optimize deep learning graph execution.",
      "domains": [
        "AI6-04"
      ],
      "subtask_category": [
        "compiler_optimization",
        "kernel_fusion"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/dmlc/nnvm-fusion",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "nnvm",
        "compiler",
        "optimization",
        "kernel-fusion"
      ],
      "id": 202
    },
    {
      "name": "Paracel",
      "one_line_profile": "Distributed training framework implementing the parameter server architecture",
      "detailed_description": "A distributed training framework designed to solve large-scale machine learning problems using the parameter server architecture. It supports various ML algorithms and provides a mechanism for efficient data and parameter synchronization.",
      "domains": [
        "AI6",
        "HPC"
      ],
      "subtask_category": [
        "distributed_training"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/douban/paracel",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "parameter-server",
        "distributed-systems",
        "machine-learning"
      ],
      "id": 203
    },
    {
      "name": "VisDrone-dataset-python-toolkit",
      "one_line_profile": "Toolkit for processing and using the VisDrone aerial object detection dataset",
      "detailed_description": "A Python toolkit designed to facilitate the use of the VisDrone dataset for aerial object detection. It includes pipelines for training, inference, and format conversion, supporting models like Faster R-CNN and RetinaNet.",
      "domains": [
        "Computer Vision",
        "Data Processing"
      ],
      "subtask_category": [
        "data_processing",
        "object_detection"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/dronefreak/VisDrone-dataset-python-toolkit",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "visdrone",
        "aerial-imagery",
        "dataset-tools",
        "pytorch"
      ],
      "id": 204
    },
    {
      "name": "gemlite",
      "one_line_profile": "Library for fast low-bit matrix multiplication kernels using Triton",
      "detailed_description": "A library providing optimized low-bit matrix multiplication (matmul) kernels implemented in Triton. It focuses on accelerating quantized neural network operations on GPUs.",
      "domains": [
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "kernel_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/dropbox/gemlite",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "triton",
        "matmul",
        "quantization",
        "gpu-acceleration"
      ],
      "id": 205
    },
    {
      "name": "XLand-MiniGrid",
      "one_line_profile": "JAX-accelerated meta-reinforcement learning environments",
      "detailed_description": "A library of JAX-accelerated reinforcement learning environments inspired by XLand and MiniGrid. It is designed for meta-reinforcement learning research, enabling fast and scalable simulation on hardware accelerators.",
      "domains": [
        "Reinforcement Learning",
        "AI6"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "simulation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/dunnolab/xland-minigrid",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "jax",
        "reinforcement-learning",
        "environment",
        "simulation"
      ],
      "id": 206
    },
    {
      "name": "XLand-MiniGrid Datasets",
      "one_line_profile": "Large-scale multi-task dataset for in-context reinforcement learning",
      "detailed_description": "A large-scale dataset (XLand-100B) designed for multi-task and in-context reinforcement learning research, generated using the XLand-MiniGrid environments.",
      "domains": [
        "Reinforcement Learning"
      ],
      "subtask_category": [
        "reinforcement_learning"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/dunnolab/xland-minigrid-datasets",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "dataset",
        "reinforcement-learning",
        "jax"
      ],
      "id": 207
    },
    {
      "name": "NanoLLM",
      "one_line_profile": "Optimized local inference library for LLMs and multimodal models on NVIDIA Jetson/Edge",
      "detailed_description": "A lightweight and efficient library for running Large Language Models (LLMs), Vision-Language Models (VLMs), and multimodal agents locally on NVIDIA Jetson and edge devices. It supports quantization (AWQ, GPTQ), multimodal pipelines, and integrates with vector databases for RAG applications.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "quantization",
        "edge_computing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/dusty-nv/NanoLLM",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "edge-ai",
        "quantization",
        "nvidia-jetson"
      ],
      "id": 208
    },
    {
      "name": "jetson-inference",
      "one_line_profile": "High-performance deep learning inference library for NVIDIA Jetson devices",
      "detailed_description": "A C++ library and collection of utilities for deploying deep learning inference networks (image classification, detection, segmentation, pose estimation) on NVIDIA Jetson devices using TensorRT. It provides optimized primitives for vision tasks and streamlined deployment workflows.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "computer_vision",
        "edge_computing"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/dusty-nv/jetson-inference",
      "help_website": [
        "https://github.com/dusty-nv/jetson-inference/blob/master/docs/html/index.md"
      ],
      "license": "MIT",
      "tags": [
        "tensorrt",
        "edge-ai",
        "inference",
        "computer-vision"
      ],
      "id": 209
    },
    {
      "name": "ros_deep_learning",
      "one_line_profile": "Deep learning inference nodes for ROS/ROS2 integration",
      "detailed_description": "Provides ROS and ROS2 nodes that wrap deep learning inference capabilities (using TensorRT and NVIDIA Jetson), enabling robotic systems to perform real-time object detection, classification, and segmentation within the ROS ecosystem.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "robotics_integration",
        "inference_acceleration"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/dusty-nv/ros_deep_learning",
      "help_website": [],
      "license": null,
      "tags": [
        "ros",
        "ros2",
        "robotics",
        "tensorrt"
      ],
      "id": 210
    },
    {
      "name": "Q-LLM",
      "one_line_profile": "Query-aware inference acceleration library for Large Language Models",
      "detailed_description": "Implementation of the QuickLLaMA method, focusing on query-aware inference acceleration for LLMs. It provides techniques to optimize the inference process by leveraging query information to reduce computational redundancy.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "model_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/dvlab-research/Q-LLM",
      "help_website": [],
      "license": null,
      "tags": [
        "llm",
        "inference",
        "acceleration"
      ],
      "id": 211
    },
    {
      "name": "mixtral-offloading",
      "one_line_profile": "Efficient offloading inference engine for Mixtral-8x7B models",
      "detailed_description": "A specialized inference tool enabling the execution of large Mixture-of-Experts (MoE) models like Mixtral-8x7B on consumer-grade hardware or limited memory environments (e.g., Colab) through efficient CPU/GPU offloading strategies.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "memory_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/dvmazur/mixtral-offloading",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "moe",
        "offloading",
        "inference",
        "mixtral"
      ],
      "id": 212
    },
    {
      "name": "Nx",
      "one_line_profile": "Numerical computing and tensor library for Elixir",
      "detailed_description": "A foundational library for numerical computing in Elixir, providing multi-dimensional arrays (tensors) and automatic differentiation. It serves as the core infrastructure for machine learning and scientific computing within the Elixir ecosystem.",
      "domains": [
        "AI6"
      ],
      "subtask_category": [
        "numerical_computing",
        "tensor_operations"
      ],
      "application_level": "library",
      "primary_language": "Elixir",
      "repo_url": "https://github.com/elixir-nx/nx",
      "help_website": [
        "https://hexdocs.pm/nx/Nx.html"
      ],
      "license": null,
      "tags": [
        "elixir",
        "tensors",
        "numerical-computing"
      ],
      "id": 213
    },
    {
      "name": "Ortex",
      "one_line_profile": "ONNX Runtime bindings for Elixir",
      "detailed_description": "Provides Elixir bindings for ONNX Runtime, enabling the execution of machine learning models trained in other frameworks (PyTorch, TensorFlow) within Elixir applications for high-performance inference.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_runtime",
        "interoperability"
      ],
      "application_level": "library",
      "primary_language": "Elixir",
      "repo_url": "https://github.com/elixir-nx/ortex",
      "help_website": [
        "https://hexdocs.pm/ortex"
      ],
      "license": "MIT",
      "tags": [
        "onnx",
        "elixir",
        "inference"
      ],
      "id": 214
    },
    {
      "name": "XLA (Elixir)",
      "one_line_profile": "XLA (Accelerated Linear Algebra) compiler extension for Elixir Nx",
      "detailed_description": "Integrates Google's XLA compiler with Elixir Nx, allowing tensor operations to be just-in-time compiled to optimized machine code for CPUs, GPUs, and TPUs, significantly accelerating numerical computations.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "compilation",
        "acceleration"
      ],
      "application_level": "library",
      "primary_language": "Elixir",
      "repo_url": "https://github.com/elixir-nx/xla",
      "help_website": [
        "https://hexdocs.pm/xla"
      ],
      "license": "Apache-2.0",
      "tags": [
        "xla",
        "compiler",
        "acceleration",
        "elixir"
      ],
      "id": 215
    },
    {
      "name": "tensorflow-serving-arm",
      "one_line_profile": "Cross-compilation toolkit for TensorFlow Serving on ARM architecture",
      "detailed_description": "A utility project providing scripts and configurations to cross-compile TensorFlow Serving for ARM-based devices (e.g., Raspberry Pi, edge devices), facilitating the deployment of ML models on edge infrastructure.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "deployment",
        "cross_compilation"
      ],
      "application_level": "utility",
      "primary_language": "C++",
      "repo_url": "https://github.com/emacski/tensorflow-serving-arm",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "tensorflow-serving",
        "arm",
        "cross-compile"
      ],
      "id": 216
    },
    {
      "name": "yolo-tensorrt",
      "one_line_profile": "TensorRT implementation and conversion tool for YOLO object detection models",
      "detailed_description": "A C++ library and toolset for converting and running YOLO series models (v3, v4, v5) using NVIDIA TensorRT. It optimizes inference speed for real-time object detection applications.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "model_conversion"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/enazoe/yolo-tensorrt",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "tensorrt",
        "yolo",
        "inference",
        "acceleration"
      ],
      "id": 217
    },
    {
      "name": "imgcomp-cvpr",
      "one_line_profile": "Deep image compression model implementation based on Conditional Probability Models",
      "detailed_description": "A TensorFlow implementation of deep image compression techniques using conditional probability models. While a paper implementation, it serves as a baseline solver for image compression research and data processing.",
      "domains": [
        "AI6"
      ],
      "subtask_category": [
        "image_compression",
        "data_processing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/fab-jul/imgcomp-cvpr",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "image-compression",
        "deep-learning",
        "tensorflow"
      ],
      "id": 218
    },
    {
      "name": "FBTT-Embedding",
      "one_line_profile": "Tensor Train compression library for sparse embedding tables",
      "detailed_description": "A library for compressing sparse embedding tables in large-scale recommendation and NLP models using Tensor Train decomposition. It significantly reduces memory footprint and enables efficient runtime decompression.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_compression",
        "memory_optimization"
      ],
      "application_level": "library",
      "primary_language": "Cuda",
      "repo_url": "https://github.com/facebookresearch/FBTT-Embedding",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "embedding-compression",
        "tensor-train",
        "dlrm"
      ],
      "id": 219
    },
    {
      "name": "LLM-QAT",
      "one_line_profile": "Data-Free Quantization Aware Training framework for LLMs",
      "detailed_description": "A research tool enabling Quantization Aware Training (QAT) for Large Language Models without requiring original training data (data-free). It facilitates the production of quantized models with preserved accuracy.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "model_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/LLM-QAT",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "qat",
        "quantization",
        "llm"
      ],
      "id": 220
    },
    {
      "name": "bitsandbytes",
      "one_line_profile": "Lightweight wrapper for 8-bit optimizers and quantization routines",
      "detailed_description": "A critical library providing 8-bit optimizers, matrix multiplication routines, and quantization functions. It is widely used to reduce memory usage during training and inference of large models (LLMs) on GPUs.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "memory_optimization",
        "training_acceleration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/bitsandbytes",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "quantization",
        "8-bit",
        "optimization",
        "cuda"
      ],
      "id": 221
    },
    {
      "name": "diffq",
      "one_line_profile": "Differentiable quantization library for model compression",
      "detailed_description": "A library for differentiable quantization using pseudo quantization noise. It allows automatic tuning of bit-width per weight or group to achieve optimal trade-offs between model size and accuracy.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "model_compression"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/diffq",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "quantization",
        "differentiable-programming",
        "compression"
      ],
      "id": 222
    },
    {
      "name": "flashy",
      "one_line_profile": "Lightweight framework for deep learning training loops",
      "detailed_description": "A flexible framework for constructing deep learning training loops, handling checkpointing, logging, and distributed training (Dora compatibility), designed to streamline research experimentation.",
      "domains": [
        "AI6"
      ],
      "subtask_category": [
        "training_framework",
        "experiment_management"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/flashy",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "training-loop",
        "distributed-training",
        "pytorch"
      ],
      "id": 223
    },
    {
      "name": "Felafax",
      "one_line_profile": "Infrastructure platform for AI workloads on non-NVIDIA GPUs",
      "detailed_description": "A platform and toolkit designed to facilitate the running and fine-tuning of AI models on diverse hardware backends, specifically targeting non-NVIDIA GPUs (e.g., AMD, TPUs) to democratize AI compute access.",
      "domains": [
        "AI6"
      ],
      "subtask_category": [
        "hardware_abstraction",
        "distributed_training"
      ],
      "application_level": "platform",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/felafax/felafax",
      "help_website": [
        "https://felafax.ai"
      ],
      "license": "Apache-2.0",
      "tags": [
        "amd-gpu",
        "tpu",
        "infrastructure",
        "fine-tuning"
      ],
      "id": 224
    },
    {
      "name": "fhelipe",
      "one_line_profile": "Tensor compiler for Fully Homomorphic Encryption (FHE)",
      "detailed_description": "A compiler designed to transform high-level tensor operations into Fully Homomorphic Encryption (FHE) compatible circuits, enabling privacy-preserving computation on encrypted data.",
      "domains": [
        "AI6"
      ],
      "subtask_category": [
        "compilation",
        "privacy_preserving_computing"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/fhelipe-compiler/fhelipe",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "fhe",
        "compiler",
        "privacy",
        "tensor"
      ],
      "id": 225
    },
    {
      "name": "Finch.jl",
      "one_line_profile": "Sparse and structured tensor compiler for Julia",
      "detailed_description": "A compiler for sparse and structured tensor operations in Julia. It optimizes tensor loops to generate high-performance code for complex sparse array computations, essential for scientific computing and graph analysis.",
      "domains": [
        "AI6"
      ],
      "subtask_category": [
        "compilation",
        "sparse_computing"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/finch-tensor/Finch.jl",
      "help_website": [
        "https://willow-ahrens.io/Finch.jl/stable/"
      ],
      "license": "MIT",
      "tags": [
        "julia",
        "sparse-tensors",
        "compiler",
        "hpc"
      ],
      "id": 226
    },
    {
      "name": "native-sparse-attention",
      "one_line_profile": "Hardware-aligned sparse attention kernels in Triton",
      "detailed_description": "A library of efficient Triton implementations for native sparse attention mechanisms. It provides optimized kernels that align with hardware characteristics to accelerate training and inference of sparse transformers.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "kernel_optimization",
        "sparse_attention"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/fla-org/native-sparse-attention",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "triton",
        "sparse-attention",
        "acceleration"
      ],
      "id": 227
    },
    {
      "name": "FlagAttention",
      "one_line_profile": "Memory-efficient attention operators library",
      "detailed_description": "A collection of high-performance, memory-efficient attention operators implemented in Triton. It aims to optimize the core attention mechanism in Large Language Models for better throughput and lower memory usage.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "kernel_optimization",
        "attention_mechanism"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/flagos-ai/FlagAttention",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "triton",
        "attention",
        "optimization"
      ],
      "id": 228
    },
    {
      "name": "FlagGems",
      "one_line_profile": "Triton-based operator library for Large Language Models",
      "detailed_description": "A library of general-purpose and specialized operators for deep learning, implemented in Triton. It serves as a high-performance backend for LLM inference and training, offering alternatives to standard CUDA kernels.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "kernel_optimization",
        "operator_library"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/flagos-ai/FlagGems",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "triton",
        "operators",
        "llm"
      ],
      "id": 229
    },
    {
      "name": "FlagTree",
      "one_line_profile": "Unified compiler for custom DL operations on multiple AI chips",
      "detailed_description": "A fork of the Triton compiler designed to support multiple AI chip backends. It provides a unified compilation framework for custom deep learning operations, facilitating portability across different hardware architectures.",
      "domains": [
        "AI6"
      ],
      "subtask_category": [
        "compilation",
        "hardware_abstraction"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/flagos-ai/flagtree",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "compiler",
        "triton",
        "multi-backend"
      ],
      "id": 230
    },
    {
      "name": "SAX",
      "one_line_profile": "S-parameter based frequency domain circuit simulation with JAX",
      "detailed_description": "A circuit simulation and optimization tool based on S-parameters, built on top of JAX. It leverages JAX's autograd and XLA capabilities for efficient frequency domain analysis and optimization of photonic and electrical circuits.",
      "domains": [
        "AI6"
      ],
      "subtask_category": [
        "scientific_modeling",
        "simulation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/flaport/sax",
      "help_website": [
        "https://flaport.github.io/sax/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "jax",
        "circuit-simulation",
        "photonics",
        "autograd"
      ],
      "id": 231
    },
    {
      "name": "Deep Learning on Flink",
      "one_line_profile": "Distributed deep learning framework on Apache Flink",
      "detailed_description": "Integrates deep learning frameworks (TensorFlow, PyTorch) with Apache Flink, enabling distributed training and inference workflows on Flink clusters for large-scale data processing pipelines.",
      "domains": [
        "AI6"
      ],
      "subtask_category": [
        "distributed_computing",
        "workflow_orchestration"
      ],
      "application_level": "platform",
      "primary_language": "Java",
      "repo_url": "https://github.com/flink-extended/dl-on-flink",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "flink",
        "distributed-training",
        "tensorflow",
        "pytorch"
      ],
      "id": 232
    },
    {
      "name": "fms-acceleration",
      "one_line_profile": "Acceleration libraries for foundation model fine-tuning",
      "detailed_description": "A collection of libraries and plugins designed to accelerate the fine-tuning and training of foundation models within the Foundation Model Stack ecosystem. It provides optimizations for training loops and hardware utilization.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "training_acceleration",
        "fine_tuning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/foundation-model-stack/fms-acceleration",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "foundation-models",
        "acceleration",
        "fine-tuning"
      ],
      "id": 233
    },
    {
      "name": "GPTQ-triton",
      "one_line_profile": "Triton kernel implementation for GPTQ inference",
      "detailed_description": "A specialized implementation of the GPTQ (Generative Pre-trained Transformer Quantization) inference kernel using OpenAI's Triton language. It enables high-performance inference of quantized models on GPUs.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "quantization"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/fpgaminer/GPTQ-triton",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "gptq",
        "triton",
        "quantization",
        "inference"
      ],
      "id": 234
    },
    {
      "name": "Galois",
      "one_line_profile": "Tensor computing compiler based on tile programming for GPU/CPU/TPU",
      "detailed_description": "A tensor computing compiler that utilizes tile programming to optimize execution on various hardware backends including GPUs, CPUs, and TPUs.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "compiler",
        "tensor_computing"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/galois-stack/galois",
      "help_website": [],
      "license": null,
      "tags": [
        "compiler",
        "tensor",
        "gpu",
        "tpu"
      ],
      "id": 235
    },
    {
      "name": "CLAMP-ViT",
      "one_line_profile": "Contrastive data-free learning for adaptive post-training quantization of ViTs",
      "detailed_description": "A tool implementing contrastive data-free learning techniques for adaptive post-training quantization specifically designed for Vision Transformers (ViTs).",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "model_compression"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/georgia-tech-synergy-lab/CLAMP-ViT",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "quantization",
        "vit",
        "post-training-quantization"
      ],
      "id": 236
    },
    {
      "name": "SparQ",
      "one_line_profile": "Post-training sparsity-aware quantization framework",
      "detailed_description": "A framework for post-training quantization that takes sparsity into account to optimize model performance and size.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "model_compression"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/gilshm/sparq",
      "help_website": [],
      "license": null,
      "tags": [
        "quantization",
        "sparsity",
        "post-training"
      ],
      "id": 237
    },
    {
      "name": "Orion",
      "one_line_profile": "ONNX Runtime in Cairo for verifiable ML inference using STARK",
      "detailed_description": "An implementation of the ONNX Runtime in Cairo 1.0, enabling verifiable machine learning inference through STARK proofs.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_runtime",
        "verifiable_ml"
      ],
      "application_level": "solver",
      "primary_language": "Cairo",
      "repo_url": "https://github.com/gizatechxyz/orion",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "onnx",
        "cairo",
        "zkml",
        "inference"
      ],
      "id": 238
    },
    {
      "name": "GoMLX",
      "one_line_profile": "Accelerated Machine Learning Framework for Go",
      "detailed_description": "A machine learning framework for the Go programming language that provides acceleration capabilities, likely via XLA or similar backends.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "ml_framework",
        "training",
        "inference"
      ],
      "application_level": "platform",
      "primary_language": "Go",
      "repo_url": "https://github.com/gomlx/gomlx",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "go",
        "machine-learning",
        "framework"
      ],
      "id": 239
    },
    {
      "name": "AI Edge Quantizer",
      "one_line_profile": "Flexible post-training quantization for LiteRT models",
      "detailed_description": "A tool provided by Google AI Edge for performing flexible post-training quantization on LiteRT (formerly TensorFlow Lite) models.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "edge_computing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/google-ai-edge/ai-edge-quantizer",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "quantization",
        "litert",
        "tensorflow-lite"
      ],
      "id": 240
    },
    {
      "name": "GPUStack",
      "one_line_profile": "GPU cluster manager for optimized AI model deployment",
      "detailed_description": "A cluster management tool designed to orchestrate and optimize the deployment of AI models across GPU clusters.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "cluster_management",
        "deployment",
        "orchestration"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/gpustack/gpustack",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "gpu",
        "cluster-manager",
        "deployment"
      ],
      "id": 241
    },
    {
      "name": "Horovod Ansible",
      "one_line_profile": "Ansible roles for deploying Horovod clusters",
      "detailed_description": "Infrastructure-as-Code tool using Ansible to automate the deployment and configuration of Horovod distributed training clusters.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "deployment",
        "infrastructure_provisioning"
      ],
      "application_level": "workflow",
      "primary_language": "HCL",
      "repo_url": "https://github.com/graykode/horovod-ansible",
      "help_website": [],
      "license": null,
      "tags": [
        "ansible",
        "horovod",
        "distributed-training"
      ],
      "id": 242
    },
    {
      "name": "Inference (C#)",
      "one_line_profile": "C# deployment wrapper for OpenVINO, TensorRT, and ONNX Runtime",
      "detailed_description": "A library providing C# interfaces for deploying models using various high-performance inference engines like OpenVINO, TensorRT, and ONNX Runtime.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_wrapper",
        "deployment"
      ],
      "application_level": "library",
      "primary_language": "C#",
      "repo_url": "https://github.com/guojin-yan/Inference",
      "help_website": [],
      "license": null,
      "tags": [
        "csharp",
        "openvino",
        "tensorrt",
        "onnx"
      ],
      "id": 243
    },
    {
      "name": "BERT-pre-training",
      "one_line_profile": "Multi-GPU pre-training implementation for BERT without Horovod",
      "detailed_description": "A specialized implementation for pre-training BERT models using multi-GPU data parallelism on a single machine without requiring Horovod.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_training",
        "parallelism"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/guotong1988/BERT-pre-training",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "bert",
        "pre-training",
        "multi-gpu"
      ],
      "id": 244
    },
    {
      "name": "APPy",
      "one_line_profile": "Annotated Parallelism for Python to GPU compiler",
      "detailed_description": "A compiler tool that allows users to annotate Python loops and tensor expressions for automatic compilation into efficient GPU kernels.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "compiler",
        "parallelization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/habanero-lab/APPy",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "compiler",
        "gpu",
        "python",
        "parallelism"
      ],
      "id": 245
    },
    {
      "name": "docker-tensorflow-builder",
      "one_line_profile": "Docker environment for compiling TensorFlow from source",
      "detailed_description": "A set of Docker configurations and scripts to facilitate the compilation of TensorFlow from source, enabling custom builds.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "build_tool",
        "compilation"
      ],
      "application_level": "workflow",
      "primary_language": "Shell",
      "repo_url": "https://github.com/hadim/docker-tensorflow-builder",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "docker",
        "tensorflow",
        "compilation"
      ],
      "id": 246
    },
    {
      "name": "PTQ4ViT",
      "one_line_profile": "Post-Training Quantization for Vision Transformers",
      "detailed_description": "A tool implementing post-training quantization techniques specifically optimized for Vision Transformer (ViT) architectures.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "model_compression"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/hahnyuan/PTQ4ViT",
      "help_website": [],
      "license": null,
      "tags": [
        "quantization",
        "vit",
        "vision-transformer"
      ],
      "id": 247
    },
    {
      "name": "RPTQ4LLM",
      "one_line_profile": "Reorder-based post-training quantization for LLMs",
      "detailed_description": "A quantization tool for Large Language Models that uses a reorder-based approach to maintain accuracy after post-training quantization.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "llm_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/hahnyuan/RPTQ4LLM",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "quantization",
        "llm",
        "post-training"
      ],
      "id": 248
    },
    {
      "name": "Hailo Model Zoo",
      "one_line_profile": "Pre-trained models and build environment for Hailo hardware",
      "detailed_description": "A collection of pre-trained models along with a build and evaluation environment optimized for deployment on Hailo AI processors.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_deployment",
        "benchmarking"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/hailo-ai/hailo_model_zoo",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "model-zoo",
        "hailo",
        "deployment"
      ],
      "id": 249
    },
    {
      "name": "Catgrad",
      "one_line_profile": "Categorical deep learning compiler",
      "detailed_description": "A deep learning compiler built using principles from category theory, written in Rust.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "compiler",
        "deep_learning"
      ],
      "application_level": "solver",
      "primary_language": "Rust",
      "repo_url": "https://github.com/hellas-ai/catgrad",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "compiler",
        "rust",
        "category-theory"
      ],
      "id": 250
    },
    {
      "name": "SWIFT",
      "one_line_profile": "On-the-Fly Self-Speculative Decoding for LLM Inference Acceleration",
      "detailed_description": "A tool implementing self-speculative decoding to accelerate the inference of Large Language Models.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "decoding"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/hemingkx/SWIFT",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "inference",
        "acceleration",
        "llm",
        "speculative-decoding"
      ],
      "id": 251
    },
    {
      "name": "Quantization.MXNet",
      "one_line_profile": "Quantization simulation for MXNet-Gluon models",
      "detailed_description": "A tool to simulate quantization and perform quantization-aware training for models built with MXNet-Gluon.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "simulation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/hey-yahei/Quantization.MXNet",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mxnet",
        "quantization",
        "gluon"
      ],
      "id": 252
    },
    {
      "name": "VodaScheduler",
      "one_line_profile": "GPU scheduler for elastic distributed deep learning in Kubernetes",
      "detailed_description": "A GPU scheduling tool designed for managing elastic and distributed deep learning workloads within Kubernetes clusters.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "scheduling",
        "cluster_management"
      ],
      "application_level": "platform",
      "primary_language": "Go",
      "repo_url": "https://github.com/heyfey/vodascheduler",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "kubernetes",
        "scheduler",
        "gpu",
        "distributed-training"
      ],
      "id": 253
    },
    {
      "name": "tensorflow-cpp",
      "one_line_profile": "TensorFlow C++ library compilation for CMake projects",
      "detailed_description": "A utility to compile TensorFlow into a C++ library suitable for integration into CMake-based projects.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "build_tool",
        "integration"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/hhzrz/tensorflow-cpp",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "tensorflow",
        "cpp",
        "cmake"
      ],
      "id": 254
    },
    {
      "name": "Hidet",
      "one_line_profile": "Efficient deep learning framework and compiler",
      "detailed_description": "An open-source deep learning framework and compiler written in Python, designed for high efficiency.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "compiler",
        "dl_framework"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/hidet-org/hidet",
      "help_website": [
        "https://hidet.org"
      ],
      "license": "Apache-2.0",
      "tags": [
        "compiler",
        "deep-learning",
        "python"
      ],
      "id": 255
    },
    {
      "name": "Higgsfield",
      "one_line_profile": "Fault-tolerant GPU orchestration and training framework",
      "detailed_description": "A scalable GPU orchestration platform and machine learning framework designed for training large-scale models with fault tolerance.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "orchestration",
        "training_framework"
      ],
      "application_level": "platform",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/higgsfield-ai/higgsfield",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "orchestration",
        "gpu",
        "training"
      ],
      "id": 256
    },
    {
      "name": "Caten",
      "one_line_profile": "Polyhedral-based Deep Learning Compiler",
      "detailed_description": "A deep learning compiler leveraging polyhedral compilation techniques, lightweight IRs, and pattern matching for optimization.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "compiler",
        "optimization"
      ],
      "application_level": "solver",
      "primary_language": "Common Lisp",
      "repo_url": "https://github.com/hikettei/Caten",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "compiler",
        "polyhedral",
        "lisp"
      ],
      "id": 257
    },
    {
      "name": "Parallel Prompt Decoding",
      "one_line_profile": "Efficient LLM Inference Acceleration using Prompting",
      "detailed_description": "A tool implementing parallel prompt decoding techniques to accelerate the inference process of Large Language Models.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "decoding"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/hmarkc/parallel-prompt-decoding",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "inference",
        "acceleration",
        "llm"
      ],
      "id": 258
    },
    {
      "name": "Horovod",
      "one_line_profile": "Distributed training framework for TensorFlow, Keras, PyTorch, and MXNet",
      "detailed_description": "A widely used distributed deep learning training framework that supports multiple backends including TensorFlow, PyTorch, and MXNet, enabling easy scaling of training jobs.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "distributed_training",
        "parallelism"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/horovod/horovod",
      "help_website": [
        "https://horovod.ai"
      ],
      "license": "NOASSERTION",
      "tags": [
        "distributed-training",
        "mpi",
        "deep-learning"
      ],
      "id": 259
    },
    {
      "name": "LLM-Pruner",
      "one_line_profile": "Structural Pruning tool for Large Language Models",
      "detailed_description": "A tool for structural pruning of Large Language Models (LLMs) such as Llama, BLOOM, and Vicuna to reduce model size and improve efficiency.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_compression",
        "pruning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/horseee/LLM-Pruner",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "pruning",
        "llm",
        "compression"
      ],
      "id": 260
    },
    {
      "name": "SINQ",
      "one_line_profile": "Fast and high-quality quantization method for LLMs",
      "detailed_description": "A quantization tool designed to compress Large Language Models while preserving accuracy, implementing the SINQ method.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "model_compression"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/huawei-csl/SINQ",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "quantization",
        "llm",
        "compression"
      ],
      "id": 261
    },
    {
      "name": "Huawei Noah PLM",
      "one_line_profile": "Pretrained language models and optimization techniques",
      "detailed_description": "A repository containing pretrained language models and implementations of optimization techniques (like PanGu-Alpha) developed by Huawei Noah's Ark Lab.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_optimization",
        "pretraining"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/huawei-noah/Pretrained-Language-Model",
      "help_website": [],
      "license": null,
      "tags": [
        "pretrained-models",
        "optimization",
        "huawei"
      ],
      "id": 262
    },
    {
      "name": "Accelerate",
      "one_line_profile": "Library for easy distributed training and mixed precision in PyTorch",
      "detailed_description": "A library by Hugging Face that abstracts the boilerplate code for distributed training and mixed precision, making it easy to run PyTorch scripts on various hardware configurations.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "distributed_training",
        "acceleration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/huggingface/accelerate",
      "help_website": [
        "https://huggingface.co/docs/accelerate"
      ],
      "license": "Apache-2.0",
      "tags": [
        "pytorch",
        "distributed-training",
        "mixed-precision"
      ],
      "id": 263
    },
    {
      "name": "Optimum ONNX",
      "one_line_profile": "Export and inference tool for ONNX Runtime via Hugging Face",
      "detailed_description": "A tool to export Hugging Face models to ONNX format and run inference using ONNX Runtime, optimizing performance.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_optimization",
        "model_export"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/huggingface/optimum-onnx",
      "help_website": [
        "https://huggingface.co/docs/optimum"
      ],
      "license": "Apache-2.0",
      "tags": [
        "onnx",
        "inference",
        "optimization"
      ],
      "id": 264
    },
    {
      "name": "Picotron",
      "one_line_profile": "Minimalistic 4D-parallelism distributed training framework",
      "detailed_description": "A distributed training framework implementing 4D-parallelism, designed for educational purposes but functional as a reference implementation for advanced parallelism techniques.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "distributed_training",
        "parallelism"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/huggingface/picotron",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "distributed-training",
        "parallelism",
        "educational"
      ],
      "id": 265
    },
    {
      "name": "tensorflow-yolov4-tflite",
      "one_line_profile": "YOLOv4 implementation with conversion tools for TFLite and TensorRT",
      "detailed_description": "A comprehensive repository providing implementations of YOLOv4, YOLOv4-tiny, YOLOv3, and YOLOv3-tiny in TensorFlow 2.0, along with utilities to convert these models to TensorFlow Lite (.tflite) and TensorRT formats for accelerated inference on edge devices.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_conversion",
        "inference_acceleration"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/hunglc007/tensorflow-yolov4-tflite",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "yolov4",
        "tflite",
        "tensorrt",
        "conversion"
      ],
      "id": 266
    },
    {
      "name": "PD-Quant",
      "one_line_profile": "Post-training quantization method based on prediction difference metric",
      "detailed_description": "Implementation of the PD-Quant method (CVPR 2023), a post-training quantization technique that uses a prediction difference metric to optimize quantization parameters without requiring retraining.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "model_compression"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/hustvl/PD-Quant",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "post-training-quantization",
        "cvpr-2023",
        "model-compression"
      ],
      "id": 267
    },
    {
      "name": "yzma",
      "one_line_profile": "Go bindings for llama.cpp enabling local LLM inference",
      "detailed_description": "A Go library that integrates with llama.cpp to provide hardware-accelerated local inference for Large Language Models (LLMs) within Go applications.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "llm_inference"
      ],
      "application_level": "library",
      "primary_language": "Go",
      "repo_url": "https://github.com/hybridgroup/yzma",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "llama.cpp",
        "go",
        "llm",
        "inference"
      ],
      "id": 268
    },
    {
      "name": "AutoRound",
      "one_line_profile": "Advanced quantization toolkit for LLMs and VLMs",
      "detailed_description": "An advanced quantization toolkit designed for Large Language Models (LLMs) and Vision-Language Models (VLMs), supporting various schemes like WOQ, MXFP4, and NVFP4. It integrates with major frameworks like Transformers and vLLM to facilitate low-bit model deployment.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "model_compression"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/intel/auto-round",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "quantization",
        "llm",
        "vlm",
        "intel"
      ],
      "id": 269
    },
    {
      "name": "Intel Extension for DeepSpeed",
      "one_line_profile": "DeepSpeed extension for Intel XPU acceleration",
      "detailed_description": "An extension for the DeepSpeed optimization library that enables support for Intel GPU (XPU) devices using SYCL kernels, facilitating accelerated distributed training on Intel hardware.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "training_acceleration",
        "distributed_training"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/intel/intel-extension-for-deepspeed",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "deepspeed",
        "intel-xpu",
        "sycl",
        "distributed-training"
      ],
      "id": 270
    },
    {
      "name": "Intel XPU Backend for Triton",
      "one_line_profile": "OpenAI Triton compiler backend for Intel GPUs",
      "detailed_description": "A backend implementation for the OpenAI Triton compiler, enabling Triton kernels to run efficiently on Intel GPUs (XPU), supporting high-performance deep learning primitives.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "compiler_backend",
        "inference_acceleration"
      ],
      "application_level": "library",
      "primary_language": "MLIR",
      "repo_url": "https://github.com/intel/intel-xpu-backend-for-triton",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "triton",
        "intel-gpu",
        "compiler",
        "acceleration"
      ],
      "id": 271
    },
    {
      "name": "IPEX-LLM",
      "one_line_profile": "LLM inference and finetuning acceleration library for Intel XPU",
      "detailed_description": "A library for accelerating local inference and fine-tuning of Large Language Models (LLMs) on Intel hardware (CPUs, iGPUs, discrete GPUs). It integrates with popular frameworks like HuggingFace, LangChain, and vLLM.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "finetuning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/intel/ipex-llm",
      "help_website": [
        "https://ipex-llm.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "intel",
        "inference",
        "finetuning"
      ],
      "id": 272
    },
    {
      "name": "Intel Neural Compressor",
      "one_line_profile": "Model compression tool for quantization and sparsity",
      "detailed_description": "A model compression tool that provides state-of-the-art quantization (INT8, FP8, INT4, etc.) and sparsity techniques for PyTorch, TensorFlow, and ONNX Runtime, aiming to accelerate inference with minimal accuracy loss.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "model_compression",
        "sparsity"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/intel/neural-compressor",
      "help_website": [
        "https://intel.github.io/neural-compressor/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "quantization",
        "compression",
        "intel",
        "optimization"
      ],
      "id": 273
    },
    {
      "name": "NeuroVectorizer",
      "one_line_profile": "RL-based framework for optimal compiler vectorization",
      "detailed_description": "A framework that utilizes deep reinforcement learning to predict optimal vectorization compiler pragmas for loops in C and C++ code, automating the tuning of compiler optimizations.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "compiler_optimization",
        "vectorization"
      ],
      "application_level": "solver",
      "primary_language": "C",
      "repo_url": "https://github.com/intel/neuro-vectorizer",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "compiler",
        "reinforcement-learning",
        "vectorization",
        "optimization"
      ],
      "id": 274
    },
    {
      "name": "PKD-for-BERT",
      "one_line_profile": "Patient Knowledge Distillation for BERT model compression",
      "detailed_description": "PyTorch implementation of Patient Knowledge Distillation (PKD), a method for compressing BERT models by distilling knowledge from intermediate layers, enabling efficient inference.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "knowledge_distillation",
        "model_compression"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/intersun/PKD-for-BERT-Model-Compression",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "bert",
        "knowledge-distillation",
        "compression"
      ],
      "id": 275
    },
    {
      "name": "IREE",
      "one_line_profile": "Retargetable MLIR-based machine learning compiler and runtime",
      "detailed_description": "A compiler and runtime toolkit based on MLIR that lowers machine learning models to a unified intermediate representation, enabling execution on diverse hardware targets including mobile, edge, and datacenter devices.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "compiler",
        "runtime",
        "inference_acceleration"
      ],
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/iree-org/iree",
      "help_website": [
        "https://iree.dev/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "mlir",
        "compiler",
        "runtime",
        "cross-platform"
      ],
      "id": 276
    },
    {
      "name": "yolov4-triton-tensorrt",
      "one_line_profile": "Deployment workflow for YOLOv4 on Triton Inference Server via TensorRT",
      "detailed_description": "A repository providing the necessary configurations and scripts to deploy YOLOv4 models as optimized TensorRT engines within the Triton Inference Server environment.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_deployment",
        "inference_serving"
      ],
      "application_level": "workflow",
      "primary_language": "C++",
      "repo_url": "https://github.com/isarsoft/yolov4-triton-tensorrt",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "yolov4",
        "triton-inference-server",
        "tensorrt",
        "deployment"
      ],
      "id": 277
    },
    {
      "name": "Tzer",
      "one_line_profile": "Coverage-guided tensor compiler fuzzer for TVM",
      "detailed_description": "A fuzzing tool designed for tensor compilers (specifically TVM) that uses coverage guidance and joint IR-pass mutation to detect bugs and correctness issues in the compilation process.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "compiler_testing",
        "fuzzing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ise-uiuc/tzer",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "tvm",
        "fuzzing",
        "compiler-testing"
      ],
      "id": 278
    },
    {
      "name": "CalibTIP",
      "one_line_profile": "Layer-wise calibration for post-training neural quantization",
      "detailed_description": "Implementation of a post-training quantization method that utilizes layer-wise calibration and integer programming to improve the accuracy of quantized neural networks.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "calibration"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/itayhubara/CalibTIP",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "quantization",
        "calibration",
        "optimization"
      ],
      "id": 279
    },
    {
      "name": "dl-benchmark",
      "one_line_profile": "Multi-framework deep learning inference benchmark tool",
      "detailed_description": "A benchmarking tool for evaluating deep learning inference performance across multiple frameworks including OpenVINO, TensorFlow, ONNX Runtime, PyTorch, and TVM.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_analysis"
      ],
      "application_level": "solver",
      "primary_language": "HTML",
      "repo_url": "https://github.com/itlab-vision/dl-benchmark",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "benchmark",
        "inference",
        "deep-learning"
      ],
      "id": 280
    },
    {
      "name": "yolov5-onnxruntime",
      "one_line_profile": "C++ inference implementation for YOLOv5 using ONNX Runtime",
      "detailed_description": "A C++ implementation for running inference on YOLOv5 models using the ONNX Runtime, providing a lightweight solution for deploying object detection models.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "model_deployment"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/itsnine/yolov5-onnxruntime",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "yolov5",
        "onnx-runtime",
        "cpp",
        "inference"
      ],
      "id": 281
    },
    {
      "name": "InferenceHelper",
      "one_line_profile": "C++ helper library for multiple deep learning inference frameworks",
      "detailed_description": "A C++ wrapper library that provides a unified interface for various deep learning inference frameworks such as TensorFlow Lite, TensorRT, OpenVINO, ONNX Runtime, and ncnn, simplifying application development.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_abstraction",
        "deployment"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/iwatake2222/InferenceHelper",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "inference",
        "wrapper",
        "cpp",
        "multi-framework"
      ],
      "id": 282
    },
    {
      "name": "model_compression",
      "one_line_profile": "PyTorch library for model compression techniques",
      "detailed_description": "A PyTorch library implementing various model compression techniques, including pruning and quantization, to reduce model size and improve inference speed.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_compression",
        "pruning",
        "quantization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/j-marple-dev/model_compression",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pytorch",
        "compression",
        "pruning"
      ],
      "id": 283
    },
    {
      "name": "Alpaca-LoRA-RLHF-PyTorch",
      "one_line_profile": "Pipeline for finetuning Alpaca LLM with LoRA and RLHF",
      "detailed_description": "A complete pipeline for fine-tuning the Alpaca Large Language Model using Low-Rank Adaptation (LoRA) and Reinforcement Learning with Human Feedback (RLHF) on consumer hardware.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "finetuning",
        "rlhf",
        "training_pipeline"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/jackaduma/Alpaca-LoRA-RLHF-PyTorch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "alpaca",
        "lora",
        "rlhf",
        "finetuning"
      ],
      "id": 284
    },
    {
      "name": "ChatGLM-LoRA-RLHF-PyTorch",
      "one_line_profile": "Pipeline for finetuning ChatGLM with LoRA and RLHF",
      "detailed_description": "A pipeline designed to fine-tune the ChatGLM model using LoRA and RLHF techniques, enabling efficient customization of the model on consumer-grade hardware.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "finetuning",
        "rlhf",
        "training_pipeline"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/jackaduma/ChatGLM-LoRA-RLHF-PyTorch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "chatglm",
        "lora",
        "rlhf",
        "finetuning"
      ],
      "id": 285
    },
    {
      "name": "YOLOv8-qat",
      "one_line_profile": "Quantization Aware Training implementation for YOLOv8",
      "detailed_description": "A tool implementing Quantization Aware Training (QAT) specifically for YOLOv8 models, allowing for the creation of quantized models that retain high accuracy.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization_aware_training",
        "model_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/jahongir7174/YOLOv8-qat",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "yolov8",
        "qat",
        "quantization"
      ],
      "id": 286
    },
    {
      "name": "jax-triton",
      "one_line_profile": "Integration library between JAX and OpenAI Triton",
      "detailed_description": "A library that provides integrations between JAX and OpenAI Triton, allowing users to write custom Triton kernels and use them within JAX computations for accelerated machine learning.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "compiler_integration",
        "acceleration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/jax-ml/jax-triton",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "jax",
        "triton",
        "kernel",
        "acceleration"
      ],
      "id": 287
    },
    {
      "name": "xllm",
      "one_line_profile": "High-performance inference engine for LLMs on diverse accelerators",
      "detailed_description": "A high-performance inference engine designed for Large Language Models (LLMs), optimized to run efficiently on various AI accelerators and hardware platforms.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_engine",
        "llm_acceleration"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/jd-opensource/xllm",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "llm",
        "inference-engine",
        "acceleration"
      ],
      "id": 288
    },
    {
      "name": "QSNNs",
      "one_line_profile": "Quantization-aware training for spiking neural networks",
      "detailed_description": "A library/tool for performing quantization-aware training specifically tailored for Spiking Neural Networks (SNNs), enabling the deployment of efficient neuromorphic models.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "neuromorphic_computing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/jeshraghian/QSNNs",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "snn",
        "quantization",
        "spiking-neural-networks"
      ],
      "id": 289
    },
    {
      "name": "tensorrt_demos",
      "one_line_profile": "Collection of TensorRT implementation examples and utilities",
      "detailed_description": "A widely used collection of examples and utility scripts for deploying various deep learning models (YOLO, SSD, etc.) using TensorRT on NVIDIA Jetson and x86 platforms.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_deployment",
        "tensorrt_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/jkjung-avt/tensorrt_demos",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "tensorrt",
        "jetson",
        "inference",
        "demo"
      ],
      "id": 290
    },
    {
      "name": "DeepDetect",
      "one_line_profile": "Open source deep learning API and server",
      "detailed_description": "A deep learning API and server written in C++14 that supports multiple backends (PyTorch, TensorRT, NCNN, etc.), providing a unified platform for training and inference integration.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_server",
        "model_serving"
      ],
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/jolibrain/deepdetect",
      "help_website": [
        "https://www.deepdetect.com/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "inference-server",
        "api",
        "deep-learning"
      ],
      "id": 291
    },
    {
      "name": "pykaldi2",
      "one_line_profile": "Speech recognition toolkit based on Kaldi and PyTorch",
      "detailed_description": "A Python wrapper and toolkit for the Kaldi speech recognition system, integrating it with PyTorch for flexible model training and inference.",
      "domains": [
        "AI6",
        "AI4"
      ],
      "subtask_category": [
        "speech_recognition",
        "model_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/jzlianglu/pykaldi2",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "kaldi",
        "speech-recognition",
        "pytorch-wrapper"
      ],
      "id": 292
    },
    {
      "name": "trident",
      "one_line_profile": "Performance library for machine learning applications",
      "detailed_description": "A library designed to accelerate machine learning applications, providing optimized kernels and utilities for performance enhancement.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "training_acceleration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/kakaobrain/trident",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "acceleration",
        "performance",
        "machine-learning"
      ],
      "id": 293
    },
    {
      "name": "ps-dnn",
      "one_line_profile": "Distributed deep learning training and prediction framework",
      "detailed_description": "A lightweight distributed deep learning framework based on Parameter Server (PS-Lite), supporting feature extraction and distributed training.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "distributed_training",
        "inference"
      ],
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/kangshantong/ps-dnn",
      "help_website": [],
      "license": null,
      "tags": [
        "parameter-server",
        "distributed-training",
        "c++"
      ],
      "id": 294
    },
    {
      "name": "sparrow",
      "one_line_profile": "Structured data extraction tool using ML and LLMs",
      "detailed_description": "A tool for extracting structured data from documents and images using machine learning and Large Language Models, useful for scientific data processing.",
      "domains": [
        "AI1",
        "AI6"
      ],
      "subtask_category": [
        "data_extraction",
        "document_processing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/katanaml/sparrow",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "data-extraction",
        "llm",
        "ocr"
      ],
      "id": 295
    },
    {
      "name": "nncase",
      "one_line_profile": "Deep learning compiler stack for AI accelerators",
      "detailed_description": "An open compiler stack designed to optimize and deploy deep learning models onto Kendryte AI accelerators.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_compilation",
        "inference_acceleration"
      ],
      "application_level": "solver",
      "primary_language": "C#",
      "repo_url": "https://github.com/kendryte/nncase",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "compiler",
        "ai-accelerator",
        "kendryte"
      ],
      "id": 296
    },
    {
      "name": "onnxruntime-server",
      "one_line_profile": "Server for ONNX inference via REST APIs",
      "detailed_description": "A server application that provides TCP and HTTP/HTTPS REST APIs for performing inference using ONNX Runtime.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_serving",
        "inference"
      ],
      "application_level": "service",
      "primary_language": "C++",
      "repo_url": "https://github.com/kibae/onnxruntime-server",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "onnx",
        "serving",
        "rest-api"
      ],
      "id": 297
    },
    {
      "name": "kserve",
      "one_line_profile": "Standardized distributed AI inference platform on Kubernetes",
      "detailed_description": "A platform for serving machine learning models on Kubernetes, supporting serverless inference, canary rollouts, and multi-framework deployment.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_serving",
        "inference_management"
      ],
      "application_level": "platform",
      "primary_language": "Shell",
      "repo_url": "https://github.com/kserve/kserve",
      "help_website": [
        "https://kserve.github.io/website/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "kubernetes",
        "inference-serving",
        "mlops"
      ],
      "id": 298
    },
    {
      "name": "kubeai",
      "one_line_profile": "AI Inference Operator for Kubernetes",
      "detailed_description": "A Kubernetes operator designed to simplify the deployment and serving of AI models, including LLMs and VLMs, in production environments.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_serving",
        "deployment_automation"
      ],
      "application_level": "platform",
      "primary_language": "Go",
      "repo_url": "https://github.com/kubeai-project/kubeai",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "kubernetes-operator",
        "inference",
        "llm-serving"
      ],
      "id": 299
    },
    {
      "name": "mpi-operator",
      "one_line_profile": "Kubernetes Operator for MPI-based applications",
      "detailed_description": "A Kubernetes operator that manages the lifecycle of MPI jobs, facilitating distributed training and HPC workloads on Kubernetes clusters.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "distributed_training",
        "hpc_job_management"
      ],
      "application_level": "platform",
      "primary_language": "Go",
      "repo_url": "https://github.com/kubeflow/mpi-operator",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "mpi",
        "kubernetes",
        "hpc"
      ],
      "id": 300
    },
    {
      "name": "Mooncake",
      "one_line_profile": "High-performance LLM serving platform with KVCache separation",
      "detailed_description": "A serving platform optimized for Large Language Models, featuring KVCache separation to enhance inference performance and scalability.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_serving",
        "inference_acceleration"
      ],
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/kvcache-ai/Mooncake",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-serving",
        "kv-cache",
        "inference"
      ],
      "id": 301
    },
    {
      "name": "pytorch2c",
      "one_line_profile": "Compiler for converting PyTorch graphs to C",
      "detailed_description": "A tool that compiles PyTorch computational graphs into C code, enabling standalone execution of models without the Python runtime.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_compilation",
        "inference_acceleration"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/lantiga/pytorch2c",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pytorch",
        "compiler",
        "c-generation"
      ],
      "id": 302
    },
    {
      "name": "TensorRT-YOLO",
      "one_line_profile": "Deployment toolkit for YOLO models using TensorRT",
      "detailed_description": "A toolkit designed to facilitate the deployment of YOLO object detection models on NVIDIA GPUs using TensorRT for acceleration.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "model_deployment"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/laugh12321/TensorRT-YOLO",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "tensorrt",
        "yolo",
        "deployment"
      ],
      "id": 303
    },
    {
      "name": "Liger-Kernel",
      "one_line_profile": "Efficient Triton kernels for LLM training",
      "detailed_description": "A library of highly optimized Triton kernels designed to accelerate the training of Large Language Models.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "training_acceleration",
        "kernel_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/linkedin/Liger-Kernel",
      "help_website": [],
      "license": "BSD-2-Clause",
      "tags": [
        "triton",
        "kernels",
        "llm-training"
      ],
      "id": 304
    },
    {
      "name": "Tempo",
      "one_line_profile": "Declarative compiled dynamic deep learning system",
      "detailed_description": "A system for end-to-end compiled dynamic deep learning, offering declarative interfaces and efficient execution.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_compilation",
        "deep_learning_system"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/lsds/Tempo",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "compiler",
        "deep-learning",
        "system"
      ],
      "id": 305
    },
    {
      "name": "pytorch-quantity",
      "one_line_profile": "Automated 8-bit quantization conversion tool for PyTorch",
      "detailed_description": "A tool for performing post-training quantization on PyTorch models, specifically using KL divergence for calibration.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "model_compression"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/lswzjuer/pytorch-quantity",
      "help_website": [],
      "license": null,
      "tags": [
        "quantization",
        "pytorch",
        "post-training-quantization"
      ],
      "id": 306
    },
    {
      "name": "SOBER",
      "one_line_profile": "Fast Bayesian optimization with GPU acceleration",
      "detailed_description": "A tool for performing fast Bayesian optimization, quadrature, and inference over arbitrary domains, leveraging GPU acceleration.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "optimization",
        "scientific_modeling"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/ma921/SOBER",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "bayesian-optimization",
        "gpu-acceleration",
        "inference"
      ],
      "id": 307
    },
    {
      "name": "altius",
      "one_line_profile": "Small ONNX inference runtime written in Rust",
      "detailed_description": "A lightweight inference runtime for ONNX models, implemented in Rust, focusing on portability and performance.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_runtime",
        "model_execution"
      ],
      "application_level": "solver",
      "primary_language": "Rust",
      "repo_url": "https://github.com/maekawatoshiki/altius",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "onnx",
        "runtime",
        "rust"
      ],
      "id": 308
    },
    {
      "name": "DeepStream-Yolo",
      "one_line_profile": "NVIDIA DeepStream implementation for YOLO models",
      "detailed_description": "A widely used integration tool that enables running YOLO object detection models within the NVIDIA DeepStream SDK pipeline.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "pipeline_integration"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/marcoslucianops/DeepStream-Yolo",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "deepstream",
        "yolo",
        "nvidia"
      ],
      "id": 309
    },
    {
      "name": "ONN-QAT-SQL",
      "one_line_profile": "Simulation scripts for Optical Neural Networks under photon shot noise",
      "detailed_description": "A set of scripts for training neural networks resistant to photon shot noise using quantization-aware training, including simulation of neural network performance under shot noise conditions.",
      "domains": [
        "AI6-04",
        "Physics"
      ],
      "subtask_category": [
        "simulation",
        "quantization_aware_training"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/mcmahon-lab/ONN-QAT-SQL",
      "help_website": [],
      "license": "CC-BY-4.0",
      "tags": [
        "optical-neural-networks",
        "simulation",
        "quantization"
      ],
      "id": 310
    },
    {
      "name": "FQ-ViT",
      "one_line_profile": "Post-Training Quantization for Fully Quantized Vision Transformers",
      "detailed_description": "Implementation of FQ-ViT, a method for post-training quantization specifically designed for Vision Transformers to achieve full quantization.",
      "domains": [
        "AI6-04",
        "CV"
      ],
      "subtask_category": [
        "quantization",
        "model_compression"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/megvii-research/FQ-ViT",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "vision-transformer",
        "post-training-quantization",
        "ptq"
      ],
      "id": 311
    },
    {
      "name": "Sparsebit",
      "one_line_profile": "Model compression and acceleration toolbox for PyTorch",
      "detailed_description": "A comprehensive toolbox for model compression and acceleration based on PyTorch, facilitating quantization and pruning tasks.",
      "domains": [
        "AI6-04"
      ],
      "subtask_category": [
        "model_compression",
        "acceleration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/megvii-research/Sparsebit",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "compression",
        "acceleration",
        "pytorch"
      ],
      "id": 312
    },
    {
      "name": "MSLK",
      "one_line_profile": "Meta Superintelligence Labs Kernels for GenAI optimization",
      "detailed_description": "A collection of PyTorch GPU operator libraries designed and optimized for GenAI training and inference, including FP8 row-wise quantization and collective communications.",
      "domains": [
        "AI6-04",
        "HPC"
      ],
      "subtask_category": [
        "kernel_optimization",
        "quantization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/meta-pytorch/MSLK",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "gpu-kernels",
        "fp8",
        "genai"
      ],
      "id": 313
    },
    {
      "name": "Tritonbench",
      "one_line_profile": "Benchmarking framework for PyTorch operators and Triton kernels",
      "detailed_description": "A collection of PyTorch custom operators with example inputs designed to measure their performance, serving as a benchmarking tool for Triton kernels.",
      "domains": [
        "AI6-04"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_profiling"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/meta-pytorch/tritonbench",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "triton",
        "benchmarking",
        "pytorch"
      ],
      "id": 314
    },
    {
      "name": "TritonParse",
      "one_line_profile": "Compiler Tracer and Visualizer for Triton Kernels",
      "detailed_description": "A tool for tracing, visualizing, and reproducing Triton compiler behavior, aiding in the debugging and optimization of Triton kernels.",
      "domains": [
        "AI6-04"
      ],
      "subtask_category": [
        "compiler_debugging",
        "visualization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/meta-pytorch/tritonparse",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "triton",
        "compiler",
        "visualization"
      ],
      "id": 315
    },
    {
      "name": "Olive",
      "one_line_profile": "Model optimization toolchain for hardware acceleration",
      "detailed_description": "A tool to simplify ML model finetuning, conversion, quantization, and optimization for deployment on CPUs, GPUs, and NPUs.",
      "domains": [
        "AI6-04"
      ],
      "subtask_category": [
        "model_optimization",
        "quantization"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/Olive",
      "help_website": [
        "https://microsoft.github.io/Olive/"
      ],
      "license": "MIT",
      "tags": [
        "onnx",
        "optimization",
        "quantization"
      ],
      "id": 316
    },
    {
      "name": "SwitchML",
      "one_line_profile": "Switch-based training acceleration for machine learning",
      "detailed_description": "A project implementing switch-based training acceleration for machine learning workloads, optimizing communication in distributed training.",
      "domains": [
        "AI6",
        "HPC"
      ],
      "subtask_category": [
        "distributed_training",
        "acceleration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/SwitchML",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "distributed-training",
        "networking",
        "acceleration"
      ],
      "id": 317
    },
    {
      "name": "TileFusion",
      "one_line_profile": "C++ macro kernel template library for CUDA tile processing",
      "detailed_description": "An experimental C++ macro kernel template library that elevates the abstraction level in CUDA C for efficient tile processing.",
      "domains": [
        "AI6-04",
        "HPC"
      ],
      "subtask_category": [
        "kernel_optimization",
        "cuda_programming"
      ],
      "application_level": "library",
      "primary_language": "Cuda",
      "repo_url": "https://github.com/microsoft/TileFusion",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "cuda",
        "kernel-templates",
        "optimization"
      ],
      "id": 318
    },
    {
      "name": "Antares",
      "one_line_profile": "Automatic engine for multi-platform kernel generation",
      "detailed_description": "An automatic engine for multi-platform kernel generation and optimization, supporting various backends like CPU, CUDA, ROCm, and OpenCL.",
      "domains": [
        "AI6-04"
      ],
      "subtask_category": [
        "kernel_generation",
        "compiler"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/microsoft/antares",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "kernel-generation",
        "compiler",
        "multi-platform"
      ],
      "id": 319
    },
    {
      "name": "Hummingbird",
      "one_line_profile": "Compiles traditional ML models into tensor computation",
      "detailed_description": "A library that compiles trained traditional ML models (like decision trees) into tensor computation for faster inference on deep learning frameworks.",
      "domains": [
        "AI6-04"
      ],
      "subtask_category": [
        "model_compilation",
        "inference_acceleration"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/hummingbird",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ml-compilation",
        "tensor-computation",
        "inference"
      ],
      "id": 320
    },
    {
      "name": "NNI",
      "one_line_profile": "AutoML toolkit for model compression and NAS",
      "detailed_description": "An open source AutoML toolkit for automating machine learning lifecycle, including feature engineering, neural architecture search (NAS), model compression, and hyper-parameter tuning.",
      "domains": [
        "AI6-04"
      ],
      "subtask_category": [
        "automl",
        "model_compression",
        "nas"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/nni",
      "help_website": [
        "https://nni.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "automl",
        "nas",
        "hyperparameter-tuning"
      ],
      "id": 321
    },
    {
      "name": "ONNX Runtime",
      "one_line_profile": "High performance ML inferencing and training accelerator",
      "detailed_description": "A cross-platform, high performance machine learning inferencing and training accelerator that supports models from PyTorch, TensorFlow/Keras, TFLite, scikit-learn, and other frameworks.",
      "domains": [
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "runtime"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/microsoft/onnxruntime",
      "help_website": [
        "https://onnxruntime.ai/"
      ],
      "license": "MIT",
      "tags": [
        "inference",
        "onnx",
        "acceleration"
      ],
      "id": 322
    },
    {
      "name": "onnxruntime-extensions",
      "one_line_profile": "Pre- and post-processing library for ONNX Runtime",
      "detailed_description": "A specialized library providing custom operators for pre- and post-processing to support ONNX Runtime, enabling end-to-end model execution.",
      "domains": [
        "AI6-04"
      ],
      "subtask_category": [
        "data_processing",
        "custom_operators"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/microsoft/onnxruntime-extensions",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "onnx",
        "preprocessing",
        "extensions"
      ],
      "id": 323
    },
    {
      "name": "triton-shared",
      "one_line_profile": "Shared Middle-Layer for Triton Compilation",
      "detailed_description": "A shared middle-layer infrastructure for Triton compilation, facilitating the development of Triton backends and compiler optimizations.",
      "domains": [
        "AI6-04"
      ],
      "subtask_category": [
        "compiler_infrastructure"
      ],
      "application_level": "library",
      "primary_language": "MLIR",
      "repo_url": "https://github.com/microsoft/triton-shared",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "triton",
        "compiler",
        "mlir"
      ],
      "id": 324
    },
    {
      "name": "Vidur",
      "one_line_profile": "Large-scale simulation framework for LLM inference",
      "detailed_description": "A large-scale simulation framework designed to model and analyze the performance of Large Language Model (LLM) inference systems.",
      "domains": [
        "AI6-04"
      ],
      "subtask_category": [
        "performance_simulation",
        "profiling"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/vidur",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "simulation",
        "inference"
      ],
      "id": 325
    },
    {
      "name": "BoxMOT",
      "one_line_profile": "Pluggable multi-object tracking modules",
      "detailed_description": "A collection of state-of-the-art multi-object tracking modules pluggable into segmentation, object detection, and pose estimation models.",
      "domains": [
        "AI6",
        "CV"
      ],
      "subtask_category": [
        "object_tracking",
        "image_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/mikel-brostrom/boxmot",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "tracking",
        "computer-vision",
        "mot"
      ],
      "id": 326
    },
    {
      "name": "MindNLP",
      "one_line_profile": "NLP library for MindSpore with Huggingface compatibility",
      "detailed_description": "An NLP library that enables running Transformers/Diffusers models on MindSpore with seamless compatibility and acceleration.",
      "domains": [
        "AI6-04",
        "NLP"
      ],
      "subtask_category": [
        "natural_language_processing",
        "model_training"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/mindspore-lab/mindnlp",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "mindspore",
        "nlp",
        "transformers"
      ],
      "id": 327
    },
    {
      "name": "AMC",
      "one_line_profile": "AutoML for Model Compression on Mobile Devices",
      "detailed_description": "A tool implementing AutoML for Model Compression (AMC), leveraging reinforcement learning to automate the compression of deep neural networks for mobile devices.",
      "domains": [
        "AI6-04"
      ],
      "subtask_category": [
        "model_compression",
        "automl"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/mit-han-lab/amc",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "compression",
        "automl",
        "mobile-ai"
      ],
      "id": 328
    },
    {
      "name": "DistriFusion",
      "one_line_profile": "Distributed Parallel Inference for Diffusion Models",
      "detailed_description": "A framework for distributed parallel inference enabling high-resolution diffusion model generation by splitting computation across devices.",
      "domains": [
        "AI6-04"
      ],
      "subtask_category": [
        "distributed_inference",
        "image_generation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/mit-han-lab/distrifuser",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "diffusion-models",
        "distributed-inference",
        "parallel-computing"
      ],
      "id": 329
    },
    {
      "name": "AWQ",
      "one_line_profile": "Activation-aware Weight Quantization for LLMs",
      "detailed_description": "A tool for Activation-aware Weight Quantization (AWQ), enabling efficient compression and acceleration of Large Language Models.",
      "domains": [
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "model_compression"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/mit-han-lab/llm-awq",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "quantization",
        "awq"
      ],
      "id": 330
    },
    {
      "name": "SmoothQuant",
      "one_line_profile": "Post-Training Quantization for Large Language Models",
      "detailed_description": "A framework for accurate and efficient post-training quantization of Large Language Models, enabling 8-bit weight and activation quantization.",
      "domains": [
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "model_compression"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/mit-han-lab/smoothquant",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "quantization",
        "post-training"
      ],
      "id": 331
    },
    {
      "name": "TorchSparse",
      "one_line_profile": "Efficient Sparse Convolution Framework on GPUs",
      "detailed_description": "A high-performance computing framework for efficient training and inference of sparse convolutions on GPUs.",
      "domains": [
        "AI6-04",
        "HPC"
      ],
      "subtask_category": [
        "sparse_computation",
        "acceleration"
      ],
      "application_level": "library",
      "primary_language": "Cuda",
      "repo_url": "https://github.com/mit-han-lab/torchsparse",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "sparse-convolution",
        "gpu",
        "acceleration"
      ],
      "id": 332
    },
    {
      "name": "WebLLM",
      "one_line_profile": "High-performance in-browser LLM inference engine",
      "detailed_description": "A high-performance in-browser inference engine for Large Language Models, bringing hardware-accelerated AI to web browsers.",
      "domains": [
        "AI6-04"
      ],
      "subtask_category": [
        "inference_engine",
        "web_ai"
      ],
      "application_level": "solver",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/mlc-ai/web-llm",
      "help_website": [
        "https://webllm.mlc.ai/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "webgpu",
        "inference"
      ],
      "id": 333
    },
    {
      "name": "mpi4jax",
      "one_line_profile": "Zero-copy MPI communication for JAX",
      "detailed_description": "A library enabling zero-copy MPI communication of JAX arrays, facilitating the development of turbo-charged HPC applications in Python.",
      "domains": [
        "AI6",
        "HPC"
      ],
      "subtask_category": [
        "distributed_computing",
        "hpc"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/mpi4jax/mpi4jax",
      "help_website": [
        "https://mpi4jax.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "jax",
        "mpi",
        "hpc"
      ],
      "id": 334
    },
    {
      "name": "FastVGGT",
      "one_line_profile": "Training-free acceleration library for Visual Geometry Transformers",
      "detailed_description": "A library implementing training-free acceleration techniques for Visual Geometry Transformers (VGGT), optimizing inference speed without retraining.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "model_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/mystorm16/FastVGGT",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "transformer",
        "acceleration",
        "vision"
      ],
      "id": 335
    },
    {
      "name": "onnxruntime-rs",
      "one_line_profile": "Rust bindings for ONNX Runtime inference engine",
      "detailed_description": "A Rust wrapper for Microsoft's ONNX Runtime, enabling high-performance machine learning inference within Rust applications.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_runtime",
        "model_deployment"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/nbigaouette/onnxruntime-rs",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rust",
        "onnx",
        "inference"
      ],
      "id": 336
    },
    {
      "name": "DeepSparse",
      "one_line_profile": "Sparsity-aware deep learning inference runtime for CPUs",
      "detailed_description": "An inference runtime engine optimized for sparse neural networks, delivering GPU-class performance on commodity CPUs by leveraging sparsity.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "inference_runtime"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/neuralmagic/deepsparse",
      "help_website": [
        "https://docs.neuralmagic.com/deepsparse/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "sparsity",
        "inference",
        "cpu-optimization"
      ],
      "id": 337
    },
    {
      "name": "SparseZoo",
      "one_line_profile": "Repository of sparse models and recipes for inference optimization",
      "detailed_description": "A repository and library providing pre-sparsified models and optimization recipes to facilitate the deployment of high-performance sparse neural networks.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_optimization",
        "model_repository"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/neuralmagic/sparsezoo",
      "help_website": [
        "https://docs.neuralmagic.com/sparsezoo/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "sparse-models",
        "optimization-recipes",
        "neural-networks"
      ],
      "id": 338
    },
    {
      "name": "Sparsify",
      "one_line_profile": "Tool for model sparsification and pruning to accelerate inference",
      "detailed_description": "A tool enabling users to apply sparsification technologies (pruning and quantization) to neural networks to accelerate inference performance.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_compression",
        "quantization",
        "pruning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/neuralmagic/sparsify",
      "help_website": [
        "https://docs.neuralmagic.com/sparsify/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "pruning",
        "quantization",
        "optimization"
      ],
      "id": 339
    },
    {
      "name": "vLLM-gfx906",
      "one_line_profile": "Port of vLLM inference engine for AMD gfx906 GPUs",
      "detailed_description": "A specialized fork of the vLLM library optimized for AMD gfx906 GPUs (e.g., Radeon VII, MI50, MI60), enabling high-throughput LLM inference on this hardware.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_runtime",
        "llm_serving"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/nlzy/vllm-gfx906",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "amd",
        "rocm",
        "vllm",
        "llm"
      ],
      "id": 340
    },
    {
      "name": "nndeploy",
      "one_line_profile": "Cross-platform high-performance AI model deployment framework",
      "detailed_description": "A comprehensive framework for AI model deployment that supports multiple inference backends (TensorRT, ONNX Runtime, OpenVINO, etc.) and provides a unified C++ interface.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_deployment",
        "inference_runtime"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/nndeploy/nndeploy",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "deployment",
        "inference",
        "cross-platform"
      ],
      "id": 341
    },
    {
      "name": "NNTile",
      "one_line_profile": "Task-based parallel neural network training framework for HPC",
      "detailed_description": "A neural network training framework built on a task-based parallel programming paradigm, designed for high-performance computing environments.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "distributed_training",
        "hpc_acceleration"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/nntile/nntile",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "hpc",
        "parallel-computing",
        "training"
      ],
      "id": 342
    },
    {
      "name": "tf-to-xla-to-wasm",
      "one_line_profile": "Toolchain to compile TensorFlow graphs to WebAssembly via XLA",
      "detailed_description": "A utility that compiles TensorFlow graphs into WebAssembly modules using XLA, enabling model execution in web environments.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_compilation",
        "webassembly_export"
      ],
      "application_level": "solver",
      "primary_language": "Shell",
      "repo_url": "https://github.com/nuchi/tf-to-xla-to-wasm",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "tensorflow",
        "xla",
        "webassembly",
        "compiler"
      ],
      "id": 343
    },
    {
      "name": "DeepCompressor",
      "one_line_profile": "Compression toolbox for LLMs and diffusion models",
      "detailed_description": "A model compression toolbox specifically designed for Large Language Models and Diffusion Models, offering quantization and optimization techniques.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_compression",
        "quantization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/nunchaku-tech/deepcompressor",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "diffusion",
        "compression"
      ],
      "id": 344
    },
    {
      "name": "Nunchaku",
      "one_line_profile": "Inference engine and quantization library for 4-bit diffusion models",
      "detailed_description": "A library implementing SVDQuant for 4-bit quantization of diffusion models, enabling efficient inference by absorbing outliers into low-rank components.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "inference_acceleration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/nunchaku-tech/nunchaku",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "diffusion-models",
        "quantization",
        "svdquant"
      ],
      "id": 345
    },
    {
      "name": "ComfyUI-FSampler",
      "one_line_profile": "Acceleration layer for diffusion sampling in ComfyUI",
      "detailed_description": "A training-free, sampler-agnostic acceleration layer integrated into ComfyUI to speed up diffusion model sampling.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "image_generation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/obisin/ComfyUI-FSampler",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "comfyui",
        "diffusion",
        "acceleration"
      ],
      "id": 346
    },
    {
      "name": "ONNX-TensorRT",
      "one_line_profile": "TensorRT backend for parsing and executing ONNX models",
      "detailed_description": "A library that parses ONNX models and executes them using the NVIDIA TensorRT inference engine, serving as a critical bridge for deploying ONNX models on NVIDIA GPUs.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_compilation",
        "inference_acceleration"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/onnx/onnx-tensorrt",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "onnx",
        "tensorrt",
        "nvidia"
      ],
      "id": 347
    },
    {
      "name": "OpenVINO Training Extensions",
      "one_line_profile": "Framework for training, optimizing, and deploying CV models with OpenVINO",
      "detailed_description": "A framework that provides workflows for training, evaluating, optimizing, and deploying computer vision models, specifically tailored for the OpenVINO toolkit.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_optimization",
        "deployment_workflow"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/open-edge-platform/training_extensions",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "openvino",
        "computer-vision",
        "optimization"
      ],
      "id": 348
    },
    {
      "name": "MMDeploy",
      "one_line_profile": "Model deployment framework supporting export to multiple inference backends",
      "detailed_description": "A comprehensive model deployment toolbox that provides a unified pipeline to export OpenMMLab models to various inference backends like ONNX Runtime, TensorRT, and ncnn.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_deployment",
        "model_conversion"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/open-mmlab/mmdeploy",
      "help_website": [
        "https://mmdeploy.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "deployment",
        "openmmlab",
        "conversion"
      ],
      "id": 349
    },
    {
      "name": "MMRazor",
      "one_line_profile": "Model compression toolbox for pruning, quantization, and distillation",
      "detailed_description": "A model compression toolbox that includes algorithms for neural network pruning, quantization, knowledge distillation, and architecture search.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_compression",
        "quantization",
        "pruning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/open-mmlab/mmrazor",
      "help_website": [
        "https://mmrazor.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "compression",
        "distillation",
        "nas"
      ],
      "id": 350
    },
    {
      "name": "DI-hpc",
      "one_line_profile": "High-performance operator library for Reinforcement Learning",
      "detailed_description": "A library of optimized CUDA and Triton kernels specifically designed to accelerate Reinforcement Learning operations.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "kernel_optimization",
        "hpc_acceleration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/opendilab/DI-hpc",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "reinforcement-learning",
        "cuda",
        "triton"
      ],
      "id": 351
    },
    {
      "name": "ReaLHF",
      "one_line_profile": "Efficient RLHF training system for LLMs",
      "detailed_description": "A system for efficient Reinforcement Learning from Human Feedback (RLHF) training of Large Language Models, utilizing parameter reallocation techniques.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "distributed_training",
        "llm_optimization"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/openpsi-project/ReaLHF",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rlhf",
        "llm",
        "training-system"
      ],
      "id": 352
    },
    {
      "name": "XLA",
      "one_line_profile": "Domain-specific compiler for linear algebra and machine learning models",
      "detailed_description": "A machine learning compiler that optimizes linear algebra computations for execution on GPUs, CPUs, and ML accelerators.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_compilation",
        "hardware_acceleration"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/openxla/xla",
      "help_website": [
        "https://openxla.org/xla"
      ],
      "license": "Apache-2.0",
      "tags": [
        "compiler",
        "linear-algebra",
        "optimization"
      ],
      "id": 353
    },
    {
      "name": "sd4j",
      "one_line_profile": "Java implementation of Stable Diffusion pipeline using ONNX Runtime",
      "detailed_description": "A library that provides a Stable Diffusion inference pipeline in Java, leveraging ONNX Runtime for execution.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_runtime",
        "image_generation"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/oracle/sd4j",
      "help_website": [],
      "license": "UPL-1.0",
      "tags": [
        "java",
        "stable-diffusion",
        "onnx"
      ],
      "id": 354
    },
    {
      "name": "kvcached",
      "one_line_profile": "Virtualized elastic KV cache system for dynamic GPU sharing",
      "detailed_description": "A system that provides a virtualized, elastic Key-Value (KV) cache to enable efficient dynamic GPU sharing for Large Language Model inference.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_optimization",
        "resource_management"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/ovg-project/kvcached",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "kv-cache",
        "llm",
        "gpu-sharing"
      ],
      "id": 355
    },
    {
      "name": "onnx_transformers",
      "one_line_profile": "Library for accelerated NLP inference using ONNX Runtime",
      "detailed_description": "A library that provides accelerated NLP pipelines by integrating Hugging Face Transformers with ONNX Runtime for faster CPU inference.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "nlp_pipeline"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/patil-suraj/onnx_transformers",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "nlp",
        "onnx",
        "transformers"
      ],
      "id": 356
    },
    {
      "name": "BambooAI",
      "one_line_profile": "Library for conversational data analysis using LLMs",
      "detailed_description": "A Python library that leverages Large Language Models to perform conversational data discovery, analysis, and visualization.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "data_analysis",
        "scientific_visualization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pgalko/BambooAI",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "data-analysis",
        "llm",
        "visualization"
      ],
      "id": 357
    },
    {
      "name": "neural-imaging",
      "one_line_profile": "Toolbox for modeling and optimizing photo acquisition pipelines",
      "detailed_description": "A Python toolbox for modeling, simulating, and optimizing various stages of photo acquisition and distribution pipelines, including ISP and compression.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "image_processing",
        "pipeline_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pkorus/neural-imaging",
      "help_website": [],
      "license": null,
      "tags": [
        "imaging",
        "isp",
        "optimization"
      ],
      "id": 358
    },
    {
      "name": "FastV",
      "one_line_profile": "Inference acceleration library for Large Vision-Language Models",
      "detailed_description": "A plug-and-play inference acceleration library for Large Vision-Language Models (LVLMs) that reduces computational cost via token pruning.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "token_pruning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pkunlp-icler/FastV",
      "help_website": [],
      "license": null,
      "tags": [
        "lvlm",
        "acceleration",
        "pruning"
      ],
      "id": 359
    },
    {
      "name": "PlaidML",
      "one_line_profile": "Tensor compiler and runtime for deep learning on diverse hardware",
      "detailed_description": "A tensor compiler and runtime framework that enables deep learning on a wide range of hardware devices by using OpenCL and other backends.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_compilation",
        "inference_runtime"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/plaidml/plaidml",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "compiler",
        "opencl",
        "deep-learning"
      ],
      "id": 360
    },
    {
      "name": "YOLOv5-Lite",
      "one_line_profile": "Lightweight YOLOv5 implementation optimized for edge devices with NPU/GPU support",
      "detailed_description": "A lightweight version of YOLOv5 designed for edge computing devices like Raspberry Pi. It features model pruning and quantization (int8/fp16) to achieve high inference speeds (e.g., 15 FPS on RPi 4B) while maintaining reasonable accuracy, suitable for real-time object detection in resource-constrained scientific or field environments.",
      "domains": [
        "AI6-04",
        "Computer Vision"
      ],
      "subtask_category": [
        "inference_optimization",
        "quantization",
        "edge_computing"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/ppogg/YOLOv5-Lite",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "yolo",
        "quantization",
        "edge-ai",
        "ncnn",
        "mnn"
      ],
      "id": 361
    },
    {
      "name": "DinkyTrain",
      "one_line_profile": "Lightweight pre-training library with DeepSpeed integration",
      "detailed_description": "A pre-training library from Princeton NLP based on fairseq, integrated with DeepSpeed kernels. It is designed to facilitate efficient training of language models, providing a streamlined interface for research into model pre-training and optimization.",
      "domains": [
        "AI6-04",
        "NLP"
      ],
      "subtask_category": [
        "training_acceleration",
        "model_pretraining"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/princeton-nlp/DinkyTrain",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "deepspeed",
        "training",
        "nlp",
        "fairseq"
      ],
      "id": 362
    },
    {
      "name": "Prometheus-Eval",
      "one_line_profile": "LLM evaluation toolkit using Prometheus and GPT-4",
      "detailed_description": "A toolkit for evaluating Large Language Models (LLMs) by leveraging Prometheus (an open-source evaluator LLM) and GPT-4. It provides a framework for assessing model responses, enabling researchers to perform quality control and comparative analysis of generative models.",
      "domains": [
        "AI11",
        "NLP"
      ],
      "subtask_category": [
        "model_evaluation",
        "quality_control"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/prometheus-eval/prometheus-eval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-evaluation",
        "benchmark",
        "prometheus"
      ],
      "id": 363
    },
    {
      "name": "Torch-TensorRT",
      "one_line_profile": "PyTorch compiler for NVIDIA GPUs using TensorRT",
      "detailed_description": "A compiler for PyTorch/TorchScript/FX that targets NVIDIA GPUs using NVIDIA's TensorRT deep learning optimizer and runtime. It accelerates inference for PyTorch models by leveraging TensorRT's optimizations while maintaining the ease of use of the PyTorch ecosystem.",
      "domains": [
        "AI6-04",
        "AI Infra"
      ],
      "subtask_category": [
        "compiler",
        "inference_acceleration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pytorch/TensorRT",
      "help_website": [
        "https://pytorch.org/TensorRT/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "pytorch",
        "tensorrt",
        "inference",
        "compiler",
        "gpu"
      ],
      "id": 364
    },
    {
      "name": "PyTorch-ORT",
      "one_line_profile": "PyTorch acceleration via ONNX Runtime",
      "detailed_description": "A library that accelerates PyTorch model training and inference by integrating with ONNX Runtime. It allows users to run PyTorch models with the performance benefits of ONNX Runtime's optimizations without leaving the PyTorch environment.",
      "domains": [
        "AI6-04",
        "AI Infra"
      ],
      "subtask_category": [
        "inference_acceleration",
        "training_acceleration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pytorch/ort",
      "help_website": [
        "https://onnxruntime.ai/"
      ],
      "license": "MIT",
      "tags": [
        "onnx-runtime",
        "pytorch",
        "acceleration"
      ],
      "id": 365
    },
    {
      "name": "PyTorch XLA",
      "one_line_profile": "PyTorch support for XLA devices (TPU/GPU)",
      "detailed_description": "A Python package that connects PyTorch to the XLA (Accelerated Linear Algebra) compiler, enabling PyTorch models to run on high-performance XLA devices such as Google TPUs and GPUs. It provides the necessary runtime and compiler integration for scaling PyTorch workloads.",
      "domains": [
        "AI6-04",
        "AI Infra"
      ],
      "subtask_category": [
        "compiler",
        "training_acceleration",
        "inference_acceleration"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/pytorch/xla",
      "help_website": [
        "https://github.com/pytorch/xla"
      ],
      "license": "NOASSERTION",
      "tags": [
        "xla",
        "tpu",
        "pytorch",
        "compiler"
      ],
      "id": 366
    },
    {
      "name": "async_cosyvoice",
      "one_line_profile": "Accelerated inference for CosyVoice2 using vLLM",
      "detailed_description": "A tool designed to accelerate the inference of the CosyVoice2 text-to-speech model by leveraging vLLM (a high-throughput and memory-efficient LLM serving engine). It optimizes the runtime performance for audio generation tasks.",
      "domains": [
        "AI6-04",
        "Audio"
      ],
      "subtask_category": [
        "inference_acceleration",
        "audio_generation"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/qi-hua/async_cosyvoice",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "vllm",
        "cosyvoice",
        "inference",
        "tts"
      ],
      "id": 367
    },
    {
      "name": "AIMET",
      "one_line_profile": "AI Model Efficiency Toolkit for quantization and compression",
      "detailed_description": "A library that provides advanced model quantization and compression techniques for trained neural networks. It helps in reducing model size and improving inference latency on edge devices while maintaining accuracy, supporting both post-training quantization and quantization-aware training.",
      "domains": [
        "AI6-04",
        "AI Infra"
      ],
      "subtask_category": [
        "quantization",
        "model_compression"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/quic/aimet",
      "help_website": [
        "https://quic.github.io/aimet-pages/index.html"
      ],
      "license": "NOASSERTION",
      "tags": [
        "quantization",
        "compression",
        "neural-networks"
      ],
      "id": 368
    },
    {
      "name": "Chatterbox-vLLM",
      "one_line_profile": "vLLM port of the Chatterbox TTS model",
      "detailed_description": "A port of the Chatterbox Text-to-Speech model to the vLLM inference engine. This tool enables high-performance serving of the Chatterbox model, leveraging vLLM's memory management and batching capabilities for efficient audio synthesis.",
      "domains": [
        "AI6-04",
        "Audio"
      ],
      "subtask_category": [
        "inference_acceleration",
        "inference_serving"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/randombk/chatterbox-vllm",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "vllm",
        "tts",
        "inference"
      ],
      "id": 369
    },
    {
      "name": "ComfyUI_Step1X-Edit",
      "one_line_profile": "TeaCache acceleration plugin for ComfyUI",
      "detailed_description": "A plugin for ComfyUI that implements TeaCache acceleration, enabling up to 2x faster inference for generative models with minimal quality loss. It serves as an optimization tool for image generation workflows.",
      "domains": [
        "AI6-04",
        "Generative AI"
      ],
      "subtask_category": [
        "inference_acceleration",
        "caching"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/raykindle/ComfyUI_Step1X-Edit",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "comfyui",
        "acceleration",
        "teacache"
      ],
      "id": 370
    },
    {
      "name": "RZV DRP-AI TVM",
      "one_line_profile": "Apache TVM extension for Renesas DRP-AI accelerators",
      "detailed_description": "An extension package for the Apache TVM machine learning compiler, specifically designed to support Renesas DRP-AI accelerators. It enables the compilation and optimization of deep learning models for deployment on Renesas hardware.",
      "domains": [
        "AI6-04",
        "Embedded AI"
      ],
      "subtask_category": [
        "compiler",
        "hardware_acceleration"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/renesas-rz/rzv_drp-ai_tvm",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "tvm",
        "compiler",
        "drp-ai",
        "renesas"
      ],
      "id": 371
    },
    {
      "name": "FreeTensor",
      "one_line_profile": "Language and compiler for irregular tensor programs",
      "detailed_description": "A domain-specific language and compiler designed for optimizing irregular tensor programs. It targets high-performance computing scenarios where standard tensor compilers may struggle with irregular data structures or access patterns.",
      "domains": [
        "AI6-04",
        "HPC"
      ],
      "subtask_category": [
        "compiler",
        "tensor_computation"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/roastduck/FreeTensor",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "compiler",
        "tensor",
        "hpc",
        "irregular-computation"
      ],
      "id": 372
    },
    {
      "name": "Roboflow Inference",
      "one_line_profile": "High-performance inference server for computer vision",
      "detailed_description": "A comprehensive inference server and library for running computer vision models on various devices (edge to cloud). It provides a unified interface for deploying models, managing inference pipelines, and optimizing runtime performance.",
      "domains": [
        "AI6-04",
        "Computer Vision"
      ],
      "subtask_category": [
        "inference_serving",
        "deployment"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/roboflow/inference",
      "help_website": [
        "https://inference.roboflow.com/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "inference-server",
        "computer-vision",
        "deployment"
      ],
      "id": 373
    },
    {
      "name": "l2hmc-qcd",
      "one_line_profile": "L2HMC algorithm implementation for Lattice QCD simulations",
      "detailed_description": "An implementation of the L2HMC (Learning to Hamiltonian Monte Carlo) algorithm specifically applied to simulations in Lattice Quantum Chromodynamics (QCD). It serves as a computational tool for physics research, enabling more efficient sampling in high-dimensional spaces.",
      "domains": [
        "AI4S",
        "Physics"
      ],
      "subtask_category": [
        "simulation",
        "lattice_qcd",
        "sampling"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/saforem2/l2hmc-qcd",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "l2hmc",
        "lattice-qcd",
        "physics-simulation",
        "mcmc"
      ],
      "id": 374
    },
    {
      "name": "HLOEnv",
      "one_line_profile": "RL environment for XLA compiler optimization research",
      "detailed_description": "A research environment based on XLA (Accelerated Linear Algebra) designed for deep learning compiler optimization. It allows researchers to apply reinforcement learning techniques to optimize HLO (High Level Optimizer) passes and graph transformations.",
      "domains": [
        "AI6-04",
        "Compiler Research"
      ],
      "subtask_category": [
        "compiler_optimization",
        "reinforcement_learning"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/sail-sg/hloenv",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "xla",
        "compiler",
        "reinforcement-learning",
        "optimization"
      ],
      "id": 375
    },
    {
      "name": "Sbnb Linux",
      "one_line_profile": "Linux distribution optimized for AI workloads",
      "detailed_description": "A specialized Linux distribution designed for AI computers, automating the setup and execution of AI workloads like vLLM, SGLang, and RAG pipelines. It serves as an infrastructure platform to facilitate reproducible AI research and deployment.",
      "domains": [
        "AI6",
        "AI Infra"
      ],
      "subtask_category": [
        "environment_management",
        "infrastructure"
      ],
      "application_level": "platform",
      "primary_language": "Shell",
      "repo_url": "https://github.com/sbnb-io/sbnb",
      "help_website": [
        "https://sbnb.io"
      ],
      "license": "MIT",
      "tags": [
        "linux-distro",
        "ai-infrastructure",
        "vllm"
      ],
      "id": 376
    },
    {
      "name": "Red Candle",
      "one_line_profile": "Ruby interface for local LLM inference via Candle",
      "detailed_description": "A Ruby gem that provides an interface to run state-of-the-art language models locally, powered by the Rust-based Candle framework. It supports Metal/CUDA acceleration, enabling Ruby-based scientific workflows to leverage efficient local LLM inference.",
      "domains": [
        "AI6-04",
        "NLP"
      ],
      "subtask_category": [
        "inference_acceleration",
        "inference_serving"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/scientist-labs/red-candle",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ruby",
        "candle",
        "llm",
        "inference"
      ],
      "id": 377
    },
    {
      "name": "ScatterMoE",
      "one_line_profile": "Triton-based Sparse Mixture of Experts implementation",
      "detailed_description": "A high-performance implementation of Sparse Mixture of Experts (MoE) using OpenAI's Triton language. It provides efficient kernels for training and inference of MoE models, serving as a building block for large-scale model acceleration.",
      "domains": [
        "AI6-04",
        "AI Infra"
      ],
      "subtask_category": [
        "training_acceleration",
        "inference_acceleration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/shawntan/scattermoe",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "triton",
        "moe",
        "mixture-of-experts",
        "acceleration"
      ],
      "id": 378
    },
    {
      "name": "TensorRT_Pro",
      "one_line_profile": "C++ library for easy TensorRT integration",
      "detailed_description": "A C++ library that simplifies the integration and usage of NVIDIA TensorRT. It provides a high-level API for loading models, managing memory, and executing inference, aiming to streamline the deployment of accelerated deep learning models.",
      "domains": [
        "AI6-04",
        "AI Infra"
      ],
      "subtask_category": [
        "inference_acceleration",
        "deployment"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/shouxieai/tensorRT_Pro",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "tensorrt",
        "c++",
        "inference",
        "wrapper"
      ],
      "id": 379
    },
    {
      "name": "chatGLM-6B-QLoRA",
      "one_line_profile": "Efficient fine-tuning and quantization implementation for ChatGLM models",
      "detailed_description": "A specific implementation for 4-bit QLoRA fine-tuning of ChatGLM-6B/ChatGLM2-6B models, including LoRA model merging and 4-bit quantization capabilities.",
      "domains": [
        "AI6-04"
      ],
      "subtask_category": [
        "model_quantization",
        "fine_tuning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/shuxueslpi/chatGLM-6B-QLoRA",
      "help_website": [],
      "license": null,
      "tags": [
        "qlora",
        "quantization",
        "llm",
        "chatglm"
      ],
      "id": 380
    },
    {
      "name": "Tacker",
      "one_line_profile": "Tensor-CUDA Core Kernel Fusion for Improving GPU Utilization",
      "detailed_description": "A kernel fusion tool that leverages Tensor Cores and CUDA Cores to improve GPU utilization while ensuring QoS, specifically designed for optimizing deep learning workloads.",
      "domains": [
        "AI6-04"
      ],
      "subtask_category": [
        "kernel_optimization",
        "gpu_acceleration"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/sjtu-epcc/Tacker",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "kernel-fusion",
        "cuda",
        "gpu-optimization"
      ],
      "id": 381
    },
    {
      "name": "ArcticInference",
      "one_line_profile": "vLLM plugin for high-throughput, low-latency inference",
      "detailed_description": "A plugin for vLLM designed to enable high-throughput and low-latency inference, specifically optimized for Snowflake's Arctic models and enterprise-grade workloads.",
      "domains": [
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "model_serving"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/snowflakedb/ArcticInference",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "vllm",
        "inference",
        "llm"
      ],
      "id": 382
    },
    {
      "name": "llms_tool",
      "one_line_profile": "Toolkit for LLM training, testing, quantization and deployment",
      "detailed_description": "A comprehensive tool based on HuggingFace for Large Language Model training (Pre-training, SFT, RM, PPO, DPO), testing, quantization, and model fusion.",
      "domains": [
        "AI6-04"
      ],
      "subtask_category": [
        "model_training",
        "model_quantization",
        "fine_tuning"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/stanleylsx/llms_tool",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "quantization",
        "fine-tuning",
        "rlhf"
      ],
      "id": 383
    },
    {
      "name": "catgrad",
      "one_line_profile": "A categorical deep learning compiler",
      "detailed_description": "A compiler for deep learning that leverages categorical concepts, providing a framework for compiling and optimizing neural network models.",
      "domains": [
        "AI6-04"
      ],
      "subtask_category": [
        "compiler",
        "model_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/statusfailed/catgrad",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "compiler",
        "deep-learning",
        "category-theory"
      ],
      "id": 384
    },
    {
      "name": "llm_finetuning",
      "one_line_profile": "Wrapper for LLM fine-tuning and inference with quantization",
      "detailed_description": "A convenient wrapper tool for fine-tuning and running inference on Large Language Models, supporting quantization techniques like GPTQ and bitsandbytes.",
      "domains": [
        "AI6-04"
      ],
      "subtask_category": [
        "fine_tuning",
        "model_quantization",
        "inference"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/taprosoft/llm_finetuning",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "gptq",
        "bitsandbytes",
        "fine-tuning"
      ],
      "id": 385
    },
    {
      "name": "Cube Studio",
      "one_line_profile": "Cloud-native one-stop machine learning and MLOps platform",
      "detailed_description": "An open-source cloud-native AI platform supporting the full MLOps lifecycle, including distributed training, hyperparameter search, inference serving, and pipeline orchestration. Supports various frameworks and hardware ecosystems.",
      "domains": [
        "AI6"
      ],
      "subtask_category": [
        "mlops",
        "distributed_training",
        "inference_serving"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/tencentmusic/cube-studio",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "mlops",
        "distributed-training",
        "cloud-native",
        "kubernetes"
      ],
      "id": 386
    },
    {
      "name": "Tensara",
      "one_line_profile": "Competitive GPU kernel optimization platform",
      "detailed_description": "A platform designed for optimizing GPU kernels, likely providing tools or frameworks to benchmark and improve kernel performance for deep learning or scientific computing.",
      "domains": [
        "AI6-04"
      ],
      "subtask_category": [
        "kernel_optimization",
        "gpu_acceleration"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/tensara/tensara",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "gpu",
        "optimization",
        "kernel"
      ],
      "id": 387
    },
    {
      "name": "taco",
      "one_line_profile": "The Tensor Algebra Compiler",
      "detailed_description": "A compiler that computes sparse tensor expressions on CPUs and GPUs, automatically generating efficient code for complex tensor algebra operations used in scientific computing and machine learning.",
      "domains": [
        "AI6-04"
      ],
      "subtask_category": [
        "compiler",
        "tensor_algebra",
        "sparse_computation"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/tensor-compiler/taco",
      "help_website": [
        "http://tensor-compiler.org"
      ],
      "license": "NOASSERTION",
      "tags": [
        "compiler",
        "tensor-algebra",
        "sparse-tensors"
      ],
      "id": 388
    },
    {
      "name": "TensorFlow Model Optimization Toolkit",
      "one_line_profile": "Toolkit to optimize ML models for deployment",
      "detailed_description": "A suite of tools for optimizing machine learning models for deployment and execution, including techniques like quantization and pruning to reduce model size and improve latency.",
      "domains": [
        "AI6-04"
      ],
      "subtask_category": [
        "model_quantization",
        "model_pruning",
        "optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/tensorflow/model-optimization",
      "help_website": [
        "https://www.tensorflow.org/model_optimization"
      ],
      "license": "Apache-2.0",
      "tags": [
        "tensorflow",
        "quantization",
        "pruning",
        "optimization"
      ],
      "id": 389
    },
    {
      "name": "HyperPose",
      "one_line_profile": "Library for Fast and Flexible Human Pose Estimation",
      "detailed_description": "A library built on TensorLayer for high-performance human pose estimation, focusing on acceleration and flexibility for real-time applications and research.",
      "domains": [
        "AI6-04"
      ],
      "subtask_category": [
        "pose_estimation",
        "inference_acceleration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/tensorlayer/HyperPose",
      "help_website": [],
      "license": null,
      "tags": [
        "pose-estimation",
        "tensorlayer",
        "acceleration"
      ],
      "id": 390
    },
    {
      "name": "tt-forge-fe",
      "one_line_profile": "Graph compiler for Tenstorrent hardware",
      "detailed_description": "A graph compiler frontend designed to optimize and transform computational graphs for deep learning models, specifically targeting Tenstorrent's AI hardware accelerators.",
      "domains": [
        "AI6-04"
      ],
      "subtask_category": [
        "compiler",
        "graph_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/tenstorrent/tt-forge-fe",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "compiler",
        "tenstorrent",
        "graph-optimization"
      ],
      "id": 391
    },
    {
      "name": "tt-xla",
      "one_line_profile": "PJRT device implementation for Tenstorrent AI Compiler",
      "detailed_description": "Implementation of a PJRT (Pretty Just-In-Time Runtime) device for Tenstorrent's AI compiler stack, enabling XLA (Accelerated Linear Algebra) compatibility.",
      "domains": [
        "AI6-04"
      ],
      "subtask_category": [
        "compiler",
        "backend_integration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/tenstorrent/tt-xla",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "xla",
        "compiler",
        "tenstorrent"
      ],
      "id": 392
    },
    {
      "name": "SageAttention",
      "one_line_profile": "Quantized Attention acceleration library",
      "detailed_description": "A library implementing Quantized Attention that achieves significant speedups compared to FlashAttention without losing accuracy, applicable to language, image, and video models.",
      "domains": [
        "AI6-04"
      ],
      "subtask_category": [
        "attention_acceleration",
        "quantization"
      ],
      "application_level": "library",
      "primary_language": "Cuda",
      "repo_url": "https://github.com/thu-ml/SageAttention",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "attention",
        "quantization",
        "acceleration",
        "cuda"
      ],
      "id": 393
    },
    {
      "name": "SpargeAttn",
      "one_line_profile": "Training-free sparse attention for inference acceleration",
      "detailed_description": "A library implementing SpargeAttention, a training-free sparse attention mechanism designed to accelerate model inference across various architectures.",
      "domains": [
        "AI6-04"
      ],
      "subtask_category": [
        "attention_acceleration",
        "sparse_computation"
      ],
      "application_level": "library",
      "primary_language": "Cuda",
      "repo_url": "https://github.com/thu-ml/SpargeAttn",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "sparse-attention",
        "inference-acceleration",
        "cuda"
      ],
      "id": 394
    },
    {
      "name": "DAMO-YOLO",
      "one_line_profile": "Fast and accurate object detection method with NAS and distillation",
      "detailed_description": "A high-performance object detection framework incorporating Neural Architecture Search (NAS) backbones, efficient RepGFPN, and distillation enhancement for accelerated inference.",
      "domains": [
        "AI6-04"
      ],
      "subtask_category": [
        "object_detection",
        "model_distillation",
        "nas"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/tinyvision/DAMO-YOLO",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "object-detection",
        "yolo",
        "nas",
        "distillation"
      ],
      "id": 395
    },
    {
      "name": "TonY",
      "one_line_profile": "Framework to natively run deep learning on Apache Hadoop",
      "detailed_description": "TonY (TensorFlow on YARN) is a framework for running distributed deep learning jobs (TensorFlow, PyTorch, etc.) on Apache Hadoop clusters, enabling scalable scientific computing.",
      "domains": [
        "AI6"
      ],
      "subtask_category": [
        "distributed_training",
        "job_scheduling"
      ],
      "application_level": "platform",
      "primary_language": "Java",
      "repo_url": "https://github.com/tony-framework/TonY",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "hadoop",
        "distributed-training",
        "yarn"
      ],
      "id": 396
    },
    {
      "name": "llama.onnx",
      "one_line_profile": "ONNX export and quantization tools for LLaMa/RWKV",
      "detailed_description": "A set of tools and scripts for converting LLaMa and RWKV models to ONNX format and performing quantization to enable efficient inference.",
      "domains": [
        "AI6-04"
      ],
      "subtask_category": [
        "model_quantization",
        "model_conversion"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/tpoisonooo/llama.onnx",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "onnx",
        "quantization",
        "llama",
        "rwkv"
      ],
      "id": 397
    },
    {
      "name": "GDS3D",
      "one_line_profile": "3D hardware accelerated viewer for GDSII IC layouts",
      "detailed_description": "An application that interprets IC layouts (GDSII files) and renders them in 3D, allowing real-time control over camera position and angle for inspecting chip designs.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "scientific_visualization",
        "chip_design"
      ],
      "application_level": "application",
      "primary_language": "C++",
      "repo_url": "https://github.com/trilomix/GDS3D",
      "help_website": [],
      "license": "GPL-2.0",
      "tags": [
        "gdsii",
        "3d-rendering",
        "ic-layout",
        "visualization"
      ],
      "id": 398
    },
    {
      "name": "YOLOv8-TensorRT",
      "one_line_profile": "TensorRT implementation for YOLOv8 acceleration",
      "detailed_description": "A C++ and Python implementation for accelerating YOLOv8 object detection models using NVIDIA TensorRT, optimized for high-performance inference.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "computer_vision"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/triple-Mu/YOLOv8-TensorRT",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "tensorrt",
        "yolov8",
        "inference",
        "acceleration"
      ],
      "id": 399
    },
    {
      "name": "Triton Backend",
      "one_line_profile": "Utilities for creating custom Triton Inference Server backends",
      "detailed_description": "Common source code, scripts, and utilities designed to facilitate the creation and maintenance of backends for the Triton Inference Server.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_serving",
        "backend_development"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/triton-inference-server/backend",
      "help_website": [
        "https://github.com/triton-inference-server/server"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "triton",
        "inference-server",
        "backend"
      ],
      "id": 400
    },
    {
      "name": "Triton Client",
      "one_line_profile": "Client libraries for Triton Inference Server",
      "detailed_description": "Provides Python, C++, and Java client libraries, along with GRPC-generated examples, to interact with the Triton Inference Server for sending inference requests.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_serving",
        "client_interface"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/triton-inference-server/client",
      "help_website": [
        "https://github.com/triton-inference-server/server"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "triton",
        "client",
        "grpc",
        "http"
      ],
      "id": 401
    },
    {
      "name": "Triton Model Analyzer",
      "one_line_profile": "Profiling tool for Triton model compute and memory requirements",
      "detailed_description": "A CLI tool that helps users understand the compute and memory requirements of models served by Triton Inference Server, enabling configuration optimization.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "profiling",
        "optimization"
      ],
      "application_level": "application",
      "primary_language": "Python",
      "repo_url": "https://github.com/triton-inference-server/model_analyzer",
      "help_website": [
        "https://github.com/triton-inference-server/server"
      ],
      "license": "Apache-2.0",
      "tags": [
        "profiling",
        "optimization",
        "triton",
        "inference"
      ],
      "id": 402
    },
    {
      "name": "Triton Model Navigator",
      "one_line_profile": "Toolkit for optimizing and deploying DL models on NVIDIA GPUs",
      "detailed_description": "An inference toolkit designed to automate the process of moving deep learning models from training to deployment, focusing on optimization for NVIDIA GPUs within the Triton ecosystem.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_optimization",
        "deployment"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/triton-inference-server/model_navigator",
      "help_website": [
        "https://github.com/triton-inference-server/server"
      ],
      "license": "Apache-2.0",
      "tags": [
        "deployment",
        "optimization",
        "nvidia",
        "triton"
      ],
      "id": 403
    },
    {
      "name": "Triton ONNX Runtime Backend",
      "one_line_profile": "ONNX Runtime integration for Triton Inference Server",
      "detailed_description": "The backend implementation that allows Triton Inference Server to execute models using the ONNX Runtime engine.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_serving",
        "runtime_integration"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/triton-inference-server/onnxruntime_backend",
      "help_website": [
        "https://github.com/triton-inference-server/server"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "onnx",
        "triton",
        "backend"
      ],
      "id": 404
    },
    {
      "name": "Triton Python Backend",
      "one_line_profile": "Python execution backend for Triton Inference Server",
      "detailed_description": "Enables pre-processing, post-processing, and other custom logic to be implemented in Python and executed within the Triton Inference Server pipeline.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_serving",
        "custom_logic"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/triton-inference-server/python_backend",
      "help_website": [
        "https://github.com/triton-inference-server/server"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "python",
        "triton",
        "backend"
      ],
      "id": 405
    },
    {
      "name": "PyTriton",
      "one_line_profile": "Python interface for deploying models with Triton",
      "detailed_description": "A Flask/FastAPI-like interface that simplifies the deployment of Python models and functions using Triton Inference Server, allowing developers to bind Python functions to Triton endpoints.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_serving",
        "deployment"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/triton-inference-server/pytriton",
      "help_website": [
        "https://triton-inference-server.github.io/pytriton"
      ],
      "license": "Apache-2.0",
      "tags": [
        "python",
        "serving",
        "triton",
        "deployment"
      ],
      "id": 406
    },
    {
      "name": "Triton Inference Server",
      "one_line_profile": "High-performance inference serving platform",
      "detailed_description": "An open-source inference serving software that streamlines AI inference by enabling teams to deploy, run, and scale trained AI models from any framework on any GPU- or CPU-based infrastructure.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_serving",
        "platform"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/triton-inference-server/server",
      "help_website": [
        "https://developer.nvidia.com/nvidia-triton-inference-server"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "inference",
        "server",
        "gpu",
        "cloud",
        "edge"
      ],
      "id": 407
    },
    {
      "name": "Triton TensorRT-LLM Backend",
      "one_line_profile": "TensorRT-LLM integration for Triton Inference Server",
      "detailed_description": "A backend for Triton Inference Server that enables the execution of Large Language Models (LLMs) optimized with NVIDIA TensorRT-LLM.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_serving",
        "llm_acceleration"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/triton-inference-server/tensorrtllm_backend",
      "help_website": [
        "https://github.com/triton-inference-server/server"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "tensorrt",
        "triton",
        "backend"
      ],
      "id": 408
    },
    {
      "name": "Triton",
      "one_line_profile": "Language and compiler for custom Deep Learning primitives",
      "detailed_description": "A language and compiler for writing highly efficient custom Deep Learning primitives. It aims to provide an open-source environment to write fast code at higher productivity than CUDA.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "compiler",
        "kernel_optimization"
      ],
      "application_level": "library",
      "primary_language": "MLIR",
      "repo_url": "https://github.com/triton-lang/triton",
      "help_website": [
        "https://triton-lang.org/"
      ],
      "license": "MIT",
      "tags": [
        "compiler",
        "gpu",
        "cuda",
        "optimization",
        "deep-learning"
      ],
      "id": 409
    },
    {
      "name": "TrustGraph",
      "one_line_profile": "Graph-based tool to reduce AI hallucinations",
      "detailed_description": "A tool designed to eliminate hallucinations from AI agents by leveraging knowledge graphs and trusted data sources to ground the generation process.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "knowledge_graph",
        "ai_reliability"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/trustgraph-ai/trustgraph",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "hallucination",
        "knowledge-graph",
        "ai-agent",
        "reliability"
      ],
      "id": 410
    },
    {
      "name": "Petastorm",
      "one_line_profile": "Data access library for deep learning from Parquet",
      "detailed_description": "A library that enables single machine or distributed training and evaluation of deep learning models directly from datasets in Apache Parquet format, supporting TensorFlow, PyTorch, and PySpark.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "data_loading",
        "distributed_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/uber/petastorm",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "parquet",
        "data-loading",
        "deep-learning",
        "distributed-training"
      ],
      "id": 411
    },
    {
      "name": "Telamon",
      "one_line_profile": "Optimization framework for GPU computational kernels",
      "detailed_description": "A framework designed to explore and find optimal combinations of optimizations for computational kernels running on GPUs, acting as an auto-tuning tool.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "kernel_optimization",
        "auto_tuning"
      ],
      "application_level": "solver",
      "primary_language": "Rust",
      "repo_url": "https://github.com/ulysseB/telamon",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "gpu",
        "optimization",
        "kernel",
        "auto-tuning"
      ],
      "id": 412
    },
    {
      "name": "SparseTIR",
      "one_line_profile": "Sparse tensor compiler for deep learning",
      "detailed_description": "A compiler infrastructure for sparse tensor algebra in deep learning, enabling efficient execution of sparse operations on hardware accelerators.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "compiler",
        "sparse_computation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/uwsampl/SparseTIR",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "compiler",
        "sparse-tensor",
        "deep-learning",
        "optimization"
      ],
      "id": 413
    },
    {
      "name": "Cache-DiT",
      "one_line_profile": "Inference engine with cache acceleration for Diffusion Transformers",
      "detailed_description": "A PyTorch-native inference engine that utilizes hybrid cache acceleration and parallelism to speed up Diffusion Transformer (DiT) models like Z-Image and FLUX2.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "diffusion_models"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/vipshop/cache-dit",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "diffusion-transformer",
        "inference",
        "cache",
        "acceleration"
      ],
      "id": 414
    },
    {
      "name": "LLM Compressor",
      "one_line_profile": "Library for compressing LLMs for vLLM deployment",
      "detailed_description": "A library compatible with Transformers for applying various compression algorithms (quantization, sparsification) to Large Language Models to optimize them for deployment with vLLM.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_compression",
        "quantization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/vllm-project/llm-compressor",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "compression",
        "quantization",
        "vllm"
      ],
      "id": 415
    },
    {
      "name": "Semantic Router",
      "one_line_profile": "Routing layer for Mixture-of-Models and LLM agents",
      "detailed_description": "An intelligent routing tool for AI systems that directs queries to the most appropriate model or agent based on semantic meaning, optimizing resource usage and response quality.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_routing",
        "model_orchestration"
      ],
      "application_level": "library",
      "primary_language": "Go",
      "repo_url": "https://github.com/vllm-project/semantic-router",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "routing",
        "llm",
        "mixture-of-models",
        "agent"
      ],
      "id": 416
    },
    {
      "name": "vLLM",
      "one_line_profile": "High-throughput memory-efficient LLM inference engine",
      "detailed_description": "A high-performance library for LLM inference and serving, featuring PagedAttention for efficient memory management and continuous batching for high throughput.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_serving",
        "llm_acceleration"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/vllm-project/vllm",
      "help_website": [
        "https://docs.vllm.ai/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "inference",
        "serving",
        "pagedattention"
      ],
      "id": 417
    },
    {
      "name": "vLLM Ascend",
      "one_line_profile": "Ascend NPU hardware plugin for vLLM",
      "detailed_description": "A community-maintained plugin that enables vLLM to run on Huawei Ascend hardware, providing hardware-specific optimizations for LLM inference.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "hardware_acceleration",
        "inference_serving"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/vllm-project/vllm-ascend",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ascend",
        "npu",
        "vllm",
        "hardware-support"
      ],
      "id": 418
    },
    {
      "name": "vLLM Omni",
      "one_line_profile": "Inference framework for omni-modality models",
      "detailed_description": "An extension of the vLLM framework designed to support efficient inference for omni-modality models (processing text, image, audio, etc. simultaneously).",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "multimodal_inference",
        "serving"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/vllm-project/vllm-omni",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "multimodal",
        "inference",
        "vllm",
        "omni-modality"
      ],
      "id": 419
    },
    {
      "name": "PytorchAutoDrive",
      "one_line_profile": "Toolkit for segmentation and lane detection in autonomous driving",
      "detailed_description": "A PyTorch-based toolkit providing implementations of various segmentation and lane detection models (ERFNet, SCNN, etc.) along with tools for training, visualization, benchmarking, and deployment.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "computer_vision",
        "autonomous_driving"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/voldemortX/pytorch-auto-drive",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "autonomous-driving",
        "segmentation",
        "lane-detection",
        "pytorch"
      ],
      "id": 420
    },
    {
      "name": "TensorRTx",
      "one_line_profile": "TensorRT implementations of popular deep learning networks",
      "detailed_description": "A comprehensive collection of TensorRT network definition API implementations for popular deep learning models, serving as a reference and library for accelerating these models.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "model_implementation"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/wang-xinyu/tensorrtx",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "tensorrt",
        "deep-learning",
        "acceleration",
        "inference"
      ],
      "id": 421
    },
    {
      "name": "3d-model-convert-to-gltf",
      "one_line_profile": "Converter for 3D models to glTF format",
      "detailed_description": "A utility tool to convert various 3D model formats (STL, IGES, STEP, OBJ, FBX) into the glTF format, often used for efficient transmission and loading of 3D scenes.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "data_conversion",
        "scientific_visualization"
      ],
      "application_level": "application",
      "primary_language": "Python",
      "repo_url": "https://github.com/wangerzi/3d-model-convert-to-gltf",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "3d-model",
        "converter",
        "gltf",
        "visualization"
      ],
      "id": 422
    },
    {
      "name": "QLLM",
      "one_line_profile": "Quantization toolbox for Large Language Models",
      "detailed_description": "A general quantization toolbox supporting 2-8 bit quantization methods like GPTQ, AWQ, HQQ, and VPTQ, with capabilities to export models to ONNX/ONNX Runtime.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "model_compression"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/wejoncy/QLLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "quantization",
        "llm",
        "onnx",
        "compression"
      ],
      "id": 423
    },
    {
      "name": "QDrop",
      "one_line_profile": "Post-training quantization method implementation",
      "detailed_description": "The official implementation of the QDrop method for randomly dropping quantization during post-training quantization to achieve better performance with low-bit models.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "model_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/wimh966/QDrop",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "quantization",
        "ptq",
        "deep-learning",
        "optimization"
      ],
      "id": 424
    },
    {
      "name": "QwT",
      "one_line_profile": "Post-training quantization library with lightweight linear compensation",
      "detailed_description": "A PyTorch implementation of 'Quantization without Tears' (QwT) that provides fast and accurate post-training network quantization using lightweight linear compensation layers to minimize accuracy loss.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "model_compression"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/wujx2001/QwT",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "quantization",
        "pytorch",
        "post-training-quantization"
      ],
      "id": 425
    },
    {
      "name": "rknn-3588-npu-yolo-accelerate",
      "one_line_profile": "YOLOv5 inference acceleration on RK3588 NPU",
      "detailed_description": "A C++ implementation for deploying and accelerating YOLOv5 object detection models on Rockchip RK3588 NPUs, utilizing thread pools for efficient inference.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "edge_computing"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/wzxzhuxi/rknn-3588-npu-yolo-accelerate",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rknn",
        "npu",
        "yolov5",
        "inference"
      ],
      "id": 426
    },
    {
      "name": "libonnx",
      "one_line_profile": "Lightweight pure C99 ONNX inference engine",
      "detailed_description": "A portable and lightweight ONNX inference engine written in pure C99, designed for embedded devices with support for hardware acceleration.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_engine",
        "embedded_ai"
      ],
      "application_level": "library",
      "primary_language": "C",
      "repo_url": "https://github.com/xboot/libonnx",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "onnx",
        "inference",
        "embedded",
        "c99"
      ],
      "id": 427
    },
    {
      "name": "xlang",
      "one_line_profile": "High-performance dynamic language for AI and IoT",
      "detailed_description": "A next-generation dynamic programming language designed for AI and IoT applications, featuring built-in distributed computing capabilities and high performance.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "compiler",
        "programming_language"
      ],
      "application_level": "platform",
      "primary_language": "C",
      "repo_url": "https://github.com/xlang-foundation/xlang",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "language",
        "compiler",
        "distributed-computing"
      ],
      "id": 428
    },
    {
      "name": "lite.ai.toolkit",
      "one_line_profile": "C++ AI toolkit wrapping MNN, ONNXRuntime, and TensorRT",
      "detailed_description": "A lightweight C++ toolkit that integrates multiple inference backends (MNN, ONNXRuntime, TensorRT) to support over 100 models for tasks like detection, segmentation, and generation.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "model_deployment"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/xlite-dev/lite.ai.toolkit",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "inference",
        "mnn",
        "tensorrt",
        "onnxruntime"
      ],
      "id": 429
    },
    {
      "name": "onnx_runtime_cpp",
      "one_line_profile": "Simplified C++ wrapper for ONNX Runtime",
      "detailed_description": "A small C++ library designed to facilitate the quick deployment of machine learning models using ONNX Runtime, providing a simplified interface for inference.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_deployment",
        "model_serving"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/xmba15/onnx_runtime_cpp",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "onnx-runtime",
        "cpp",
        "inference"
      ],
      "id": 430
    },
    {
      "name": "Xinference",
      "one_line_profile": "Unified inference platform for LLMs and multimodal models",
      "detailed_description": "A production-ready inference platform that allows running open-source LLMs, speech, and multimodal models on cloud or on-premise infrastructure with a unified API compatible with GPT.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_serving",
        "llm_inference"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/xorbitsai/inference",
      "help_website": [
        "https://inference.readthedocs.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "inference-server",
        "distributed-inference"
      ],
      "id": 431
    },
    {
      "name": "local-dream",
      "one_line_profile": "Stable Diffusion inference on Android with NPU acceleration",
      "detailed_description": "An application and toolkit for running Stable Diffusion models on Android devices, leveraging Snapdragon NPU acceleration as well as CPU/GPU support.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "mobile_inference",
        "generative_ai"
      ],
      "application_level": "solver",
      "primary_language": "Kotlin",
      "repo_url": "https://github.com/xororz/local-dream",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "android",
        "stable-diffusion",
        "npu",
        "inference"
      ],
      "id": 432
    },
    {
      "name": "GlobalCom2",
      "one_line_profile": "Plug-and-play inference acceleration for LVLMs",
      "detailed_description": "A tool implementing 'Global Compression Commander' for accelerating high-resolution Large Vision-Language Models (LVLMs) inference through token compression.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "model_compression"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/xuyang-liu16/GlobalCom2",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "lvlm",
        "compression",
        "acceleration"
      ],
      "id": 433
    },
    {
      "name": "VidCom2",
      "one_line_profile": "Inference acceleration for Video LLMs",
      "detailed_description": "A plug-and-play tool for accelerating Video Large Language Models inference via video compression techniques, as presented at EMNLP 2025.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "video_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/xuyang-liu16/VidCom2",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "video-llm",
        "compression",
        "inference"
      ],
      "id": 434
    },
    {
      "name": "LOPQ",
      "one_line_profile": "Locally Optimized Product Quantization for ANN search",
      "detailed_description": "A library for training Locally Optimized Product Quantization (LOPQ) models to enable efficient approximate nearest neighbor search for high-dimensional data.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "ann_search"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/yahoo/lopq",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "quantization",
        "ann",
        "search",
        "spark"
      ],
      "id": 435
    },
    {
      "name": "face-parsing",
      "one_line_profile": "Real-time face parsing inference with ONNX Runtime",
      "detailed_description": "A tool for real-time face parsing using BiSeNet, providing a pipeline from PyTorch training to ONNX Runtime inference.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "image_segmentation",
        "inference_deployment"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/yakhyo/face-parsing",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "face-parsing",
        "onnx",
        "segmentation"
      ],
      "id": 436
    },
    {
      "name": "face-reidentification",
      "one_line_profile": "Face re-identification pipeline with FAISS and ONNX",
      "detailed_description": "A complete pipeline for face re-identification integrating SCRFD for detection, ArcFace for recognition, and FAISS for vector search, optimized for ONNX Runtime inference.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "face_recognition",
        "inference_pipeline"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/yakhyo/face-reidentification",
      "help_website": [],
      "license": null,
      "tags": [
        "face-reid",
        "faiss",
        "onnx"
      ],
      "id": 437
    },
    {
      "name": "gaze-estimation",
      "one_line_profile": "Real-time gaze estimation inference models",
      "detailed_description": "A collection of real-time gaze estimation models (ResNet, MobileNet, etc.) optimized for inference using ONNX Runtime.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "gaze_estimation",
        "inference_deployment"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/yakhyo/gaze-estimation",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gaze-estimation",
        "onnx",
        "real-time"
      ],
      "id": 438
    },
    {
      "name": "Monocular_Depth_Estimation_TRT",
      "one_line_profile": "TensorRT optimization for monocular depth estimation",
      "detailed_description": "A toolkit for optimizing monocular depth estimation models using TensorRT, including model conversion and inference acceleration for 3D reconstruction tasks.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "depth_estimation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/yester31/Monocular_Depth_Estimation_TRT",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "tensorrt",
        "depth-estimation",
        "acceleration"
      ],
      "id": 439
    },
    {
      "name": "LLM-SFT",
      "one_line_profile": "Framework for Chinese LLM supervised fine-tuning",
      "detailed_description": "A comprehensive framework for Supervised Fine-Tuning (SFT) of Large Language Models (LLMs), supporting multiple base models (ChatGLM, LLaMA, etc.) and acceleration techniques like LoRA, QLoRA, and DeepSpeed.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "model_training",
        "fine_tuning"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/yongzhuo/LLM-SFT",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "sft",
        "fine-tuning",
        "deepspeed"
      ],
      "id": 440
    },
    {
      "name": "SimVQ",
      "one_line_profile": "Vector quantization method to address representation collapse",
      "detailed_description": "An implementation of SimVQ, a method to improve vector quantized models by addressing representation collapse using a linear layer approach, as presented at ICCV 2025.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "representation_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/youngsheen/SimVQ",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "vector-quantization",
        "iccv",
        "computer-vision"
      ],
      "id": 441
    },
    {
      "name": "rknn-cpp-yolo",
      "one_line_profile": "Optimized YOLOv11 inference on RK3588",
      "detailed_description": "A C++ project for efficient real-time inference of YOLOv11 on RK3588 platforms using RKNN and RGA hardware acceleration.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_acceleration",
        "edge_computing"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/yuunnn-w/rknn-cpp-yolo",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "rknn",
        "yolo",
        "rk3588",
        "inference"
      ],
      "id": 442
    },
    {
      "name": "Optimizing-SGEMM-on-NVIDIA-Turing-GPUs",
      "one_line_profile": "High-performance SGEMM kernels for NVIDIA GPUs",
      "detailed_description": "A collection of highly optimized SGEMM (Single-Precision General Matrix Multiply) kernel functions for NVIDIA Turing GPUs, achieving performance close to cuBLAS.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "hpc_kernels",
        "matrix_multiplication"
      ],
      "application_level": "library",
      "primary_language": "Cuda",
      "repo_url": "https://github.com/yzhaiustc/Optimizing-SGEMM-on-NVIDIA-Turing-GPUs",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "cuda",
        "sgemm",
        "optimization",
        "hpc"
      ],
      "id": 443
    },
    {
      "name": "AI-research-SKILLs",
      "one_line_profile": "Library of AI research skills for agents",
      "detailed_description": "A comprehensive library of AI research and engineering skills designed to be packaged into AI agents (like Claude or Gemini) to enhance their capabilities in conducting AI research.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "agent_tools",
        "research_automation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/zechenzhangAGI/AI-research-SKILLs",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ai-agent",
        "research-tools",
        "skills-library"
      ],
      "id": 444
    },
    {
      "name": "ZhiLight",
      "one_line_profile": "High-performance LLM inference engine",
      "detailed_description": "A highly optimized inference acceleration engine specifically designed for Llama and its variants, providing efficient serving capabilities.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_engine",
        "llm_serving"
      ],
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/zhihu/ZhiLight",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "inference",
        "acceleration",
        "llama"
      ],
      "id": 445
    },
    {
      "name": "yolort",
      "one_line_profile": "Runtime stack for YOLOv5 on accelerators",
      "detailed_description": "A runtime stack that simplifies the deployment of YOLOv5 models on specialized accelerators such as TensorRT, LibTorch, ONNX Runtime, TVM, and NCNN.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "inference_runtime",
        "model_deployment"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/zhiqwang/yolort",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "yolov5",
        "tensorrt",
        "tvm",
        "inference"
      ],
      "id": 446
    },
    {
      "name": "PTQD",
      "one_line_profile": "Post-training quantization for diffusion models",
      "detailed_description": "The official implementation of PTQD, a method for accurate post-training quantization specifically tailored for diffusion models.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "diffusion_models"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/ziplab/PTQD",
      "help_website": [],
      "license": null,
      "tags": [
        "quantization",
        "diffusion",
        "post-training"
      ],
      "id": 447
    },
    {
      "name": "KnowLM",
      "one_line_profile": "Knowledgeable Large Language Model Framework",
      "detailed_description": "An open-source framework for building and training Knowledgeable Large Language Models, focusing on integrating knowledge graphs and structured data.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "llm_framework",
        "knowledge_integration"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/zjunlp/KnowLM",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "knowledge-graph",
        "framework"
      ],
      "id": 448
    },
    {
      "name": "RepQ-ViT",
      "one_line_profile": "Post-training quantization for Vision Transformers",
      "detailed_description": "A tool implementing RepQ-ViT, a scale reparameterization method for post-training quantization of Vision Transformers (ViTs).",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "quantization",
        "vision_transformer"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/zkkli/RepQ-ViT",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "quantization",
        "vit",
        "post-training"
      ],
      "id": 449
    },
    {
      "name": "zml",
      "one_line_profile": "High-performance AI compiler and runtime stack",
      "detailed_description": "A machine learning compiler and runtime stack built with Zig, OpenXLA, and MLIR, designed to run any model on any hardware with zero compromise.",
      "domains": [
        "AI6",
        "AI6-04"
      ],
      "subtask_category": [
        "compiler",
        "runtime"
      ],
      "application_level": "platform",
      "primary_language": "Zig",
      "repo_url": "https://github.com/zml/zml",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "compiler",
        "zig",
        "mlir",
        "openxla"
      ],
      "id": 450
    }
  ]
}
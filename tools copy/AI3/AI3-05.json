{
  "generated_at": "2025-12-16T08:25:25.569588+08:00",
  "metadata": {
    "leaf_cluster": {
      "leaf_cluster_id": "AI3",
      "leaf_cluster_name": "科研领域模型训练与微调生态",
      "domain": "AI Toolchain",
      "typical_objects": "domain corpora",
      "task_chain": "数据→训练→微调→对齐→评测→发布",
      "tool_form": "训练栈 + 数据管线 + serving"
    },
    "unit": {
      "unit_id": "AI3-05",
      "unit_name": "模型发布/服务/推理优化",
      "target_scale": "150–300",
      "coverage_tools": "serving、quantization"
    },
    "search": {
      "target_candidates": 300,
      "queries": [
        "[GH] DeepSpeed-MII",
        "[GH] LMDeploy",
        "[GH] AutoGPTQ",
        "[GH] ONNX Runtime",
        "[GH] Triton Inference Server",
        "[GH] llama.cpp",
        "[GH] TensorRT-LLM",
        "[GH] Text Generation Inference (TGI)",
        "[GH] vLLM",
        "[GH] llm serving",
        "[GH] inference server",
        "[GH] model quantization",
        "[GH] inference optimization",
        "[GH] tensorrt-llm",
        "[GH] vllm",
        "[GH] gptq",
        "[GH] awq",
        "[GH] onnx runtime",
        "[GH] triton inference server",
        "[GH] model compression",
        "[GH] pagedattention",
        "[WEB] llm inference serving framework github",
        "[WEB] model quantization tools github",
        "[WEB] deep learning inference optimization github",
        "[WEB] awesome llm serving github",
        "[WEB] large model deployment tools github"
      ],
      "total_candidates": 772,
      "tool_candidates": 584,
      "final_tools": 310
    }
  },
  "tools": [
    {
      "name": "HyperGen",
      "one_line_profile": "Optimized inference and fine-tuning framework for diffusion models",
      "detailed_description": "A framework designed to optimize the inference and fine-tuning of diffusion models (image and video), offering significant speed improvements and reduced VRAM usage compared to standard implementations.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_optimization",
        "model_finetuning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/0xCrunchyy/hypergen",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "diffusion-models",
        "inference-optimization",
        "fine-tuning"
      ],
      "id": 1
    },
    {
      "name": "micronet",
      "one_line_profile": "Model compression and deployment library",
      "detailed_description": "A library for neural network model compression and deployment, supporting quantization (QAT, PTQ, Low-Bit), pruning (channel pruning), and deployment optimization via TensorRT.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_compression",
        "quantization",
        "pruning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/666DZY666/micronet",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "quantization",
        "pruning",
        "tensorrt",
        "model-compression"
      ],
      "id": 2
    },
    {
      "name": "JetStream",
      "one_line_profile": "Throughput and memory optimized LLM inference engine for XLA devices",
      "detailed_description": "A high-performance inference engine designed for Large Language Models (LLMs), optimized for throughput and memory efficiency on XLA devices such as TPUs.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_serving",
        "throughput_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/AI-Hypercomputer/JetStream",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-inference",
        "tpu",
        "xla",
        "optimization"
      ],
      "id": 3
    },
    {
      "name": "optimum-transformers",
      "one_line_profile": "Accelerated NLP pipelines for fast inference on CPU and GPU",
      "detailed_description": "A library providing accelerated inference pipelines for NLP models, built upon Hugging Face Transformers, Optimum, and ONNX Runtime to enhance performance on CPUs and GPUs.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_acceleration",
        "nlp_pipeline"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/AlekseyKorshuk/optimum-transformers",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "nlp",
        "inference",
        "onnx",
        "optimization"
      ],
      "id": 4
    },
    {
      "name": "llumnix",
      "one_line_profile": "Efficient multi-instance LLM serving system",
      "detailed_description": "A serving system designed for efficient and easy deployment of multiple Large Language Model (LLM) instances, optimizing resource utilization and request handling.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "resource_scheduling"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/AlibabaPAI/llumnix",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-serving",
        "distributed-systems",
        "inference"
      ],
      "id": 5
    },
    {
      "name": "REASONING_COMPILER",
      "one_line_profile": "LLM-guided optimizations for efficient model serving",
      "detailed_description": "A compiler-based approach that utilizes LLMs to guide optimizations for efficient model serving, as presented in NeurIPS 2025.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "serving_optimization",
        "compiler_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Anna-Bele/REASONING_COMPILER",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "compiler",
        "serving-optimization"
      ],
      "id": 6
    },
    {
      "name": "AutoGPTQ",
      "one_line_profile": "Easy-to-use LLM quantization package based on GPTQ",
      "detailed_description": "A library providing user-friendly APIs for quantizing Large Language Models (LLMs) using the GPTQ algorithm, enabling efficient inference on consumer hardware.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_quantization",
        "inference_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/AutoGPTQ/AutoGPTQ",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gptq",
        "quantization",
        "llm"
      ],
      "id": 7
    },
    {
      "name": "BitNet-Transformers",
      "one_line_profile": "1-bit Transformer implementation for LLMs",
      "detailed_description": "A PyTorch implementation of the BitNet architecture, enabling 1-bit scaling for Large Language Models within the Hugging Face Transformers ecosystem.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_architecture",
        "quantization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Beomi/BitNet-Transformers",
      "help_website": [],
      "license": null,
      "tags": [
        "bitnet",
        "1-bit",
        "transformers",
        "quantization"
      ],
      "id": 8
    },
    {
      "name": "LMDeploy-Jetson",
      "one_line_profile": "Offline LLM deployment tools for NVIDIA Jetson platform",
      "detailed_description": "A toolkit and set of scripts for deploying Large Language Models (LLMs) offline on NVIDIA Jetson edge devices, facilitating embodied intelligence applications.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "edge_deployment",
        "inference_serving"
      ],
      "application_level": "workflow",
      "primary_language": null,
      "repo_url": "https://github.com/BestAnHongjun/LMDeploy-Jetson",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "jetson",
        "edge-computing",
        "llm-deployment"
      ],
      "id": 9
    },
    {
      "name": "QuIP",
      "one_line_profile": "2-Bit Quantization of Large Language Models",
      "detailed_description": "Implementation of the QuIP algorithm for 2-bit quantization of Large Language Models with theoretical guarantees, enabling extreme model compression.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_quantization",
        "compression"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Cornell-RelaxML/QuIP",
      "help_website": [],
      "license": null,
      "tags": [
        "quantization",
        "2-bit",
        "llm"
      ],
      "id": 10
    },
    {
      "name": "Audio-Denoiser-ONNX",
      "one_line_profile": "Audio denoising tool using ONNX Runtime",
      "detailed_description": "A tool utilizing ONNX Runtime to perform audio denoising, applicable for cleaning scientific audio data or speech signals.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "signal_processing",
        "audio_denoising"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/DakeQQ/Audio-Denoiser-ONNX",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "audio-processing",
        "onnx",
        "denoising"
      ],
      "id": 11
    },
    {
      "name": "F5-TTS-ONNX",
      "one_line_profile": "F5-TTS implementation using ONNX Runtime",
      "detailed_description": "An implementation of the F5-TTS text-to-speech model optimized for inference using ONNX Runtime.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "speech_synthesis",
        "inference_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/DakeQQ/F5-TTS-ONNX",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "tts",
        "onnx",
        "inference"
      ],
      "id": 12
    },
    {
      "name": "ReflectionFlow",
      "one_line_profile": "Inference-time optimization for text-to-image diffusion models",
      "detailed_description": "A method and tool for scaling inference-time optimization for text-to-image diffusion models via Reflection Tuning, improving generation quality.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_optimization",
        "image_generation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Diffusion-CoT/ReflectionFlow",
      "help_website": [],
      "license": null,
      "tags": [
        "diffusion-models",
        "inference-time-optimization"
      ],
      "id": 13
    },
    {
      "name": "keras_compressor",
      "one_line_profile": "Model compression CLI tool for Keras",
      "detailed_description": "A Command Line Interface tool for compressing Keras models, facilitating efficient deployment.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_compression",
        "deployment"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/DwangoMediaVillage/keras_compressor",
      "help_website": [],
      "license": null,
      "tags": [
        "keras",
        "compression",
        "cli"
      ],
      "id": 14
    },
    {
      "name": "transformer-deploy",
      "one_line_profile": "Efficient CPU/GPU inference server for Transformer models",
      "detailed_description": "An enterprise-grade inference server designed for efficient and scalable deployment of Hugging Face transformer models on CPUs and GPUs.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_serving",
        "model_deployment"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/ELS-RD/transformer-deploy",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "transformers",
        "inference-server",
        "gpu-acceleration"
      ],
      "id": 15
    },
    {
      "name": "ENOVA",
      "one_line_profile": "Serverless LLM serving with autoscaling",
      "detailed_description": "A deployment, monitoring, and autoscaling service framework designed for serverless serving of Large Language Models.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "autoscaling"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/Emerging-AI/ENOVA",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "serverless",
        "llm-serving",
        "autoscaling"
      ],
      "id": 16
    },
    {
      "name": "candle-vllm",
      "one_line_profile": "Efficient local LLM inference and serving platform",
      "detailed_description": "A platform for efficient inference and serving of local LLMs, written in Rust and providing an OpenAI-compatible API server.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_serving",
        "local_inference"
      ],
      "application_level": "service",
      "primary_language": "Rust",
      "repo_url": "https://github.com/EricLBuehler/candle-vllm",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rust",
        "llm-serving",
        "inference"
      ],
      "id": 17
    },
    {
      "name": "FaceONNX",
      "one_line_profile": "Face recognition library based on ONNX Runtime",
      "detailed_description": "A library for face recognition and analytics utilizing Deep Neural Networks and ONNX Runtime for inference.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "computer_vision",
        "biometrics",
        "inference"
      ],
      "application_level": "library",
      "primary_language": "C#",
      "repo_url": "https://github.com/FaceONNX/FaceONNX",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "face-recognition",
        "onnx",
        "c-sharp"
      ],
      "id": 18
    },
    {
      "name": "fasterai",
      "one_line_profile": "Model pruning and distillation library for FastAI/PyTorch",
      "detailed_description": "A library to prune and distill neural network models using FastAI and PyTorch, aiming to reduce model size and increase inference speed.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_pruning",
        "knowledge_distillation"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/FasterAI-Labs/fasterai",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "pruning",
        "distillation",
        "fastai",
        "optimization"
      ],
      "id": 19
    },
    {
      "name": "RoboBrain",
      "one_line_profile": "Unified brain model for robotic manipulation",
      "detailed_description": "A unified model framework for robotic manipulation, bridging abstract reasoning to concrete control, useful for robotics research and simulation.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "robotics_modeling",
        "control_inference"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/FlagOpen/RoboBrain",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "robotics",
        "manipulation",
        "foundation-model"
      ],
      "id": 20
    },
    {
      "name": "nano-vllm",
      "one_line_profile": "Lightweight implementation of vLLM for LLM inference",
      "detailed_description": "A lightweight version or implementation of the vLLM library designed for efficient Large Language Model serving and inference.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_serving"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/GeeeekExplorer/nano-vllm",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "inference",
        "vllm",
        "serving"
      ],
      "id": 21
    },
    {
      "name": "Depths-CPP",
      "one_line_profile": "High-performance C++ inference for depth estimation models",
      "detailed_description": "A C++ application and header library for real-time metric depth estimation using Depth-Anything-V2 models, supporting ONNX Runtime and OpenCV.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_serving",
        "depth_estimation"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/Geekgineer/Depths-CPP",
      "help_website": [],
      "license": null,
      "tags": [
        "depth-estimation",
        "cpp",
        "onnx",
        "inference"
      ],
      "id": 22
    },
    {
      "name": "YOLOs-CPP",
      "one_line_profile": "C++ inference headers for YOLO object detection models",
      "detailed_description": "High-performance C++ headers for real-time object detection and segmentation using various YOLO model versions, leveraging ONNX Runtime.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_serving",
        "object_detection"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/Geekgineer/YOLOs-CPP",
      "help_website": [],
      "license": null,
      "tags": [
        "yolo",
        "object-detection",
        "cpp",
        "onnx"
      ],
      "id": 23
    },
    {
      "name": "furnace",
      "one_line_profile": "Rust-based ML inference server using Burn framework",
      "detailed_description": "A machine learning inference server built with Rust and the Burn framework, designed for high performance.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_serving"
      ],
      "application_level": "service",
      "primary_language": "Rust",
      "repo_url": "https://github.com/Gilfeather/furnace",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rust",
        "inference-server",
        "burn-framework"
      ],
      "id": 24
    },
    {
      "name": "parallax",
      "one_line_profile": "Distributed model serving framework for AI clusters",
      "detailed_description": "A distributed model serving framework that enables building AI clusters for serving large models across distributed infrastructure.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_serving",
        "distributed_computing"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/GradientHQ/parallax",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "model-serving",
        "distributed-systems",
        "llm"
      ],
      "id": 25
    },
    {
      "name": "PiSSA",
      "one_line_profile": "Parameter-efficient fine-tuning method for LLMs",
      "detailed_description": "Implementation of PiSSA (Principal Singular Values and Singular Vectors Adaptation), a method for parameter-efficient fine-tuning of Large Language Models.",
      "domains": [
        "AI3"
      ],
      "subtask_category": [
        "model_finetuning",
        "optimization"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/GraphPKU/PiSSA",
      "help_website": [],
      "license": null,
      "tags": [
        "peft",
        "llm",
        "fine-tuning",
        "optimization"
      ],
      "id": 26
    },
    {
      "name": "BurstGPT",
      "one_line_profile": "Workload traces for optimizing LLM serving systems",
      "detailed_description": "A dataset of ChatGPT and GPT-4 workload traces designed to help researchers and developers optimize Large Language Model serving systems.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "benchmarking",
        "system_optimization"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/HPMLL/BurstGPT",
      "help_website": [],
      "license": "CC-BY-4.0",
      "tags": [
        "workload-trace",
        "llm-serving",
        "optimization",
        "dataset"
      ],
      "id": 27
    },
    {
      "name": "YOLO-Multi-Backbones-Attention",
      "one_line_profile": "Compressed YOLOv3 with lightweight backbones and attention",
      "detailed_description": "A model compression tool/implementation for YOLOv3 incorporating multiple lightweight backbones (ShuffleNetV2, GhostNet), attention mechanisms, pruning, and quantization.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_compression",
        "object_detection"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/HaloTrouvaille/YOLO-Multi-Backbones-Attention",
      "help_website": [],
      "license": null,
      "tags": [
        "model-compression",
        "yolo",
        "pruning",
        "quantization"
      ],
      "id": 28
    },
    {
      "name": "FlashTTS",
      "one_line_profile": "High-quality Chinese TTS and voice cloning service",
      "detailed_description": "A text-to-speech service and library based on SparkTTS and OrpheusTTS models, providing high-quality Chinese speech synthesis and voice cloning capabilities.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_serving",
        "text_to_speech"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/HuiResearch/FlashTTS",
      "help_website": [],
      "license": null,
      "tags": [
        "tts",
        "voice-cloning",
        "speech-synthesis"
      ],
      "id": 29
    },
    {
      "name": "onnxmlir-triton-backend",
      "one_line_profile": "ONNX MLIR backend for Triton Inference Server",
      "detailed_description": "A backend component that allows the usage of ONNX MLIR compiled models with the Triton Inference Server, enabling optimized inference on supported hardware.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_serving",
        "compiler_backend"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/IBM/onnxmlir-triton-backend",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "triton-inference-server",
        "onnx",
        "mlir",
        "backend"
      ],
      "id": 30
    },
    {
      "name": "gptq",
      "one_line_profile": "Post-training quantization for generative pretrained transformers",
      "detailed_description": "The official implementation of GPTQ, a method for accurate post-training quantization of generative pretrained transformers to reduce model size and inference cost.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "quantization",
        "model_compression"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/IST-DASLab/gptq",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "quantization",
        "llm",
        "compression",
        "gptq"
      ],
      "id": 31
    },
    {
      "name": "gptq-gguf-toolkit",
      "one_line_profile": "Toolkit for GPTQ quantization with GGUF format",
      "detailed_description": "A toolkit for performing efficient non-uniform quantization using GPTQ specifically for models in the GGUF format.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "quantization",
        "model_conversion"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/IST-DASLab/gptq-gguf-toolkit",
      "help_website": [],
      "license": null,
      "tags": [
        "quantization",
        "gguf",
        "gptq"
      ],
      "id": 32
    },
    {
      "name": "qmoe",
      "one_line_profile": "Sub-1-bit compression for trillion-parameter models",
      "detailed_description": "Implementation of QMoE, a compression method designed for practical sub-1-bit compression of massive Mixture-of-Experts (MoE) models.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_compression",
        "quantization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/IST-DASLab/qmoe",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "compression",
        "moe",
        "quantization",
        "llm"
      ],
      "id": 33
    },
    {
      "name": "py-txi",
      "one_line_profile": "Python wrapper for HuggingFace TGI and TEI servers",
      "detailed_description": "A Python client wrapper for interacting with HuggingFace's Text Generation Inference (TGI) and Text Embedding Inference (TEI) servers.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_client",
        "api_wrapper"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/IlyasMoutawwakil/py-txi",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "tgi",
        "tei",
        "inference",
        "client"
      ],
      "id": 34
    },
    {
      "name": "InferenceMAX",
      "one_line_profile": "Continuous inference benchmarking for AI hardware",
      "detailed_description": "An open-source benchmarking tool for continuous evaluation of inference performance across various AI hardware accelerators (GPUs, TPUs, etc.).",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/InferenceMAX/InferenceMAX",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "benchmarking",
        "inference",
        "hardware-evaluation"
      ],
      "id": 35
    },
    {
      "name": "nlp-architect",
      "one_line_profile": "Library for exploring NLP topologies and optimization techniques",
      "detailed_description": "A model library by Intel Labs for exploring state-of-the-art deep learning topologies and techniques for optimizing Natural Language Processing neural networks.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "modeling",
        "optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/IntelLabs/nlp-architect",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "nlp",
        "optimization",
        "deep-learning",
        "intel"
      ],
      "id": 36
    },
    {
      "name": "lmdeploy",
      "one_line_profile": "Toolkit for compressing, deploying, and serving LLMs",
      "detailed_description": "A comprehensive toolkit for compressing, deploying, and serving Large Language Models, offering high-performance inference.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_serving",
        "model_compression"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/InternLM/lmdeploy",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "serving",
        "compression",
        "deployment"
      ],
      "id": 37
    },
    {
      "name": "gbnfgen",
      "one_line_profile": "TypeScript generator for llama.cpp grammars",
      "detailed_description": "A tool to generate GBNF grammars for llama.cpp directly from TypeScript interfaces, enabling structured output generation from LLMs.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_utility",
        "structured_generation"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/IntrinsicLabsAI/gbnfgen",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "grammar",
        "llama.cpp",
        "structured-output"
      ],
      "id": 38
    },
    {
      "name": "chatglm-q",
      "one_line_profile": "ChatGLM2 implementation for GPTQ quantization",
      "detailed_description": "An implementation of ChatGLM2 specifically designed to support GPTQ quantization for efficient inference.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "quantization",
        "inference_serving"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/K024/chatglm-q",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "chatglm",
        "gptq",
        "quantization"
      ],
      "id": 39
    },
    {
      "name": "fastT5",
      "one_line_profile": "Inference speed optimization for T5 models",
      "detailed_description": "A tool to boost inference speed of T5 models by converting them to ONNX and quantizing them, reducing model size and latency.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_optimization",
        "quantization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Ki6an/fastT5",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "t5",
        "onnx",
        "optimization",
        "inference"
      ],
      "id": 40
    },
    {
      "name": "csle",
      "one_line_profile": "Research platform for automated security policies using quantitative methods",
      "detailed_description": "A research platform for developing automated security policies using quantitative methods such as reinforcement learning, game theory, and causal inference.",
      "domains": [
        "AI3"
      ],
      "subtask_category": [
        "scientific_modeling",
        "simulation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/Kim-Hammar/csle",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "security",
        "reinforcement-learning",
        "game-theory",
        "simulation"
      ],
      "id": 41
    },
    {
      "name": "index-tts-vllm",
      "one_line_profile": "vLLM support integration for IndexTTS",
      "detailed_description": "A tool/library that adds vLLM support to IndexTTS, enabling faster inference for text-to-speech applications.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_serving",
        "text_to_speech"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Ksuriuri/index-tts-vllm",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "tts",
        "vllm",
        "inference"
      ],
      "id": 42
    },
    {
      "name": "dds",
      "one_line_profile": "Server-driven video streaming for deep learning inference",
      "detailed_description": "A system for server-driven video streaming optimized for deep learning inference, balancing bandwidth and accuracy.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_serving",
        "video_processing"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/KuntaiDu/dds",
      "help_website": [],
      "license": null,
      "tags": [
        "video-streaming",
        "inference",
        "deep-learning"
      ],
      "id": 43
    },
    {
      "name": "DistServe",
      "one_line_profile": "Disaggregated serving system for Large Language Models",
      "detailed_description": "A disaggregated serving system for LLMs that separates the prefill and decoding phases to optimize performance and resource utilization.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_serving",
        "distributed_computing"
      ],
      "application_level": "platform",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/LLMServe/DistServe",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-serving",
        "distributed-systems",
        "optimization"
      ],
      "id": 44
    },
    {
      "name": "LMCache",
      "one_line_profile": "High-performance KV cache layer for LLMs",
      "detailed_description": "A specialized KV cache layer designed to supercharge Large Language Model inference by optimizing memory access and caching strategies.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_optimization",
        "memory_management"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/LMCache/LMCache",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "kv-cache",
        "llm",
        "optimization",
        "inference"
      ],
      "id": 45
    },
    {
      "name": "AutoGPTQ.tvm",
      "one_line_profile": "TVM kernel for GPTQ inference",
      "detailed_description": "A TVM-based kernel implementation for efficient inference of GPTQ quantized models.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_optimization",
        "kernel_implementation"
      ],
      "application_level": "library",
      "primary_language": "Cuda",
      "repo_url": "https://github.com/LeiWang1999/AutoGPTQ.tvm",
      "help_website": [],
      "license": null,
      "tags": [
        "tvm",
        "gptq",
        "cuda",
        "inference"
      ],
      "id": 46
    },
    {
      "name": "lightning-thunder",
      "one_line_profile": "PyTorch compiler for training and inference acceleration",
      "detailed_description": "A PyTorch compiler that accelerates both training and inference through built-in optimizations for performance, memory, and parallelism.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "compiler_optimization",
        "inference_acceleration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Lightning-AI/lightning-thunder",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "pytorch",
        "compiler",
        "optimization",
        "inference"
      ],
      "id": 47
    },
    {
      "name": "lit-llama",
      "one_line_profile": "Implementation of LLaMA for training and inference",
      "detailed_description": "A clean implementation of the LLaMA language model supporting pre-training, fine-tuning (LoRA, Adapter), and quantized inference (Int8, GPTQ).",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "modeling",
        "inference_serving"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Lightning-AI/lit-llama",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llama",
        "fine-tuning",
        "quantization",
        "inference"
      ],
      "id": 48
    },
    {
      "name": "VitsServer",
      "one_line_profile": "VITS ONNX TTS server for fast inference",
      "detailed_description": "A fast inference server for VITS Text-to-Speech models using ONNX Runtime.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_serving",
        "text_to_speech"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/LlmKira/VitsServer",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "tts",
        "onnx",
        "server",
        "vits"
      ],
      "id": 49
    },
    {
      "name": "KokoroSharp",
      "one_line_profile": "Fast local TTS inference engine in C#",
      "detailed_description": "A multi-platform, multi-lingual Text-to-Speech inference engine implemented in C# using ONNX runtime.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_serving",
        "text_to_speech"
      ],
      "application_level": "library",
      "primary_language": "C#",
      "repo_url": "https://github.com/Lyrcaxis/KokoroSharp",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "tts",
        "csharp",
        "onnx",
        "inference"
      ],
      "id": 50
    },
    {
      "name": "mpeg-pcc-tmc13",
      "one_line_profile": "Geometry-based Point Cloud Compression (G-PCC) Test Model",
      "detailed_description": "The reference software implementation (Test Model) for the MPEG Geometry-based Point Cloud Compression (G-PCC) standard.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "compression",
        "point_cloud_processing"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/MPEGGroup/mpeg-pcc-tmc13",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "point-cloud",
        "compression",
        "mpeg",
        "standard"
      ],
      "id": 51
    },
    {
      "name": "mpeg-pcc-tmc2",
      "one_line_profile": "Video-based Point Cloud Compression (V-PCC) Test Model",
      "detailed_description": "The reference software implementation (Test Model) for the MPEG Video-based Point Cloud Compression (V-PCC) standard.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "compression",
        "point_cloud_processing"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/MPEGGroup/mpeg-pcc-tmc2",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "point-cloud",
        "compression",
        "mpeg",
        "standard"
      ],
      "id": 52
    },
    {
      "name": "dcsam",
      "one_line_profile": "Factored inference for discrete-continuous smoothing and mapping",
      "detailed_description": "A library for factored inference in discrete-continuous smoothing and mapping (SLAM) problems, useful in robotics and navigation.",
      "domains": [
        "AI3"
      ],
      "subtask_category": [
        "scientific_inference",
        "slam"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/MarineRoboticsGroup/dcsam",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "slam",
        "inference",
        "robotics",
        "optimization"
      ],
      "id": 53
    },
    {
      "name": "llama-cpp-agent",
      "one_line_profile": "Framework for structured interaction with LLMs",
      "detailed_description": "A framework designed to facilitate interaction with Large Language Models, supporting structured function calls and output generation.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_utility",
        "agent_framework"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Maximilian-Winter/llama-cpp-agent",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "llm",
        "agent",
        "function-calling",
        "inference"
      ],
      "id": 54
    },
    {
      "name": "shimmy",
      "one_line_profile": "Rust-based OpenAI-compatible inference server",
      "detailed_description": "A high-performance, Python-free inference server written in Rust, compatible with the OpenAI API and supporting GGUF/SafeTensors models.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_serving"
      ],
      "application_level": "service",
      "primary_language": "Rust",
      "repo_url": "https://github.com/Michael-A-Kuykendall/shimmy",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "inference-server",
        "rust",
        "openai-api",
        "gguf"
      ],
      "id": 55
    },
    {
      "name": "GPTQModel",
      "one_line_profile": "LLM model quantization toolkit with multi-backend hardware acceleration",
      "detailed_description": "A toolkit for quantizing Large Language Models (LLMs) using GPTQ, supporting hardware acceleration on Nvidia CUDA, AMD ROCm, Intel XPU, and various CPUs. It integrates with Hugging Face, vLLM, and SGLang for efficient inference.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "quantization",
        "model_compression"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ModelCloud/GPTQModel",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "quantization",
        "llm",
        "compression",
        "cuda",
        "rocm"
      ],
      "id": 56
    },
    {
      "name": "LightCompress",
      "one_line_profile": "Toolkit for compressing large models including LLMs and VLMs",
      "detailed_description": "A toolkit designed for compressing large-scale models such as Large Language Models (LLMs), Vision-Language Models (VLMs), and video generation models, aiming to reduce model size and improve inference efficiency.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_compression",
        "quantization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ModelTC/LightCompress",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "compression",
        "llm",
        "vlm",
        "model-optimization"
      ],
      "id": 57
    },
    {
      "name": "LightLLM",
      "one_line_profile": "Lightweight and scalable LLM inference and serving framework",
      "detailed_description": "A Python-based framework for Large Language Model (LLM) inference and serving, featuring a lightweight design, easy scalability, and high-speed performance for deploying LLMs.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference",
        "serving"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/ModelTC/LightLLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "inference-server",
        "llm",
        "serving",
        "high-performance"
      ],
      "id": 58
    },
    {
      "name": "MQBench",
      "one_line_profile": "Benchmark and toolkit for model quantization algorithms",
      "detailed_description": "A framework for evaluating and implementing model quantization techniques, providing a standardized benchmark for assessing the performance and accuracy of quantized models.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "quantization",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ModelTC/MQBench",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "quantization",
        "benchmark",
        "model-compression"
      ],
      "id": 59
    },
    {
      "name": "flash-tokenizer",
      "one_line_profile": "High-performance tokenizer engine for LLM inference",
      "detailed_description": "An efficient and optimized tokenizer engine designed specifically for Large Language Model (LLM) inference serving, aiming to minimize tokenization latency.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "tokenization",
        "inference_optimization"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/NLPOptimize/flash-tokenizer",
      "help_website": [],
      "license": null,
      "tags": [
        "tokenizer",
        "llm",
        "inference",
        "optimization"
      ],
      "id": 60
    },
    {
      "name": "nanoowl",
      "one_line_profile": "Optimized OWL-ViT inference with NVIDIA TensorRT",
      "detailed_description": "A project that optimizes the OWL-ViT (Open-Vocabulary Object Detection) model for real-time inference using NVIDIA TensorRT, enabling efficient deployment on edge devices.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_optimization",
        "computer_vision"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NVIDIA-AI-IOT/nanoowl",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "tensorrt",
        "owl-vit",
        "object-detection",
        "optimization"
      ],
      "id": 61
    },
    {
      "name": "NVIDIA DALI",
      "one_line_profile": "GPU-accelerated data loading and augmentation library",
      "detailed_description": "A library containing highly optimized building blocks and an execution engine for data processing to accelerate deep learning training and inference applications, particularly for image and video data.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "data_processing",
        "augmentation"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/NVIDIA/DALI",
      "help_website": [
        "https://docs.nvidia.com/deeplearning/dali/user-guide/docs/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "gpu",
        "data-loading",
        "augmentation",
        "deep-learning"
      ],
      "id": 62
    },
    {
      "name": "Model-Optimizer",
      "one_line_profile": "Unified library for SOTA model optimization techniques",
      "detailed_description": "A library providing state-of-the-art model optimization techniques such as quantization, pruning, distillation, and speculative decoding to compress deep learning models for efficient deployment on NVIDIA hardware.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_optimization",
        "quantization",
        "pruning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NVIDIA/Model-Optimizer",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "optimization",
        "compression",
        "tensorrt",
        "llm"
      ],
      "id": 63
    },
    {
      "name": "TensorRT-LLM",
      "one_line_profile": "High-performance LLM inference library for NVIDIA GPUs",
      "detailed_description": "A library that provides an easy-to-use Python API to define Large Language Models (LLMs) and supports state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference",
        "serving"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NVIDIA/TensorRT-LLM",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "inference",
        "llm",
        "nvidia",
        "gpu",
        "optimization"
      ],
      "id": 64
    },
    {
      "name": "grps",
      "one_line_profile": "High-performance deep learning deployment framework",
      "detailed_description": "A deep learning deployment framework supporting multiple backends (TF, Torch, TRT, vLLM) with dynamic batching and streaming modes, compatible with Python and C++ for scalable service deployment.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "serving",
        "deployment"
      ],
      "application_level": "framework",
      "primary_language": "C++",
      "repo_url": "https://github.com/NetEase-Media/grps",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "serving",
        "inference",
        "deployment",
        "multi-backend"
      ],
      "id": 65
    },
    {
      "name": "grps_trtllm",
      "one_line_profile": "High-performance OpenAI-compatible LLM service based on TensorRT-LLM",
      "detailed_description": "A C++ high-performance LLM service implementation using GRPS and TensorRT-LLM, supporting OpenAI API, chat, function calls, and distributed multi-GPU inference.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "serving",
        "inference"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/NetEase-Media/grps_trtllm",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-serving",
        "tensorrt-llm",
        "openai-api",
        "inference"
      ],
      "id": 66
    },
    {
      "name": "sparse_quant_llms",
      "one_line_profile": "SparseGPT and GPTQ compression tools for LLMs",
      "detailed_description": "A toolkit for applying SparseGPT and GPTQ compression techniques to Large Language Models like LLaMa, OPT, and Pythia to reduce model size and computational requirements.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_compression",
        "quantization",
        "sparsification"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NolanoOrg/sparse_quant_llms",
      "help_website": [],
      "license": null,
      "tags": [
        "compression",
        "sparsegpt",
        "gptq",
        "llm"
      ],
      "id": 67
    },
    {
      "name": "BMCook",
      "one_line_profile": "Model compression toolkit for large-scale models",
      "detailed_description": "A model compression toolkit designed for big models, providing methods for quantization, pruning, and distillation to optimize models for deployment.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_compression",
        "quantization",
        "pruning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenBMB/BMCook",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "compression",
        "large-models",
        "optimization"
      ],
      "id": 68
    },
    {
      "name": "CPM.cu",
      "one_line_profile": "Lightweight CUDA implementation for LLM inference on end-devices",
      "detailed_description": "A high-performance CUDA implementation for Large Language Models, optimized for end-device inference with support for sparse architectures, speculative sampling, and quantization.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference",
        "on-device_ai"
      ],
      "application_level": "library",
      "primary_language": "Cuda",
      "repo_url": "https://github.com/OpenBMB/CPM.cu",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "cuda",
        "inference",
        "llm",
        "edge-computing"
      ],
      "id": 69
    },
    {
      "name": "UltraRAG",
      "one_line_profile": "Framework for building complex RAG pipelines",
      "detailed_description": "A framework for constructing Retrieval Augmented Generation (RAG) pipelines, facilitating the integration of external knowledge bases with LLMs for enhanced inference and reasoning.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "rag",
        "inference_augmentation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenBMB/UltraRAG",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "retrieval",
        "llm",
        "pipeline"
      ],
      "id": 70
    },
    {
      "name": "EfficientQAT",
      "one_line_profile": "Efficient Quantization-Aware Training for LLMs",
      "detailed_description": "A library implementing efficient Quantization-Aware Training (QAT) techniques for Large Language Models, enabling the creation of high-performance quantized models.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "quantization",
        "training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenGVLab/EfficientQAT",
      "help_website": [],
      "license": null,
      "tags": [
        "qat",
        "quantization",
        "llm",
        "training"
      ],
      "id": 71
    },
    {
      "name": "VideoChat-Flash",
      "one_line_profile": "Hierarchical compression for long-context video modeling",
      "detailed_description": "A tool and model implementation for efficient long-context video modeling using hierarchical compression techniques to manage memory and computational complexity.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "video_modeling",
        "compression"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenGVLab/VideoChat-Flash",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "video",
        "compression",
        "long-context",
        "multimodal"
      ],
      "id": 72
    },
    {
      "name": "CTranslate2",
      "one_line_profile": "Fast inference engine for Transformer models",
      "detailed_description": "A C++ and Python library for efficient inference with Transformer models, supporting quantization (INT8, INT16) and hardware acceleration on CPU and GPU.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference",
        "quantization"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/OpenNMT/CTranslate2",
      "help_website": [
        "https://opennmt.net/CTranslate2/"
      ],
      "license": "MIT",
      "tags": [
        "inference",
        "transformer",
        "quantization",
        "acceleration"
      ],
      "id": 73
    },
    {
      "name": "OpenRLHF",
      "one_line_profile": "Scalable and high-performance RLHF framework",
      "detailed_description": "A framework for Reinforcement Learning from Human Feedback (RLHF) built on Ray, supporting PPO, GRPO, and other algorithms for fine-tuning large language models.",
      "domains": [
        "AI3"
      ],
      "subtask_category": [
        "fine-tuning",
        "rlhf"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenRLHF/OpenRLHF",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rlhf",
        "fine-tuning",
        "llm",
        "ray"
      ],
      "id": 74
    },
    {
      "name": "Orion",
      "one_line_profile": "Orion-14B foundation model and inference tools",
      "detailed_description": "A repository providing the Orion-14B foundation LLM and associated tools for inference, quantization, and fine-tuning, including chat and RAG capabilities.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "modeling",
        "inference"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/OrionStarAI/Orion",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "foundation-model",
        "inference",
        "quantization"
      ],
      "id": 75
    },
    {
      "name": "simple-onnx-processing-tools",
      "one_line_profile": "Tools for manipulating and optimizing ONNX models",
      "detailed_description": "A collection of tools for splitting, merging, compressing, and modifying ONNX models, facilitating model optimization and deployment workflows.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_optimization",
        "onnx_manipulation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/PINTO0309/simple-onnx-processing-tools",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "onnx",
        "model-processing",
        "optimization",
        "tools"
      ],
      "id": 76
    },
    {
      "name": "tflite2tensorflow",
      "one_line_profile": "Converter for TFLite models to other formats",
      "detailed_description": "A tool to convert .tflite models to various formats including TensorFlow, ONNX, TensorRT, and OpenVINO, supporting quantization and inverse quantization.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_conversion",
        "interoperability"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/PINTO0309/tflite2tensorflow",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "conversion",
        "tflite",
        "onnx",
        "tensorrt"
      ],
      "id": 77
    },
    {
      "name": "FastDeploy",
      "one_line_profile": "High-performance inference and deployment toolkit",
      "detailed_description": "A comprehensive toolkit for deploying Large Language Models (LLMs) and Vision-Language Models (VLMs) based on PaddlePaddle, supporting various hardware backends.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "deployment",
        "inference"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/PaddlePaddle/FastDeploy",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "deployment",
        "inference",
        "paddlepaddle",
        "llm"
      ],
      "id": 78
    },
    {
      "name": "PaddleNLP",
      "one_line_profile": "Comprehensive NLP library with LLM support",
      "detailed_description": "A library for Natural Language Processing based on PaddlePaddle, providing a model zoo of LLMs and SLMs along with tools for training, fine-tuning, and inference.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "nlp",
        "modeling",
        "inference"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/PaddlePaddle/PaddleNLP",
      "help_website": [
        "https://paddlenlp.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "nlp",
        "llm",
        "paddlepaddle",
        "model-zoo"
      ],
      "id": 79
    },
    {
      "name": "PaddleSlim",
      "one_line_profile": "Model compression and architecture search library",
      "detailed_description": "A library for deep model compression (quantization, pruning, distillation) and neural architecture search (NAS) based on PaddlePaddle.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_compression",
        "nas"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/PaddlePaddle/PaddleSlim",
      "help_website": [
        "https://paddleslim.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "compression",
        "quantization",
        "pruning",
        "nas"
      ],
      "id": 80
    },
    {
      "name": "MixQ_Tensorrt_LLM",
      "one_line_profile": "Mixed precision inference implementation using TensorRT-LLM",
      "detailed_description": "A tool enabling mixed precision inference for Large Language Models using NVIDIA's TensorRT-LLM, optimizing performance and memory usage.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_optimization",
        "mixed_precision"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/Qcompiler/MixQ_Tensorrt_LLM",
      "help_website": [],
      "license": null,
      "tags": [
        "tensorrt-llm",
        "mixed-precision",
        "inference"
      ],
      "id": 81
    },
    {
      "name": "MIVisionX",
      "one_line_profile": "AMD computer vision and machine intelligence toolkit",
      "detailed_description": "A comprehensive toolkit from AMD for computer vision and machine intelligence, including optimized implementations of OpenVX and utilities for model inference on AMD hardware.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "computer_vision",
        "inference"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/ROCm/MIVisionX",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "amd",
        "rocm",
        "openvx",
        "computer-vision"
      ],
      "id": 82
    },
    {
      "name": "rwkv.cpp",
      "one_line_profile": "CPU inference engine for RWKV models",
      "detailed_description": "A C++ implementation for efficient inference of RWKV language models on CPUs, supporting INT4, INT5, INT8, and FP16 quantization.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference",
        "quantization"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/RWKV/rwkv.cpp",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rwkv",
        "inference",
        "cpu",
        "quantization"
      ],
      "id": 83
    },
    {
      "name": "redis-inference-optimization",
      "one_line_profile": "Redis module for serving tensors and executing DL graphs",
      "detailed_description": "A Redis module (RedisAI) designed for serving tensors and executing deep learning graphs directly within Redis, enabling high-performance inference pipelines.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "serving",
        "inference"
      ],
      "application_level": "service",
      "primary_language": "C",
      "repo_url": "https://github.com/RedisAI/redis-inference-optimization",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "redis",
        "serving",
        "inference",
        "deep-learning"
      ],
      "id": 84
    },
    {
      "name": "PowerInfer",
      "one_line_profile": "High-speed LLM serving engine for local deployment",
      "detailed_description": "A high-speed inference engine for Large Language Models optimized for local deployment, leveraging activation sparsity to accelerate serving on consumer-grade hardware.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "serving",
        "inference_optimization"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/SJTU-IPADS/PowerInfer",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "inference",
        "llm",
        "sparsity",
        "local-deployment"
      ],
      "id": 85
    },
    {
      "name": "MLServer",
      "one_line_profile": "Multi-framework inference server for ML models",
      "detailed_description": "An open-source inference server that supports multiple machine learning frameworks, enabling standardized model serving and deployment in production environments.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "serving",
        "deployment"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/SeldonIO/MLServer",
      "help_website": [
        "https://mlserver.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "inference-server",
        "serving",
        "mlops",
        "multi-framework"
      ],
      "id": 86
    },
    {
      "name": "ServerlessLLM",
      "one_line_profile": "Serverless serving framework for LLMs",
      "detailed_description": "A framework designed for serverless deployment of Large Language Models, optimizing cold-start times and resource utilization for efficient LLM serving.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "serving",
        "deployment"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/ServerlessLLM/ServerlessLLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "serverless",
        "llm",
        "serving",
        "cloud-computing"
      ],
      "id": 87
    },
    {
      "name": "Model Compression Toolkit (MCT)",
      "one_line_profile": "Neural network model optimization and compression toolkit",
      "detailed_description": "An open-source project providing advanced quantization and compression tools for optimizing neural network models for deployment on efficient, constrained hardware.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_compression",
        "quantization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/SonySemiconductorSolutions/mct-model-optimization",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "compression",
        "quantization",
        "optimization",
        "neural-networks"
      ],
      "id": 88
    },
    {
      "name": "YOLOv3v4-ModelCompression",
      "one_line_profile": "Model compression scripts for YOLO object detection models",
      "detailed_description": "A collection of scripts and tools for compressing YOLOv3 and YOLOv4 models, including pruning and quantization techniques for efficient inference.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_compression",
        "object_detection"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/SpursLipu/YOLOv3v4-ModelCompression-MultidatasetTraining-Multibackbone",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "yolo",
        "compression",
        "pruning",
        "quantization"
      ],
      "id": 89
    },
    {
      "name": "Torch-TRTLLM",
      "one_line_profile": "Converter from HuggingFace models to TensorRT-LLM engines",
      "detailed_description": "A framework (Ditto) that enables direct conversion of HuggingFace PreTrainedModels into TensorRT-LLM engines, simplifying the optimization pipeline for LLMs.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_conversion",
        "inference_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/SqueezeBits/Torch-TRTLLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "tensorrt-llm",
        "huggingface",
        "conversion",
        "optimization"
      ],
      "id": 90
    },
    {
      "name": "EmbedAnything",
      "one_line_profile": "High-performance inference and ingestion pipeline in Rust",
      "detailed_description": "A modular and memory-safe library built in Rust for high-performance inference, data ingestion, and indexing, suitable for building efficient RAG and search pipelines.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference",
        "data_ingestion"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/StarlightSearch/EmbedAnything",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rust",
        "inference",
        "embedding",
        "rag"
      ],
      "id": 91
    },
    {
      "name": "torch-model-compression",
      "one_line_profile": "Automated model structure analysis and compression toolset for PyTorch",
      "detailed_description": "A toolset designed for PyTorch models that provides automated model structure analysis and a library of model compression algorithms to optimize inference efficiency.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_compression",
        "structure_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/THU-MIG/torch-model-compression",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pytorch",
        "model-compression",
        "optimization"
      ],
      "id": 92
    },
    {
      "name": "AngelSlim",
      "one_line_profile": "Model compression toolkit for enhanced usability and efficiency",
      "detailed_description": "A comprehensive model compression toolkit developed by Tencent, designed to improve the usability and efficiency of deploying deep learning models through various compression techniques.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_compression",
        "optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Tencent/AngelSlim",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "model-compression",
        "deep-learning",
        "tencent"
      ],
      "id": 93
    },
    {
      "name": "PocketFlow",
      "one_line_profile": "Automatic Model Compression (AutoMC) framework",
      "detailed_description": "An open-source framework for automatic model compression (AutoMC) that integrates various compression algorithms to develop smaller and faster AI applications with minimal human effort.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_compression",
        "automl"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/Tencent/PocketFlow",
      "help_website": [
        "https://pocketflow.github.io/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "automc",
        "model-compression",
        "tensorflow"
      ],
      "id": 94
    },
    {
      "name": "TNN",
      "one_line_profile": "High-performance deep learning inference framework",
      "detailed_description": "A uniform deep learning inference framework for mobile, desktop, and server platforms, featuring cross-platform capability, high performance, and model compression support.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_engine",
        "model_serving"
      ],
      "application_level": "framework",
      "primary_language": "C++",
      "repo_url": "https://github.com/Tencent/TNN",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "inference",
        "cross-platform",
        "mobile-ai"
      ],
      "id": 95
    },
    {
      "name": "ncnn",
      "one_line_profile": "High-performance neural network inference framework for mobile",
      "detailed_description": "A high-performance neural network inference framework optimized for mobile platforms, supporting various model formats and enabling efficient AI deployment on edge devices.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_engine",
        "edge_computing"
      ],
      "application_level": "framework",
      "primary_language": "C++",
      "repo_url": "https://github.com/Tencent/ncnn",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "inference",
        "mobile",
        "neural-network"
      ],
      "id": 96
    },
    {
      "name": "AQLM",
      "one_line_profile": "Extreme Compression of LLMs via Additive Quantization",
      "detailed_description": "Official implementation of Additive Quantization for Language Models (AQLM), providing tools for extreme compression of Large Language Models while maintaining performance.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "quantization",
        "model_compression"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Vahe1994/AQLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "quantization",
        "llm",
        "compression"
      ],
      "id": 97
    },
    {
      "name": "voltaML",
      "one_line_profile": "Lightweight library for accelerating ML/DL models",
      "detailed_description": "A lightweight library designed to convert and run machine learning and deep learning models in high-performance inference runtimes like TensorRT, TorchScript, ONNX, and TVM.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_optimization",
        "model_conversion"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/VoltaML/voltaML",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "acceleration",
        "tensorrt",
        "inference"
      ],
      "id": 98
    },
    {
      "name": "MACE",
      "one_line_profile": "Mobile AI Compute Engine",
      "detailed_description": "A deep learning inference framework optimized for mobile heterogeneous computing platforms, supporting efficient execution of neural networks on mobile devices.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_engine",
        "mobile_computing"
      ],
      "application_level": "framework",
      "primary_language": "C++",
      "repo_url": "https://github.com/XiaoMi/mace",
      "help_website": [
        "https://mace.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "mobile-ai",
        "inference",
        "heterogeneous-computing"
      ],
      "id": 99
    },
    {
      "name": "Keras-inference-time-optimizer",
      "one_line_profile": "Keras model layer structure optimizer",
      "detailed_description": "A tool to optimize the layer structure of Keras models to reduce computation time and improve inference speed.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_optimization",
        "model_profiling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ZFTurbo/Keras-inference-time-optimizer",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "keras",
        "optimization",
        "inference"
      ],
      "id": 100
    },
    {
      "name": "KVCache-Factory",
      "one_line_profile": "Unified KV Cache Compression Methods for Auto-Regressive Models",
      "detailed_description": "A unified framework and library for applying various Key-Value (KV) cache compression methods to auto-regressive models to optimize memory usage and inference speed.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "memory_optimization",
        "inference_acceleration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Zefan-Cai/KVCache-Factory",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "kv-cache",
        "compression",
        "llm"
      ],
      "id": 101
    },
    {
      "name": "llama-cpp-python",
      "one_line_profile": "Python bindings for llama.cpp",
      "detailed_description": "Python bindings for llama.cpp, enabling the execution of quantized Large Language Models (LLMs) efficiently in Python environments.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_runtime",
        "model_serving"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/abetlen/llama-cpp-python",
      "help_website": [
        "https://llama-cpp-python.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "llm",
        "inference",
        "quantization"
      ],
      "id": 102
    },
    {
      "name": "ggify",
      "one_line_profile": "Huggingface to GGML/GGUF converter",
      "detailed_description": "A utility tool to download models from Huggingface Hub and convert them to GGML/GGUF formats for use with llama.cpp.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_conversion",
        "data_preparation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/akx/ggify",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gguf",
        "conversion",
        "llama.cpp"
      ],
      "id": 103
    },
    {
      "name": "optillm",
      "one_line_profile": "Optimizing inference proxy for LLMs",
      "detailed_description": "An optimizing inference proxy that sits between LLM applications and providers, implementing techniques like mixture-of-agents and speculative decoding to improve performance and accuracy.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_optimization",
        "proxy_service"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/algorithmicsuperintelligence/optillm",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "inference",
        "proxy",
        "optimization"
      ],
      "id": 104
    },
    {
      "name": "ServeGen",
      "one_line_profile": "LLM serving workload generator",
      "detailed_description": "A framework for generating realistic Large Language Model (LLM) serving workloads to benchmark and evaluate serving systems.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "benchmarking",
        "workload_generation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/alibaba/ServeGen",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "benchmarking",
        "serving",
        "llm"
      ],
      "id": 105
    },
    {
      "name": "TinyNeuralNetwork",
      "one_line_profile": "Efficient deep learning model compression framework",
      "detailed_description": "An efficient and easy-to-use framework for deep learning model compression, supporting techniques like pruning and quantization to optimize models for deployment.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_compression",
        "optimization"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/alibaba/TinyNeuralNetwork",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "compression",
        "pruning",
        "quantization"
      ],
      "id": 106
    },
    {
      "name": "SQLFlow",
      "one_line_profile": "SQL-based machine learning bridge",
      "detailed_description": "A bridge that connects SQL engines with machine learning toolkits like TensorFlow, enabling users to perform model training, prediction, and inference using extended SQL syntax.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_training",
        "inference_interface"
      ],
      "application_level": "platform",
      "primary_language": "Go",
      "repo_url": "https://github.com/alipay/SQLFlow",
      "help_website": [
        "https://sqlflow.org/"
      ],
      "license": null,
      "tags": [
        "sql",
        "machine-learning",
        "interface"
      ],
      "id": 107
    },
    {
      "name": "byzer-llm",
      "one_line_profile": "Full-lifecycle LLM toolchain",
      "detailed_description": "A comprehensive toolchain for pretraining, fine-tuning, and serving Large Language Models (LLMs), designed to make LLM deployment accessible and efficient.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "fine_tuning"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/allwefantasy/byzer-llm",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "serving",
        "finetuning"
      ],
      "id": 108
    },
    {
      "name": "Alpa",
      "one_line_profile": "Auto-parallelization for large-scale neural networks",
      "detailed_description": "A system for training and serving large-scale neural networks that automatically generates parallelization strategies for distributed execution.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "distributed_serving",
        "distributed_training"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/alpa-projects/alpa",
      "help_website": [
        "https://alpa.ai/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "distributed-systems",
        "parallelization",
        "serving"
      ],
      "id": 109
    },
    {
      "name": "cONNXr",
      "one_line_profile": "Pure C ONNX runtime for embedded devices",
      "detailed_description": "A pure C implementation of the ONNX runtime with zero dependencies, designed for inference on embedded devices.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_runtime",
        "embedded_ai"
      ],
      "application_level": "library",
      "primary_language": "C",
      "repo_url": "https://github.com/alrevuelta/cONNXr",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "onnx",
        "embedded",
        "c"
      ],
      "id": 110
    },
    {
      "name": "RyzenAI-SW",
      "one_line_profile": "AMD Ryzen AI inference software",
      "detailed_description": "Software tools and runtime libraries provided by AMD for optimizing and deploying AI inference on Ryzen AI-powered hardware.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_optimization",
        "hardware_acceleration"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/amd/RyzenAI-SW",
      "help_website": [
        "https://ryzenai.docs.amd.com/"
      ],
      "license": "MIT",
      "tags": [
        "amd",
        "inference",
        "npu"
      ],
      "id": 111
    },
    {
      "name": "flux-fp8-api",
      "one_line_profile": "Optimized Flux diffusion model serving implementation",
      "detailed_description": "An implementation of the Flux diffusion model using quantized FP8 matrix multiplication and optimized layers for faster inference on consumer devices, exposed via an API.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_serving",
        "quantization"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/aredden/flux-fp8-api",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "diffusion",
        "quantization",
        "serving"
      ],
      "id": 112
    },
    {
      "name": "LLM-Inference-Bench",
      "one_line_profile": "Benchmark suite for LLM inference",
      "detailed_description": "A benchmarking tool developed by Argonne National Laboratory to evaluate the performance of Large Language Model inference across different configurations.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/argonne-lcf/LLM-Inference-Bench",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "benchmarking",
        "llm",
        "hpc"
      ],
      "id": 113
    },
    {
      "name": "sqlite-lembed",
      "one_line_profile": "SQLite extension for GGUF embeddings",
      "detailed_description": "A SQLite extension that enables the generation of text embeddings directly within the database using GGUF models via llama.cpp, facilitating local vector search and data processing.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "data_processing",
        "embedding_generation"
      ],
      "application_level": "library",
      "primary_language": "C",
      "repo_url": "https://github.com/asg017/sqlite-lembed",
      "help_website": [],
      "license": null,
      "tags": [
        "sqlite",
        "embedding",
        "gguf"
      ],
      "id": 114
    },
    {
      "name": "nos",
      "one_line_profile": "Fast and flexible PyTorch inference server for local and cloud deployment",
      "detailed_description": "A high-performance inference server designed to run PyTorch models efficiently on various hardware backends. It supports dynamic batching and hardware acceleration, facilitating the deployment of scientific AI models.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "inference_optimization"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/autonomi-ai/nos",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "inference-server",
        "pytorch",
        "gpu-acceleration"
      ],
      "id": 115
    },
    {
      "name": "Multi Model Server",
      "one_line_profile": "Tool for serving neural net models for inference",
      "detailed_description": "A flexible and easy-to-use tool for serving deep learning models trained with various frameworks. It provides an HTTP frontend for inference requests and manages model loading and scaling.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving"
      ],
      "application_level": "service",
      "primary_language": "Java",
      "repo_url": "https://github.com/awslabs/multi-model-server",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "inference-serving",
        "model-deployment",
        "mxnet",
        "pytorch"
      ],
      "id": 116
    },
    {
      "name": "llama-on-lambda",
      "one_line_profile": "Deployment tool for running llama.cpp compatible LLMs on AWS Lambda",
      "detailed_description": "A utility that enables the deployment of quantized Large Language Models (LLMs) using llama.cpp on serverless infrastructure (AWS Lambda), facilitating cost-effective inference for scientific NLP tasks.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "deployment"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/baileytec-labs/llama-on-lambda",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "serverless",
        "llm-inference",
        "aws-lambda",
        "quantization"
      ],
      "id": 117
    },
    {
      "name": "BentoLMDeploy",
      "one_line_profile": "Integration tool for self-hosting LLMs with LMDeploy and BentoML",
      "detailed_description": "A bridge tool that combines the high-performance inference capabilities of LMDeploy with the model serving infrastructure of BentoML, enabling efficient deployment of Large Language Models.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "inference_optimization"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/bentoml/BentoLMDeploy",
      "help_website": [],
      "license": null,
      "tags": [
        "llm-serving",
        "bentoml",
        "lmdeploy"
      ],
      "id": 118
    },
    {
      "name": "llm-optimizer",
      "one_line_profile": "Benchmark and optimize LLM inference across frameworks",
      "detailed_description": "A toolkit designed to profile, benchmark, and optimize the inference performance of Large Language Models across different serving frameworks, aiding in the efficient deployment of scientific models.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_optimization",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/bentoml/llm-optimizer",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "optimization",
        "inference-benchmark"
      ],
      "id": 119
    },
    {
      "name": "bitsandbytes",
      "one_line_profile": "Accessible large language models via k-bit quantization for PyTorch",
      "detailed_description": "A lightweight wrapper around CUDA custom functions, in particular 8-bit optimizers, matrix multiplication (LLM.int8()), and quantization functions. It is a critical dependency for efficient training and inference of large models on consumer hardware.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "quantization",
        "model_compression"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/bitsandbytes-foundation/bitsandbytes",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "quantization",
        "cuda",
        "optimization",
        "llm"
      ],
      "id": 120
    },
    {
      "name": "X-LLM",
      "one_line_profile": "Library for cutting edge and easy LLM finetuning",
      "detailed_description": "A library designed to simplify the fine-tuning process of Large Language Models, supporting various optimization techniques to adapt models for specific scientific or domain tasks.",
      "domains": [
        "AI3"
      ],
      "subtask_category": [
        "model_training",
        "fine_tuning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/bobazooba/xllm",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "fine-tuning",
        "training-framework"
      ],
      "id": 121
    },
    {
      "name": "ByteTransformer",
      "one_line_profile": "Optimized BERT transformer inference on NVIDIA GPU",
      "detailed_description": "A high-performance inference engine for Transformer models (like BERT), providing highly optimized CUDA kernels for variable-length sequence processing, significantly accelerating scientific text analysis tasks.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_optimization",
        "model_serving"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/bytedance/ByteTransformer",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "transformer",
        "inference-acceleration",
        "cuda",
        "bert"
      ],
      "id": 122
    },
    {
      "name": "AutoAWQ",
      "one_line_profile": "Implementation of the AWQ algorithm for 4-bit quantization",
      "detailed_description": "A library implementing Activation-aware Weight Quantization (AWQ) for LLMs, enabling significant speedups and memory reduction during inference while maintaining model accuracy.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "quantization",
        "inference_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/casper-hansen/AutoAWQ",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "quantization",
        "awq",
        "llm-inference"
      ],
      "id": 123
    },
    {
      "name": "LLMServingSim",
      "one_line_profile": "HW/SW Co-Simulation Infrastructure for LLM Inference Serving",
      "detailed_description": "A simulation framework designed to evaluate and optimize the hardware/software stack for Large Language Model inference serving at scale, aiding in system architecture research.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "simulation",
        "inference_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/casys-kaist/LLMServingSim",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "simulation",
        "llm-serving",
        "system-architecture"
      ],
      "id": 124
    },
    {
      "name": "flex-nano-vllm",
      "one_line_profile": "Minimal vllm-style inference engine for fast Gemma 2 inference",
      "detailed_description": "A lightweight inference engine leveraging FlexAttention to provide fast inference capabilities for specific LLM architectures (e.g., Gemma 2), serving as a specialized tool for model execution.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "inference_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/changjonathanc/flex-nano-vllm",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "inference-engine",
        "vllm",
        "gemma"
      ],
      "id": 125
    },
    {
      "name": "model_compression",
      "one_line_profile": "Implementation of model compression with knowledge distilling",
      "detailed_description": "A toolkit implementing various model compression techniques, specifically knowledge distillation, to reduce model size and improve inference speed for deployment.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_compression",
        "distillation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/chengshengchan/model_compression",
      "help_website": [],
      "license": null,
      "tags": [
        "model-compression",
        "knowledge-distillation"
      ],
      "id": 126
    },
    {
      "name": "Comfy-WaveSpeed",
      "one_line_profile": "Inference optimization solution for ComfyUI",
      "detailed_description": "An optimization framework integrated into ComfyUI to accelerate the inference of generative AI models, facilitating faster image generation for research and creative workflows.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_optimization",
        "image_generation"
      ],
      "application_level": "plugin",
      "primary_language": "Python",
      "repo_url": "https://github.com/chengzeyi/Comfy-WaveSpeed",
      "help_website": [
        "https://wavespeed.ai/"
      ],
      "license": "MIT",
      "tags": [
        "comfyui",
        "inference-acceleration",
        "stable-diffusion"
      ],
      "id": 127
    },
    {
      "name": "stable-fast",
      "one_line_profile": "Inference performance optimization framework for HuggingFace Diffusers",
      "detailed_description": "A highly optimized inference framework for Stable Diffusion models on NVIDIA GPUs, providing significant speedups for diffusion-based generative tasks.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_optimization",
        "image_generation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/chengzeyi/stable-fast",
      "help_website": [
        "https://wavespeed.ai/"
      ],
      "license": "MIT",
      "tags": [
        "stable-diffusion",
        "inference-optimization",
        "cuda"
      ],
      "id": 128
    },
    {
      "name": "ialacol",
      "one_line_profile": "Lightweight OpenAI drop-in replacement for Kubernetes",
      "detailed_description": "A serving infrastructure tool that allows deploying open-source LLMs on Kubernetes with an OpenAI-compatible API, facilitating scalable model inference in research environments.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "deployment"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/chenhunghan/ialacol",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "kubernetes",
        "llm-serving",
        "openai-api"
      ],
      "id": 129
    },
    {
      "name": "topicGPT",
      "one_line_profile": "Prompt-Based Framework for Topic Modeling",
      "detailed_description": "A framework that utilizes Large Language Models to perform topic modeling on text data, providing a tool for qualitative and quantitative text analysis in social and computational sciences.",
      "domains": [
        "AI3"
      ],
      "subtask_category": [
        "data_analysis",
        "text_mining"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/chtmp223/topicGPT",
      "help_website": [],
      "license": null,
      "tags": [
        "topic-modeling",
        "llm",
        "nlp"
      ],
      "id": 130
    },
    {
      "name": "ComfyUI-GGUF",
      "one_line_profile": "GGUF Quantization support for native ComfyUI models",
      "detailed_description": "A plugin for ComfyUI that enables the loading and inference of models quantized in the GGUF format, allowing for efficient execution of generative models on consumer hardware.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "quantization",
        "inference_optimization"
      ],
      "application_level": "plugin",
      "primary_language": "Python",
      "repo_url": "https://github.com/city96/ComfyUI-GGUF",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "gguf",
        "comfyui",
        "quantization"
      ],
      "id": 131
    },
    {
      "name": "WhisperLive",
      "one_line_profile": "A nearly-live implementation of OpenAI's Whisper",
      "detailed_description": "A real-time implementation of the Whisper automatic speech recognition model, enabling live audio transcription and processing for research and application development.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_implementation",
        "speech_recognition"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/collabora/WhisperLive",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "whisper",
        "asr",
        "real-time-inference"
      ],
      "id": 132
    },
    {
      "name": "ramalama",
      "one_line_profile": "Tool for local serving of AI models using containers",
      "detailed_description": "A command-line tool that simplifies the management and serving of AI models using container technology (OCI), making it easier to deploy inference services reproducibly.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "deployment"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/containers/ramalama",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "containers",
        "inference-serving",
        "local-ai"
      ],
      "id": 133
    },
    {
      "name": "cuckoo",
      "one_line_profile": "Decentralized AI Model-Serving Platform",
      "detailed_description": "A platform for decentralized serving of AI models, enabling distributed inference for generative AI and LLMs, potentially utilizing shared GPU resources.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "distributed_inference"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/cuckoo-network/cuckoo",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "decentralized-ai",
        "model-serving",
        "gpu-sharing"
      ],
      "id": 134
    },
    {
      "name": "cube-studio",
      "one_line_profile": "Cloud-native one-stop machine learning/deep learning platform",
      "detailed_description": "A comprehensive MLOps platform supporting the full lifecycle of AI development, including data management, notebook development, distributed training, and model serving/inference optimization.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "platform",
        "model_training",
        "model_serving"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/data-infra/cube-studio",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "mlops",
        "distributed-training",
        "model-serving",
        "cloud-native"
      ],
      "id": 135
    },
    {
      "name": "DeepSpeed",
      "one_line_profile": "Deep learning optimization library for distributed training and inference",
      "detailed_description": "A widely used library that provides extreme optimization for large-scale deep learning training and inference, enabling the training of massive models with efficient memory and compute usage.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_training",
        "inference_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/deepspeedai/DeepSpeed",
      "help_website": [
        "https://www.deepspeed.ai/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "distributed-training",
        "optimization",
        "inference",
        "llm"
      ],
      "id": 136
    },
    {
      "name": "DeepSpeed-MII",
      "one_line_profile": "Low-latency and high-throughput inference powered by DeepSpeed",
      "detailed_description": "A library specifically designed to accelerate model inference using DeepSpeed technologies, providing easy-to-use APIs for deploying high-performance AI models.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_optimization",
        "model_serving"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/deepspeedai/DeepSpeed-MII",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "inference-acceleration",
        "deepspeed",
        "low-latency"
      ],
      "id": 137
    },
    {
      "name": "Diffbot LLM Inference Server",
      "one_line_profile": "High-performance inference server for Large Language Models",
      "detailed_description": "A dedicated server implementation for deploying and serving Large Language Models, focusing on inference capabilities.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "inference_server"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/diffbot/diffbot-llm-inference",
      "help_website": [],
      "license": null,
      "tags": [
        "llm",
        "inference-server",
        "serving"
      ],
      "id": 138
    },
    {
      "name": "NanoLLM",
      "one_line_profile": "Optimized local inference library for LLMs and multimodal agents",
      "detailed_description": "A lightweight and optimized library for running Large Language Models locally, supporting quantization, vision-language models, and RAG pipelines, particularly suitable for edge computing in scientific data collection.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_optimization",
        "quantization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/dusty-nv/NanoLLM",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "edge-ai",
        "quantization",
        "rag",
        "local-inference"
      ],
      "id": 139
    },
    {
      "name": "mixtral-offloading",
      "one_line_profile": "Efficient inference of Mixtral-8x7B on consumer hardware via offloading",
      "detailed_description": "A tool enabling the execution of large Mixture-of-Experts models (like Mixtral-8x7B) on limited memory hardware (e.g., Colab, consumer GPUs) through efficient RAM/VRAM offloading strategies.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_optimization",
        "memory_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/dvmazur/mixtral-offloading",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "offloading",
        "mixture-of-experts",
        "inference-optimization"
      ],
      "id": 140
    },
    {
      "name": "Atom",
      "one_line_profile": "Low-bit quantization framework for efficient LLM serving",
      "detailed_description": "A quantization library implementing low-bit techniques to maintain accuracy while significantly reducing the memory footprint and increasing the throughput of Large Language Model serving.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "quantization",
        "model_compression"
      ],
      "application_level": "library",
      "primary_language": "Cuda",
      "repo_url": "https://github.com/efeslab/Atom",
      "help_website": [],
      "license": null,
      "tags": [
        "quantization",
        "llm-serving",
        "low-bit"
      ],
      "id": 141
    },
    {
      "name": "Nanoflow",
      "one_line_profile": "High-throughput serving framework for Large Language Models",
      "detailed_description": "A high-performance serving system designed to maximize throughput for LLM inference, utilizing advanced scheduling and memory management techniques.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "inference_acceleration"
      ],
      "application_level": "platform",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/efeslab/Nanoflow",
      "help_website": [],
      "license": null,
      "tags": [
        "high-throughput",
        "serving-framework",
        "llm"
      ],
      "id": 142
    },
    {
      "name": "cake",
      "one_line_profile": "Distributed inference engine for LLMs and StableDiffusion",
      "detailed_description": "A Rust-based distributed inference framework supporting both Large Language Models and image generation models, designed to run across diverse hardware including mobile, desktop, and servers.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "distributed_inference",
        "model_serving"
      ],
      "application_level": "service",
      "primary_language": "Rust",
      "repo_url": "https://github.com/evilsocket/cake",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "distributed-computing",
        "rust",
        "inference-engine"
      ],
      "id": 143
    },
    {
      "name": "FBTT-Embedding",
      "one_line_profile": "Tensor Train compression library for sparse embedding tables",
      "detailed_description": "A library for compressing sparse embedding tables in large-scale machine learning models using Tensor Train decomposition, reducing memory footprint while maintaining model quality.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_compression",
        "embedding_optimization"
      ],
      "application_level": "library",
      "primary_language": "Cuda",
      "repo_url": "https://github.com/facebookresearch/FBTT-Embedding",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "tensor-train",
        "compression",
        "embeddings"
      ],
      "id": 144
    },
    {
      "name": "LLM-QAT",
      "one_line_profile": "Data-free quantization aware training for LLMs",
      "detailed_description": "A toolkit for performing Quantization Aware Training (QAT) on Large Language Models without requiring original training data, enabling the creation of efficient quantized models.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "quantization",
        "model_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/LLM-QAT",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "qat",
        "quantization",
        "data-free"
      ],
      "id": 145
    },
    {
      "name": "LayerSkip",
      "one_line_profile": "Early exit inference and self-speculative decoding for LLMs",
      "detailed_description": "An inference optimization tool that enables early exit strategies and self-speculative decoding to accelerate Large Language Model generation.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_acceleration",
        "speculative_decoding"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/LayerSkip",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "early-exit",
        "acceleration",
        "inference"
      ],
      "id": 146
    },
    {
      "name": "diffq",
      "one_line_profile": "Differentiable quantization library",
      "detailed_description": "A library for differentiable quantization using pseudo quantization noise, allowing automatic tuning of bit-width per weight to balance model size and accuracy.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "quantization",
        "model_compression"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/diffq",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "differentiable-quantization",
        "compression",
        "pytorch"
      ],
      "id": 147
    },
    {
      "name": "FlashInfer",
      "one_line_profile": "High-performance kernel library for LLM serving",
      "detailed_description": "A library providing highly optimized CUDA kernels for Large Language Model serving, focusing on accelerating attention mechanisms and other critical operations.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_acceleration",
        "kernel_optimization"
      ],
      "application_level": "library",
      "primary_language": "Cuda",
      "repo_url": "https://github.com/flashinfer-ai/flashinfer",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "cuda-kernels",
        "flash-attention",
        "serving"
      ],
      "id": 148
    },
    {
      "name": "GPTQ-triton",
      "one_line_profile": "Triton kernel implementation for GPTQ inference",
      "detailed_description": "A specialized Triton kernel implementation for running inference on models quantized with GPTQ, enabling efficient execution on GPUs.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "quantization",
        "inference_acceleration"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/fpgaminer/GPTQ-triton",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "gptq",
        "triton",
        "quantization"
      ],
      "id": 149
    },
    {
      "name": "EvaDB",
      "one_line_profile": "Database system for AI-powered applications",
      "detailed_description": "A database system designed to facilitate the management and analysis of data using AI models, supporting SQL-like queries for video, image, and text analysis.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "data_management",
        "model_inference"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/georgia-tech-db/evadb",
      "help_website": [
        "https://evadb.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "ai-database",
        "sql",
        "multimodal"
      ],
      "id": 150
    },
    {
      "name": "llama.cpp",
      "one_line_profile": "Port of Facebook's LLaMA model in C/C++ for efficient local inference",
      "detailed_description": "A widely used C/C++ implementation for running Large Language Models efficiently on consumer hardware (CPU/GPU), supporting various quantization methods and model architectures.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_inference",
        "quantization"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/ggml-org/llama.cpp",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "inference",
        "quantization",
        "cpp",
        "local-llm"
      ],
      "id": 151
    },
    {
      "name": "Orion",
      "one_line_profile": "ONNX Runtime in Cairo for verifiable ML inference",
      "detailed_description": "An implementation of the ONNX Runtime in Cairo, enabling verifiable machine learning inference using STARK proofs, useful for ensuring integrity in scientific computation.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_inference",
        "verifiable_computing"
      ],
      "application_level": "library",
      "primary_language": "Cairo",
      "repo_url": "https://github.com/gizatechxyz/orion",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "zkml",
        "onnx",
        "verifiable-inference"
      ],
      "id": 152
    },
    {
      "name": "XNNPACK",
      "one_line_profile": "High-efficiency floating-point neural network inference operators",
      "detailed_description": "A library of highly optimized neural network inference operators for ARM, x86, and WebAssembly, serving as a backend for frameworks like TensorFlow Lite and PyTorch.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_acceleration",
        "kernel_optimization"
      ],
      "application_level": "library",
      "primary_language": "C",
      "repo_url": "https://github.com/google/XNNPACK",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "inference-operators",
        "optimization",
        "simd"
      ],
      "id": 153
    },
    {
      "name": "llama.go",
      "one_line_profile": "Pure Golang implementation of llama.cpp",
      "detailed_description": "A pure Go implementation of the LLaMA inference engine, providing an alternative runtime for deploying LLMs in Go-based scientific or infrastructure environments.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_inference"
      ],
      "application_level": "solver",
      "primary_language": "Go",
      "repo_url": "https://github.com/gotzmann/llama.go",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "golang",
        "inference",
        "llm"
      ],
      "id": 154
    },
    {
      "name": "gpustack",
      "one_line_profile": "GPU cluster manager for optimized AI model deployment",
      "detailed_description": "A platform for managing GPU clusters to efficiently deploy and serve AI models, handling scheduling and resource allocation for inference workloads.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "resource_management"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/gpustack/gpustack",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "gpu-cluster",
        "deployment",
        "serving"
      ],
      "id": 155
    },
    {
      "name": "llama-box",
      "one_line_profile": "LM inference server implementation based on llama.cpp",
      "detailed_description": "A lightweight inference server wrapping llama.cpp, providing API endpoints for serving Large Language Models.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "inference_server"
      ],
      "application_level": "service",
      "primary_language": "C++",
      "repo_url": "https://github.com/gpustack/llama-box",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "inference-server",
        "llama.cpp",
        "api"
      ],
      "id": 156
    },
    {
      "name": "gLLM",
      "one_line_profile": "Global Balanced Pipeline Parallelism System for Distributed LLM Serving",
      "detailed_description": "A distributed serving system for Large Language Models that utilizes token throttling and global pipeline parallelism to optimize throughput and latency.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "distributed_inference"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/gty111/gLLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "pipeline-parallelism",
        "distributed-serving",
        "llm"
      ],
      "id": 157
    },
    {
      "name": "llgtrt",
      "one_line_profile": "TensorRT-LLM server with Structured Outputs",
      "detailed_description": "A Rust-based server for TensorRT-LLM that specifically supports structured output generation (JSON), useful for extracting structured scientific data from LLMs.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "structured_generation"
      ],
      "application_level": "service",
      "primary_language": "Rust",
      "repo_url": "https://github.com/guidance-ai/llgtrt",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "tensorrt-llm",
        "structured-output",
        "rust"
      ],
      "id": 158
    },
    {
      "name": "h2oGPT",
      "one_line_profile": "Private local GPT platform for document analysis",
      "detailed_description": "A platform for running LLMs locally to query documents, images, and videos privately, supporting RAG and various inference backends, highly relevant for secure scientific data analysis.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "rag_platform"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/h2oai/h2ogpt",
      "help_website": [
        "https://gpt-docs.h2o.ai/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "local-gpt",
        "rag",
        "private-ai"
      ],
      "id": 159
    },
    {
      "name": "Hailo Model Zoo",
      "one_line_profile": "Pre-trained models and evaluation environment for Hailo hardware",
      "detailed_description": "A collection of pre-trained models and tools for building, compiling, and evaluating models on Hailo AI processors, facilitating edge AI deployment.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_management",
        "inference_optimization"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/hailo-ai/hailo_model_zoo",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "model-zoo",
        "edge-ai",
        "hailo"
      ],
      "id": 160
    },
    {
      "name": "LLM-Pruner",
      "one_line_profile": "Structural pruning tool for Large Language Models",
      "detailed_description": "A tool for performing structural pruning on Large Language Models to reduce model size and inference cost while preserving performance, supporting various open-source models.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_compression",
        "pruning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/horseee/LLM-Pruner",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "pruning",
        "model-compression",
        "efficient-llm"
      ],
      "id": 161
    },
    {
      "name": "FastFold",
      "one_line_profile": "Optimized training and inference for AlphaFold",
      "detailed_description": "A high-performance implementation for training and inference of AlphaFold, optimizing protein structure prediction workflows on GPU clusters.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_optimization",
        "scientific_modeling"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/hpcaitech/FastFold",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "alphafold",
        "protein-folding",
        "optimization"
      ],
      "id": 162
    },
    {
      "name": "SINQ",
      "one_line_profile": "Fast and high-quality quantization method for LLMs",
      "detailed_description": "A quantization tool designed to compress Large Language Models while preserving accuracy, offering a balance between model size and performance.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "quantization",
        "model_compression"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/huawei-csl/SINQ",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "quantization",
        "compression",
        "llm"
      ],
      "id": 163
    },
    {
      "name": "Huawei Noah Pretrained Language Model",
      "one_line_profile": "Pretrained models and optimization techniques from Huawei Noah's Ark Lab",
      "detailed_description": "A repository containing pretrained language models and associated optimization code (compression, quantization) developed by Huawei Noah's Ark Lab.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_management",
        "model_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/huawei-noah/Pretrained-Language-Model",
      "help_website": [],
      "license": null,
      "tags": [
        "pretrained-models",
        "optimization",
        "huawei"
      ],
      "id": 164
    },
    {
      "name": "Inference Benchmarker",
      "one_line_profile": "Benchmarking tool for inference servers",
      "detailed_description": "A tool designed to benchmark the performance of various inference servers, helping to evaluate latency and throughput for model serving.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Rust",
      "repo_url": "https://github.com/huggingface/inference-benchmarker",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "benchmarking",
        "inference",
        "latency"
      ],
      "id": 165
    },
    {
      "name": "Optimum",
      "one_line_profile": "Hardware optimization toolkit for Hugging Face models",
      "detailed_description": "An extension of Hugging Face Transformers that provides tools for training and inference optimization on specific hardware (ONNX Runtime, Intel Neural Compressor, OpenVINO, etc.), enabling quantization and pruning.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_optimization",
        "quantization",
        "training_acceleration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/huggingface/optimum",
      "help_website": [
        "https://huggingface.co/docs/optimum/index"
      ],
      "license": "Apache-2.0",
      "tags": [
        "optimization",
        "quantization",
        "hardware-acceleration"
      ],
      "id": 166
    },
    {
      "name": "Optimum Benchmark",
      "one_line_profile": "Unified benchmarking utility for Transformers and PEFT",
      "detailed_description": "A multi-backend benchmarking tool for Transformers, Diffusers, and PEFT models, supporting various hardware optimizations and quantization schemes to evaluate inference performance.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/huggingface/optimum-benchmark",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "benchmark",
        "transformers",
        "quantization"
      ],
      "id": 167
    },
    {
      "name": "Optimum Intel",
      "one_line_profile": "Intel-specific optimization for Hugging Face models",
      "detailed_description": "A library that accelerates inference and training of Hugging Face models on Intel hardware using Intel's optimization tools like OpenVINO and Neural Compressor.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_optimization",
        "quantization"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/huggingface/optimum-intel",
      "help_website": [
        "https://huggingface.co/docs/optimum/intel/index"
      ],
      "license": "Apache-2.0",
      "tags": [
        "intel",
        "openvino",
        "optimization"
      ],
      "id": 168
    },
    {
      "name": "Optimum ONNX",
      "one_line_profile": "ONNX export and inference optimization tool",
      "detailed_description": "A tool to export Hugging Face models to ONNX format and run optimized inference using ONNX Runtime.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_conversion",
        "inference_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/huggingface/optimum-onnx",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "onnx",
        "inference",
        "export"
      ],
      "id": 169
    },
    {
      "name": "yzma",
      "one_line_profile": "Go bindings for local llama.cpp inference",
      "detailed_description": "A library enabling Go applications to directly integrate llama.cpp for local inference with hardware acceleration support.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_serving",
        "model_integration"
      ],
      "application_level": "library",
      "primary_language": "Go",
      "repo_url": "https://github.com/hybridgroup/yzma",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "go",
        "llama.cpp",
        "inference"
      ],
      "id": 170
    },
    {
      "name": "ik_llama.cpp",
      "one_line_profile": "Optimized fork of llama.cpp with SOTA quantization",
      "detailed_description": "A fork of the llama.cpp inference engine featuring additional state-of-the-art quantization methods and performance improvements for running LLMs locally.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_engine",
        "quantization"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/ikawrakow/ik_llama.cpp",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "quantization",
        "inference"
      ],
      "id": 171
    },
    {
      "name": "ShaderNN",
      "one_line_profile": "Lightweight mobile inference framework for CNNs",
      "detailed_description": "A deep learning inference framework optimized for running Convolutional Neural Networks on mobile platforms using shader-based acceleration.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_engine",
        "mobile_inference"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/inferenceengine/shadernn",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "mobile",
        "cnn",
        "inference"
      ],
      "id": 172
    },
    {
      "name": "Triton Co-Pilot",
      "one_line_profile": "Glue code generator for Triton Inference Server",
      "detailed_description": "A utility to generate configuration and glue code to simplify the deployment of models on Nvidia Triton Inference Server.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "deployment_automation",
        "serving_configuration"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/inferless/triton-co-pilot",
      "help_website": [],
      "license": null,
      "tags": [
        "triton",
        "deployment",
        "code-generation"
      ],
      "id": 173
    },
    {
      "name": "Semi-PD",
      "one_line_profile": "Disaggregated LLM serving framework",
      "detailed_description": "A serving framework that disaggregates prefill and decode phases for LLMs, featuring shared GPU memory and fine-grained compute isolation to optimize throughput.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_serving",
        "resource_scheduling"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/infinigence/Semi-PD",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-serving",
        "distributed-systems",
        "optimization"
      ],
      "id": 174
    },
    {
      "name": "Intel AI Reference Models",
      "one_line_profile": "Optimized deep learning reference implementations for Intel hardware",
      "detailed_description": "A collection of deep learning model implementations optimized for Intel Xeon processors and Data Center GPUs, serving as a reference for efficient inference and training.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_optimization",
        "model_implementation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/intel/ai-reference-models",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "intel",
        "optimization",
        "reference-models"
      ],
      "id": 175
    },
    {
      "name": "Auto-Round",
      "one_line_profile": "Advanced quantization toolkit for LLMs and VLMs",
      "detailed_description": "A toolkit providing advanced quantization schemes (WOQ, MXFP4, NVFP4, etc.) for Large Language Models and Vision Language Models, integrating with transformers and vLLM.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "quantization",
        "model_compression"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/intel/auto-round",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "quantization",
        "llm",
        "compression"
      ],
      "id": 176
    },
    {
      "name": "IPEX-LLM",
      "one_line_profile": "LLM inference and finetuning accelerator for Intel XPU",
      "detailed_description": "A library to accelerate local LLM inference and fine-tuning on Intel hardware (CPUs, GPUs, NPUs), integrating with popular frameworks like llama.cpp, HuggingFace, and vLLM.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_acceleration",
        "fine_tuning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/intel/ipex-llm",
      "help_website": [
        "https://ipex-llm.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "intel",
        "xpu",
        "acceleration"
      ],
      "id": 177
    },
    {
      "name": "Intel Neural Compressor",
      "one_line_profile": "Model compression and quantization toolkit",
      "detailed_description": "A toolkit for low-bit quantization (INT8, FP8, etc.) and sparsity, providing model compression techniques for PyTorch, TensorFlow, and ONNX Runtime.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_compression",
        "quantization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/intel/neural-compressor",
      "help_website": [
        "https://intel.github.io/neural-compressor/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "compression",
        "quantization",
        "sparsity"
      ],
      "id": 178
    },
    {
      "name": "Paddler",
      "one_line_profile": "Load balancer and serving platform for LLMs",
      "detailed_description": "An open-source load balancer and serving platform designed for self-hosting Large Language Models at scale.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_serving",
        "load_balancing"
      ],
      "application_level": "platform",
      "primary_language": "Rust",
      "repo_url": "https://github.com/intentee/paddler",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "serving",
        "load-balancer",
        "llm"
      ],
      "id": 179
    },
    {
      "name": "PKD-for-BERT-Model-Compression",
      "one_line_profile": "Patient Knowledge Distillation for BERT",
      "detailed_description": "A PyTorch implementation of Patient Knowledge Distillation (PKD) for compressing BERT models.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_compression",
        "knowledge_distillation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/intersun/PKD-for-BERT-Model-Compression",
      "help_website": [],
      "license": null,
      "tags": [
        "bert",
        "compression",
        "distillation"
      ],
      "id": 180
    },
    {
      "name": "IREE",
      "one_line_profile": "MLIR-based machine learning compiler and runtime",
      "detailed_description": "A retargetable compiler and runtime toolkit based on MLIR, designed to optimize and run machine learning models on various hardware backends.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "compiler",
        "inference_runtime"
      ],
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/iree-org/iree",
      "help_website": [
        "https://iree.dev/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "compiler",
        "mlir",
        "runtime"
      ],
      "id": 181
    },
    {
      "name": "ChatGPTQAG",
      "one_line_profile": "Automated QA pair generation using ChatGPT",
      "detailed_description": "A tool for automatically generating question-answer pairs using ChatGPT, applicable for creating training datasets for NLP tasks.",
      "domains": [
        "AI3"
      ],
      "subtask_category": [
        "data_generation",
        "dataset_synthesis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/itlubber/ChatGPTQAG",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "data-generation",
        "nlp",
        "qa"
      ],
      "id": 182
    },
    {
      "name": "yolov5-onnxruntime",
      "one_line_profile": "C++ inference implementation for YOLOv5 using ONNX Runtime",
      "detailed_description": "A C++ implementation for running YOLOv5 object detection models using ONNX Runtime, serving as a reference solver for deployment.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_implementation",
        "object_detection"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/itsnine/yolov5-onnxruntime",
      "help_website": [],
      "license": null,
      "tags": [
        "yolo",
        "onnx",
        "inference"
      ],
      "id": 183
    },
    {
      "name": "InferenceHelper",
      "one_line_profile": "C++ helper library for multiple inference frameworks",
      "detailed_description": "A C++ helper class that provides a unified interface for various deep learning inference frameworks including TensorFlow Lite, TensorRT, OpenVINO, and ONNX Runtime.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_integration",
        "framework_abstraction"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/iwatake2222/InferenceHelper",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "inference",
        "cpp",
        "wrapper"
      ],
      "id": 184
    },
    {
      "name": "model_compression",
      "one_line_profile": "PyTorch model compression library",
      "detailed_description": "A library implementing various model compression techniques for PyTorch models, including pruning and quantization.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_compression",
        "pruning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/j-marple-dev/model_compression",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "compression",
        "pytorch",
        "pruning"
      ],
      "id": 185
    },
    {
      "name": "xllm",
      "one_line_profile": "High-performance LLM inference engine",
      "detailed_description": "A high-performance inference engine for Large Language Models, optimized for diverse AI accelerators.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_engine",
        "acceleration"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/jd-opensource/xllm",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "inference",
        "llm",
        "engine"
      ],
      "id": 186
    },
    {
      "name": "GPTQ-for-LLaMa-CUDA",
      "one_line_profile": "Packaged CUDA version of GPTQ for LLaMa",
      "detailed_description": "A packaged distribution of the GPTQ-for-LLaMa tool with CUDA support, facilitating the quantization and inference of LLaMa models.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "quantization",
        "inference_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/jllllll/GPTQ-for-LLaMa-CUDA",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "gptq",
        "cuda",
        "quantization"
      ],
      "id": 187
    },
    {
      "name": "openai-clip-js",
      "one_line_profile": "JavaScript port of OpenAI's CLIP model",
      "detailed_description": "A port of OpenAI's CLIP model to JavaScript using ONNX web runtime, enabling CLIP inference in web environments.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_implementation",
        "model_porting"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/josephrocca/openai-clip-js",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "clip",
        "javascript",
        "onnx"
      ],
      "id": 188
    },
    {
      "name": "Sparrow",
      "one_line_profile": "Structured data extraction using LLMs",
      "detailed_description": "A tool for structured data extraction and instruction calling using Machine Learning and Large Language Models, suitable for processing scientific documents or data.",
      "domains": [
        "AI3"
      ],
      "subtask_category": [
        "data_extraction",
        "information_retrieval"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/katanaml/sparrow",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "extraction",
        "llm",
        "data-processing"
      ],
      "id": 189
    },
    {
      "name": "gpt-llama.cpp",
      "one_line_profile": "OpenAI API adapter for local llama.cpp models",
      "detailed_description": "A utility that acts as a drop-in replacement for OpenAI's GPT endpoints, allowing applications to interface with local llama.cpp models via standard APIs.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_serving",
        "api_adaptation"
      ],
      "application_level": "service",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/keldenl/gpt-llama.cpp",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "api",
        "llama.cpp",
        "serving"
      ],
      "id": 190
    },
    {
      "name": "search",
      "one_line_profile": "Embedded vector search library using llama.cpp",
      "detailed_description": "A Go library for embedded vector search and semantic embeddings utilizing llama.cpp, enabling semantic analysis and retrieval.",
      "domains": [
        "AI3"
      ],
      "subtask_category": [
        "vector_search",
        "embedding_generation"
      ],
      "application_level": "library",
      "primary_language": "Go",
      "repo_url": "https://github.com/kelindar/search",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "search",
        "embeddings",
        "go"
      ],
      "id": 191
    },
    {
      "name": "java-llama.cpp",
      "one_line_profile": "Java bindings for llama.cpp",
      "detailed_description": "Java bindings for the llama.cpp inference engine, enabling Java applications to run LLaMA models locally.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_integration",
        "language_binding"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/kherud/java-llama.cpp",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "java",
        "llama.cpp",
        "inference"
      ],
      "id": 192
    },
    {
      "name": "ONNX Runtime Server",
      "one_line_profile": "REST API server for ONNX inference",
      "detailed_description": "A server implementation that provides TCP and HTTP/HTTPS REST APIs for running inference using ONNX Runtime.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_serving",
        "model_deployment"
      ],
      "application_level": "service",
      "primary_language": "C++",
      "repo_url": "https://github.com/kibae/onnxruntime-server",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "onnx",
        "server",
        "rest-api"
      ],
      "id": 193
    },
    {
      "name": "triton-grpc-proxy-rs",
      "one_line_profile": "Proxy server for Triton gRPC inference",
      "detailed_description": "A Rust-based proxy server for the Triton gRPC server, specifically designed for handling embedding model inference requests.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_serving",
        "proxy_service"
      ],
      "application_level": "service",
      "primary_language": "Rust",
      "repo_url": "https://github.com/kozistr/triton-grpc-proxy-rs",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "triton",
        "proxy",
        "rust"
      ],
      "id": 194
    },
    {
      "name": "KServe",
      "one_line_profile": "Standardized AI inference platform for Kubernetes",
      "detailed_description": "A standardized, distributed inference platform for deploying generative and predictive AI models on Kubernetes, supporting scalable multi-framework deployment.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_serving",
        "model_deployment"
      ],
      "application_level": "platform",
      "primary_language": "Shell",
      "repo_url": "https://github.com/kserve/kserve",
      "help_website": [
        "https://kserve.github.io/website/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "kubernetes",
        "serving",
        "mlops"
      ],
      "id": 195
    },
    {
      "name": "KubeAI",
      "one_line_profile": "Kubernetes operator for managing and scaling AI inference workloads",
      "detailed_description": "KubeAI is an AI Inference Operator for Kubernetes that simplifies serving machine learning models in production. It supports various model types including VLMs, LLMs, embeddings, and speech-to-text, providing auto-scaling and infrastructure management for AI services.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "inference_orchestration"
      ],
      "application_level": "platform",
      "primary_language": "Go",
      "repo_url": "https://github.com/kubeai-project/kubeai",
      "help_website": [
        "https://github.com/kubeai-project/kubeai"
      ],
      "license": "Apache-2.0",
      "tags": [
        "kubernetes",
        "inference-server",
        "llm-serving",
        "operator"
      ],
      "id": 196
    },
    {
      "name": "Mooncake",
      "one_line_profile": "High-performance KVCache-centric LLM serving platform",
      "detailed_description": "Mooncake is a serving platform designed for Large Language Models (LLMs), utilizing a KVCache-centric architecture to optimize inference performance. It is used in production for the Kimi LLM service, focusing on efficient resource utilization and low-latency serving.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "inference_optimization"
      ],
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/kvcache-ai/Mooncake",
      "help_website": [
        "https://github.com/kvcache-ai/Mooncake"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm-serving",
        "kv-cache",
        "inference-engine"
      ],
      "id": 197
    },
    {
      "name": "ktransformers",
      "one_line_profile": "Framework for heterogeneous LLM inference and fine-tuning optimization",
      "detailed_description": "ktransformers is a flexible framework designed to optimize Large Language Model (LLM) inference and fine-tuning on heterogeneous hardware. It provides tools for experiencing and developing advanced optimization techniques for transformer-based models.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_optimization",
        "model_finetuning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/kvcache-ai/ktransformers",
      "help_website": [
        "https://github.com/kvcache-ai/ktransformers"
      ],
      "license": "Apache-2.0",
      "tags": [
        "transformers",
        "inference-optimization",
        "llm"
      ],
      "id": 198
    },
    {
      "name": "Larq Compute Engine",
      "one_line_profile": "Highly optimized inference engine for Binarized Neural Networks (BNNs)",
      "detailed_description": "Larq Compute Engine (LCE) is a highly optimized inference engine specifically designed for Binarized Neural Networks (BNNs). It targets mobile and embedded devices, providing efficient execution of quantized models.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_acceleration",
        "quantization"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/larq/compute-engine",
      "help_website": [
        "https://docs.larq.dev/compute-engine/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "bnn",
        "inference-engine",
        "quantization",
        "mobile-ai"
      ],
      "id": 199
    },
    {
      "name": "Mobile-ID",
      "one_line_profile": "Deep face model compression and acceleration toolkit",
      "detailed_description": "A MATLAB-based toolkit for compressing deep face recognition models. It implements algorithms for model compression to enable efficient deployment on mobile devices.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_compression",
        "quantization"
      ],
      "application_level": "solver",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/liuziwei7/mobile-id",
      "help_website": [
        "https://github.com/liuziwei7/mobile-id"
      ],
      "license": null,
      "tags": [
        "model-compression",
        "face-recognition",
        "matlab"
      ],
      "id": 200
    },
    {
      "name": "LLMKit",
      "one_line_profile": "Toolkit for LLM prompt management, testing, and inference serving",
      "detailed_description": "LLMKit is a comprehensive toolkit and inference server for managing prompts, versioning, testing, and evaluating Large Language Models. It provides an OpenAI-compatible API and UI for model interaction.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "prompt_engineering"
      ],
      "application_level": "platform",
      "primary_language": "Rust",
      "repo_url": "https://github.com/llmkit-ai/llmkit",
      "help_website": [
        "https://github.com/llmkit-ai/llmkit"
      ],
      "license": "MIT",
      "tags": [
        "llm-serving",
        "prompt-management",
        "inference-server"
      ],
      "id": 201
    },
    {
      "name": "RouteLLM",
      "one_line_profile": "Framework for serving and evaluating LLM routers",
      "detailed_description": "RouteLLM is a framework designed to optimize LLM serving costs without compromising quality by intelligently routing queries between different models. It provides tools for training and evaluating these routing strategies.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "inference_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/lm-sys/RouteLLM",
      "help_website": [
        "https://github.com/lm-sys/RouteLLM"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm-routing",
        "cost-optimization",
        "model-serving"
      ],
      "id": 202
    },
    {
      "name": "MLX Omni Server",
      "one_line_profile": "Local inference server for Apple Silicon using MLX framework",
      "detailed_description": "MLX Omni Server is a local inference server built on Apple's MLX framework, optimized for Apple Silicon. It provides OpenAI-compatible API endpoints for serving LLMs locally with high efficiency.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "inference_server"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/madroidmaq/mlx-omni-server",
      "help_website": [
        "https://github.com/madroidmaq/mlx-omni-server"
      ],
      "license": "MIT",
      "tags": [
        "mlx",
        "apple-silicon",
        "inference-server",
        "llm"
      ],
      "id": 203
    },
    {
      "name": "Altius",
      "one_line_profile": "Lightweight ONNX inference runtime in Rust",
      "detailed_description": "Altius is a small, efficient ONNX inference runtime written in Rust. It is designed to run ONNX models with low overhead, suitable for embedding in applications requiring fast inference.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_runtime",
        "model_execution"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/maekawatoshiki/altius",
      "help_website": [
        "https://github.com/maekawatoshiki/altius"
      ],
      "license": "MIT",
      "tags": [
        "onnx",
        "inference-runtime",
        "rust"
      ],
      "id": 204
    },
    {
      "name": "rust-llama.cpp",
      "one_line_profile": "Rust bindings for llama.cpp inference engine",
      "detailed_description": "This library provides Rust bindings for llama.cpp, enabling Rust applications to leverage the efficient LLM inference capabilities of llama.cpp.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_runtime",
        "model_serving"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/mdrokz/rust-llama.cpp",
      "help_website": [
        "https://github.com/mdrokz/rust-llama.cpp"
      ],
      "license": "MIT",
      "tags": [
        "rust",
        "llama.cpp",
        "bindings",
        "inference"
      ],
      "id": 205
    },
    {
      "name": "Sparsebit",
      "one_line_profile": "Model compression and acceleration toolbox",
      "detailed_description": "Sparsebit is a toolbox based on PyTorch for model compression and acceleration. It provides tools for quantization, pruning, and other optimization techniques to improve inference efficiency.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_compression",
        "quantization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/megvii-research/Sparsebit",
      "help_website": [
        "https://github.com/megvii-research/Sparsebit"
      ],
      "license": "Apache-2.0",
      "tags": [
        "model-compression",
        "quantization",
        "pytorch"
      ],
      "id": 206
    },
    {
      "name": "llama_ros",
      "one_line_profile": "ROS 2 integration for llama.cpp and llava.cpp",
      "detailed_description": "llama_ros provides a set of ROS 2 packages to integrate llama.cpp (for LLMs) and llava.cpp (for VLMs) into robotics systems, enabling on-robot inference.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_integration",
        "robotics_inference"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/mgonzs13/llama_ros",
      "help_website": [
        "https://github.com/mgonzs13/llama_ros"
      ],
      "license": "MIT",
      "tags": [
        "ros2",
        "robotics",
        "llama.cpp",
        "inference"
      ],
      "id": 207
    },
    {
      "name": "Infinity",
      "one_line_profile": "High-throughput serving engine for embeddings and reranking models",
      "detailed_description": "Infinity is a high-performance serving engine specialized for text-embeddings, reranking models, and other vector-based models (CLIP, etc.). It focuses on high throughput and low latency for vector search applications.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "embedding_generation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/michaelfeil/infinity",
      "help_website": [
        "https://github.com/michaelfeil/infinity"
      ],
      "license": "MIT",
      "tags": [
        "embedding-server",
        "inference-engine",
        "vector-search"
      ],
      "id": 208
    },
    {
      "name": "Olive",
      "one_line_profile": "Automated machine learning model optimization toolkit",
      "detailed_description": "Olive is a toolkit that simplifies the process of model fine-tuning, conversion, quantization, and optimization for deployment on various hardware targets (CPUs, GPUs, NPUs). It automates the optimization pipeline.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_optimization",
        "quantization",
        "model_conversion"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/Olive",
      "help_website": [
        "https://microsoft.github.io/Olive/"
      ],
      "license": "MIT",
      "tags": [
        "model-optimization",
        "onnx",
        "quantization",
        "automl"
      ],
      "id": 209
    },
    {
      "name": "ParrotServe",
      "one_line_profile": "Efficient serving engine for LLM-based applications",
      "detailed_description": "ParrotServe is a serving system designed to optimize the execution of LLM-based applications by using semantic variables to manage context and requests efficiently.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "inference_optimization"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/ParrotServe",
      "help_website": [
        "https://github.com/microsoft/ParrotServe"
      ],
      "license": "MIT",
      "tags": [
        "llm-serving",
        "inference-system",
        "optimization"
      ],
      "id": 210
    },
    {
      "name": "NNI",
      "one_line_profile": "AutoML toolkit for neural architecture search and model compression",
      "detailed_description": "Neural Network Intelligence (NNI) is an open-source AutoML toolkit that automates feature engineering, neural architecture search (NAS), hyper-parameter tuning, and model compression to optimize model performance and efficiency.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_optimization",
        "model_compression",
        "automl"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/nni",
      "help_website": [
        "https://nni.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "automl",
        "nas",
        "model-compression",
        "hyperparameter-tuning"
      ],
      "id": 211
    },
    {
      "name": "ONNX Server Open Enclave",
      "one_line_profile": "Confidential inference server for ONNX models",
      "detailed_description": "This tool is a port of the ONNX inference server designed to run within Open Enclave, enabling confidential inference with data encryption and attestation capabilities, specifically for Azure Confidential Computing.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "confidential_computing"
      ],
      "application_level": "platform",
      "primary_language": "C",
      "repo_url": "https://github.com/microsoft/onnx-server-openenclave",
      "help_website": [
        "https://github.com/microsoft/onnx-server-openenclave"
      ],
      "license": "MIT",
      "tags": [
        "confidential-computing",
        "onnx",
        "inference-server",
        "security"
      ],
      "id": 212
    },
    {
      "name": "ONNX Runtime",
      "one_line_profile": "Cross-platform high-performance machine learning inference accelerator",
      "detailed_description": "ONNX Runtime is a high-performance inference engine for machine learning models in the ONNX format. It supports a wide range of hardware accelerators and platforms, optimizing inference latency and throughput.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_runtime",
        "model_acceleration"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/microsoft/onnxruntime",
      "help_website": [
        "https://onnxruntime.ai/"
      ],
      "license": "MIT",
      "tags": [
        "onnx",
        "inference-engine",
        "acceleration",
        "cross-platform"
      ],
      "id": 213
    },
    {
      "name": "onnxruntime-extensions",
      "one_line_profile": "Custom operators and extensions for ONNX Runtime",
      "detailed_description": "A library providing custom operators and extensions for ONNX Runtime, enabling pre- and post-processing steps (like tokenization and image processing) to be embedded directly within the ONNX model graph.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "data_processing",
        "inference_support"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/microsoft/onnxruntime-extensions",
      "help_website": [
        "https://github.com/microsoft/onnxruntime-extensions"
      ],
      "license": "MIT",
      "tags": [
        "onnx",
        "extensions",
        "preprocessing",
        "custom-operators"
      ],
      "id": 214
    },
    {
      "name": "Sarathi-Serve",
      "one_line_profile": "Low-latency and high-throughput LLM serving engine",
      "detailed_description": "Sarathi-Serve is a serving engine for Large Language Models designed to maximize throughput and minimize latency through techniques like chunked prefill and decode-maximal batching.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "inference_optimization"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/sarathi-serve",
      "help_website": [
        "https://github.com/microsoft/sarathi-serve"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm-serving",
        "inference-engine",
        "optimization"
      ],
      "id": 215
    },
    {
      "name": "vAttention",
      "one_line_profile": "Dynamic memory management system for LLM serving",
      "detailed_description": "vAttention is a system for dynamic memory management in LLM serving, enabling efficient handling of KV-cache without the need for PagedAttention, optimizing memory usage and performance.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "memory_management",
        "inference_optimization"
      ],
      "application_level": "library",
      "primary_language": "C",
      "repo_url": "https://github.com/microsoft/vattention",
      "help_website": [
        "https://github.com/microsoft/vattention"
      ],
      "license": "MIT",
      "tags": [
        "memory-management",
        "llm-serving",
        "cuda"
      ],
      "id": 216
    },
    {
      "name": "Vidur",
      "one_line_profile": "Large-scale simulation framework for LLM inference",
      "detailed_description": "Vidur is a high-fidelity simulation framework for Large Language Model (LLM) inference. It allows researchers and practitioners to estimate the performance of LLM serving systems under various configurations and workloads.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "system_simulation",
        "performance_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/vidur",
      "help_website": [
        "https://github.com/microsoft/vidur"
      ],
      "license": "MIT",
      "tags": [
        "simulation",
        "llm-inference",
        "performance-modeling"
      ],
      "id": 217
    },
    {
      "name": "AMC",
      "one_line_profile": "AutoML for Model Compression on mobile devices",
      "detailed_description": "AMC (AutoML for Model Compression) leverages reinforcement learning to automatically find the optimal compression strategy (pruning ratio, quantization bits) for deep neural networks, targeting mobile device constraints.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_compression",
        "automl"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/mit-han-lab/amc",
      "help_website": [
        "https://github.com/mit-han-lab/amc"
      ],
      "license": "MIT",
      "tags": [
        "model-compression",
        "reinforcement-learning",
        "automl"
      ],
      "id": 218
    },
    {
      "name": "LLM-AWQ",
      "one_line_profile": "Activation-aware Weight Quantization for LLMs",
      "detailed_description": "AWQ is a quantization toolkit for Large Language Models that protects salient weights based on activation magnitude. It enables efficient 4-bit quantization with minimal performance degradation.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_quantization",
        "inference_acceleration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/mit-han-lab/llm-awq",
      "help_website": [
        "https://github.com/mit-han-lab/llm-awq"
      ],
      "license": "MIT",
      "tags": [
        "quantization",
        "llm",
        "compression"
      ],
      "id": 219
    },
    {
      "name": "OmniServe",
      "one_line_profile": "Unified efficient serving system for LLMs",
      "detailed_description": "OmniServe (encompassing QServe and LServe) is a serving system designed for efficient LLM inference. It features advanced quantization (W4A8KV4) and system co-design, as well as optimizations for long-sequence serving.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "inference_optimization"
      ],
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/mit-han-lab/omniserve",
      "help_website": [
        "https://github.com/mit-han-lab/omniserve"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm-serving",
        "quantization",
        "long-context"
      ],
      "id": 220
    },
    {
      "name": "SmoothQuant",
      "one_line_profile": "Post-training quantization for Large Language Models",
      "detailed_description": "SmoothQuant is a post-training quantization framework that enables 8-bit weight, 8-bit activation (W8A8) quantization for LLMs by smoothing activation outliers, maintaining accuracy while improving efficiency.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_quantization",
        "inference_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/mit-han-lab/smoothquant",
      "help_website": [
        "https://github.com/mit-han-lab/smoothquant"
      ],
      "license": "MIT",
      "tags": [
        "quantization",
        "llm",
        "post-training-optimization"
      ],
      "id": 221
    },
    {
      "name": "MLC LLM",
      "one_line_profile": "Universal LLM deployment engine using machine learning compilation",
      "detailed_description": "A universal deployment solution that enables large language models to run natively on a diverse set of hardware backends and native applications, utilizing machine learning compilation technology (TVM).",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_optimization",
        "model_deployment",
        "quantization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/mlc-ai/mlc-llm",
      "help_website": [
        "https://llm.mlc.ai/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "compilation",
        "inference",
        "edge-computing"
      ],
      "id": 222
    },
    {
      "name": "Stopwatch",
      "one_line_profile": "Benchmarking tool for LLMs on Modal",
      "detailed_description": "A utility for benchmarking the performance (latency, throughput) of Large Language Models running on the Modal serverless platform.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/modal-labs/stopwatch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "benchmarking",
        "llm",
        "latency"
      ],
      "id": 223
    },
    {
      "name": "Mosec",
      "one_line_profile": "High-performance ML model serving framework",
      "detailed_description": "A machine learning model serving framework that offers dynamic batching and efficient CPU/GPU pipelines to maximize hardware utilization for inference.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "inference_optimization"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/mosecorg/mosec",
      "help_website": [
        "https://mosecorg.github.io/mosec/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "serving",
        "dynamic-batching",
        "inference"
      ],
      "id": 224
    },
    {
      "name": "Llama Swap",
      "one_line_profile": "Model swapping utility for local LLM servers",
      "detailed_description": "A proxy service that enables reliable model swapping for OpenAI/Anthropic compatible local inference servers like llama.cpp or vllm, facilitating dynamic model management.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_management",
        "serving_utility"
      ],
      "application_level": "service",
      "primary_language": "Go",
      "repo_url": "https://github.com/mostlygeek/llama-swap",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "proxy",
        "model-swapping",
        "llm-serving"
      ],
      "id": 225
    },
    {
      "name": "Splitwise Sim",
      "one_line_profile": "LLM serving cluster simulator",
      "detailed_description": "A simulation tool for modeling and analyzing the behavior of LLM serving clusters, useful for research into scheduling and resource allocation strategies.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "simulation",
        "system_modeling"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/mutinifni/splitwise-sim",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "simulation",
        "cluster",
        "llm-serving"
      ],
      "id": 226
    },
    {
      "name": "AutoGPTQ-API",
      "one_line_profile": "API wrapper for AutoGPTQ inference",
      "detailed_description": "A tool to host GPTQ quantized models using AutoGPTQ as an API, compatible with text generation UI APIs, facilitating the serving of quantized models.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "quantization"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/mzbac/AutoGPTQ-API",
      "help_website": [],
      "license": null,
      "tags": [
        "gptq",
        "api",
        "serving"
      ],
      "id": 227
    },
    {
      "name": "GPTQ-for-LLaMa-API",
      "one_line_profile": "API for GPT-QLLama models",
      "detailed_description": "A lightweight API implementation for serving GPT-QLLama models, enabling programmatic access to quantized LLaMa inference.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "quantization"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/mzbac/GPTQ-for-LLaMa-API",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llama",
        "gptq",
        "api"
      ],
      "id": 228
    },
    {
      "name": "MLX LLM Server",
      "one_line_profile": "Local LLM serving using MLX framework",
      "detailed_description": "A server implementation for inferring and serving local Large Language Models using Apple's MLX framework, optimized for Apple Silicon.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "inference_optimization"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/mzbac/mlx-llm-server",
      "help_website": [],
      "license": null,
      "tags": [
        "mlx",
        "apple-silicon",
        "serving"
      ],
      "id": 229
    },
    {
      "name": "MLX-Textgen",
      "one_line_profile": "OpenAI-compatible API for MLX LLM serving",
      "detailed_description": "A Python package for serving LLMs on OpenAI-compatible API endpoints with features like prompt caching, utilizing the MLX framework.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "inference_optimization"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/nath1295/MLX-Textgen",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mlx",
        "api",
        "serving"
      ],
      "id": 230
    },
    {
      "name": "onnxruntime-rs",
      "one_line_profile": "Rust wrapper for ONNX Runtime",
      "detailed_description": "A Rust language binding for Microsoft's ONNX Runtime, enabling high-performance inference of ONNX models within Rust applications.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_runtime",
        "language_binding"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/nbigaouette/onnxruntime-rs",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rust",
        "onnx",
        "inference"
      ],
      "id": 231
    },
    {
      "name": "DeepSparse",
      "one_line_profile": "Sparsity-aware deep learning inference runtime",
      "detailed_description": "A CPU inference runtime that leverages sparsity to accelerate deep learning models, offering significant performance improvements for sparse models.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_optimization",
        "sparsity"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/neuralmagic/deepsparse",
      "help_website": [
        "https://docs.neuralmagic.com/deepsparse/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "sparsity",
        "cpu-inference",
        "acceleration"
      ],
      "id": 232
    },
    {
      "name": "SparseZoo",
      "one_line_profile": "Repository for sparse and quantized models",
      "detailed_description": "A repository and tooling for accessing highly sparse and sparse-quantized neural network models, along with their sparsification recipes.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_repository",
        "model_management"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/neuralmagic/sparsezoo",
      "help_website": [
        "https://docs.neuralmagic.com/sparsezoo/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "sparse-models",
        "model-zoo",
        "quantization"
      ],
      "id": 233
    },
    {
      "name": "Sparsify",
      "one_line_profile": "Model optimization tool for inference acceleration",
      "detailed_description": "A product and toolset for optimizing machine learning models through sparsification and quantization to accelerate inference.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_optimization",
        "quantization",
        "pruning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/neuralmagic/sparsify",
      "help_website": [
        "https://docs.neuralmagic.com/sparsify/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "optimization",
        "pruning",
        "inference"
      ],
      "id": 234
    },
    {
      "name": "Wllama",
      "one_line_profile": "WebAssembly binding for llama.cpp",
      "detailed_description": "A WebAssembly binding for llama.cpp that enables running LLM inference directly in the browser or other Wasm runtimes, facilitating client-side scientific computing.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_runtime",
        "webassembly"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/ngxson/wllama",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "wasm",
        "llama.cpp",
        "browser-inference"
      ],
      "id": 235
    },
    {
      "name": "vLLM-gfx906",
      "one_line_profile": "vLLM port for AMD gfx906 GPUs",
      "detailed_description": "A specialized port of the vLLM serving engine optimized for AMD gfx906 GPUs (e.g., Radeon VII, MI50, MI60), enabling high-throughput serving on this hardware.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "inference_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/nlzy/vllm-gfx906",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "amd",
        "rocm",
        "vllm",
        "serving"
      ],
      "id": 236
    },
    {
      "name": "pygpt4all",
      "one_line_profile": "Python bindings for llama.cpp and gpt4all",
      "detailed_description": "Official Python bindings for the gpt4all and llama.cpp ecosystem, allowing developers to run quantized LLMs locally via Python.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_runtime",
        "language_binding"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/nomic-ai/pygpt4all",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gpt4all",
        "llama.cpp",
        "bindings"
      ],
      "id": 237
    },
    {
      "name": "openai_trtllm",
      "one_line_profile": "OpenAI compatible API for TensorRT-LLM",
      "detailed_description": "A backend that exposes TensorRT-LLM via an OpenAI-compatible API, facilitating the integration of optimized TensorRT inference into standard LLM workflows.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "inference_optimization"
      ],
      "application_level": "service",
      "primary_language": "Rust",
      "repo_url": "https://github.com/npuichigo/openai_trtllm",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "tensorrt-llm",
        "api",
        "serving"
      ],
      "id": 238
    },
    {
      "name": "DeepCompressor",
      "one_line_profile": "Model compression toolbox for LLMs and Diffusion Models",
      "detailed_description": "A toolbox for compressing Large Language Models and Diffusion Models, enabling efficient storage and inference through advanced quantization and compression techniques.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_compression",
        "quantization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/nunchaku-tech/deepcompressor",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "compression",
        "llm",
        "diffusion"
      ],
      "id": 239
    },
    {
      "name": "Nunchaku",
      "one_line_profile": "SVDQuant inference engine for 4-bit Diffusion Models",
      "detailed_description": "An inference engine implementing SVDQuant, a technique to absorb outliers by low-rank components, enabling efficient 4-bit quantization for diffusion models.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_optimization",
        "quantization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/nunchaku-tech/nunchaku",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "diffusion",
        "quantization",
        "svdquant"
      ],
      "id": 240
    },
    {
      "name": "Whisper TFLite",
      "one_line_profile": "Optimized Whisper TFLite port for edge inference",
      "detailed_description": "A port of OpenAI's Whisper model to TensorFlow Lite, optimized for efficient offline inference on edge devices.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_optimization",
        "edge_computing"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/nyadla-sys/whisper.tflite",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "whisper",
        "tflite",
        "edge-inference"
      ],
      "id": 241
    },
    {
      "name": "ORT Builder",
      "one_line_profile": "ONNX Runtime static library builder",
      "detailed_description": "A utility to build static libraries for ONNX Runtime, facilitating the embedding of the inference engine into various applications and systems.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "build_tool",
        "inference_deployment"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/olilarkin/ort-builder",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "onnx-runtime",
        "build-tool",
        "static-library"
      ],
      "id": 242
    },
    {
      "name": "Anomalib",
      "one_line_profile": "Deep learning library for anomaly detection",
      "detailed_description": "A library comprising state-of-the-art algorithms for anomaly detection, including features for experiment management, hyper-parameter optimization, and edge inference.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "anomaly_detection",
        "inference_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/open-edge-platform/anomalib",
      "help_website": [
        "https://anomalib.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "anomaly-detection",
        "computer-vision",
        "edge-inference"
      ],
      "id": 243
    },
    {
      "name": "OpenVINO Training Extensions",
      "one_line_profile": "Toolbox for training and optimizing CV models via OpenVINO",
      "detailed_description": "A toolkit to train, evaluate, optimize, and deploy computer vision models, specifically designed to work with the OpenVINO ecosystem for efficient inference.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_optimization",
        "training_pipeline"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/open-edge-platform/training_extensions",
      "help_website": [
        "https://github.com/openvinotoolkit/training_extensions"
      ],
      "license": "Apache-2.0",
      "tags": [
        "openvino",
        "computer-vision",
        "optimization"
      ],
      "id": 244
    },
    {
      "name": "MMRazor",
      "one_line_profile": "OpenMMLab model compression toolbox",
      "detailed_description": "A model compression toolbox that provides various algorithms for network pruning, knowledge distillation, and neural architecture search to optimize models for inference.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_compression",
        "pruning",
        "distillation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/open-mmlab/mmrazor",
      "help_website": [
        "https://mmrazor.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "compression",
        "openmmlab",
        "optimization"
      ],
      "id": 245
    },
    {
      "name": "OpenVINO Model Server",
      "one_line_profile": "Scalable inference server for OpenVINO models",
      "detailed_description": "A high-performance system for serving machine learning models optimized with OpenVINO, supporting scalable inference via gRPC and REST endpoints.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "inference_optimization"
      ],
      "application_level": "service",
      "primary_language": "C++",
      "repo_url": "https://github.com/openvinotoolkit/model_server",
      "help_website": [
        "https://docs.openvino.ai/latest/ovms_what_is_openvino_model_server.html"
      ],
      "license": "Apache-2.0",
      "tags": [
        "serving",
        "openvino",
        "inference"
      ],
      "id": 246
    },
    {
      "name": "OpenVINO",
      "one_line_profile": "Toolkit for optimizing and deploying AI inference",
      "detailed_description": "An open-source toolkit for optimizing and deploying deep learning models across various hardware platforms, focusing on high-performance inference.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_optimization",
        "model_deployment"
      ],
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/openvinotoolkit/openvino",
      "help_website": [
        "https://docs.openvino.ai/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "inference",
        "optimization",
        "intel"
      ],
      "id": 247
    },
    {
      "name": "sd4j",
      "one_line_profile": "Stable Diffusion pipeline in Java using ONNX Runtime",
      "detailed_description": "A Java implementation of the Stable Diffusion pipeline leveraging ONNX Runtime for inference, enabling image generation within Java environments.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_pipeline",
        "image_generation"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/oracle/sd4j",
      "help_website": [],
      "license": "UPL-1.0",
      "tags": [
        "java",
        "stable-diffusion",
        "onnx"
      ],
      "id": 248
    },
    {
      "name": "KVCached",
      "one_line_profile": "Virtualized elastic KV cache for LLM serving",
      "detailed_description": "A system for virtualized and elastic Key-Value (KV) cache management, designed to optimize dynamic GPU sharing and memory usage in LLM serving.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_optimization",
        "memory_management"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ovg-project/kvcached",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "kv-cache",
        "llm-serving",
        "optimization"
      ],
      "id": 249
    },
    {
      "name": "ONNX Transformers",
      "one_line_profile": "Accelerated NLP pipelines using ONNX Runtime",
      "detailed_description": "A collection of pipelines and utilities for running fast NLP inference on CPUs using Hugging Face Transformers and ONNX Runtime.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_optimization",
        "nlp_pipeline"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/patil-suraj/onnx_transformers",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "onnx",
        "transformers",
        "nlp"
      ],
      "id": 250
    },
    {
      "name": "BambooAI",
      "one_line_profile": "LLM-powered library for data discovery and analysis",
      "detailed_description": "A Python library that uses Large Language Models to assist in conversational data discovery, analysis, and visualization, acting as an AI agent for data science tasks.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "data_analysis",
        "scientific_visualization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/pgalko/BambooAI",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "data-analysis",
        "llm-agent",
        "pandas"
      ],
      "id": 251
    },
    {
      "name": "llama.clj",
      "one_line_profile": "Clojure wrapper for llama.cpp",
      "detailed_description": "A Clojure language wrapper for llama.cpp, enabling the use of local Large Language Models within Clojure applications and research workflows.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_binding",
        "language_binding"
      ],
      "application_level": "library",
      "primary_language": "Clojure",
      "repo_url": "https://github.com/phronmophobic/llama.clj",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "clojure",
        "llama.cpp",
        "inference"
      ],
      "id": 252
    },
    {
      "name": "Neural Imaging",
      "one_line_profile": "Toolbox for modeling photo acquisition pipelines",
      "detailed_description": "A Python toolbox for modeling and optimizing photo acquisition and distribution pipelines, including camera ISP, compression, and manipulation detection.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "image_processing",
        "simulation",
        "modeling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pkorus/neural-imaging",
      "help_website": [],
      "license": null,
      "tags": [
        "imaging",
        "isp",
        "simulation"
      ],
      "id": 253
    },
    {
      "name": "PowerServe",
      "one_line_profile": "High-speed LLM serving framework",
      "detailed_description": "A high-performance framework for serving Large Language Models locally, designed for speed and ease of use in deployment scenarios.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "inference_optimization"
      ],
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/powerserve-project/PowerServe",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "serving",
        "llm",
        "inference"
      ],
      "id": 254
    },
    {
      "name": "LoRAX",
      "one_line_profile": "Multi-LoRA inference server for scaling fine-tuned LLMs",
      "detailed_description": "LoRAX is an inference server that allows serving thousands of fine-tuned Low-Rank Adaptation (LoRA) models on a single GPU cluster, optimizing resource usage for scientific model deployment.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "inference_optimization"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/predibase/lorax",
      "help_website": [
        "https://predibase.github.io/lorax/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "lora",
        "inference-server",
        "llm-serving"
      ],
      "id": 255
    },
    {
      "name": "Prometheus-Eval",
      "one_line_profile": "Evaluation library for LLM responses using Prometheus and GPT-4",
      "detailed_description": "Prometheus-Eval provides tools to evaluate the quality of Large Language Model responses, serving as a critical component for validating scientific models and generated data.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_evaluation",
        "quality_assessment"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/prometheus-eval/prometheus-eval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "evaluation",
        "llm",
        "benchmark"
      ],
      "id": 256
    },
    {
      "name": "Punica",
      "one_line_profile": "System for serving multiple LoRA fine-tuned LLMs efficiently",
      "detailed_description": "Punica is a serving system designed to run multiple LoRA fine-tuned Large Language Models simultaneously on shared GPUs, optimizing throughput for diverse scientific inference tasks.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "inference_optimization"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/punica-ai/punica",
      "help_website": [
        "https://punica-ai.github.io/punica/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "lora",
        "multi-tenant",
        "serving"
      ],
      "id": 257
    },
    {
      "name": "PyTorch-ORT",
      "one_line_profile": "Acceleration library for PyTorch models using ONNX Runtime",
      "detailed_description": "PyTorch-ORT enables the acceleration of PyTorch model training and inference by leveraging ONNX Runtime, optimizing computational efficiency for scientific AI workloads.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_acceleration",
        "model_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pytorch/ort",
      "help_website": [
        "https://cloudblogs.microsoft.com/opensource/2021/07/13/accelerate-pytorch-training-with-torch-ort/"
      ],
      "license": "MIT",
      "tags": [
        "onnx",
        "pytorch",
        "acceleration"
      ],
      "id": 258
    },
    {
      "name": "Qualcomm AI Hub Models",
      "one_line_profile": "Library for accessing and deploying optimized ML models on Qualcomm devices",
      "detailed_description": "This tool provides a Python API to access, optimize, and deploy a collection of state-of-the-art machine learning models, facilitating edge AI inference for scientific data collection and processing.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_deployment",
        "edge_inference"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/quic/ai-hub-models",
      "help_website": [
        "https://aihub.qualcomm.com/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "edge-ai",
        "model-zoo",
        "optimization"
      ],
      "id": 259
    },
    {
      "name": "AIMET",
      "one_line_profile": "AI Model Efficiency Toolkit for quantization and compression",
      "detailed_description": "AIMET (AI Model Efficiency Toolkit) provides advanced model quantization and compression techniques to optimize neural networks for efficient inference in scientific applications.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "quantization",
        "model_compression"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/quic/aimet",
      "help_website": [
        "https://quic.github.io/aimet-pages/index.html"
      ],
      "license": "NOASSERTION",
      "tags": [
        "quantization",
        "compression",
        "efficiency"
      ],
      "id": 260
    },
    {
      "name": "GPTQ-for-LLaMa",
      "one_line_profile": "4-bit quantization implementation for LLaMA models using GPTQ",
      "detailed_description": "This tool implements the GPTQ algorithm for 4-bit quantization of LLaMA models, enabling efficient inference of large language models on consumer-grade hardware for research purposes.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "quantization",
        "inference_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/qwopqwop200/GPTQ-for-LLaMa",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "gptq",
        "quantization",
        "llama"
      ],
      "id": 261
    },
    {
      "name": "Ray",
      "one_line_profile": "Unified compute framework for scaling AI and Python applications",
      "detailed_description": "Ray is a distributed computing framework that provides the infrastructure for scaling AI workloads, including model training, tuning, and serving, essential for large-scale scientific AI.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "distributed_computing",
        "model_serving"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/ray-project/ray",
      "help_website": [
        "https://docs.ray.io/en/latest/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "distributed-systems",
        "scaling",
        "serving"
      ],
      "id": 262
    },
    {
      "name": "Tritony",
      "one_line_profile": "Simplified client library for Triton Inference Server",
      "detailed_description": "Tritony is a helper library that simplifies the configuration and interaction with NVIDIA Triton Inference Server, facilitating the deployment of scientific models.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "inference_client"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/rtzr/tritony",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "triton",
        "inference",
        "client"
      ],
      "id": 263
    },
    {
      "name": "OpenCvSharp Mini Runtime",
      "one_line_profile": "Minimal runtime for OpenCVSharp optimized for server inference",
      "detailed_description": "A lightweight runtime library for OpenCVSharp, designed to facilitate computer vision model inference in server environments for image processing tasks.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "image_processing",
        "inference_runtime"
      ],
      "application_level": "library",
      "primary_language": "C#",
      "repo_url": "https://github.com/sdcb/opencvsharp-mini-runtime",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "opencv",
        "runtime",
        "inference"
      ],
      "id": 264
    },
    {
      "name": "SpecForge",
      "one_line_profile": "Tool for training and porting speculative decoding models",
      "detailed_description": "SpecForge simplifies the training of speculative decoding models and their deployment to serving systems like SGLang, optimizing inference latency for LLMs.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_optimization",
        "speculative_decoding"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/sgl-project/SpecForge",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "speculative-decoding",
        "inference",
        "optimization"
      ],
      "id": 265
    },
    {
      "name": "GenAI-Bench",
      "one_line_profile": "Benchmark tool for evaluating LLM serving systems",
      "detailed_description": "GenAI-Bench is a benchmarking suite designed to evaluate the token-level performance, throughput, and latency of Large Language Model serving systems.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/sgl-project/genai-bench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "benchmark",
        "serving",
        "llm"
      ],
      "id": 266
    },
    {
      "name": "SGLang",
      "one_line_profile": "Fast serving framework for LLMs and VLMs",
      "detailed_description": "SGLang is a high-performance serving framework for large language and vision-language models, offering optimized runtime execution for scientific AI applications.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "inference_engine"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/sgl-project/sglang",
      "help_website": [
        "https://sgl-project.github.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "serving",
        "inference",
        "vlm"
      ],
      "id": 267
    },
    {
      "name": "GPT Server",
      "one_line_profile": "Production-grade serving framework for multimodal AI models",
      "detailed_description": "gpt_server is an open-source framework for deploying various AI models including LLMs, Embeddings, and Rerankers, facilitating the creation of scientific inference services.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "multimodal_inference"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/shell-nlp/gpt_server",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "serving",
        "deployment",
        "multimodal"
      ],
      "id": 268
    },
    {
      "name": "llm-llama-cpp",
      "one_line_profile": "Plugin to run llama.cpp models via the LLM CLI",
      "detailed_description": "This tool is a plugin for the 'llm' command-line utility, enabling the execution of models using the llama.cpp backend, facilitating local inference for researchers.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_runtime",
        "model_execution"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/simonw/llm-llama-cpp",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llama.cpp",
        "plugin",
        "inference"
      ],
      "id": 269
    },
    {
      "name": "Nano-PEARL",
      "one_line_profile": "Parallel speculative decoding serving system",
      "detailed_description": "Nano-PEARL is a serving system implementing Draft-Target Disaggregation via Parallel Speculative Decoding, designed to accelerate LLM inference.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_optimization",
        "speculative_decoding"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/smart-lty/nano-PEARL",
      "help_website": [],
      "license": null,
      "tags": [
        "speculative-decoding",
        "serving",
        "acceleration"
      ],
      "id": 270
    },
    {
      "name": "ArcticInference",
      "one_line_profile": "vLLM plugin for high-throughput inference of Arctic models",
      "detailed_description": "A specialized plugin for the vLLM serving engine designed to enable high-throughput and low-latency inference specifically for Snowflake's Arctic series of Large Language Models.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "inference_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/snowflakedb/ArcticInference",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "vllm",
        "inference",
        "arctic-model"
      ],
      "id": 271
    },
    {
      "name": "Cube Studio",
      "one_line_profile": "Cloud-native one-stop MLOps and AI platform",
      "detailed_description": "A comprehensive cloud-native AI platform supporting the full MLOps lifecycle, including distributed training, hyperparameter search, and inference serving for deep learning and large language models.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "mlops",
        "model_training",
        "model_serving"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/tencentmusic/cube-studio",
      "help_website": [
        "https://github.com/tencentmusic/cube-studio/wiki"
      ],
      "license": "NOASSERTION",
      "tags": [
        "mlops",
        "distributed-training",
        "inference-server"
      ],
      "id": 272
    },
    {
      "name": "OpenModelZ",
      "one_line_profile": "Autoscaling infrastructure for LLM inference on Kubernetes",
      "detailed_description": "A deployment tool designed to automate the scaling and management of Large Language Model (LLM) inference servers (like vLLM and SGLang) on Kubernetes clusters.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "infrastructure_scaling"
      ],
      "application_level": "platform",
      "primary_language": "Go",
      "repo_url": "https://github.com/tensorchord/openmodelz",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "kubernetes",
        "autoscaling",
        "llm-serving"
      ],
      "id": 273
    },
    {
      "name": "TensorFlow Model Optimization Toolkit",
      "one_line_profile": "Toolkit to optimize ML models for deployment",
      "detailed_description": "A suite of tools for optimizing machine learning models for deployment and execution, supporting techniques such as quantization and pruning to reduce model size and improve latency.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_optimization",
        "quantization",
        "pruning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/tensorflow/model-optimization",
      "help_website": [
        "https://www.tensorflow.org/model_optimization"
      ],
      "license": "Apache-2.0",
      "tags": [
        "tensorflow",
        "optimization",
        "quantization"
      ],
      "id": 274
    },
    {
      "name": "Indexify",
      "one_line_profile": "Realtime serving engine for data-intensive AI applications",
      "detailed_description": "A serving engine and extraction framework designed for building data-intensive generative AI applications, facilitating the ingestion and processing of unstructured data for RAG and inference pipelines.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "data_extraction",
        "rag"
      ],
      "application_level": "service",
      "primary_language": "Rust",
      "repo_url": "https://github.com/tensorlakeai/indexify",
      "help_website": [
        "https://docs.getindexify.ai"
      ],
      "license": "Apache-2.0",
      "tags": [
        "serving-engine",
        "rag",
        "unstructured-data"
      ],
      "id": 275
    },
    {
      "name": "llamacpp-python",
      "one_line_profile": "Python bindings for llama.cpp inference engine",
      "detailed_description": "Python bindings for the llama.cpp library, enabling efficient local inference of Large Language Models directly within Python environments commonly used for scientific computing.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_inference",
        "local_deployment"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/thomasantony/llamacpp-python",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "llama.cpp",
        "inference",
        "python-binding"
      ],
      "id": 276
    },
    {
      "name": "SageAttention",
      "one_line_profile": "Quantized attention library for model acceleration",
      "detailed_description": "A library implementing quantized attention mechanisms to accelerate inference speed for language, image, and video models without significant loss in accuracy.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_acceleration",
        "quantization"
      ],
      "application_level": "library",
      "primary_language": "Cuda",
      "repo_url": "https://github.com/thu-ml/SageAttention",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "attention-mechanism",
        "quantization",
        "acceleration"
      ],
      "id": 277
    },
    {
      "name": "SpargeAttention",
      "one_line_profile": "Training-free sparse attention for inference acceleration",
      "detailed_description": "A library providing a training-free sparse attention mechanism to accelerate the inference of transformer-based models by optimizing attention computation.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_acceleration",
        "sparse_attention"
      ],
      "application_level": "library",
      "primary_language": "Cuda",
      "repo_url": "https://github.com/thu-ml/SpargeAttn",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "sparse-attention",
        "inference",
        "optimization"
      ],
      "id": 278
    },
    {
      "name": "llama.onnx",
      "one_line_profile": "Tools for converting and running LLaMA/RWKV models in ONNX",
      "detailed_description": "A utility toolkit for exporting LLaMA and RWKV language models to the ONNX format, including quantization support and test cases for verification.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_conversion",
        "quantization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/tpoisonooo/llama.onnx",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "onnx",
        "model-conversion",
        "llama"
      ],
      "id": 279
    },
    {
      "name": "Triton Core",
      "one_line_profile": "Core library for Triton Inference Server",
      "detailed_description": "The core library and API implementation for the NVIDIA Triton Inference Server, providing the fundamental infrastructure for model serving.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "infrastructure"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/triton-inference-server/core",
      "help_website": [
        "https://github.com/triton-inference-server/server/blob/main/docs/README.md"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "triton",
        "inference-server",
        "core"
      ],
      "id": 280
    },
    {
      "name": "Triton FIL Backend",
      "one_line_profile": "Forest Inference Library backend for Triton",
      "detailed_description": "A backend for the Triton Inference Server that enables high-performance inference of tree-based models (e.g., XGBoost, LightGBM, Scikit-Learn) using the Forest Inference Library (FIL).",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "inference_backend"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/triton-inference-server/fil_backend",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "triton",
        "xgboost",
        "random-forest"
      ],
      "id": 281
    },
    {
      "name": "Triton Local Cache",
      "one_line_profile": "In-memory cache implementation for Triton",
      "detailed_description": "A local in-memory cache implementation for the Triton Inference Server, designed to reduce latency by caching inference responses.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "caching"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/triton-inference-server/local_cache",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "triton",
        "cache",
        "optimization"
      ],
      "id": 282
    },
    {
      "name": "Triton Model Analyzer",
      "one_line_profile": "Profiling tool for Triton Inference Server models",
      "detailed_description": "A CLI tool for profiling and analyzing the compute and memory requirements of models served by Triton, helping to optimize configuration for performance.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "performance_profiling",
        "resource_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/triton-inference-server/model_analyzer",
      "help_website": [
        "https://github.com/triton-inference-server/model_analyzer/blob/main/docs/README.md"
      ],
      "license": "Apache-2.0",
      "tags": [
        "profiling",
        "triton",
        "optimization"
      ],
      "id": 283
    },
    {
      "name": "Triton Model Navigator",
      "one_line_profile": "Toolkit for optimizing and deploying models on NVIDIA GPUs",
      "detailed_description": "An inference toolkit that automates the process of moving models from training to deployment, including optimization and conversion for NVIDIA GPUs within the Triton ecosystem.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_deployment",
        "model_optimization"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/triton-inference-server/model_navigator",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "deployment",
        "optimization",
        "nvidia"
      ],
      "id": 284
    },
    {
      "name": "Triton ONNX Runtime Backend",
      "one_line_profile": "ONNX Runtime backend for Triton",
      "detailed_description": "The backend integration that allows the Triton Inference Server to execute models using the ONNX Runtime engine.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "inference_backend"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/triton-inference-server/onnxruntime_backend",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "triton",
        "onnx",
        "backend"
      ],
      "id": 285
    },
    {
      "name": "Triton Inference Server",
      "one_line_profile": "High-performance inference serving software",
      "detailed_description": "An open-source inference serving software that streamlines AI inference by enabling teams to deploy, run, and scale trained AI models from any framework on any GPU- or CPU-based infrastructure.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "inference_server"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/triton-inference-server/server",
      "help_website": [
        "https://developer.nvidia.com/nvidia-triton-inference-server"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "inference-server",
        "nvidia",
        "deployment"
      ],
      "id": 286
    },
    {
      "name": "Triton TensorRT-LLM Backend",
      "one_line_profile": "TensorRT-LLM backend for Triton",
      "detailed_description": "The backend for Triton Inference Server that enables optimized serving of Large Language Models using NVIDIA's TensorRT-LLM library.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "llm_inference"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/triton-inference-server/tensorrtllm_backend",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "tensorrt-llm",
        "triton",
        "backend"
      ],
      "id": 287
    },
    {
      "name": "Triton CLI",
      "one_line_profile": "Command line interface for Triton Inference Server",
      "detailed_description": "A command-line tool to simplify the creation, deployment, and profiling of models served by the Triton Inference Server.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_management",
        "deployment_tools"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/triton-inference-server/triton_cli",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "cli",
        "triton",
        "management"
      ],
      "id": 288
    },
    {
      "name": "TrustGraph",
      "one_line_profile": "Graph-based RAG tool for AI reliability",
      "detailed_description": "A tool designed to eliminate hallucinations in AI agents by constructing and utilizing knowledge graphs, enhancing the reliability of scientific information retrieval and inference.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "rag",
        "knowledge_graph",
        "inference_reliability"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/trustgraph-ai/trustgraph",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "knowledge-graph",
        "hallucination-reduction"
      ],
      "id": 289
    },
    {
      "name": "LLaVA C++ Server",
      "one_line_profile": "Server implementation for LLaVA multimodal models",
      "detailed_description": "A lightweight server implementation for LLaVA (Large Language-and-Vision Assistant) models based on llama.cpp, enabling multimodal inference.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "multimodal_inference"
      ],
      "application_level": "service",
      "primary_language": "C++",
      "repo_url": "https://github.com/trzy/llava-cpp-server",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llava",
        "multimodal",
        "inference-server"
      ],
      "id": 290
    },
    {
      "name": "ExLlamaV3",
      "one_line_profile": "Optimized quantization and inference library for local LLMs",
      "detailed_description": "A highly optimized library for quantization and inference of Large Language Models on consumer-class GPUs, focusing on performance and memory efficiency.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_quantization",
        "inference_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/turboderp-org/exllamav3",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "quantization",
        "inference",
        "exllama"
      ],
      "id": 291
    },
    {
      "name": "Pinferencia",
      "one_line_profile": "Simple Python model deployment library",
      "detailed_description": "A lightweight library for deploying Python machine learning models as inference services with minimal configuration.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "deployment"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/underneathall/pinferencia",
      "help_website": [
        "https://pinferencia.underneathall.app"
      ],
      "license": "Apache-2.0",
      "tags": [
        "deployment",
        "inference",
        "python"
      ],
      "id": 292
    },
    {
      "name": "Super JSON Mode",
      "one_line_profile": "Library for structured JSON generation from LLMs",
      "detailed_description": "A framework designed to ensure low-latency and reliable JSON output generation from Large Language Models, facilitating structured data extraction from scientific text.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "data_extraction",
        "structured_generation"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/varunshenoy/super-json-mode",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "json",
        "llm-output",
        "structured-data"
      ],
      "id": 293
    },
    {
      "name": "ScaleLLM",
      "one_line_profile": "High-performance LLM inference system",
      "detailed_description": "A high-performance inference system designed for deploying Large Language Models in production environments, optimizing throughput and latency.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "inference_system"
      ],
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/vectorch-ai/ScaleLLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "inference",
        "llm",
        "production"
      ],
      "id": 294
    },
    {
      "name": "Orkhon",
      "one_line_profile": "Rust-based ML inference framework",
      "detailed_description": "A machine learning inference framework and server runtime written in Rust, focusing on performance and safety for model deployment.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "inference_runtime"
      ],
      "application_level": "framework",
      "primary_language": "Rust",
      "repo_url": "https://github.com/vertexclique/orkhon",
      "help_website": [
        "https://orkhon.neocities.org"
      ],
      "license": "MIT",
      "tags": [
        "rust",
        "inference",
        "machine-learning"
      ],
      "id": 295
    },
    {
      "name": "OnnxStream",
      "one_line_profile": "Lightweight ONNX inference library for edge devices",
      "detailed_description": "A lightweight, dependency-free inference library for ONNX models, optimized for low-resource environments like Raspberry Pi and edge devices.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "edge_inference",
        "model_serving"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/vitoplantamura/OnnxStream",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "onnx",
        "edge-computing",
        "inference"
      ],
      "id": 296
    },
    {
      "name": "LLM Compressor",
      "one_line_profile": "Library for LLM compression and optimization",
      "detailed_description": "A library compatible with Transformers for applying compression algorithms (quantization, pruning) to Large Language Models to optimize them for deployment with vLLM.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_compression",
        "quantization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/vllm-project/llm-compressor",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "compression",
        "quantization",
        "vllm"
      ],
      "id": 297
    },
    {
      "name": "vLLM Production Stack",
      "one_line_profile": "Reference stack for K8s-native vLLM deployment",
      "detailed_description": "A reference implementation and toolkit for deploying vLLM on Kubernetes clusters, providing best practices for production-grade model serving.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_deployment",
        "infrastructure"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/vllm-project/production-stack",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "kubernetes",
        "deployment",
        "vllm"
      ],
      "id": 298
    },
    {
      "name": "Semantic Router",
      "one_line_profile": "Routing layer for LLM inference pipelines",
      "detailed_description": "A superfast decision-making layer for LLMs and agents that routes requests to the appropriate model or prompt based on semantic meaning, optimizing inference costs and performance.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_routing",
        "pipeline_optimization"
      ],
      "application_level": "library",
      "primary_language": "Go",
      "repo_url": "https://github.com/vllm-project/semantic-router",
      "help_website": [
        "https://semantic-router.readthedocs.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "routing",
        "semantic-search",
        "inference"
      ],
      "id": 299
    },
    {
      "name": "vLLM",
      "one_line_profile": "High-throughput LLM inference and serving engine",
      "detailed_description": "A state-of-the-art library for fast and memory-efficient inference and serving of Large Language Models, featuring PagedAttention and continuous batching.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "model_serving",
        "inference_engine"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/vllm-project/vllm",
      "help_website": [
        "https://docs.vllm.ai"
      ],
      "license": "Apache-2.0",
      "tags": [
        "inference",
        "serving",
        "paged-attention"
      ],
      "id": 300
    },
    {
      "name": "vllm-ascend",
      "one_line_profile": "Hardware backend plugin enabling vLLM execution on Huawei Ascend NPUs",
      "detailed_description": "A community-maintained hardware abstraction layer that extends the vLLM inference engine to support Huawei Ascend AI processors (NPUs), enabling high-throughput LLM serving on this specific hardware architecture.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_optimization",
        "serving"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/vllm-project/vllm-ascend",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "vllm",
        "ascend",
        "npu",
        "inference"
      ],
      "id": 301
    },
    {
      "name": "vllm-omni",
      "one_line_profile": "Inference framework for omni-modality large models",
      "detailed_description": "An extension of the vLLM architecture designed to support efficient inference for multimodal models (audio, video, text), handling the complexities of cross-modal generation and serving.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "serving",
        "inference_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/vllm-project/vllm-omni",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "multimodal",
        "inference",
        "vllm",
        "serving"
      ],
      "id": 302
    },
    {
      "name": "QLLM",
      "one_line_profile": "Quantization toolbox for Large Language Models",
      "detailed_description": "A general-purpose quantization library supporting multiple algorithms (GPTQ, AWQ, HQQ, VPTQ) for compressing Large Language Models to 2-8 bits, with support for exporting to ONNX/ONNX Runtime.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "quantization",
        "model_compression"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/wejoncy/QLLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "quantization",
        "gptq",
        "awq",
        "onnx"
      ],
      "id": 303
    },
    {
      "name": "quantkit",
      "one_line_profile": "CLI tool for LLM quantization formats",
      "detailed_description": "A command-line interface utility for quantizing Large Language Models into various formats including GGUF, GPTQ, AWQ, HQQ, and EXL2, facilitating model deployment on consumer hardware.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "quantization",
        "model_conversion"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/xhedit/quantkit",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "cli",
        "quantization",
        "gguf",
        "gptq"
      ],
      "id": 304
    },
    {
      "name": "onnx_runtime_cpp",
      "one_line_profile": "C++ wrapper for ONNX Runtime deployment",
      "detailed_description": "A lightweight C++ library designed to simplify the deployment of ONNX models using ONNX Runtime, providing an abstraction layer for inference integration in C++ applications.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_optimization",
        "deployment"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/xmba15/onnx_runtime_cpp",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "onnx",
        "cpp",
        "inference",
        "deployment"
      ],
      "id": 305
    },
    {
      "name": "Xinference",
      "one_line_profile": "Unified inference serving platform for LLMs and multimodal models",
      "detailed_description": "A comprehensive inference server that supports running open-source LLMs, speech recognition, and multimodal models. It provides a unified API compatible with OpenAI's interface and supports distributed deployment.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "serving",
        "inference_optimization"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/xorbitsai/inference",
      "help_website": [
        "https://inference.readthedocs.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "serving",
        "llm",
        "distributed-inference",
        "api"
      ],
      "id": 306
    },
    {
      "name": "LOPQ",
      "one_line_profile": "Locally Optimized Product Quantization for ANN search",
      "detailed_description": "A library for training Locally Optimized Product Quantization (LOPQ) models, enabling efficient approximate nearest neighbor search for high-dimensional data, suitable for large-scale scientific data retrieval.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "quantization",
        "data_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/yahoo/lopq",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ann",
        "quantization",
        "search",
        "high-dimensional"
      ],
      "id": 307
    },
    {
      "name": "EasyLM",
      "one_line_profile": "JAX/Flax library for LLM training and serving",
      "detailed_description": "A one-stop solution for pre-training, fine-tuning, evaluating, and serving Large Language Models using JAX and Flax, designed to leverage TPU/GPU parallelism efficiently.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "serving",
        "training",
        "fine-tuning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/young-geng/EasyLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "jax",
        "flax",
        "llm",
        "serving"
      ],
      "id": 308
    },
    {
      "name": "ZhiLight",
      "one_line_profile": "High-performance inference engine for Llama models",
      "detailed_description": "A highly optimized inference acceleration engine specifically designed for Llama and its variants, focusing on high throughput and low latency serving in production environments.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "serving",
        "inference_optimization"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/zhihu/ZhiLight",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "inference",
        "llama",
        "acceleration",
        "serving"
      ],
      "id": 309
    },
    {
      "name": "yolort",
      "one_line_profile": "Runtime stack for YOLOv5 deployment",
      "detailed_description": "A runtime library that unifies the deployment of YOLOv5 object detection models across various accelerators including TensorRT, LibTorch, ONNX Runtime, and TVM.",
      "domains": [
        "AI3",
        "AI3-05"
      ],
      "subtask_category": [
        "inference_optimization",
        "deployment"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/zhiqwang/yolort",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "yolo",
        "tensorrt",
        "onnx",
        "deployment"
      ],
      "id": 310
    }
  ]
}
{
  "generated_at": "2025-12-16T07:44:49.627448+08:00",
  "metadata": {
    "leaf_cluster": {
      "leaf_cluster_id": "AI3",
      "leaf_cluster_name": "科研领域模型训练与微调生态",
      "domain": "AI Toolchain",
      "typical_objects": "domain corpora",
      "task_chain": "数据→训练→微调→对齐→评测→发布",
      "tool_form": "训练栈 + 数据管线 + serving"
    },
    "unit": {
      "unit_id": "AI3-04",
      "unit_name": "模型评测/红队/鲁棒性",
      "target_scale": "150–350",
      "coverage_tools": "eval harness"
    },
    "search": {
      "target_candidates": 350,
      "queries": [
        "[GH] PromptBench",
        "[GH] Garak",
        "[GH] TextAttack",
        "[GH] Giskard",
        "[GH] Ragas",
        "[GH] DeepEval",
        "[GH] OpenCompass",
        "[GH] HELM",
        "[GH] lm-evaluation-harness",
        "[GH] llm evaluation",
        "[GH] eval harness",
        "[GH] red teaming",
        "[GH] model robustness",
        "[GH] adversarial attack",
        "[GH] safety benchmark",
        "[GH] hallucination detection",
        "[GH] prompt injection",
        "[GH] jailbreak detection",
        "[GH] bias evaluation",
        "[GH] trustworthy ai",
        "[GH] llm-as-a-judge",
        "[GH] automated metrics",
        "[WEB] llm evaluation harness github",
        "[WEB] red teaming framework llm github",
        "[WEB] model robustness benchmark github",
        "[WEB] adversarial attack library github",
        "[WEB] ai safety evaluation tools github"
      ],
      "total_candidates": 1231,
      "tool_candidates": 789,
      "final_tools": 331
    }
  },
  "tools": [
    {
      "name": "GraphRAG Agent",
      "one_line_profile": "Integrated framework for GraphRAG construction and custom evaluation",
      "detailed_description": "A comprehensive tool that integrates GraphRAG, LightRAG, and Neo4j for knowledge graph construction and search. It includes a custom evaluation framework specifically designed for assessing GraphRAG performance, making it relevant for model evaluation and retrieval robustness.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "rag_evaluation",
        "knowledge_graph_construction",
        "retrieval_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/1517005260/graph-rag-agent",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "graphrag",
        "evaluation",
        "neo4j"
      ],
      "id": 1
    },
    {
      "name": "Prompt Injection Generator",
      "one_line_profile": "Generator for prompt injection attacks to test model robustness",
      "detailed_description": "A tool designed to generate prompt injection payloads. It serves as a red teaming utility to test the robustness of Large Language Models against adversarial inputs.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "red_teaming",
        "adversarial_attack_generation",
        "robustness_testing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/1celand/prompt-injection-generator",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "prompt-injection",
        "red-teaming",
        "security"
      ],
      "id": 2
    },
    {
      "name": "Indic-Bias",
      "one_line_profile": "Benchmark for evaluating fairness of LLMs in Indian contexts",
      "detailed_description": "A comprehensive benchmark and evaluation suite designed to assess the fairness and bias of Large Language Models specifically within Indian cultural and linguistic contexts. Developed by AI4Bharat.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "bias_evaluation",
        "fairness_benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/AI4Bharat/indic-bias",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bias",
        "fairness",
        "llm-evaluation",
        "indic-languages"
      ],
      "id": 3
    },
    {
      "name": "AISBench Benchmark",
      "one_line_profile": "Model evaluation tool extending OpenCompass for service-based models",
      "detailed_description": "A model evaluation tool built upon the OpenCompass framework. It maintains compatibility with OpenCompass's configuration and datasets while extending support for evaluating service-based models, facilitating standardized benchmarking.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "performance_benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/AISBench/benchmark",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "benchmark",
        "opencompass",
        "model-evaluation"
      ],
      "id": 4
    },
    {
      "name": "Agenta",
      "one_line_profile": "Open-source LLMOps platform for prompt management and evaluation",
      "detailed_description": "A platform for LLM operations that provides tools for prompt management, experimentation, and systematic evaluation of LLM applications. It enables researchers and developers to test and benchmark model performance.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "prompt_engineering",
        "llm_ops"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/Agenta-AI/agenta",
      "help_website": [
        "https://agenta.ai"
      ],
      "license": "NOASSERTION",
      "tags": [
        "llmops",
        "evaluation",
        "prompt-management"
      ],
      "id": 5
    },
    {
      "name": "Strata-Sword",
      "one_line_profile": "Hierarchical Chinese-English jailbreak safety benchmark",
      "detailed_description": "A safety benchmark developed by Alibaba-AAIG that evaluates the robustness of LLMs against jailbreak attacks. It uses 'reasoning complexity' as a dimension to assess safety boundaries in both Chinese and English contexts.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "safety_benchmarking",
        "jailbreak_testing",
        "robustness_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/Alibaba-AAIG/Strata-Sword",
      "help_website": [],
      "license": null,
      "tags": [
        "jailbreak",
        "safety",
        "benchmark",
        "llm"
      ],
      "id": 6
    },
    {
      "name": "ALERT",
      "one_line_profile": "Benchmark for assessing LLM safety through red teaming",
      "detailed_description": "A comprehensive benchmark designed to assess the safety of Large Language Models through red teaming methodologies. It provides a framework for evaluating model responses to adversarial and harmful inputs.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "safety_benchmarking",
        "red_teaming",
        "robustness_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/Babelscape/ALERT",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "red-teaming",
        "safety",
        "benchmark",
        "llm"
      ],
      "id": 7
    },
    {
      "name": "PromptInjectionBench",
      "one_line_profile": "Benchmark for prompt injection attacks against LLMs",
      "detailed_description": "A benchmarking tool for evaluating the robustness of various Large Language Models (including GPT-4, Gemini, Azure) against prompt injection attacks and jailbreak attempts.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "prompt_injection_testing",
        "robustness_benchmarking",
        "red_teaming"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/BenderScript/PromptInjectionBench",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "prompt-injection",
        "benchmark",
        "security"
      ],
      "id": 8
    },
    {
      "name": "ko-lm-evaluation-harness",
      "one_line_profile": "Evaluation harness for Korean Large Language Models",
      "detailed_description": "A fork of the EleutherAI lm-evaluation-harness, specifically adapted for evaluating Korean Large Language Models. It provides a standardized framework for benchmarking Korean language understanding and generation capabilities.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "language_specific_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Beomi/ko-lm-evaluation-harness",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "evaluation",
        "korean",
        "llm",
        "benchmark"
      ],
      "id": 9
    },
    {
      "name": "HonestyMeter",
      "one_line_profile": "NLP framework for evaluating objectivity and bias in media",
      "detailed_description": "An NLP-powered framework designed to evaluate objectivity and detect bias in text content. It identifies manipulative techniques and provides feedback, serving as a tool for analyzing model outputs or training data for fairness.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "bias_evaluation",
        "objectivity_analysis",
        "fairness_testing"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/BetterForAll/HonestyMeter",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "bias-detection",
        "nlp",
        "fairness",
        "evaluation"
      ],
      "id": 10
    },
    {
      "name": "prompt-injector",
      "one_line_profile": "Library for research-informed prompt injection attacks",
      "detailed_description": "A TypeScript library that implements various research-informed prompt injection attack patterns. It is designed to help developers and researchers test the vulnerability of their LLM applications to prompt injection.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "prompt_injection_testing",
        "red_teaming",
        "vulnerability_assessment"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/BlueprintLabIO/prompt-injector",
      "help_website": [],
      "license": null,
      "tags": [
        "prompt-injection",
        "security",
        "testing"
      ],
      "id": 11
    },
    {
      "name": "Spell Whisperer",
      "one_line_profile": "Platform for prompt injection challenges and red teaming",
      "detailed_description": "A platform designed for prompt injection challenges, serving as a gamified environment for red teaming and understanding LLM vulnerabilities. It facilitates the collection of adversarial prompts and testing of model robustness.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "red_teaming",
        "prompt_injection_challenges",
        "robustness_training"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/CX330Blake/Spell-Whisperer",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "prompt-injection",
        "red-teaming",
        "ctf"
      ],
      "id": 12
    },
    {
      "name": "OmniVerifier",
      "one_line_profile": "Generative Universal Verifier as Multimodal Meta-Reasoner",
      "detailed_description": "A framework serving as a generative universal verifier, utilizing multimodal meta-reasoning to enhance the verification process of AI models.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_verification",
        "reasoning_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Cominclip/OmniVerifier",
      "help_website": [],
      "license": null,
      "tags": [
        "verification",
        "multimodal",
        "meta-reasoning"
      ],
      "id": 13
    },
    {
      "name": "NormalyzerDE",
      "one_line_profile": "Tools for normalization and differential expression analysis of omics data",
      "detailed_description": "A framework for normalization, outlier evaluation, technical bias assessment, batch effect handling, and differential expression analysis in proteomics and other omics data.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "normalization",
        "differential_expression",
        "quality_control"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/ComputationalProteomics/NormalyzerDE",
      "help_website": [],
      "license": null,
      "tags": [
        "proteomics",
        "normalization",
        "differential-expression",
        "r-package"
      ],
      "id": 14
    },
    {
      "name": "CartAI",
      "one_line_profile": "Open-source AI supervisor agent for lifecycle oversight and compliance",
      "detailed_description": "An intelligent agent designed for end-to-end oversight and compliance in the AI lifecycle, ensuring trustworthy AI development and deployment.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "ai_governance",
        "compliance_checking",
        "model_monitoring"
      ],
      "application_level": "platform",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/ContrastoAI/cartai",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ai-safety",
        "compliance",
        "agent",
        "trustworthy-ai"
      ],
      "id": 15
    },
    {
      "name": "DeepRobust",
      "one_line_profile": "PyTorch adversarial library for attack and defense on images and graphs",
      "detailed_description": "A comprehensive PyTorch library for adversarial attacks and defenses, covering both image processing and graph neural networks to evaluate and improve model robustness.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_attack",
        "adversarial_defense",
        "robustness_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/DSE-MSU/DeepRobust",
      "help_website": [
        "https://deeprobust.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "adversarial-learning",
        "pytorch",
        "graph-neural-networks",
        "robustness"
      ],
      "id": 16
    },
    {
      "name": "DreamLayer",
      "one_line_profile": "Benchmarking and evaluation automation for diffusion models",
      "detailed_description": "A tool to automate evaluations, seed management, and metric calculation for benchmarking diffusion models, ensuring reproducible results.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_benchmarking",
        "evaluation_automation",
        "reproducibility"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/DreamLayer-AI/DreamLayer",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "diffusion-models",
        "benchmarking",
        "evaluation"
      ],
      "id": 17
    },
    {
      "name": "lm-evaluation-harness",
      "one_line_profile": "Framework for few-shot evaluation of language models",
      "detailed_description": "A widely used framework for evaluating autoregressive language models on a large number of tasks, supporting few-shot evaluation and providing a standardized interface for model comparison.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "few_shot_testing",
        "benchmark_suite"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/EleutherAI/lm-evaluation-harness",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "evaluation",
        "nlp",
        "benchmark"
      ],
      "id": 18
    },
    {
      "name": "JamAIBase",
      "one_line_profile": "Collaborative spreadsheet interface for AI pipeline creation and evaluation",
      "detailed_description": "A platform that combines a spreadsheet interface with AI capabilities, allowing users to chain cells into pipelines, experiment with prompts, and evaluate LLM responses in real-time.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "prompt_engineering",
        "response_evaluation",
        "pipeline_orchestration"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/EmbeddedLLM/JamAIBase",
      "help_website": [
        "https://jamaibase.com"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm-ops",
        "spreadsheet-ui",
        "evaluation",
        "collaboration"
      ],
      "id": 19
    },
    {
      "name": "Cognitive-Hijacking-in-Long-Context-LLMs",
      "one_line_profile": "Implementation of prompt injection via forged internal states",
      "detailed_description": "A research tool demonstrating a novel prompt injection method that exploits forged internal states in long-context Large Language Models.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "red_teaming",
        "prompt_injection",
        "vulnerability_research"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Eric-Terminal/Cognitive-Hijacking-in-Long-Context-LLMs",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "prompt-injection",
        "llm-security",
        "exploit"
      ],
      "id": 20
    },
    {
      "name": "universal-triggers",
      "one_line_profile": "Universal Adversarial Triggers for Attacking and Analyzing NLP",
      "detailed_description": "Code for generating universal adversarial triggers that can cause specific predictions when concatenated to any input, used for analyzing NLP model robustness.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_attack",
        "robustness_analysis",
        "nlp_security"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Eric-Wallace/universal-triggers",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "adversarial-nlp",
        "triggers",
        "robustness"
      ],
      "id": 21
    },
    {
      "name": "xai",
      "one_line_profile": "Explainability toolbox for machine learning",
      "detailed_description": "A library designed to provide explainability and fairness analysis for machine learning models, helping to evaluate model behavior and detect bias.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "explainability",
        "fairness_evaluation",
        "model_interpretation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/EthicalML/xai",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "explainable-ai",
        "fairness",
        "machine-learning"
      ],
      "id": 22
    },
    {
      "name": "OpenFed",
      "one_line_profile": "Comprehensive Federated Learning Framework",
      "detailed_description": "A versatile open-source framework for federated learning, enabling distributed model training and evaluation while preserving data privacy.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "federated_learning",
        "distributed_training",
        "privacy_preserving_ml"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/FederalLab/OpenFed",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "federated-learning",
        "privacy",
        "distributed-systems"
      ],
      "id": 23
    },
    {
      "name": "LLMZoo",
      "one_line_profile": "Data, models, and evaluation benchmark for LLMs",
      "detailed_description": "A project providing a collection of instruction-tuned models, datasets, and evaluation benchmarks to facilitate research and development of Large Language Models.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_benchmarking",
        "instruction_tuning",
        "dataset_provision"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/FreedomIntelligence/LLMZoo",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "benchmark",
        "instruction-tuning"
      ],
      "id": 24
    },
    {
      "name": "LRV-Instruction",
      "one_line_profile": "Mitigating Hallucination in Large Multi-Modal Models",
      "detailed_description": "Code and resources for robust instruction tuning aimed at mitigating hallucinations in Large Multi-Modal Models (LMMs).",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "hallucination_mitigation",
        "instruction_tuning",
        "robustness"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/FuxiaoLiu/LRV-Instruction",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "multimodal",
        "hallucination",
        "instruction-tuning"
      ],
      "id": 25
    },
    {
      "name": "LLM-Check",
      "one_line_profile": "Detection of Hallucinations in Large Language Models",
      "detailed_description": "Implementation of methods for investigating and detecting hallucinations in Large Language Models, as presented at NeurIPS 2024.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "hallucination_detection",
        "model_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/GaurangSriramanan/LLM_Check_Hallucination_Detection",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "hallucination",
        "llm",
        "neurips-2024"
      ],
      "id": 26
    },
    {
      "name": "mcp-guard",
      "one_line_profile": "Security tool for Model Context Protocol (MCP) clients",
      "detailed_description": "A security tool designed to protect Model Context Protocol (MCP) clients from prompt injection attacks and other vulnerabilities.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "prompt_injection_defense",
        "client_security",
        "ai_protocol_security"
      ],
      "application_level": "solver",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/General-Analysis/mcp-guard",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mcp",
        "prompt-injection",
        "security"
      ],
      "id": 27
    },
    {
      "name": "giskard-client",
      "one_line_profile": "API Client for Giskard AI evaluation platform",
      "detailed_description": "The Python client library for interacting with the Giskard platform, enabling programmatic control over AI model testing and evaluation workflows.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_testing",
        "evaluation_workflow",
        "api_client"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Giskard-AI/giskard-client",
      "help_website": [
        "https://docs.giskard.ai/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "giskard",
        "testing",
        "client-library"
      ],
      "id": 28
    },
    {
      "name": "giskard-hub",
      "one_line_profile": "SDK for Giskard Enterprise platform",
      "detailed_description": "SDK for the Giskard Hub, facilitating enterprise-level LLM agent testing, team collaboration, and continuous red teaming.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "red_teaming",
        "collaborative_testing",
        "enterprise_sdk"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/Giskard-AI/giskard-hub",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "giskard",
        "enterprise",
        "red-teaming"
      ],
      "id": 29
    },
    {
      "name": "giskard-oss",
      "one_line_profile": "Open-Source Evaluation & Testing library for LLM Agents",
      "detailed_description": "A comprehensive open-source library for testing and evaluating LLM agents and AI models, focusing on detecting vulnerabilities, hallucinations, and performance issues.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_testing",
        "vulnerability_scanning",
        "quality_assurance"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Giskard-AI/giskard-oss",
      "help_website": [
        "https://docs.giskard.ai/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm-testing",
        "evaluation",
        "quality-assurance"
      ],
      "id": 30
    },
    {
      "name": "giskard-vision",
      "one_line_profile": "Evaluation & Testing for Computer Vision AI systems",
      "detailed_description": "A specialized module within the Giskard ecosystem for evaluating and testing computer vision models for robustness and correctness.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "computer_vision_testing",
        "robustness_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Giskard-AI/giskard-vision",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "computer-vision",
        "testing",
        "giskard"
      ],
      "id": 31
    },
    {
      "name": "phare",
      "one_line_profile": "LLM benchmark for security and safety dimensions",
      "detailed_description": "A benchmark suite designed to evaluate Large Language Models across key dimensions of AI security and safety.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "safety_benchmarking",
        "security_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/Giskard-AI/phare",
      "help_website": [],
      "license": null,
      "tags": [
        "benchmark",
        "ai-safety",
        "llm"
      ],
      "id": 32
    },
    {
      "name": "prompt-injections",
      "one_line_profile": "Collection of prompt injections for AI scanning",
      "detailed_description": "A dataset and collection of prompt injection patterns used by the Giskard Scan tool to test LLMs for security vulnerabilities.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "prompt_injection_testing",
        "security_dataset"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/Giskard-AI/prompt-injections",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "prompt-injection",
        "dataset",
        "security"
      ],
      "id": 33
    },
    {
      "name": "Knowledge-Constrained-Decoding",
      "one_line_profile": "Knowledge-Constrained Tree Search Decoding with Token-Level Hallucination Detection",
      "detailed_description": "Implementation of the KCTS method for decoding in language models, incorporating token-level hallucination detection to improve factual accuracy.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "decoding_strategy",
        "hallucination_detection",
        "factuality"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/HKUST-KnowComp/Knowledge-Constrained-Decoding",
      "help_website": [],
      "license": null,
      "tags": [
        "decoding",
        "hallucination",
        "nlp"
      ],
      "id": 34
    },
    {
      "name": "Graph Adversarial Attack",
      "one_line_profile": "Library for adversarial attacks on graph structured data",
      "detailed_description": "A Python library providing implementations of various adversarial attack methods specifically designed for Graph Neural Networks (GNNs), enabling robustness evaluation of graph-based models in scientific domains like chemistry and biology.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_attack",
        "robustness_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Hanjun-Dai/graph_adversarial_attack",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "graph-neural-networks",
        "adversarial-attacks",
        "robustness"
      ],
      "id": 35
    },
    {
      "name": "PGD-pytorch",
      "one_line_profile": "PyTorch implementation of Projected Gradient Descent (PGD) attacks",
      "detailed_description": "A focused implementation of the Projected Gradient Descent (PGD) adversarial attack method in PyTorch, widely used as a standard baseline for evaluating the robustness of deep learning models against adversarial perturbations.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_attack",
        "robustness_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/Harry24k/PGD-pytorch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pytorch",
        "adversarial-attacks",
        "pgd"
      ],
      "id": 36
    },
    {
      "name": "TorchAttacks",
      "one_line_profile": "Comprehensive PyTorch library for adversarial attacks",
      "detailed_description": "A lightweight and comprehensive PyTorch library providing a wide range of adversarial attack algorithms (FGSM, PGD, CW, etc.) to evaluate the robustness of neural networks.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_attack",
        "robustness_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Harry24k/adversarial-attacks-pytorch",
      "help_website": [
        "https://adversarial-attacks-pytorch.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "pytorch",
        "adversarial-attacks",
        "security"
      ],
      "id": 37
    },
    {
      "name": "Helicone",
      "one_line_profile": "Open-source observability and evaluation platform for LLMs",
      "detailed_description": "A platform designed for monitoring, logging, and evaluating Large Language Model (LLM) interactions, providing tools for tracking costs, latency, and quality metrics to ensure model reliability in production and research.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_monitoring",
        "performance_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/Helicone/helicone",
      "help_website": [
        "https://docs.helicone.ai/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm-ops",
        "observability",
        "evaluation"
      ],
      "id": 38
    },
    {
      "name": "Robust Tube MPC",
      "one_line_profile": "Implementation of Robust Model Predictive Control using Tube methods",
      "detailed_description": "A MATLAB implementation of Robust Model Predictive Control (MPC) utilizing tube-based methods to handle uncertainties in system dynamics, applicable in control theory research and robotics.",
      "domains": [
        "AI3"
      ],
      "subtask_category": [
        "control_simulation",
        "scientific_modeling"
      ],
      "application_level": "solver",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/HiroIshida/robust-tube-mpc",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mpc",
        "control-theory",
        "robust-control"
      ],
      "id": 39
    },
    {
      "name": "TrustGPT",
      "one_line_profile": "Benchmark for evaluating toxicity, bias, and value-alignment in LLMs",
      "detailed_description": "A benchmark framework designed to assess the trustworthiness of Large Language Models by evaluating them across dimensions such as toxicity, bias, and value alignment, ensuring responsible AI development.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "safety_benchmark",
        "bias_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/HowieHwong/TrustGPT",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "benchmark",
        "trustworthiness"
      ],
      "id": 40
    },
    {
      "name": "TrustLLM",
      "one_line_profile": "Comprehensive toolkit for evaluating trustworthiness in Large Language Models",
      "detailed_description": "A comprehensive framework and benchmark for evaluating the trustworthiness of LLMs, covering multiple dimensions including truthfulness, safety, fairness, and robustness.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "safety_benchmark",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/HowieHwong/TrustLLM",
      "help_website": [
        "https://trustllmbenchmark.github.io/TrustLLM-Website/"
      ],
      "license": "MIT",
      "tags": [
        "llm",
        "safety",
        "evaluation"
      ],
      "id": 41
    },
    {
      "name": "TrustRAG",
      "one_line_profile": "Framework for enhancing and evaluating robustness in RAG systems",
      "detailed_description": "A research tool focused on improving and evaluating the trustworthiness and robustness of Retrieval-Augmented Generation (RAG) systems, addressing issues like retrieval errors and generation hallucinations.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "rag_evaluation",
        "robustness_enhancement"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/HuichiZhou/TrustRAG",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "robustness",
        "trustworthiness"
      ],
      "id": 42
    },
    {
      "name": "Tensor Trust",
      "one_line_profile": "Platform for collecting prompt injection data for robust ML research",
      "detailed_description": "A gamified platform and dataset designed to crowdsource adversarial prompts (prompt injections) to facilitate research into the robustness and security of Large Language Models.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "data_collection",
        "adversarial_attack"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/HumanCompatibleAI/tensor-trust",
      "help_website": [
        "https://tensortrust.ai/"
      ],
      "license": "BSD-2-Clause",
      "tags": [
        "prompt-injection",
        "security",
        "crowdsourcing"
      ],
      "id": 43
    },
    {
      "name": "xFinder",
      "one_line_profile": "Automated evaluator using LLMs for reliable evaluation",
      "detailed_description": "A tool that leverages Large Language Models as automated evaluators to assess the quality and correctness of outputs from other models, aiming to improve the reliability of automated evaluation metrics.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "automated_evaluation",
        "llm_as_judge"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/IAAR-Shanghai/xFinder",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "evaluation",
        "llm",
        "automation"
      ],
      "id": 44
    },
    {
      "name": "xVerify",
      "one_line_profile": "Efficient answer verifier for reasoning model evaluations",
      "detailed_description": "A tool designed to verify the correctness of answers generated by reasoning models, facilitating efficient and accurate evaluation of complex reasoning tasks.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "result_verification",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/IAAR-Shanghai/xVerify",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "reasoning",
        "verification",
        "evaluation"
      ],
      "id": 45
    },
    {
      "name": "SCERL",
      "one_line_profile": "Safety Constrained Environments for Reinforcement Learning",
      "detailed_description": "A collection of benchmark environments and datasets for evaluating Reinforcement Learning algorithms under safety constraints, supporting research into safe and robust RL agents.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "safety_benchmark",
        "reinforcement_learning"
      ],
      "application_level": "dataset",
      "primary_language": "Inform 7",
      "repo_url": "https://github.com/IBM/SCERL",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "reinforcement-learning",
        "safety",
        "benchmark"
      ],
      "id": 46
    },
    {
      "name": "EvalAssist",
      "one_line_profile": "Tool for refining LLM-as-a-Judge evaluation criteria",
      "detailed_description": "An open-source tool that assists users in iteratively refining evaluation criteria and prompts when using Large Language Models as evaluators (LLM-as-a-Judge), featuring a web-based interface for analysis.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "evaluation_workflow",
        "prompt_engineering"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/IBM/eval-assist",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-as-a-judge",
        "evaluation",
        "ui"
      ],
      "id": 47
    },
    {
      "name": "S-Eval",
      "one_line_profile": "Automated and comprehensive safety evaluation framework for LLMs",
      "detailed_description": "A framework designed for the automated and comprehensive safety evaluation of Large Language Models, providing metrics and datasets to assess various safety risks.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "safety_evaluation",
        "risk_assessment"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/IS2Lab/S-Eval",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "safety",
        "llm",
        "evaluation"
      ],
      "id": 48
    },
    {
      "name": "Infosys Responsible AI Toolkit",
      "one_line_profile": "Toolkit for AI safety, security, explainability, and fairness",
      "detailed_description": "A comprehensive toolkit incorporating features for safety, security, explainability, fairness, bias, and hallucination detection to ensure the development of trustworthy and transparent AI solutions.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "responsible_ai",
        "model_audit"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Infosys/Infosys-Responsible-AI-Toolkit",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "responsible-ai",
        "explainability",
        "fairness"
      ],
      "id": 49
    },
    {
      "name": "Factorio Learning Environment",
      "one_line_profile": "Open-ended environment for evaluating LLMs and agents in Factorio",
      "detailed_description": "A non-saturating, open-ended simulation environment based on the game Factorio, designed for evaluating the planning and problem-solving capabilities of Large Language Models and Reinforcement Learning agents.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "agent_evaluation",
        "simulation_environment"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/JackHopkins/factorio-learning-environment",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "rl",
        "llm-agent",
        "simulation"
      ],
      "id": 50
    },
    {
      "name": "JailbreakBench",
      "one_line_profile": "Benchmark for jailbreaking language models",
      "detailed_description": "An open robustness benchmark specifically designed for evaluating the susceptibility of Language Models to jailbreaking attacks, providing a standardized dataset and evaluation protocol.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "safety_benchmark",
        "adversarial_attack"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/JailbreakBench/jailbreakbench",
      "help_website": [
        "https://jailbreakbench.github.io/"
      ],
      "license": "MIT",
      "tags": [
        "jailbreak",
        "robustness",
        "benchmark"
      ],
      "id": 51
    },
    {
      "name": "Multilingual Safety Benchmark",
      "one_line_profile": "Safety benchmark for Large Language Models across multiple languages",
      "detailed_description": "A benchmark dataset and framework for evaluating the safety and robustness of Large Language Models in multilingual contexts, addressing the gap in non-English safety evaluation.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "safety_benchmark",
        "multilingual_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/Jarviswang94/Multilingual_safety_benchmark",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "multilingual",
        "safety",
        "llm"
      ],
      "id": 52
    },
    {
      "name": "Model Predictive Control (MPC)",
      "one_line_profile": "C++ implementation of MPC for vehicle control simulation",
      "detailed_description": "A C++ implementation of Model Predictive Control (MPC) designed to drive a vehicle in a simulator by optimizing steering and throttle commands, useful for research in control systems and autonomous driving.",
      "domains": [
        "AI3"
      ],
      "subtask_category": [
        "control_simulation",
        "scientific_modeling"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/JunshengFu/Model-Predictive-Control",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mpc",
        "control",
        "simulation"
      ],
      "id": 53
    },
    {
      "name": "LettuceDetect",
      "one_line_profile": "Hallucination detection framework for RAG applications",
      "detailed_description": "A framework designed to detect hallucinations in Retrieval-Augmented Generation (RAG) applications, helping to ensure the factual accuracy and reliability of generated content.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "hallucination_detection",
        "rag_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/KRLabsOrg/LettuceDetect",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "hallucination",
        "evaluation"
      ],
      "id": 54
    },
    {
      "name": "Mithra Scanner",
      "one_line_profile": "API testing tool for prompt injection and LLM security benchmarking",
      "detailed_description": "An interactive security testing tool for Large Language Model endpoints, supporting prompt injection testing, refusal detection, and security benchmarking via CLI and REST API integration.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "security_scanning",
        "red_teaming"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/KadirArslan/Mithra-Scanner",
      "help_website": [],
      "license": null,
      "tags": [
        "security",
        "red-teaming",
        "prompt-injection"
      ],
      "id": 55
    },
    {
      "name": "HouYi",
      "one_line_profile": "Automated prompt injection framework for LLM applications",
      "detailed_description": "A framework for automating prompt injection attacks against LLM-integrated applications, serving as a red-teaming tool to identify security vulnerabilities in AI systems.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "red_teaming",
        "adversarial_attack"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/LLMSecurity/HouYi",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "prompt-injection",
        "security",
        "red-teaming"
      ],
      "id": 56
    },
    {
      "name": "aac-metrics",
      "one_line_profile": "Metrics for evaluating Automated Audio Captioning systems",
      "detailed_description": "A PyTorch-based library providing a collection of metrics (SPIDEr, FENSE, etc.) for evaluating the performance of Automated Audio Captioning (AAC) systems.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "audio_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Labbeti/aac-metrics",
      "help_website": [
        "https://aac-metrics.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "audio-captioning",
        "metrics",
        "evaluation"
      ],
      "id": 57
    },
    {
      "name": "BERT-Attack",
      "one_line_profile": "Adversarial attack method against BERT models",
      "detailed_description": "An implementation of the BERT-Attack method, which generates high-quality adversarial samples to attack BERT-based models, used for evaluating the robustness of NLP models.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_attack",
        "robustness_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/LinyangLee/BERT-Attack",
      "help_website": [],
      "license": null,
      "tags": [
        "nlp",
        "adversarial-attack",
        "bert"
      ],
      "id": 58
    },
    {
      "name": "R-Judge",
      "one_line_profile": "Benchmark for safety risk awareness in LLM agents",
      "detailed_description": "A benchmark designed to evaluate the safety risk awareness of Large Language Model (LLM) agents, focusing on their ability to identify and handle risky scenarios.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "safety_benchmark",
        "agent_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/Lordog/R-Judge",
      "help_website": [
        "https://r-judge.github.io/"
      ],
      "license": null,
      "tags": [
        "llm-agent",
        "safety",
        "benchmark"
      ],
      "id": 59
    },
    {
      "name": "Trustworthy AI Fetal Brain Segmentation",
      "one_line_profile": "Trustworthy AI method for fetal brain MRI segmentation",
      "detailed_description": "An implementation of a trustworthy AI method based on Dempster-Shafer theory for the segmentation of fetal brains in 3D T2w MRI scans, providing uncertainty estimation for medical imaging analysis.",
      "domains": [
        "AI3"
      ],
      "subtask_category": [
        "medical_image_segmentation",
        "scientific_data_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/LucasFidon/trustworthy-ai-fetal-brain-segmentation",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "medical-imaging",
        "segmentation",
        "uncertainty-estimation"
      ],
      "id": 60
    },
    {
      "name": "GenoArmory",
      "one_line_profile": "Unified evaluation framework for adversarial attacks on genomic foundation models",
      "detailed_description": "A comprehensive framework designed to evaluate the robustness of genomic foundation models against adversarial attacks. It provides a standardized environment for testing model vulnerabilities in bioinformatics contexts.",
      "domains": [
        "AI3-04",
        "Bioinformatics"
      ],
      "subtask_category": [
        "adversarial_attack",
        "model_evaluation",
        "robustness"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/MAGICS-LAB/GenoArmory",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "genomics",
        "adversarial-attacks",
        "foundation-models"
      ],
      "id": 61
    },
    {
      "name": "MJ-Bench",
      "one_line_profile": "Benchmark for evaluating multimodal reward models in text-to-image generation",
      "detailed_description": "A benchmark suite designed to assess whether multimodal reward models effectively judge the quality of text-to-image generation outputs, providing datasets and evaluation scripts.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "benchmarking",
        "reward_model_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/MJ-Bench/MJ-Bench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "multimodal",
        "reward-model",
        "text-to-image"
      ],
      "id": 62
    },
    {
      "name": "nlg-eval",
      "one_line_profile": "Evaluation code for unsupervised automated metrics for Natural Language Generation",
      "detailed_description": "A library providing implementations for various unsupervised automated metrics (such as BLEU, METEOR, ROUGE, CIDEr) used to evaluate Natural Language Generation (NLG) models.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "nlg_evaluation",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Maluuba/nlg-eval",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "nlg",
        "evaluation-metrics",
        "nlp"
      ],
      "id": 63
    },
    {
      "name": "AutoRAG",
      "one_line_profile": "Automated framework for RAG evaluation and optimization",
      "detailed_description": "An AutoML-style framework designed to automatically evaluate and optimize Retrieval-Augmented Generation (RAG) pipelines, helping researchers select the best RAG modules and parameters.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "rag_evaluation",
        "optimization",
        "automl"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/Marker-Inc-Korea/AutoRAG",
      "help_website": [
        "https://docs.autorag.ai/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "evaluation",
        "automl"
      ],
      "id": 64
    },
    {
      "name": "pytector",
      "one_line_profile": "Python package for LLM prompt injection detection",
      "detailed_description": "A lightweight Python package designed to detect prompt injection attacks in Large Language Model (LLM) inputs, enhancing the security and robustness of LLM applications.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "prompt_injection_detection",
        "security"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/MaxMLang/pytector",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "prompt-injection",
        "security",
        "llm"
      ],
      "id": 65
    },
    {
      "name": "Dingo",
      "one_line_profile": "Comprehensive AI data, model, and application quality evaluation tool",
      "detailed_description": "A tool designed to evaluate the quality of AI data, models, and applications, providing metrics and insights to ensure the reliability and performance of AI systems.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "data_quality"
      ],
      "application_level": "tool",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/MigoXLab/dingo",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "evaluation",
        "quality-assurance",
        "ai-testing"
      ],
      "id": 66
    },
    {
      "name": "summarization-eval",
      "one_line_profile": "Reference-free automatic summarization evaluation tool",
      "detailed_description": "A toolkit for evaluating text summarization models without reference summaries, including features for potential hallucination detection.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "summarization_evaluation",
        "hallucination_detection"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Muhtasham/summarization-eval",
      "help_website": [],
      "license": null,
      "tags": [
        "summarization",
        "evaluation",
        "hallucination"
      ],
      "id": 67
    },
    {
      "name": "garak",
      "one_line_profile": "LLM vulnerability scanner and red teaming tool",
      "detailed_description": "A comprehensive vulnerability scanner for Large Language Models (LLMs), designed to probe for hallucinations, data leakage, prompt injection, and other security weaknesses.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "vulnerability_scanning",
        "red_teaming",
        "robustness"
      ],
      "application_level": "tool",
      "primary_language": "Python",
      "repo_url": "https://github.com/NVIDIA/garak",
      "help_website": [
        "https://garak.ai"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "security",
        "red-teaming"
      ],
      "id": 68
    },
    {
      "name": "RADTTS",
      "one_line_profile": "Flow-based TTS models with robust alignment learning",
      "detailed_description": "A library providing training and inference recipes for RADTTS and RADTTS++, enabling robust text-to-speech synthesis with fine-grained control over speech attributes.",
      "domains": [
        "AI3"
      ],
      "subtask_category": [
        "text_to_speech",
        "generative_model",
        "model_training"
      ],
      "application_level": "library",
      "primary_language": "Roff",
      "repo_url": "https://github.com/NVIDIA/radtts",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "tts",
        "speech-synthesis",
        "generative-ai"
      ],
      "id": 69
    },
    {
      "name": "sentiment-discovery",
      "one_line_profile": "Unsupervised language modeling for robust sentiment classification",
      "detailed_description": "A library implementing unsupervised language modeling techniques at scale to achieve robust sentiment classification, useful for analyzing large-scale text data.",
      "domains": [
        "AI3"
      ],
      "subtask_category": [
        "sentiment_analysis",
        "language_modeling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NVIDIA/sentiment-discovery",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "sentiment-analysis",
        "nlp",
        "unsupervised-learning"
      ],
      "id": 70
    },
    {
      "name": "LLM Colosseum",
      "one_line_profile": "Benchmark for evaluating LLMs via game-playing interactions",
      "detailed_description": "A unique benchmarking tool that evaluates the quality and reasoning capabilities of Large Language Models (LLMs) by having them compete in Street Fighter 3, providing a dynamic evaluation environment.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "benchmarking",
        "llm_evaluation",
        "game_based_eval"
      ],
      "application_level": "tool",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/OpenGenerativeAI/llm-colosseum",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "benchmark",
        "game-ai"
      ],
      "id": 71
    },
    {
      "name": "EasyDetect",
      "one_line_profile": "Hallucination detection framework for LLMs",
      "detailed_description": "An easy-to-use framework designed to detect hallucinations in Large Language Model outputs, supporting various detection methods to ensure model reliability.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "hallucination_detection",
        "model_evaluation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenKG-ORG/EasyDetect",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "hallucination",
        "llm",
        "reliability"
      ],
      "id": 72
    },
    {
      "name": "Bag-of-Tricks-for-AT",
      "one_line_profile": "Collection of empirical tricks for adversarial training",
      "detailed_description": "A library implementing various empirical tricks and best practices for training robust deep learning models using adversarial training techniques.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_training",
        "robustness",
        "model_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/P2333/Bag-of-Tricks-for-AT",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "adversarial-training",
        "robustness",
        "deep-learning"
      ],
      "id": 73
    },
    {
      "name": "Safety-Gymnasium",
      "one_line_profile": "Unified benchmark environments for safe reinforcement learning",
      "detailed_description": "A highly scalable and customizable benchmark suite for Safe Reinforcement Learning, providing environments to evaluate the safety and performance of RL agents.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "safety_benchmark",
        "evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/PKU-Alignment/safety-gymnasium",
      "help_website": [
        "https://www.safety-gymnasium.com/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "reinforcement-learning",
        "safety",
        "benchmark"
      ],
      "id": 74
    },
    {
      "name": "LIFT3D",
      "one_line_profile": "Foundation policy for robust 3D robotic manipulation",
      "detailed_description": "A framework that lifts 2D large-scale pretrained models to 3D for robust robotic manipulation, serving as a foundation policy for robotics research.",
      "domains": [
        "AI3",
        "Robotics"
      ],
      "subtask_category": [
        "robotic_manipulation",
        "3d_policy",
        "model_training"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/PKU-HMI-Lab/LIFT3D",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "robotics",
        "3d-manipulation",
        "foundation-model"
      ],
      "id": 75
    },
    {
      "name": "Themis",
      "one_line_profile": "Reference-free NLG evaluation language model",
      "detailed_description": "A model-based evaluation tool for Natural Language Generation that operates without references, offering flexibility and interpretability in assessing text quality.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "nlg_evaluation",
        "model_based_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/PKU-ONELab/Themis",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "nlg",
        "evaluation",
        "llm"
      ],
      "id": 76
    },
    {
      "name": "rko_lio",
      "one_line_profile": "Robust LiDAR-Inertial Odometry without sensor-specific modelling",
      "detailed_description": "A robust implementation of LiDAR-Inertial Odometry (LIO) that does not require sensor-specific modeling, useful for robotic navigation and mapping tasks.",
      "domains": [
        "Robotics"
      ],
      "subtask_category": [
        "lidar_inertial_odometry",
        "slam",
        "navigation"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/PRBonn/rko_lio",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "slam",
        "lidar",
        "robotics"
      ],
      "id": 77
    },
    {
      "name": "SafeLife",
      "one_line_profile": "Safety benchmarks for reinforcement learning agents",
      "detailed_description": "A set of environments and benchmarks designed to test the safety and robustness of reinforcement learning agents in complex, dynamic gridworlds.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "safety_benchmark"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/PartnershipOnAI/safelife",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "reinforcement-learning",
        "safety",
        "benchmark"
      ],
      "id": 78
    },
    {
      "name": "Parrot Paraphraser",
      "one_line_profile": "Paraphrasing framework for NLU data augmentation",
      "detailed_description": "A practical framework for generating paraphrases to augment training data for Natural Language Understanding (NLU) models, helping to build more robust conversational agents.",
      "domains": [
        "AI3"
      ],
      "subtask_category": [
        "data_augmentation",
        "paraphrasing",
        "model_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/PrithivirajDamodaran/Parrot_Paraphraser",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "data-augmentation",
        "nlp",
        "paraphrasing"
      ],
      "id": 79
    },
    {
      "name": "TextAttack",
      "one_line_profile": "Framework for adversarial attacks and data augmentation in NLP",
      "detailed_description": "A comprehensive Python framework for generating adversarial attacks, performing data augmentation, and training robust models in Natural Language Processing (NLP).",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_attack",
        "data_augmentation",
        "robustness"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/QData/TextAttack",
      "help_website": [
        "https://textattack.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "nlp",
        "adversarial-attacks",
        "data-augmentation"
      ],
      "id": 80
    },
    {
      "name": "TextAttack-A2T",
      "one_line_profile": "Implementation of A2T adversarial training method for NLP models",
      "detailed_description": "A repository containing the implementation of the 'A2T: Towards Improving Adversarial Training of NLP Models' method, designed to enhance the robustness of Natural Language Processing models against adversarial attacks.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_training",
        "model_robustness"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/QData/TextAttack-A2T",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nlp",
        "adversarial-training",
        "robustness"
      ],
      "id": 81
    },
    {
      "name": "TextAttack-Search-Benchmark",
      "one_line_profile": "Benchmark for search algorithms in NLP adversarial example generation",
      "detailed_description": "A benchmarking suite for evaluating various search algorithms used to generate adversarial examples in Natural Language Processing, supporting research into the efficiency and effectiveness of adversarial attacks.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_attack",
        "benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/QData/TextAttack-Search-Benchmark",
      "help_website": [],
      "license": null,
      "tags": [
        "nlp",
        "adversarial-attacks",
        "benchmarking"
      ],
      "id": 82
    },
    {
      "name": "RLAIF-V",
      "one_line_profile": "Framework for AI feedback on Vision-Language Models",
      "detailed_description": "An open-source framework implementing Reinforcement Learning from AI Feedback (RLAIF) specifically for Vision-Language Models (VLMs) to improve their trustworthiness and alignment.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "alignment",
        "rlhf",
        "multimodal_evaluation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/RLHF-V/RLAIF-V",
      "help_website": [],
      "license": null,
      "tags": [
        "vlm",
        "rlaif",
        "trustworthiness"
      ],
      "id": 83
    },
    {
      "name": "LLMBox",
      "one_line_profile": "Unified library for LLM training and evaluation",
      "detailed_description": "A comprehensive library for implementing Large Language Models, featuring a unified pipeline for training and extensive model evaluation capabilities.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_training",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/RUCAIBox/LLMBox",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "training-pipeline",
        "evaluation"
      ],
      "id": 84
    },
    {
      "name": "RobustBench",
      "one_line_profile": "Standardized benchmark for adversarial robustness",
      "detailed_description": "A standardized benchmark and library for evaluating the adversarial robustness of image classification models, providing a leaderboard and easy access to robust models.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/RobustBench/robustbench",
      "help_website": [
        "https://robustbench.github.io/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "adversarial-robustness",
        "computer-vision",
        "benchmark"
      ],
      "id": 85
    },
    {
      "name": "RouterArena",
      "one_line_profile": "Evaluation framework for LLM routing strategies",
      "detailed_description": "An open framework designed to evaluate Large Language Model routers, featuring standardized datasets, metrics, and an automated evaluation pipeline.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_routing",
        "evaluation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/RouteWorks/RouterArena",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-routing",
        "evaluation-framework",
        "benchmark"
      ],
      "id": 86
    },
    {
      "name": "RAG-evaluation-harnesses",
      "one_line_profile": "Evaluation suite for Retrieval-Augmented Generation",
      "detailed_description": "A comprehensive evaluation suite specifically designed for assessing the performance of Retrieval-Augmented Generation (RAG) systems.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "rag_evaluation",
        "retrieval_augmented_generation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/RulinShao/RAG-evaluation-harnesses",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "evaluation",
        "llm"
      ],
      "id": 87
    },
    {
      "name": "SORRY-Bench",
      "one_line_profile": "Benchmark for evaluating LLM safety refusal",
      "detailed_description": "A benchmark and evaluation toolkit for systematically assessing the safety refusal mechanisms of Large Language Models.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "safety_evaluation",
        "alignment"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/SORRY-Bench/sorry-bench",
      "help_website": [
        "https://sorry-bench.github.io/"
      ],
      "license": "MIT",
      "tags": [
        "llm-safety",
        "benchmark",
        "refusal"
      ],
      "id": 88
    },
    {
      "name": "JailBreakV-28K",
      "one_line_profile": "Benchmark for LLM to MLLM jailbreak transferability",
      "detailed_description": "A comprehensive benchmark designed to evaluate the transferability of jailbreak attacks from Large Language Models to Multimodal Large Language Models, assessing robustness and safety.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "red_teaming",
        "safety_evaluation",
        "multimodal"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/SaFo-Lab/JailBreakV_28K",
      "help_website": [
        "https://jailbreakv.github.io/"
      ],
      "license": null,
      "tags": [
        "jailbreak",
        "mllm",
        "safety-benchmark"
      ],
      "id": 89
    },
    {
      "name": "Robust Gymnasium",
      "one_line_profile": "Unified benchmark for robust Reinforcement Learning",
      "detailed_description": "A unified modular benchmark designed to evaluate the robustness of Reinforcement Learning agents under various perturbations and conditions.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "robustness_evaluation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/SafeRL-Lab/Robust-Gymnasium",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "reinforcement-learning",
        "robustness",
        "benchmark"
      ],
      "id": 90
    },
    {
      "name": "Langtrace",
      "one_line_profile": "Observability and evaluation tool for LLM applications",
      "detailed_description": "An open-source observability tool based on Open Telemetry for LLM applications, providing real-time tracing, evaluations, and metrics.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "observability",
        "model_evaluation",
        "monitoring"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/Scale3-Labs/langtrace",
      "help_website": [
        "https://docs.langtrace.ai/"
      ],
      "license": "AGPL-3.0",
      "tags": [
        "llm-ops",
        "observability",
        "evaluation"
      ],
      "id": 91
    },
    {
      "name": "giskardpy",
      "one_line_profile": "Robot motion control library",
      "detailed_description": "The core Python library of the Giskard framework for constraint- and optimization-based robot motion control.",
      "domains": [
        "AI_Robotics"
      ],
      "subtask_category": [
        "motion_control",
        "robotics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/SemRoCo/giskardpy",
      "help_website": [],
      "license": "LGPL-3.0",
      "tags": [
        "robotics",
        "motion-control",
        "optimization"
      ],
      "id": 92
    },
    {
      "name": "Universal-Prompt-Injection",
      "one_line_profile": "Implementation of universal prompt injection attacks",
      "detailed_description": "Official implementation of the paper 'Automatic and Universal Prompt Injection Attacks against Large Language Models', providing methods to generate prompt injection attacks.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "prompt_injection",
        "red_teaming"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/SheltonLiu-N/Universal-Prompt-Injection",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "prompt-injection",
        "llm-security",
        "adversarial-attacks"
      ],
      "id": 93
    },
    {
      "name": "JailDAM",
      "one_line_profile": "Jailbreak detection for Vision-Language Models",
      "detailed_description": "Implementation of 'JailDAM: Jailbreak Detection with Adaptive Memory for Vision-Language Model', a method for detecting jailbreak attempts in multimodal models.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "jailbreak_detection",
        "safety_defense"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ShenzheZhu/JailDAM",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "jailbreak-detection",
        "vlm",
        "safety"
      ],
      "id": 94
    },
    {
      "name": "JudgeDeceiver",
      "one_line_profile": "Prompt injection attack against LLM-as-a-Judge",
      "detailed_description": "Implementation of an optimization-based prompt injection attack specifically targeting LLM-as-a-Judge systems.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "prompt_injection",
        "red_teaming",
        "llm_as_a_judge"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ShiJiawenwen/JudgeDeceiver",
      "help_website": [],
      "license": null,
      "tags": [
        "prompt-injection",
        "llm-judge",
        "adversarial"
      ],
      "id": 95
    },
    {
      "name": "Gorilla",
      "one_line_profile": "Training and evaluation for LLM function calling",
      "detailed_description": "A framework and model suite for training and evaluating Large Language Models on function calling (tool use) capabilities.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "function_calling",
        "model_evaluation",
        "tool_use"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/ShishirPatil/gorilla",
      "help_website": [
        "https://gorilla.cs.berkeley.edu/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "function-calling",
        "api"
      ],
      "id": 96
    },
    {
      "name": "StruQ",
      "one_line_profile": "Defense against prompt injection using structured queries",
      "detailed_description": "Official implementation of StruQ, a method for defending against prompt injection attacks by utilizing structured queries.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "prompt_injection_defense",
        "safety"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Sizhe-Chen/StruQ",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "prompt-injection",
        "defense",
        "llm-security"
      ],
      "id": 97
    },
    {
      "name": "Skywork",
      "one_line_profile": "Open-source bilingual LLM suite and evaluation tools",
      "detailed_description": "A suite of large language models pre-trained on multilingual data, including open-source models, training data, and evaluation methods.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_training",
        "model_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/SkyworkAI/Skywork",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "llm",
        "bilingual",
        "foundation-model"
      ],
      "id": 98
    },
    {
      "name": "EmoLLM",
      "one_line_profile": "Mental health LLM framework and evaluation",
      "detailed_description": "A comprehensive framework for mental health Large Language Models, covering pre-training, post-training, datasets, and evaluation.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "domain_adaptation",
        "mental_health",
        "model_evaluation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/SmartFlowAI/EmoLLM",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mental-health",
        "llm",
        "domain-specific"
      ],
      "id": 99
    },
    {
      "name": "AI-Safety_Benchmark",
      "one_line_profile": "Benchmark for guided jailbreak attacks",
      "detailed_description": "The official repository for a guided jailbreak benchmark, designed to evaluate the safety of AI models against sophisticated attacks.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "safety_benchmarking",
        "jailbreak_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/SproutNan/AI-Safety_Benchmark",
      "help_website": [],
      "license": null,
      "tags": [
        "ai-safety",
        "benchmark",
        "jailbreak"
      ],
      "id": 100
    },
    {
      "name": "Bullet-Safety-Gym",
      "one_line_profile": "Safety benchmark framework for Reinforcement Learning",
      "detailed_description": "An open-source framework based on Bullet Physics to benchmark and assess safety specifications in Reinforcement Learning problems.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "safety_evaluation",
        "simulation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/SvenGronauer/Bullet-Safety-Gym",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rl",
        "safety",
        "benchmark"
      ],
      "id": 101
    },
    {
      "name": "HPCPerfStats",
      "one_line_profile": "HPC resource-usage monitoring and analysis tool",
      "detailed_description": "An automated resource-usage monitoring and analysis package designed for High Performance Computing (HPC) clusters.",
      "domains": [
        "Scientific_Computing"
      ],
      "subtask_category": [
        "performance_monitoring",
        "hpc_analysis"
      ],
      "application_level": "platform",
      "primary_language": "C",
      "repo_url": "https://github.com/TACC/HPCPerfStats",
      "help_website": [],
      "license": "LGPL-2.1",
      "tags": [
        "hpc",
        "monitoring",
        "performance"
      ],
      "id": 102
    },
    {
      "name": "Omni-SafetyBench",
      "one_line_profile": "Safety benchmark for Audio-Visual LLMs",
      "detailed_description": "A benchmark designed for the safety evaluation of Audio-Visual Large Language Models, assessing risks across multiple modalities.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "safety_benchmarking",
        "multimodal_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/THU-BPM/Omni-SafetyBench",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "multimodal",
        "safety",
        "benchmark"
      ],
      "id": 103
    },
    {
      "name": "AgentBench",
      "one_line_profile": "Benchmark to evaluate LLMs as Agents",
      "detailed_description": "A comprehensive benchmark framework to evaluate the capabilities of Large Language Models acting as autonomous agents across various environments.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "agent_evaluation",
        "benchmarking"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/THUDM/AgentBench",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-agent",
        "benchmark",
        "evaluation"
      ],
      "id": 104
    },
    {
      "name": "FeverSymmetric",
      "one_line_profile": "Symmetric evaluation dataset for fact verification",
      "detailed_description": "A symmetric evaluation dataset based on the FEVER (fact verification) dataset, designed to test model consistency and bias.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "fact_verification",
        "dataset_creation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/TalSchuster/FeverSymmetric",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fever",
        "fact-verification",
        "dataset"
      ],
      "id": 105
    },
    {
      "name": "AI-Infra-Guard",
      "one_line_profile": "AI Red Teaming platform",
      "detailed_description": "A comprehensive, intelligent, and easy-to-use AI Red Teaming platform developed to secure AI infrastructure and models.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "red_teaming",
        "security_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/Tencent/AI-Infra-Guard",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "red-teaming",
        "ai-security",
        "platform"
      ],
      "id": 106
    },
    {
      "name": "AICGSecEval",
      "one_line_profile": "AI-generated code security evaluation benchmark",
      "detailed_description": "A repository-level benchmark for evaluating the security of code generated by AI models.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "code_security",
        "model_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/Tencent/AICGSecEval",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "code-generation",
        "security",
        "benchmark"
      ],
      "id": 107
    },
    {
      "name": "WeKnora",
      "one_line_profile": "RAG framework for document understanding",
      "detailed_description": "An LLM-powered framework for deep document understanding, semantic retrieval, and context-aware answers using the RAG paradigm.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "rag",
        "document_understanding",
        "retrieval"
      ],
      "application_level": "framework",
      "primary_language": "Go",
      "repo_url": "https://github.com/Tencent/WeKnora",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "rag",
        "llm",
        "document-analysis"
      ],
      "id": 108
    },
    {
      "name": "PedestrianDetection-NohNMS",
      "one_line_profile": "NOH-NMS algorithm for pedestrian detection",
      "detailed_description": "Implementation of 'NOH-NMS: Improving Pedestrian Detection by Nearby Objects Hallucination', improving detection performance in crowded scenes.",
      "domains": [
        "AI_Computer_Vision"
      ],
      "subtask_category": [
        "object_detection",
        "pedestrian_detection"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/TencentYoutuResearch/PedestrianDetection-NohNMS",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "computer-vision",
        "pedestrian-detection",
        "nms"
      ],
      "id": 109
    },
    {
      "name": "trust-safety-evals",
      "one_line_profile": "Reference stack for AI model trust and safety evaluation",
      "detailed_description": "A project defining a reference stack for AI model and system evaluation, including evaluations, benchmarks, and leaderboards for trust and safety.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "trust_safety",
        "model_evaluation"
      ],
      "application_level": "framework",
      "primary_language": "Makefile",
      "repo_url": "https://github.com/The-AI-Alliance/trust-safety-evals",
      "help_website": [],
      "license": null,
      "tags": [
        "ai-safety",
        "evaluation",
        "trust"
      ],
      "id": 110
    },
    {
      "name": "Ensemble-Pytorch",
      "one_line_profile": "Unified ensemble framework for PyTorch models",
      "detailed_description": "A library to implement, train, and evaluate ensemble learning methods in PyTorch, aiming to improve the performance and robustness of deep learning models.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "robustness_enhancement",
        "model_ensemble"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/TorchEnsemble-Community/Ensemble-Pytorch",
      "help_website": [
        "https://ensemble-pytorch.readthedocs.io/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "pytorch",
        "ensemble-learning",
        "robustness",
        "deep-learning"
      ],
      "id": 111
    },
    {
      "name": "TrustJudge",
      "one_line_profile": "Probabilistic evaluation framework for LLM-as-a-judge systems",
      "detailed_description": "A framework designed to reduce score-comparison and pairwise transitivity inconsistencies in Large Language Model (LLM) based evaluation systems, enhancing the reliability of automated judging.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "llm_as_a_judge"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/TrustJudge/TrustJudge",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm-evaluation",
        "reliability",
        "benchmarking"
      ],
      "id": 112
    },
    {
      "name": "Adversarial Robustness Toolbox (ART)",
      "one_line_profile": "Python library for machine learning security and robustness",
      "detailed_description": "A comprehensive library for machine learning security, providing tools for evasion, poisoning, extraction, and inference attacks, as well as defenses and robustness certification for red and blue teams.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_attack",
        "robustness_testing",
        "model_defense"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox",
      "help_website": [
        "https://adversarial-robustness-toolbox.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "adversarial-ml",
        "security",
        "robustness",
        "red-teaming"
      ],
      "id": 113
    },
    {
      "name": "TransferAttack",
      "one_line_profile": "Framework for transferable adversarial attacks",
      "detailed_description": "A PyTorch-based framework specifically designed to benchmark and boost the transferability of adversarial attacks in image classification tasks.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_attack",
        "robustness_testing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Trustworthy-AI-Group/TransferAttack",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "adversarial-transferability",
        "pytorch",
        "security"
      ],
      "id": 114
    },
    {
      "name": "vllm-safety-benchmark",
      "one_line_profile": "Safety evaluation benchmark for Vision LLMs",
      "detailed_description": "A benchmark suite for evaluating the safety of Vision Large Language Models (VLLMs), focusing on detecting vulnerabilities and unsafe outputs in multimodal contexts.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "safety_evaluation",
        "multimodal_benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/UCSC-VLAA/vllm-safety-benchmark",
      "help_website": [],
      "license": null,
      "tags": [
        "vllm",
        "safety",
        "benchmark",
        "vision-language"
      ],
      "id": 115
    },
    {
      "name": "Inspect",
      "one_line_profile": "Framework for large language model evaluations",
      "detailed_description": "An open-source framework developed by the UK AI Safety Institute for creating and running evaluations of large language models, supporting diverse metrics and safety checks.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "safety_testing"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/UKGovernmentBEIS/inspect_ai",
      "help_website": [
        "https://inspect.ai-safety-institute.org.uk/"
      ],
      "license": "MIT",
      "tags": [
        "llm-eval",
        "ai-safety",
        "benchmarking"
      ],
      "id": 116
    },
    {
      "name": "LLM-judge-reporting",
      "one_line_profile": "Statistical reporting framework for LLM-as-a-judge",
      "detailed_description": "A plug-in framework that corrects bias and computes confidence intervals for LLM-as-a-judge evaluations, including adaptive algorithms for sample allocation.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "evaluation_statistics",
        "bias_correction"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/UW-Madison-Lee-Lab/LLM-judge-reporting",
      "help_website": [],
      "license": null,
      "tags": [
        "statistics",
        "llm-evaluation",
        "confidence-intervals"
      ],
      "id": 117
    },
    {
      "name": "Pangaea-Bench",
      "one_line_profile": "Evaluation benchmark for geospatial foundation models",
      "detailed_description": "A benchmark suite designed to evaluate the robustness and performance of foundation models in geospatial and earth science tasks.",
      "domains": [
        "AI3",
        "AI3-04",
        "Earth Science"
      ],
      "subtask_category": [
        "domain_benchmarking",
        "robustness_testing"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/VMarsocci/pangaea-bench",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "geospatial",
        "foundation-models",
        "earth-science",
        "benchmark"
      ],
      "id": 118
    },
    {
      "name": "Fair Sense AI",
      "one_line_profile": "Bias detection and risk management tool for AI",
      "detailed_description": "An AI-powered tool for detecting bias and managing risks in AI systems, promoting sustainable and trustworthy AI development.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "bias_detection",
        "risk_management"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/VectorInstitute/fair-sense-ai",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "fairness",
        "bias-detection",
        "trustworthy-ai"
      ],
      "id": 119
    },
    {
      "name": "owl-eval",
      "one_line_profile": "Evaluation harness for diffusion world models",
      "detailed_description": "A TypeScript-based evaluation harness specifically designed for assessing the performance of diffusion-based world models.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "diffusion_models"
      ],
      "application_level": "framework",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/Wayfarer-Labs/owl-eval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "world-models",
        "diffusion",
        "evaluation"
      ],
      "id": 120
    },
    {
      "name": "CValues",
      "one_line_profile": "Evaluation and alignment for Chinese LLM values",
      "detailed_description": "A project focused on evaluating and aligning the values of Chinese Large Language Models, providing datasets and methodologies for value assessment.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "value_alignment",
        "model_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/X-PLUG/CValues",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "alignment",
        "chinese-llm",
        "evaluation"
      ],
      "id": 121
    },
    {
      "name": "VLBiasBench",
      "one_line_profile": "Benchmark for social biases in Large Vision-Language Models",
      "detailed_description": "A large-scale dataset and benchmark composed of synthetic images aimed at evaluating and analyzing social biases in Large Vision-Language Models (LVLMs).",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "bias_evaluation",
        "multimodal_benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/Xiangkui-Cao/VLBiasBench",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "bias",
        "lvlm",
        "synthetic-data",
        "benchmark"
      ],
      "id": 122
    },
    {
      "name": "DASH",
      "one_line_profile": "Detection and Assessment of Systematic Hallucinations of VLMs",
      "detailed_description": "A tool and framework for detecting and assessing systematic hallucinations in Vision-Language Models, helping to quantify model reliability.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "hallucination_detection",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/YanNeu/DASH",
      "help_website": [],
      "license": null,
      "tags": [
        "hallucination",
        "vlm",
        "evaluation"
      ],
      "id": 123
    },
    {
      "name": "Veridical Flow",
      "one_line_profile": "Framework for trustworthy data-science pipelines",
      "detailed_description": "A tool to facilitate building stable and trustworthy data-science pipelines based on the PCS (Predictability, Computability, Stability) framework.",
      "domains": [
        "AI3",
        "AI3-04",
        "Data Science"
      ],
      "subtask_category": [
        "pipeline_stability",
        "trustworthiness"
      ],
      "application_level": "framework",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/Yu-Group/veridical-flow",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "data-science",
        "pcs-framework",
        "stability"
      ],
      "id": 124
    },
    {
      "name": "Whisper-AT",
      "one_line_profile": "Noise-robust automatic speech recognition and audio tagging",
      "detailed_description": "A unified model and toolkit for automatic speech recognition (ASR) and audio tagging, emphasizing noise robustness and multi-task learning capabilities.",
      "domains": [
        "AI3",
        "Audio"
      ],
      "subtask_category": [
        "speech_recognition",
        "audio_tagging"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/YuanGongND/whisper-at",
      "help_website": [],
      "license": "BSD-2-Clause",
      "tags": [
        "asr",
        "audio-tagging",
        "whisper",
        "robustness"
      ],
      "id": 125
    },
    {
      "name": "ECC",
      "one_line_profile": "Affective bias-inspired measures for visual emotion recognition",
      "detailed_description": "Implementation of evaluation measures for Visual Emotion Recognition (VER) that account for affective biases, aiming to err more like humans.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "evaluation_metric",
        "emotion_recognition"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ZhaoChenxi-nku/ECC",
      "help_website": [],
      "license": null,
      "tags": [
        "emotion-recognition",
        "evaluation-metric",
        "bias"
      ],
      "id": 126
    },
    {
      "name": "TransferAttackEval",
      "one_line_profile": "Evaluation framework for transferable adversarial images",
      "detailed_description": "A codebase for revisiting and evaluating transferable adversarial images, providing tools to assess attack performance across different models.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_evaluation",
        "robustness_testing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ZhengyuZhao/TransferAttackEval",
      "help_website": [],
      "license": null,
      "tags": [
        "adversarial-attack",
        "transferability",
        "evaluation"
      ],
      "id": 127
    },
    {
      "name": "math-evaluation-harness",
      "one_line_profile": "Benchmark toolkit for LLM mathematical reasoning",
      "detailed_description": "A simple toolkit designed for benchmarking Large Language Models on various mathematical reasoning tasks.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_benchmarking",
        "math_reasoning"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/ZubinGou/math-evaluation-harness",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "math",
        "llm-benchmark",
        "reasoning"
      ],
      "id": 128
    },
    {
      "name": "ZoneEval",
      "one_line_profile": "Evaluation tool for spatial bias in object detection",
      "detailed_description": "A tool for revealing and evaluating spatial bias in object detection models, providing insights into performance variations across different image zones.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "bias_analysis",
        "object_detection_eval"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/Zzh-tju/ZoneEval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "object-detection",
        "spatial-bias",
        "evaluation"
      ],
      "id": 129
    },
    {
      "name": "ACADO Toolkit",
      "one_line_profile": "Toolkit for automatic control and dynamic optimization",
      "detailed_description": "A software environment and algorithm collection for automatic control and dynamic optimization, supporting model predictive control and parameter estimation.",
      "domains": [
        "Scientific Computing",
        "Control Theory"
      ],
      "subtask_category": [
        "optimal_control",
        "dynamic_optimization"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/acado/acado",
      "help_website": [
        "http://acado.github.io/"
      ],
      "license": "LGPL-3.0",
      "tags": [
        "optimization",
        "control-theory",
        "mpc",
        "c++"
      ],
      "id": 130
    },
    {
      "name": "AdvBox",
      "one_line_profile": "Adversarial example generation and robustness benchmark toolbox",
      "detailed_description": "A toolbox to generate adversarial examples to fool neural networks across multiple frameworks (PaddlePaddle, PyTorch, etc.) and benchmark the robustness of machine learning models.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_attack",
        "robustness_benchmarking"
      ],
      "application_level": "framework",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/advboxes/AdvBox",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "adversarial-examples",
        "robustness",
        "paddlepaddle",
        "pytorch"
      ],
      "id": 131
    },
    {
      "name": "T2ISafety",
      "one_line_profile": "Benchmark for assessing fairness, toxicity, and privacy in image generation",
      "detailed_description": "A benchmark suite for evaluating safety aspects such as fairness, toxicity, and privacy in Text-to-Image (T2I) generation models.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "safety_benchmarking",
        "image_generation_eval"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/adwardlee/t2i_safety",
      "help_website": [],
      "license": null,
      "tags": [
        "text-to-image",
        "safety",
        "fairness",
        "benchmark"
      ],
      "id": 132
    },
    {
      "name": "PromptInject",
      "one_line_profile": "Framework for adversarial prompt attacks on LLMs",
      "detailed_description": "A modular framework for assembling prompts to quantitatively analyze the robustness of Large Language Models (LLMs) to adversarial prompt injection attacks.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "prompt_injection",
        "robustness_analysis"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/agencyenterprise/PromptInject",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "prompt-injection",
        "llm-security",
        "red-teaming"
      ],
      "id": 133
    },
    {
      "name": "AgentFence",
      "one_line_profile": "Platform for AI agent security testing",
      "detailed_description": "An open-source platform for automatically testing the security of AI agents, identifying vulnerabilities like prompt injection, secret leakage, and instruction exposure.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "agent_security",
        "vulnerability_scanning"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/agentfence/agentfence",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ai-agents",
        "security-testing",
        "red-teaming"
      ],
      "id": 134
    },
    {
      "name": "judgy",
      "one_line_profile": "Confidence interval estimator for LLM-as-a-Judge metrics",
      "detailed_description": "A Python package for estimating confidence intervals for metrics evaluated by LLM-as-a-Judge systems, helping to quantify uncertainty in automated evaluations.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "evaluation_statistics",
        "uncertainty_quantification"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ai-evals-course/judgy",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "statistics",
        "llm-evaluation",
        "confidence-intervals"
      ],
      "id": 135
    },
    {
      "name": "AIMon Python SDK",
      "one_line_profile": "SDK for detecting LLM hallucinations and quality issues",
      "detailed_description": "The Python SDK for AIMon, a system for detecting Large Language Model (LLM) quality issues such as hallucinations during evaluation or continuous monitoring.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "hallucination_detection",
        "model_monitoring"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aimonlabs/aimon-python-sdk",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "hallucination",
        "llm-monitoring",
        "quality-control"
      ],
      "id": 136
    },
    {
      "name": "ABLE",
      "one_line_profile": "Ableism Bias Language Evaluation tool",
      "detailed_description": "A project to research and evaluate how AI systems reproduce discriminatory content and biases related to ableism.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "bias_evaluation",
        "ethics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aktionmensch/ABLE",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bias",
        "ableism",
        "ai-ethics"
      ],
      "id": 137
    },
    {
      "name": "safety-eval",
      "one_line_profile": "Evaluation toolkit for generative language models and safety classifiers",
      "detailed_description": "A simple evaluation framework designed to assess the safety and robustness of generative language models and safety classifiers, providing metrics for potential harms.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "safety_check"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/allenai/safety-eval",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "safety",
        "evaluation",
        "llm",
        "generative-ai"
      ],
      "id": 138
    },
    {
      "name": "last_layer",
      "one_line_profile": "Fast LLM prompt injection and jailbreak detection library",
      "detailed_description": "A lightweight and ultra-fast Python library designed to detect prompt injections and jailbreak attempts in Large Language Models (LLMs), suitable for real-time security monitoring.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "security_monitoring",
        "jailbreak_detection"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/arekusandr/last_layer",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "prompt-injection",
        "jailbreak-detection",
        "llm-security",
        "robustness"
      ],
      "id": 139
    },
    {
      "name": "OD-test",
      "one_line_profile": "Evaluation framework for Out-of-Distribution detectors",
      "detailed_description": "A library for evaluating Out-of-Distribution (OOD) detection methods in a less biased manner, providing standardized metrics and datasets for robust model assessment.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "ood_detection",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ashafaei/OD-test",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ood",
        "out-of-distribution",
        "evaluation",
        "pytorch"
      ],
      "id": 140
    },
    {
      "name": "BEIR",
      "one_line_profile": "Heterogeneous benchmark for information retrieval",
      "detailed_description": "A heterogeneous benchmark for zero-shot evaluation of information retrieval models across diverse datasets, facilitating robust assessment of retrieval systems.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "information_retrieval",
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/beir-cellar/beir",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "information-retrieval",
        "benchmark",
        "zero-shot",
        "evaluation"
      ],
      "id": 141
    },
    {
      "name": "Foolbox",
      "one_line_profile": "Adversarial attacks toolbox for neural networks",
      "detailed_description": "A Python toolbox to create adversarial examples that fool neural networks, supporting PyTorch, TensorFlow, and JAX, used for evaluating model robustness.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_attack",
        "robustness_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/bethgelab/foolbox",
      "help_website": [
        "https://foolbox.readthedocs.io"
      ],
      "license": "MIT",
      "tags": [
        "adversarial-attacks",
        "robustness",
        "pytorch",
        "tensorflow",
        "jax"
      ],
      "id": 142
    },
    {
      "name": "model-vs-human",
      "one_line_profile": "Benchmark for comparing model robustness against human perception",
      "detailed_description": "A benchmark framework to evaluate machine learning models on out-of-distribution datasets with collected human comparison data, assessing robustness and alignment with human perception.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "ood_evaluation",
        "human_comparison",
        "robustness"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/bethgelab/model-vs-human",
      "help_website": [],
      "license": null,
      "tags": [
        "benchmark",
        "ood",
        "human-alignment",
        "computer-vision"
      ],
      "id": 143
    },
    {
      "name": "BigCode Evaluation Harness",
      "one_line_profile": "Evaluation framework for code generation models",
      "detailed_description": "A framework for the evaluation of autoregressive code generation language models, supporting various coding benchmarks and metrics.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "code_generation_eval",
        "model_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/bigcode-project/bigcode-evaluation-harness",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "code-generation",
        "evaluation",
        "llm",
        "benchmark"
      ],
      "id": 144
    },
    {
      "name": "gmmreg",
      "one_line_profile": "Robust point set registration using Gaussian Mixture Models",
      "detailed_description": "C++ implementation of the robust point set registration algorithm using Gaussian Mixture Models (GMM), used for aligning point cloud data in computer vision and medical imaging.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "point_set_registration",
        "data_alignment"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/bing-jian/gmmreg",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "point-cloud",
        "registration",
        "gmm",
        "computer-vision"
      ],
      "id": 145
    },
    {
      "name": "nn_robust_attacks",
      "one_line_profile": "Robust evasion attacks implementation (Carlini & Wagner)",
      "detailed_description": "Implementation of robust evasion attacks against neural networks (including the Carlini & Wagner attack), serving as a standard baseline for evaluating model robustness.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_attack",
        "robustness_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/carlini/nn_robust_attacks",
      "help_website": [],
      "license": "BSD-2-Clause",
      "tags": [
        "adversarial-attacks",
        "robustness",
        "carlini-wagner",
        "neural-networks"
      ],
      "id": 146
    },
    {
      "name": "yet-another-applied-llm-benchmark",
      "one_line_profile": "Applied benchmark for evaluating LLM problem-solving capabilities",
      "detailed_description": "A benchmark suite designed to evaluate Large Language Models on practical, applied questions and problem-solving tasks.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_benchmarking",
        "llm_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/carlini/yet-another-applied-llm-benchmark",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "llm",
        "benchmark",
        "evaluation",
        "applied-ai"
      ],
      "id": 147
    },
    {
      "name": "HarmBench",
      "one_line_profile": "Standardized evaluation framework for automated red teaming",
      "detailed_description": "A standardized evaluation framework for automated red teaming and robust refusal, designed to assess the safety and security of Large Language Models.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "red_teaming",
        "safety_evaluation",
        "robustness"
      ],
      "application_level": "framework",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/centerforaisafety/HarmBench",
      "help_website": [
        "https://www.harmbench.org"
      ],
      "license": "MIT",
      "tags": [
        "red-teaming",
        "safety",
        "llm",
        "evaluation"
      ],
      "id": 148
    },
    {
      "name": "Simple Black-box Adversarial Attacks",
      "one_line_profile": "Implementation of simple black-box adversarial attacks for deep learning models",
      "detailed_description": "A Python implementation of the query-efficient black-box adversarial attack method proposed in the ICML 2019 paper. It allows researchers to evaluate the robustness of machine learning models against black-box attacks where gradients are unavailable.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_attack",
        "robustness_testing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/cg563/simple-blackbox-attack",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "adversarial-attacks",
        "black-box-attacks",
        "robustness",
        "deep-learning"
      ],
      "id": 149
    },
    {
      "name": "CleverHans",
      "one_line_profile": "Adversarial example library for constructing attacks and building defenses",
      "detailed_description": "A Python library to benchmark machine learning systems' vulnerability to adversarial examples. It provides standard implementations of state-of-the-art attack algorithms and defenses to facilitate robust model development.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_attack",
        "robustness_testing",
        "defense_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/cleverhans-lab/cleverhans",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "adversarial-machine-learning",
        "security",
        "robustness",
        "benchmarking"
      ],
      "id": 150
    },
    {
      "name": "Opik",
      "one_line_profile": "Platform for evaluating, testing, and monitoring LLM applications",
      "detailed_description": "An open-source platform designed to debug, evaluate, and monitor Large Language Model (LLM) applications. It provides tracing, automated evaluation metrics, and dashboards to ensure the reliability and performance of RAG systems and agents.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "monitoring",
        "tracing"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/comet-ml/opik",
      "help_website": [
        "https://www.comet.com/docs/opik/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm-evaluation",
        "observability",
        "rag",
        "monitoring"
      ],
      "id": 151
    },
    {
      "name": "DeepEval",
      "one_line_profile": "Unit testing and evaluation framework for LLMs",
      "detailed_description": "An open-source evaluation framework for Large Language Models (LLMs). It allows developers to build and run unit tests for LLM applications, measuring metrics such as hallucination, answer relevancy, and faithfulness to ensure model quality.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "unit_testing",
        "hallucination_detection"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/confident-ai/deepeval",
      "help_website": [
        "https://docs.confident-ai.com/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm-evaluation",
        "testing",
        "metrics",
        "rag"
      ],
      "id": 152
    },
    {
      "name": "DeepTeam",
      "one_line_profile": "Automated red teaming framework for LLMs",
      "detailed_description": "A framework designed to automate the red teaming process for Large Language Models. It helps identify vulnerabilities, biases, and safety issues in LLM systems through automated adversarial testing.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "red_teaming",
        "safety_testing",
        "vulnerability_scanning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/confident-ai/deepteam",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "red-teaming",
        "llm-safety",
        "adversarial-testing"
      ],
      "id": 153
    },
    {
      "name": "3DFuse",
      "one_line_profile": "Robust text-to-3D generation using 2D diffusion models",
      "detailed_description": "A framework that leverages pre-trained 2D diffusion models to generate 3D consistent assets from text prompts. It introduces a method to inject 3D consistency into 2D models, enabling robust 3D generation for scientific and creative applications.",
      "domains": [
        "AI3",
        "AI3-03"
      ],
      "subtask_category": [
        "3d_generation",
        "generative_modeling"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/cvlab-kaist/3DFuse",
      "help_website": [],
      "license": null,
      "tags": [
        "text-to-3d",
        "diffusion-models",
        "3d-generation",
        "nerf"
      ],
      "id": 154
    },
    {
      "name": "UQLM",
      "one_line_profile": "Uncertainty quantification and hallucination detection for LLMs",
      "detailed_description": "A Python package for quantifying uncertainty in Large Language Models to detect hallucinations. It provides methods to estimate the confidence of model generations, aiding in the reliability assessment of LLM outputs.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "uncertainty_quantification",
        "hallucination_detection",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/cvs-health/uqlm",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "uncertainty-quantification",
        "hallucination",
        "llm",
        "reliability"
      ],
      "id": 155
    },
    {
      "name": "Simple LLM Eval",
      "one_line_profile": "Lightweight library for LLM evaluation using LLM-as-a-Judge",
      "detailed_description": "A simple Python library that implements the 'LLM-as-a-Judge' paradigm for evaluating Large Language Model outputs. It facilitates the creation of automated evaluation pipelines using stronger models to judge weaker ones.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "llm_as_a_judge"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/cyberark/simple-llm-eval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-evaluation",
        "automation",
        "testing"
      ],
      "id": 156
    },
    {
      "name": "GNN Meta Attack",
      "one_line_profile": "Adversarial attacks on Graph Neural Networks via meta-learning",
      "detailed_description": "An implementation of adversarial attacks on Graph Neural Networks (GNNs) using meta-learning techniques. It allows researchers to evaluate the robustness of GNN models against perturbations in graph structure and node features.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_attack",
        "graph_neural_networks",
        "robustness_testing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/danielzuegner/gnn-meta-attack",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gnn",
        "adversarial-attacks",
        "meta-learning",
        "graph-mining"
      ],
      "id": 157
    },
    {
      "name": "Nettack",
      "one_line_profile": "Adversarial attacks on Neural Networks for Graph Data",
      "detailed_description": "A tool for generating adversarial attacks on Graph Neural Networks, specifically targeting node classification tasks. It helps in assessing the vulnerability of graph-based models to structural and feature perturbations.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_attack",
        "graph_neural_networks",
        "robustness_testing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/danielzuegner/nettack",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gnn",
        "adversarial-attacks",
        "graph-data",
        "security"
      ],
      "id": 158
    },
    {
      "name": "MAGSAC",
      "one_line_profile": "Robust model fitting algorithm without inlier-outlier threshold",
      "detailed_description": "An implementation of the MAGSAC (Model Agnostic Sample Consensus) algorithm for robust model estimation in computer vision. It improves upon RANSAC by eliminating the need for a manually tuned inlier-outlier threshold.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_fitting",
        "robust_estimation",
        "computer_vision"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/danini/magsac",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "ransac",
        "robust-estimation",
        "computer-vision",
        "model-fitting"
      ],
      "id": 159
    },
    {
      "name": "Bisheng",
      "one_line_profile": "Open LLM DevOps platform for application development and evaluation",
      "detailed_description": "A comprehensive platform for developing, managing, and evaluating Large Language Model applications. It includes features for RAG, agent workflows, model evaluation, and dataset management, facilitating the full lifecycle of enterprise AI apps.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "llmops",
        "rag_management"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/dataelement/bisheng",
      "help_website": [
        "https://bisheng.dataelement.com/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llmops",
        "rag",
        "evaluation",
        "agent"
      ],
      "id": 160
    },
    {
      "name": "HuMoR",
      "one_line_profile": "3D Human Motion Model for Robust Pose Estimation",
      "detailed_description": "A library implementing a generative model for 3D human motion, used for robust pose estimation and motion generation. It enables the recovery of plausible 3D human motion from noisy or partial observations.",
      "domains": [
        "AI3",
        "AI3-03"
      ],
      "subtask_category": [
        "pose_estimation",
        "motion_modeling",
        "3d_reconstruction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/davrempe/humor",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "human-motion",
        "pose-estimation",
        "generative-model",
        "3d-vision"
      ],
      "id": 161
    },
    {
      "name": "WEFE",
      "one_line_profile": "Word Embeddings Fairness Evaluation Framework",
      "detailed_description": "A framework for measuring and mitigating bias in word embedding models. It standardizes various fairness metrics to evaluate gender, racial, and other biases in pre-trained embeddings.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "bias_evaluation",
        "fairness_metrics",
        "embedding_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/dccuchile/wefe",
      "help_website": [
        "https://wefe.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "fairness",
        "bias-detection",
        "word-embeddings",
        "nlp"
      ],
      "id": 162
    },
    {
      "name": "Vigil LLM",
      "one_line_profile": "Security scanner for detecting prompt injections and jailbreaks in LLMs",
      "detailed_description": "A security tool designed to detect and mitigate risks in Large Language Model inputs, such as prompt injections and jailbreak attempts. It acts as a firewall or scanner for LLM applications.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "prompt_injection_detection",
        "jailbreak_detection",
        "security_scanning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/deadbits/vigil-llm",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-security",
        "prompt-injection",
        "jailbreak",
        "defense"
      ],
      "id": 163
    },
    {
      "name": "DebiAI",
      "one_line_profile": "Bias detection and contextual evaluation tool for AI projects",
      "detailed_description": "A tool for visualizing and analyzing AI model performance to detect biases and errors. It allows users to explore model results in context, identify underperforming subsets of data, and ensure model fairness.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "bias_detection",
        "model_evaluation",
        "error_analysis"
      ],
      "application_level": "platform",
      "primary_language": "Vue",
      "repo_url": "https://github.com/debiai/DebiAI",
      "help_website": [
        "https://debiai.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "bias-detection",
        "visualization",
        "evaluation",
        "fairness"
      ],
      "id": 164
    },
    {
      "name": "HaloScope",
      "one_line_profile": "Hallucination detection using unlabeled LLM generations",
      "detailed_description": "A tool implementing the HaloScope method for detecting hallucinations in Large Language Models. It leverages unlabeled generations to identify inconsistencies and factual errors without requiring extensive labeled datasets.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "hallucination_detection",
        "model_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/deeplearning-wisc/haloscope",
      "help_website": [],
      "license": null,
      "tags": [
        "hallucination",
        "llm",
        "evaluation",
        "consistency"
      ],
      "id": 165
    },
    {
      "name": "LLM Prompt Injection Filtering",
      "one_line_profile": "Safety filter for LLM inputs using LLM-based classification",
      "detailed_description": "A Python tool that uses a secondary LLM call to evaluate user inputs for safety, specifically filtering out prompt injection attacks and dangerous queries before they reach the main model.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "input_filtering",
        "safety_guardrails",
        "prompt_injection_defense"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/derwiki/llm-prompt-injection-filtering",
      "help_website": [],
      "license": null,
      "tags": [
        "prompt-injection",
        "safety",
        "filtering",
        "llm"
      ],
      "id": 166
    },
    {
      "name": "PHUDGE",
      "one_line_profile": "Scalable LLM-as-a-Judge framework using Phi-3",
      "detailed_description": "A framework for evaluating LLMs using the Phi-3 model as a scalable judge. It supports custom rubrics, reference-based and reference-free evaluation, and hallucination detection.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "llm_as_a_judge",
        "grading"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/deshwalmahesh/PHUDGE",
      "help_website": [],
      "license": null,
      "tags": [
        "llm-evaluation",
        "judge",
        "phi-3",
        "grading"
      ],
      "id": 167
    },
    {
      "name": "Ollama Grid Search",
      "one_line_profile": "Desktop application for evaluating and comparing local LLMs",
      "detailed_description": "A cross-platform desktop tool designed to perform grid search evaluations on local LLMs (via Ollama). It allows users to compare model outputs across different parameters and prompts to assess performance.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "parameter_tuning",
        "comparison"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/dezoito/ollama-grid-search",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm-evaluation",
        "ollama",
        "grid-search",
        "local-llm"
      ],
      "id": 168
    },
    {
      "name": "Diffusion Classifier",
      "one_line_profile": "Zero-shot classification using pretrained diffusion models",
      "detailed_description": "A method and library for performing zero-shot image classification by leveraging the density estimates of pretrained diffusion models, without requiring additional training or fine-tuning.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "zero_shot_classification",
        "model_inference",
        "diffusion_models"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/diffusion-classifier/diffusion-classifier",
      "help_website": [],
      "license": null,
      "tags": [
        "diffusion-models",
        "classification",
        "zero-shot",
        "inference"
      ],
      "id": 169
    },
    {
      "name": "Docling SDG",
      "one_line_profile": "Synthetic data generation from documents for AI training",
      "detailed_description": "A toolkit for generating synthetic training data from document sources. It helps in creating labeled datasets for training or evaluating AI models when real data is scarce or sensitive.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "synthetic_data_generation",
        "data_augmentation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/docling-project/docling-sdg",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "synthetic-data",
        "document-processing",
        "data-generation"
      ],
      "id": 170
    },
    {
      "name": "AI Eval System",
      "one_line_profile": "Frontend UI system for OpenCompass model evaluation",
      "detailed_description": "A web-based evaluation system built on top of OpenCompass, providing a user interface for configuring and running AI model evaluations. It simplifies the process of benchmarking models for researchers.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking_platform"
      ],
      "application_level": "platform",
      "primary_language": "Vue",
      "repo_url": "https://github.com/domonic18/ai-eval-system",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "evaluation-ui",
        "opencompass",
        "benchmarking"
      ],
      "id": 171
    },
    {
      "name": "Non-Targeted Adversarial Attacks",
      "one_line_profile": "Implementation of momentum-based non-targeted adversarial attacks",
      "detailed_description": "Code implementation for the NIPS 2017 competition winning method on non-targeted adversarial attacks. It provides algorithms to generate adversarial examples that mislead models into incorrect classifications.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_attack",
        "robustness_testing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/dongyp13/Non-Targeted-Adversarial-Attacks",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "adversarial-attacks",
        "deep-learning",
        "robustness"
      ],
      "id": 172
    },
    {
      "name": "Targeted Adversarial Attack",
      "one_line_profile": "Implementation of momentum-based targeted adversarial attacks",
      "detailed_description": "Code implementation for the NIPS 2017 competition winning method on targeted adversarial attacks. It enables the generation of perturbations that force a model to classify an input as a specific target class.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_attack",
        "robustness_testing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/dongyp13/Targeted-Adversarial-Attack",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "adversarial-attacks",
        "targeted-attack",
        "robustness"
      ],
      "id": 173
    },
    {
      "name": "Translation-Invariant Attacks",
      "one_line_profile": "Translation-invariant adversarial attack method for transferability",
      "detailed_description": "Implementation of the translation-invariant attack method designed to improve the transferability of adversarial examples across different models. It is useful for black-box robustness testing.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_attack",
        "transferability",
        "robustness_testing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/dongyp13/Translation-Invariant-Attacks",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "adversarial-attacks",
        "transferability",
        "robustness"
      ],
      "id": 174
    },
    {
      "name": "SafeDialBench",
      "one_line_profile": "Multi-turn dialogue benchmark for evaluating LLM safety",
      "detailed_description": "A comprehensive multi-turn dialogue benchmark dataset and evaluation framework designed to assess the safety of Large Language Models across various risk categories.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "safety_evaluation",
        "benchmark_dataset"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/drivetosouth/SafeDialBench-Dataset",
      "help_website": [],
      "license": null,
      "tags": [
        "llm-safety",
        "benchmark",
        "dialogue-systems"
      ],
      "id": 175
    },
    {
      "name": "FormatBiasEval",
      "one_line_profile": "Evaluation framework for output format bias in LLMs",
      "detailed_description": "Official implementation for evaluating and mitigating output format bias in Large Language Models, providing metrics to quantify how format constraints affect model reasoning and performance.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "bias_evaluation",
        "model_robustness"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/dxlong2000/FormatBiasEval",
      "help_website": [],
      "license": null,
      "tags": [
        "llm-bias",
        "format-bias",
        "evaluation-metric"
      ],
      "id": 176
    },
    {
      "name": "ibicus",
      "one_line_profile": "Bias correction toolkit for climate models",
      "detailed_description": "A flexible Python toolkit for the bias correction of climate models and associated evaluation, implementing various state-of-the-art methods for meteorological data adjustment.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "bias_correction",
        "climate_modeling",
        "evaluation"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/ecmwf-projects/ibicus",
      "help_website": [
        "https://ibicus.readthedocs.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "climate-science",
        "bias-correction",
        "meteorology"
      ],
      "id": 177
    },
    {
      "name": "STEGOSAURUS-WRECKS",
      "one_line_profile": "Steganographic prompt injection tool for AI red teaming",
      "detailed_description": "A tool for automatically encoding images with steganographic payloads to act as prompt injections or jailbreaks for multimodal AI systems with code interpreter and vision capabilities.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "red_teaming",
        "adversarial_attack",
        "prompt_injection"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/elder-plinius/STEGOSAURUS-WRECKS",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "steganography",
        "prompt-injection",
        "red-teaming",
        "multimodal-security"
      ],
      "id": 178
    },
    {
      "name": "acceptance-bench",
      "one_line_profile": "LLM evaluation framework for acceptance vs refusal behavior",
      "detailed_description": "A robust evaluation framework for measuring Large Language Model acceptance versus refusal rates across difficulty levels, featuring multi-prompt variation testing and LLM-as-judge evaluation mechanisms.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "safety_evaluation",
        "refusal_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ellydee/acceptance-bench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm-evaluation",
        "safety-alignment",
        "benchmarking"
      ],
      "id": 179
    },
    {
      "name": "RL2Grid",
      "one_line_profile": "Reinforcement learning benchmark for power grid operations",
      "detailed_description": "A standardized benchmark for reinforcement learning agents in realistic power grid environments, modeling real-time operations, topology optimization, and safety-critical constraints.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "rl_benchmark",
        "power_grid_simulation",
        "safety_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/emarche/RL2Grid",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "reinforcement-learning",
        "power-grid",
        "benchmark"
      ],
      "id": 180
    },
    {
      "name": "ChatProtect",
      "one_line_profile": "Hallucination detection and mitigation for LLMs",
      "detailed_description": "Implementation of methods for evaluating, detecting, and mitigating self-contradictory hallucinations in Large Language Models, focusing on improving model reliability.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "hallucination_detection",
        "model_evaluation",
        "mitigation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/eth-sri/ChatProtect",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "hallucination",
        "llm-reliability",
        "evaluation"
      ],
      "id": 181
    },
    {
      "name": "diffai",
      "one_line_profile": "Certifiable defense against adversarial examples",
      "detailed_description": "A system for training neural networks to be provably robust against adversarial examples, providing certifiable defenses for deep learning models.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_defense",
        "robustness_verification",
        "model_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/eth-sri/diffai",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "adversarial-robustness",
        "formal-verification",
        "deep-learning"
      ],
      "id": 182
    },
    {
      "name": "EvalPlus",
      "one_line_profile": "Rigorous evaluation framework for LLM-synthesized code",
      "detailed_description": "A framework for rigorously evaluating code generation models (like HumanEval+) by generating large amounts of test cases to ensure functional correctness and robustness.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "code_evaluation",
        "model_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/evalplus/evalplus",
      "help_website": [
        "https://evalplus.github.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "code-generation",
        "evaluation",
        "llm"
      ],
      "id": 183
    },
    {
      "name": "Evidently",
      "one_line_profile": "ML and LLM observability and evaluation framework",
      "detailed_description": "An open-source framework to evaluate, test, and monitor ML models and LLMs, providing metrics for data drift, model performance, and data quality analysis.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_monitoring",
        "data_drift_analysis",
        "evaluation"
      ],
      "application_level": "platform",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/evidentlyai/evidently",
      "help_website": [
        "https://docs.evidentlyai.com"
      ],
      "license": "Apache-2.0",
      "tags": [
        "observability",
        "ml-monitoring",
        "data-drift"
      ],
      "id": 184
    },
    {
      "name": "decoding-biases",
      "one_line_profile": "Bias evaluation scripts for NLG models",
      "detailed_description": "A collection of scripts and tools to evaluate various bias metrics for Natural Language Generation models across different decoding algorithms.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "bias_evaluation",
        "nlg_assessment"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ewsheng/decoding-biases",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bias-metrics",
        "nlg",
        "evaluation"
      ],
      "id": 185
    },
    {
      "name": "Meta SecAlign",
      "one_line_profile": "Secure foundation LLM against prompt injection",
      "detailed_description": "Implementation of Meta SecAlign, a method to create secure foundation LLMs that are robust against prompt injection attacks through safety alignment techniques.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "safety_alignment",
        "prompt_injection_defense"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/Meta_SecAlign",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "llm-safety",
        "prompt-injection",
        "alignment"
      ],
      "id": 186
    },
    {
      "name": "SecAlign",
      "one_line_profile": "Preference optimization for prompt injection defense",
      "detailed_description": "Code for the research paper 'SecAlign', providing methods to defend against prompt injection attacks using preference optimization techniques.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "safety_defense",
        "preference_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/SecAlign",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "prompt-injection",
        "defense",
        "llm"
      ],
      "id": 187
    },
    {
      "name": "rl-injector",
      "one_line_profile": "RL-based prompt injection attack generation",
      "detailed_description": "A reinforcement learning approach to generating stronger prompt injection attacks for evaluating the robustness of Large Language Models.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "red_teaming",
        "adversarial_attack",
        "rl_generation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/rl-injector",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "red-teaming",
        "reinforcement-learning",
        "prompt-injection"
      ],
      "id": 188
    },
    {
      "name": "unibench",
      "one_line_profile": "Robustness evaluation library for VLM models",
      "detailed_description": "A Python library designed to evaluate the robustness of Vision-Language Models (VLMs) across diverse benchmarks and multimodal tasks.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "vlm_evaluation",
        "robustness_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/facebookresearch/unibench",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "vlm",
        "robustness",
        "benchmarking"
      ],
      "id": 189
    },
    {
      "name": "Bank Account Fraud Dataset",
      "one_line_profile": "Datasets for evaluating ML fairness and robustness in fraud detection",
      "detailed_description": "A suite of biased, imbalanced, and dynamic tabular datasets designed for evaluating machine learning models in fraud detection scenarios, specifically focusing on fairness and robustness metrics.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "fairness_evaluation",
        "dataset_benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/feedzai/bank-account-fraud",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "ml-fairness",
        "fraud-detection",
        "evaluation-dataset"
      ],
      "id": 190
    },
    {
      "name": "NewsWCL50",
      "one_line_profile": "Evaluation dataset for media bias identification",
      "detailed_description": "An open-access evaluation dataset for methods to identify bias by word choice and labeling in news media, facilitating the development of bias detection algorithms.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "bias_detection",
        "dataset_benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": null,
      "repo_url": "https://github.com/fhamborg/NewsWCL50",
      "help_website": [],
      "license": "CC-BY-SA-4.0",
      "tags": [
        "media-bias",
        "nlp-dataset",
        "evaluation"
      ],
      "id": 191
    },
    {
      "name": "Fiddler Auditor",
      "one_line_profile": "Tool for evaluating language model robustness and bias",
      "detailed_description": "An open-source tool designed to evaluate Large Language Models for robustness, bias, and correctness through perturbation testing and other evaluation techniques.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "robustness_testing",
        "bias_detection"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/fiddler-labs/fiddler-auditor",
      "help_website": [
        "https://docs.fiddler.ai"
      ],
      "license": "NOASSERTION",
      "tags": [
        "llm-evaluation",
        "robustness",
        "red-teaming"
      ],
      "id": 192
    },
    {
      "name": "Video-SafetyBench",
      "one_line_profile": "Benchmark for safety evaluation of Video LVLMs",
      "detailed_description": "A benchmark suite specifically designed for evaluating the safety of Video Large Vision-Language Models (LVLMs), covering various safety dimensions and risk categories.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "safety_benchmarking",
        "video_llm_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/flageval-baai/Video-SafetyBench",
      "help_website": [],
      "license": null,
      "tags": [
        "video-llm",
        "safety-benchmark",
        "evaluation"
      ],
      "id": 193
    },
    {
      "name": "AutoAttack",
      "one_line_profile": "Ensemble of parameter-free attacks for robustness evaluation",
      "detailed_description": "A reliable evaluation framework for adversarial robustness using an ensemble of diverse parameter-free attacks (APGD, etc.) to test deep learning models.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_attack",
        "robustness_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/fra31/auto-attack",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "adversarial-robustness",
        "deep-learning",
        "attack-ensemble"
      ],
      "id": 194
    },
    {
      "name": "gender-bias",
      "one_line_profile": "Library for detecting gender bias in text",
      "detailed_description": "A Python library designed to detect and measure gender bias in natural language text, useful for evaluating NLP datasets and model outputs.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "bias_detection",
        "text_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/gender-bias/gender-bias",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gender-bias",
        "nlp",
        "fairness"
      ],
      "id": 195
    },
    {
      "name": "TOG",
      "one_line_profile": "Adversarial objectness gradient attacks for object detection",
      "detailed_description": "Implementation of TOG (Targeted Adversarial Objectness Gradient) attacks, a suite of adversarial attacks designed to deceive object detection systems (vanishing, fabrication, mislabeling).",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_attack",
        "object_detection_robustness"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/git-disl/TOG",
      "help_website": [],
      "license": null,
      "tags": [
        "adversarial-attack",
        "object-detection",
        "computer-vision"
      ],
      "id": 196
    },
    {
      "name": "camel-prompt-injection",
      "one_line_profile": "Code for defeating prompt injections by design",
      "detailed_description": "Research code and implementation for the paper 'Defeating Prompt Injections by Design', providing methods and evaluations for securing LLMs against injection attacks.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "prompt_injection_defense",
        "safety_research"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/google-research/camel-prompt-injection",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "prompt-injection",
        "llm-security",
        "defense"
      ],
      "id": 197
    },
    {
      "name": "lapeigvals",
      "one_line_profile": "Hallucination detection using spectral features of attention maps",
      "detailed_description": "Implementation of a method to detect hallucinations in Large Language Models by analyzing the spectral features (Laplacian eigenvalues) of their attention maps.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "hallucination_detection",
        "model_interpretability"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/graphml-lab-pwr/lapeigvals",
      "help_website": [],
      "license": null,
      "tags": [
        "hallucination",
        "spectral-analysis",
        "attention-maps"
      ],
      "id": 198
    },
    {
      "name": "PromptCARE",
      "one_line_profile": "Prompt copyright protection via watermark injection",
      "detailed_description": "A tool for protecting the copyright of prompts in LLMs by injecting and verifying watermarks, ensuring intellectual property protection for prompt engineering.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "copyright_protection",
        "watermarking",
        "model_security"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/grasses/PromptCARE",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "watermarking",
        "prompt-protection",
        "security"
      ],
      "id": 199
    },
    {
      "name": "MM-Eval",
      "one_line_profile": "Multilingual meta-evaluation benchmark for LLM-as-a-Judge and reward models",
      "detailed_description": "A benchmark designed to evaluate the performance of LLM-as-a-Judge systems and reward models across multiple languages, focusing on meta-evaluation metrics.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "meta_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/guijinSON/MM-Eval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-as-a-judge",
        "multilingual",
        "benchmark"
      ],
      "id": 200
    },
    {
      "name": "Verdict",
      "one_line_profile": "Inference-time scaling library for LLM-as-a-judge systems",
      "detailed_description": "A library that implements inference-time scaling techniques to improve the accuracy and reliability of Large Language Models when used as evaluators (LLM-as-a-judge).",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "inference_optimization"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/haizelabs/verdict",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm-as-a-judge",
        "scaling",
        "evaluation"
      ],
      "id": 201
    },
    {
      "name": "DDDM-VC",
      "one_line_profile": "Decoupled Denoising Diffusion Models for robust voice conversion",
      "detailed_description": "Official implementation of DDDM-VC, a voice conversion model using decoupled denoising diffusion models with disentangled representation and prior mixup.",
      "domains": [
        "AI2",
        "AI3"
      ],
      "subtask_category": [
        "voice_conversion",
        "generative_modeling"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/hayeong0/DDDM-VC",
      "help_website": [],
      "license": null,
      "tags": [
        "diffusion-models",
        "voice-conversion",
        "audio-processing"
      ],
      "id": 202
    },
    {
      "name": "Adversarial Explainable AI",
      "one_line_profile": "Library for adversarial attacks on Explainable AI (XAI) methods",
      "detailed_description": "A collection of implementations for performing adversarial attacks on model explanations and methods to defend against them, facilitating robustness research in XAI.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_attack",
        "explainability_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/hbaniecki/adversarial-explainable-ai",
      "help_website": [],
      "license": "CC-BY-SA-4.0",
      "tags": [
        "xai",
        "adversarial-attacks",
        "robustness"
      ],
      "id": 203
    },
    {
      "name": "HalluciDet",
      "one_line_profile": "Hallucination-based RGB modality generation for person detection",
      "detailed_description": "Implementation of a method that hallucinates RGB modality from other inputs to improve person detection, using privileged information during training.",
      "domains": [
        "AI2",
        "Computer Vision"
      ],
      "subtask_category": [
        "object_detection",
        "modality_hallucination"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/heitorrapela/HalluciDet",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "computer-vision",
        "person-detection",
        "multimodal"
      ],
      "id": 204
    },
    {
      "name": "EvalView",
      "one_line_profile": "Pytest-style test harness for AI agent evaluation",
      "detailed_description": "A testing framework for AI agents that supports YAML scenarios, tool-call checks, cost/latency tracking, and safety evaluations in a CI-friendly format.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "agent_evaluation",
        "test_harness"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/hidai25/eval-view",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ai-agents",
        "evaluation",
        "testing-framework"
      ],
      "id": 205
    },
    {
      "name": "HolisticAI",
      "one_line_profile": "Library for assessing and improving AI system trustworthiness",
      "detailed_description": "An open-source tool to measure and mitigate risks in AI systems, focusing on bias, efficacy, robustness, and explainability.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "trustworthiness_assessment",
        "bias_detection"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/holistic-ai/holisticai",
      "help_website": [
        "https://holisticai.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "ai-safety",
        "bias",
        "fairness"
      ],
      "id": 206
    },
    {
      "name": "Circular Bias Detection",
      "one_line_profile": "Statistical framework for detecting circular reasoning bias in AI evaluation",
      "detailed_description": "A comprehensive statistical framework designed to detect circular reasoning bias in the evaluation of AI algorithms.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "evaluation_bias",
        "statistical_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/hongping-zh/circular-bias-detection",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bias-detection",
        "evaluation",
        "statistics"
      ],
      "id": 207
    },
    {
      "name": "TrustworthyAI",
      "one_line_profile": "Collection of projects and tools for Trustworthy AI research",
      "detailed_description": "A repository containing various tools and libraries developed by Huawei Noah's Ark Lab for research in trustworthy AI, including robustness, explainability, and fairness.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "trustworthy_ai",
        "robustness",
        "explainability"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/huawei-noah/trustworthyAI",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "trustworthy-ai",
        "robustness",
        "causality"
      ],
      "id": 208
    },
    {
      "name": "Alignment Handbook",
      "one_line_profile": "Recipes and tools for aligning language models with human preferences",
      "detailed_description": "A library providing robust recipes and scripts for aligning Large Language Models (LLMs) using techniques like DPO and RLHF.",
      "domains": [
        "AI3",
        "AI3-03"
      ],
      "subtask_category": [
        "model_alignment",
        "fine_tuning"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/huggingface/alignment-handbook",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "alignment",
        "rlhf",
        "dpo"
      ],
      "id": 209
    },
    {
      "name": "LightEval",
      "one_line_profile": "Comprehensive toolkit for evaluating LLMs across multiple backends",
      "detailed_description": "An all-in-one evaluation toolkit designed to assess Large Language Models using various metrics and benchmarks, supporting multiple inference backends.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/huggingface/lighteval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm-evaluation",
        "benchmarking",
        "huggingface"
      ],
      "id": 210
    },
    {
      "name": "Garak Analyzer & Mitigator",
      "one_line_profile": "Tool for analyzing Garak reports and generating mitigations",
      "detailed_description": "A utility to analyze adversarial reports generated by Garak, visualize attack attempts, detect triggers, and generate system prompt mitigations.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "red_teaming_analysis",
        "mitigation_generation"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/huseyingulsin/Garak-Analyzer-Mitigator",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "red-teaming",
        "security",
        "llm-safety"
      ],
      "id": 211
    },
    {
      "name": "ChainForge",
      "one_line_profile": "Visual programming environment for prompt engineering and evaluation",
      "detailed_description": "An open-source visual environment for battle-testing, evaluating, and comparing prompts and responses from Large Language Models.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "prompt_evaluation",
        "model_comparison"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/ianarawjo/ChainForge",
      "help_website": [
        "https://chainforge.ai/"
      ],
      "license": "MIT",
      "tags": [
        "prompt-engineering",
        "evaluation",
        "visualization"
      ],
      "id": 212
    },
    {
      "name": "JudgeIt",
      "one_line_profile": "Automation framework for LLM-as-a-judge evaluation",
      "detailed_description": "A framework using LLM-as-a-judge to evaluate Agentic AI, RAG systems, and Text2SQL applications at scale.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "llm-as-a-judge"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/ibm-self-serve-assets/JudgeIt-LLM-as-a-Judge",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "evaluation",
        "rag",
        "agentic-ai"
      ],
      "id": 213
    },
    {
      "name": "PPTAgent",
      "one_line_profile": "Agent for generating and evaluating presentation slides",
      "detailed_description": "A system for generating presentations from text and evaluating them, moving beyond simple text-to-slide generation.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "content_generation",
        "evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/icip-cas/PPTAgent",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "agent",
        "presentation-generation",
        "evaluation"
      ],
      "id": 214
    },
    {
      "name": "Hallucination Detection",
      "one_line_profile": "Tool for detecting hallucinations in BART summarization models",
      "detailed_description": "A tool to automatically detect hallucinations in BART summarization models by analyzing attention maps and decoding probabilities.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "hallucination_detection",
        "model_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/idiap/hallucination-detection",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nlp",
        "hallucination",
        "summarization"
      ],
      "id": 215
    },
    {
      "name": "Innodata LLM Safety",
      "one_line_profile": "Benchmarking suite for LLM safety and factuality",
      "detailed_description": "A benchmarking tool for evaluating LLMs (Llama, Mistral, Gemma, GPT) on factuality, toxicity, bias, and hallucination propensity.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "safety_benchmarking",
        "model_evaluation"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/innodatalabs/innodata-llm-safety",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-safety",
        "benchmarking",
        "toxicity"
      ],
      "id": 216
    },
    {
      "name": "BIS",
      "one_line_profile": "Benchmark on Interactive Safety for AI agents",
      "detailed_description": "A benchmark designed to evaluate the interactive safety of AI systems.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "safety_benchmarking",
        "interactive_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/intelligent-control-lab/BIS",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "safety",
        "benchmark",
        "interactive-ai"
      ],
      "id": 217
    },
    {
      "name": "Earthquake Detection",
      "one_line_profile": "Deep metric learning algorithm for detecting dynamically triggered earthquakes",
      "detailed_description": "Code for automating the detection of dynamically triggered earthquakes using a deep metric learning algorithm.",
      "domains": [
        "Earth Science",
        "Geophysics"
      ],
      "subtask_category": [
        "earthquake_detection",
        "signal_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/interactiveaudiolab/earthquakes",
      "help_website": [],
      "license": null,
      "tags": [
        "seismology",
        "deep-learning",
        "detection"
      ],
      "id": 218
    },
    {
      "name": "SAC3",
      "one_line_profile": "Semantic-aware cross-check consistency for hallucination detection",
      "detailed_description": "Implementation of SAC3, a method for reliable hallucination detection in black-box language models via semantic-aware cross-check consistency.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "hallucination_detection",
        "model_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/intuit/sac3",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "hallucination",
        "consistency-check",
        "llm"
      ],
      "id": 219
    },
    {
      "name": "ReadabilityMetrics",
      "one_line_profile": "Library for computing text readability metrics",
      "detailed_description": "A tool that computes various readability metrics (ARI, Coleman-Liau, Flesch-Kincaid, etc.) for text analysis.",
      "domains": [
        "AI2",
        "NLP"
      ],
      "subtask_category": [
        "text_analysis",
        "metric_calculation"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/ipeirotis/ReadabilityMetrics",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "nlp",
        "readability",
        "text-analysis"
      ],
      "id": 220
    },
    {
      "name": "DALL-Eval",
      "one_line_profile": "Benchmark for reasoning skills and social biases in text-to-image models",
      "detailed_description": "A toolkit for probing the reasoning skills and social biases of text-to-image generation models.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "bias_detection"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/j-min/DallEval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "text-to-image",
        "bias",
        "evaluation"
      ],
      "id": 221
    },
    {
      "name": "LLM Warden",
      "one_line_profile": "Jailbreak detection tool for safeguarding LLMs",
      "detailed_description": "A tool designed to detect and prevent jailbreak attempts in Large Language Models.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "jailbreak_detection",
        "safety_guardrail"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/jackhhao/llm-warden",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm-safety",
        "jailbreak",
        "security"
      ],
      "id": 222
    },
    {
      "name": "Adversarial Library",
      "one_line_profile": "PyTorch library for adversarial attacks",
      "detailed_description": "A library containing PyTorch implementations of various adversarial attacks for evaluating model robustness.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_attack",
        "robustness_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/jeromerony/adversarial-library",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "adversarial-attacks",
        "pytorch",
        "robustness"
      ],
      "id": 223
    },
    {
      "name": "Fast Adversarial",
      "one_line_profile": "Efficient gradient-based L2 adversarial attacks and defenses",
      "detailed_description": "Implementation of decoupled direction and norm methods for efficient gradient-based L2 adversarial attacks and defenses.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_attack",
        "defense_mechanism"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/jeromerony/fast_adversarial",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "adversarial-attacks",
        "optimization",
        "robustness"
      ],
      "id": 224
    },
    {
      "name": "ModelNet40-C",
      "one_line_profile": "Benchmark for 3D point cloud recognition robustness",
      "detailed_description": "A benchmark dataset and codebase for evaluating the robustness of 3D point cloud recognition models against common corruptions.",
      "domains": [
        "AI3",
        "AI3-04",
        "Computer Vision"
      ],
      "subtask_category": [
        "robustness_benchmarking",
        "3d_vision"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/jiachens/ModelNet40-C",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "3d-vision",
        "robustness",
        "point-cloud"
      ],
      "id": 225
    },
    {
      "name": "runcharter",
      "one_line_profile": "Automated run chart analysis and visualization tool",
      "detailed_description": "An R package designed to automate the creation and analysis of run charts for faceted data displays across multiple metrics or locations, facilitating statistical process control and data visualization.",
      "domains": [
        "Scientific Visualization",
        "Statistics"
      ],
      "subtask_category": [
        "data_visualization",
        "statistical_analysis"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/johnmackintosh/runcharter",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "r-package",
        "visualization",
        "run-chart",
        "statistics"
      ],
      "id": 226
    },
    {
      "name": "adv_attack_capsnet",
      "one_line_profile": "Adversarial attack implementation for Capsule Networks",
      "detailed_description": "A TensorFlow implementation of adversarial attacks specifically targeting Capsule Networks, serving as a tool for evaluating the robustness of this specific neural network architecture.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_attack",
        "robustness_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/jsikyoon/adv_attack_capsnet",
      "help_website": [],
      "license": null,
      "tags": [
        "adversarial-attacks",
        "capsule-networks",
        "robustness"
      ],
      "id": 227
    },
    {
      "name": "Defense-GAN",
      "one_line_profile": "Generative model-based defense against adversarial attacks",
      "detailed_description": "An implementation of Defense-GAN, a mechanism that uses generative adversarial networks to project input images onto the range of the generator to purify adversarial perturbations before classification.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "model_defense",
        "robustness_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/kabkabm/defensegan",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "gan",
        "adversarial-defense",
        "security"
      ],
      "id": 228
    },
    {
      "name": "VSDFLOW",
      "one_line_profile": "Automated RTL-to-GDSII flow for semiconductor design",
      "detailed_description": "An automated open-source hardware design flow that converts Verilog RTL designs into GDSII layouts, integrating synthesis, placement, routing, and timing analysis tools for semiconductor engineering.",
      "domains": [
        "Electronic Design Automation",
        "Hardware Engineering"
      ],
      "subtask_category": [
        "circuit_synthesis",
        "layout_generation"
      ],
      "application_level": "workflow",
      "primary_language": "Verilog",
      "repo_url": "https://github.com/kunalg123/vsdflow",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "eda",
        "vlsi",
        "rtl-to-gds",
        "verilog"
      ],
      "id": 229
    },
    {
      "name": "limited-blackbox-attacks",
      "one_line_profile": "Black-box adversarial attacks with limited queries",
      "detailed_description": "A research codebase implementing black-box adversarial attack methods that operate under restricted query budgets and partial information, used for evaluating model robustness in realistic threat models.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_attack",
        "robustness_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/labsix/limited-blackbox-attacks",
      "help_website": [],
      "license": null,
      "tags": [
        "black-box-attacks",
        "adversarial-ml",
        "robustness"
      ],
      "id": 230
    },
    {
      "name": "pint-benchmark",
      "one_line_profile": "Benchmark for prompt injection detection systems",
      "detailed_description": "A benchmark suite designed to evaluate the effectiveness of prompt injection detection systems in Large Language Models, providing a standardized dataset and evaluation metrics for AI safety research.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "safety_evaluation",
        "prompt_injection_benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/lakeraai/pint-benchmark",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "prompt-injection",
        "benchmark",
        "llm-safety"
      ],
      "id": 231
    },
    {
      "name": "chembench",
      "one_line_profile": "Chemistry evaluation benchmark for LLMs",
      "detailed_description": "A framework and dataset for evaluating the capabilities of Large Language Models in solving chemistry-related tasks, assessing their scientific reasoning and domain knowledge.",
      "domains": [
        "AI3-04",
        "Chemistry"
      ],
      "subtask_category": [
        "domain_evaluation",
        "scientific_reasoning"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/lamalab-org/chembench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "chemistry",
        "llm-evaluation",
        "benchmark"
      ],
      "id": 232
    },
    {
      "name": "Langfuse",
      "one_line_profile": "LLM engineering platform for observability and evaluation",
      "detailed_description": "An open-source platform providing observability, metrics, and evaluation pipelines for Large Language Models, enabling researchers and engineers to trace model execution, manage datasets, and run systematic evaluations.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "observability",
        "metrics_tracking"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/langfuse/langfuse",
      "help_website": [
        "https://langfuse.com/docs"
      ],
      "license": "NOASSERTION",
      "tags": [
        "llm-ops",
        "evaluation",
        "observability"
      ],
      "id": 233
    },
    {
      "name": "LangWatch",
      "one_line_profile": "LLM Ops platform for analytics and evaluation",
      "detailed_description": "A platform for monitoring and evaluating LLM applications, offering tools for trace analysis, dataset management, and prompt optimization to ensure model quality and safety.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "analytics"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/langwatch/langwatch",
      "help_website": [
        "https://docs.langwatch.ai"
      ],
      "license": "NOASSERTION",
      "tags": [
        "llm-ops",
        "analytics",
        "evaluation"
      ],
      "id": 234
    },
    {
      "name": "Latitude",
      "one_line_profile": "Prompt engineering and evaluation platform",
      "detailed_description": "An open-source platform focused on the development, evaluation, and refinement of prompts for LLMs, providing tools to systematically test and improve model responses.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "prompt_evaluation",
        "prompt_engineering"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/latitude-dev/latitude-llm",
      "help_website": [],
      "license": "LGPL-3.0",
      "tags": [
        "prompt-engineering",
        "evaluation",
        "llm"
      ],
      "id": 235
    },
    {
      "name": "robotic_world_model",
      "one_line_profile": "Neural network simulator for robotics policy optimization",
      "detailed_description": "A neural network-based simulator designed to model robotic environments, facilitating robust policy optimization and reinforcement learning research in robotics.",
      "domains": [
        "Robotics",
        "AI3"
      ],
      "subtask_category": [
        "simulation",
        "policy_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/leggedrobotics/robotic_world_model",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "robotics",
        "simulation",
        "reinforcement-learning"
      ],
      "id": 236
    },
    {
      "name": "PIGuard",
      "one_line_profile": "Prompt injection guardrail via mitigating overdefense",
      "detailed_description": "An implementation of a defense mechanism against prompt injection attacks that specifically addresses the issue of overdefense, serving as a tool for enhancing LLM safety.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "safety_defense",
        "prompt_injection_mitigation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/leolee99/PIGuard",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "prompt-injection",
        "guardrails",
        "llm-safety"
      ],
      "id": 237
    },
    {
      "name": "Guided-Denoise",
      "one_line_profile": "Denoising-based defense against adversarial attacks",
      "detailed_description": "The winning submission for the NIPS 2017 Defense Against Adversarial Attack challenge, providing a guided denoising method to protect classifiers from adversarial perturbations.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "model_defense",
        "image_denoising"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/lfz/Guided-Denoise",
      "help_website": [],
      "license": null,
      "tags": [
        "adversarial-defense",
        "denoising",
        "nips-2017"
      ],
      "id": 238
    },
    {
      "name": "Open-Prompt-Injection",
      "one_line_profile": "Benchmark for prompt injection attacks and defenses",
      "detailed_description": "A comprehensive benchmark repository for evaluating Large Language Models against various prompt injection attacks and assessing the effectiveness of defense mechanisms.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "safety_benchmarking",
        "prompt_injection"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/liu00222/Open-Prompt-Injection",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "prompt-injection",
        "benchmark",
        "llm-security"
      ],
      "id": 239
    },
    {
      "name": "DSRL",
      "one_line_profile": "Datasets and environment wrappers for safe reinforcement learning",
      "detailed_description": "A collection of datasets and environment wrappers specifically designed for offline safe reinforcement learning research, facilitating the development and evaluation of safe RL algorithms.",
      "domains": [
        "AI3",
        "Robotics"
      ],
      "subtask_category": [
        "dataset_provision",
        "environment_wrapping"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/liuzuxin/DSRL",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "safe-rl",
        "offline-rl",
        "datasets"
      ],
      "id": 240
    },
    {
      "name": "RouteLLM",
      "one_line_profile": "Framework for serving and evaluating LLM routers",
      "detailed_description": "A framework designed to evaluate and deploy Large Language Model routers, enabling efficient model selection and cost optimization without compromising generation quality.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "model_routing",
        "efficiency_evaluation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/lm-sys/RouteLLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-routing",
        "evaluation",
        "optimization"
      ],
      "id": 241
    },
    {
      "name": "convex_adversarial",
      "one_line_profile": "Provably robust neural network training method",
      "detailed_description": "A library and method for training neural networks that are provably robust to adversarial attacks, utilizing convex relaxation techniques to certify robustness bounds.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "robust_training",
        "verification"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/locuslab/convex_adversarial",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "robustness",
        "adversarial-training",
        "verification"
      ],
      "id": 242
    },
    {
      "name": "Square Attack",
      "one_line_profile": "Query-efficient black-box adversarial attack algorithm for model robustness evaluation",
      "detailed_description": "A Python implementation of Square Attack, a score-based black-box adversarial attack that does not rely on local gradient information. It is used to evaluate the robustness of machine learning models (particularly image classifiers) against adversarial perturbations in a query-efficient manner.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_attack",
        "robustness_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/max-andr/square-attack",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "adversarial-attacks",
        "black-box-attack",
        "robustness"
      ],
      "id": 243
    },
    {
      "name": "Video-ChatGPT",
      "one_line_profile": "Video conversation model and quantitative evaluation benchmarking framework",
      "detailed_description": "A framework combining a video conversation model with a rigorous quantitative evaluation benchmarking suite. It enables the assessment of video-based conversational models using LLMs to generate meaningful conversations about videos and evaluate performance across various metrics.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "multimodal_benchmarking"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/mbzuai-oryx/Video-ChatGPT",
      "help_website": [],
      "license": "CC-BY-4.0",
      "tags": [
        "video-llm",
        "benchmarking",
        "multimodal"
      ],
      "id": 244
    },
    {
      "name": "Agent-as-a-Judge",
      "one_line_profile": "Framework for evaluating open-ended agentic tasks using agents as judges",
      "detailed_description": "A framework designed to evaluate AI agents in open-ended scenarios by employing other agents as judges. It addresses the challenges of assessing agent performance in complex, non-deterministic environments, providing a methodology for scalable and automated evaluation.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "agent_evaluation",
        "automated_benchmarking"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/metauto-ai/agent-as-a-judge",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm-as-a-judge",
        "agent-evaluation",
        "open-endedness"
      ],
      "id": 245
    },
    {
      "name": "BIPIA",
      "one_line_profile": "Benchmark for Indirect Prompt Injection Attacks on LLMs",
      "detailed_description": "A benchmark suite for evaluating the robustness of Large Language Models (LLMs) and their defenses against indirect prompt injection attacks. It provides a standardized dataset and evaluation methodology to assess security risks in LLM-integrated applications.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "security_benchmarking",
        "prompt_injection",
        "robustness_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/BIPIA",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "prompt-injection",
        "llm-security",
        "benchmark"
      ],
      "id": 246
    },
    {
      "name": "CoNLI",
      "one_line_profile": "Framework for ungrounded hallucination detection and reduction in LLMs",
      "detailed_description": "A plug-and-play framework designed to detect and reduce ungrounded hallucinations in Large Language Models. It utilizes Natural Language Inference (NLI) techniques to verify the consistency of generated text against source knowledge, improving model reliability.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "hallucination_detection",
        "model_alignment"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/CoNLI_hallucination",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "hallucination",
        "nli",
        "fact-checking"
      ],
      "id": 247
    },
    {
      "name": "HaDes",
      "one_line_profile": "Token-level reference-free hallucination detection for LLMs",
      "detailed_description": "A tool for detecting hallucinations in Large Language Model outputs at the token level without requiring reference texts. It focuses on identifying self-contradictions and logical inconsistencies within the generated content to assess model faithfulness.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "hallucination_detection",
        "quality_control"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/HaDes",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "hallucination-detection",
        "token-level",
        "llm-evaluation"
      ],
      "id": 248
    },
    {
      "name": "PromptBench",
      "one_line_profile": "Unified evaluation framework for Large Language Models",
      "detailed_description": "A comprehensive framework for evaluating Large Language Models across various dimensions, including adversarial robustness, prompt sensitivity, and task performance. It supports multiple datasets, models, and attack methods to facilitate systematic LLM assessment.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "robustness_benchmarking",
        "prompt_engineering"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/promptbench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm-evaluation",
        "adversarial-robustness",
        "benchmark"
      ],
      "id": 249
    },
    {
      "name": "Prompty",
      "one_line_profile": "Tool for managing, debugging, and evaluating LLM prompts",
      "detailed_description": "An asset class and tooling format designed to enhance the observability, understandability, and portability of LLM prompts. It facilitates the development lifecycle of prompts, including creation, management, and evaluation within AI applications.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "prompt_engineering",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/prompty",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "prompt-management",
        "observability",
        "llm-ops"
      ],
      "id": 250
    },
    {
      "name": "RobustDG",
      "one_line_profile": "Toolkit for domain generalization and robust machine learning",
      "detailed_description": "A toolkit for building machine learning models that generalize well to unseen domains and are robust against privacy attacks and other perturbations. It provides implementations of various domain generalization algorithms and evaluation metrics.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "domain_generalization",
        "robustness_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/robustdg",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "domain-generalization",
        "robustness",
        "privacy"
      ],
      "id": 251
    },
    {
      "name": "FortisAVQA",
      "one_line_profile": "Robustness evaluation and bias mitigation for Audio-Visual Question Answering",
      "detailed_description": "A framework for evaluating the robustness of Audio-Visual Question Answering (AVQA) models and mitigating biases. It includes datasets and model implementations to analyze performance under various conditions and improve model reliability.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "bias_mitigation",
        "multimodal_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/mira-ai-lab/fortisavqa",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "avqa",
        "robustness",
        "bias-mitigation"
      ],
      "id": 252
    },
    {
      "name": "MAITE",
      "one_line_profile": "Modular AI Trustworthy Engineering library for test and evaluation",
      "detailed_description": "A library providing common types, protocols, and utilities to support AI Test and Evaluation (T&E) workflows. It aims to standardize and facilitate the engineering of trustworthy AI systems through modular components.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "test_and_evaluation",
        "trustworthy_ai"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/mit-ll-ai-technology/maite",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "test-and-evaluation",
        "trustworthy-ai",
        "protocols"
      ],
      "id": 253
    },
    {
      "name": "ModelBench",
      "one_line_profile": "Safety benchmark runner for AI models",
      "detailed_description": "A tool developed by MLCommons to run safety benchmarks against AI models and generate detailed performance reports. It standardizes the evaluation of model safety across different dimensions and test sets.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "safety_benchmarking",
        "model_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/mlcommons/modelbench",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "safety",
        "benchmark",
        "mlcommons"
      ],
      "id": 254
    },
    {
      "name": "WiSE-FT",
      "one_line_profile": "Robust fine-tuning method for zero-shot models",
      "detailed_description": "An implementation of the WiSE-FT (Weight-Space Ensembles for Fine-Tuning) method, which improves the robustness of fine-tuned zero-shot models. It enables models to maintain high accuracy on distribution shifts while adapting to new tasks.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "robust_fine_tuning",
        "domain_adaptation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/mlfoundations/wise-ft",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "fine-tuning",
        "robustness",
        "zero-shot"
      ],
      "id": 255
    },
    {
      "name": "EvalScope",
      "one_line_profile": "Streamlined framework for LLM/VLM evaluation and benchmarking",
      "detailed_description": "A customizable framework for the efficient evaluation and performance benchmarking of Large Language Models (LLMs), Vision Language Models (VLMs), and AIGC models. It supports various datasets and metrics to assess model capabilities.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/modelscope/evalscope",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-evaluation",
        "benchmark",
        "modelscope"
      ],
      "id": 256
    },
    {
      "name": "DPoser-X",
      "one_line_profile": "Diffusion model-based robust 3D whole-body human pose prior",
      "detailed_description": "A tool implementing a diffusion model as a robust prior for 3D whole-body human pose estimation. It addresses challenges in pose estimation robustness and provides a method for generating realistic human poses.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "pose_estimation",
        "robust_modeling"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/moonbow721/DPoser-X",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "3d-pose",
        "diffusion-model",
        "robustness"
      ],
      "id": 257
    },
    {
      "name": "Agentic Security",
      "one_line_profile": "Vulnerability scanner and red teaming kit for Agentic LLMs",
      "detailed_description": "A security tool designed to scan for vulnerabilities in Agentic LLM systems. It serves as an AI red teaming kit, helping researchers and developers identify security flaws and robustness issues in autonomous agent deployments.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "red_teaming",
        "vulnerability_scanning"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/msoedov/agentic_security",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "red-teaming",
        "llm-security",
        "agent-evaluation"
      ],
      "id": 258
    },
    {
      "name": "Disrupting Deepfakes",
      "one_line_profile": "Adversarial attacks on conditional image translation networks for deepfake defense",
      "detailed_description": "An implementation of adversarial attack methods designed to disrupt deepfake generation models (conditional image translation networks). It serves as a defensive tool to protect images from being used to generate deepfakes.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_defense",
        "deepfake_detection"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/natanielruiz/disrupting-deepfakes",
      "help_website": [],
      "license": null,
      "tags": [
        "deepfake-defense",
        "adversarial-attack",
        "image-translation"
      ],
      "id": 259
    },
    {
      "name": "Korean Safety Benchmarks",
      "one_line_profile": "Safety benchmarks (SQuARe and KoSBi) for Korean LLMs",
      "detailed_description": "A repository containing datasets and PyTorch implementations for SQuARe and KoSBi, which are safety benchmarks specifically designed for evaluating Korean Large Language Models. It addresses social bias and safety concerns in a non-English context.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "safety_benchmarking",
        "bias_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/naver-ai/korean-safety-benchmarks",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "korean-llm",
        "safety-benchmark",
        "bias"
      ],
      "id": 260
    },
    {
      "name": "TD-MPC2",
      "one_line_profile": "Scalable and robust world models for continuous control",
      "detailed_description": "An implementation of TD-MPC2, a model-based reinforcement learning algorithm that learns scalable and robust world models for continuous control tasks. It is used for scientific modeling in robotics and control systems.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "robust_control",
        "world_modeling"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/nicklashansen/tdmpc2",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "world-models",
        "reinforcement-learning",
        "robust-control"
      ],
      "id": 261
    },
    {
      "name": "MoNA-Bench",
      "one_line_profile": "Benchmark for monocular depth estimation in UAV autonomous navigation",
      "detailed_description": "A benchmark suite designed for evaluating monocular depth estimation models in the context of unmanned aircraft autonomous navigation, focusing on obstacle avoidance and target tracking safety.",
      "domains": [
        "AI3-04",
        "Robotics"
      ],
      "subtask_category": [
        "model_evaluation",
        "safety_benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "C++",
      "repo_url": "https://github.com/npu-ius-lab/MoNA-Bench",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "depth-estimation",
        "uav",
        "benchmark",
        "safety"
      ],
      "id": 262
    },
    {
      "name": "Oak AI Auto Eval Tools",
      "one_line_profile": "LLM-as-a-judge evaluation tools for educational resources",
      "detailed_description": "A set of tools developed by Oak National Academy to perform automated evaluation of lesson plans and educational resources using Large Language Models (LLM-as-a-judge).",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "llm_as_a_judge"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/oaknational/oak-ai-autoeval-tools",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm-eval",
        "education",
        "automated-evaluation"
      ],
      "id": 263
    },
    {
      "name": "Hallucination Probes",
      "one_line_profile": "Real-time detection of hallucinated entities in long-form generation",
      "detailed_description": "A tool for detecting hallucinations in Large Language Model generations, specifically focusing on entity-level errors in long-form text using probing techniques.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "hallucination_detection",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/obalcells/hallucination_probes",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "hallucination",
        "llm",
        "probing"
      ],
      "id": 264
    },
    {
      "name": "LLM Proteomics Hallucination",
      "one_line_profile": "Hallucination risk evaluation for LLMs in clinical proteomics",
      "detailed_description": "A systematic evaluation framework and benchmark for detecting hallucinations in Large Language Models when interpreting clinical proteomics and mass spectrometry data.",
      "domains": [
        "AI3-04",
        "Bioinformatics"
      ],
      "subtask_category": [
        "hallucination_detection",
        "domain_specific_eval"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/olaflaitinen/llm-proteomics-hallucination",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "proteomics",
        "hallucination",
        "clinical-ai"
      ],
      "id": 265
    },
    {
      "name": "Meme-Safety-Bench",
      "one_line_profile": "Benchmark for evaluating safety of Vision-Language Models on memes",
      "detailed_description": "A benchmark study and dataset for assessing the safety and robustness of Vision-Language Models (VLMs) when processing meme-based content in the wild.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "safety_benchmarking",
        "multimodal_eval"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/oneonlee/Meme-Safety-Bench",
      "help_website": [],
      "license": null,
      "tags": [
        "vlm",
        "safety",
        "benchmark",
        "memes"
      ],
      "id": 266
    },
    {
      "name": "GenAIEval",
      "one_line_profile": "OPEA evaluation framework for Generative AI performance and safety",
      "detailed_description": "A comprehensive evaluation toolkit from the Open Platform for Enterprise AI (OPEA) project, covering throughput, latency, accuracy, safety, and hallucination metrics for GenAI models.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "performance_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/opea-project/GenAIEval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "genai",
        "evaluation",
        "opea",
        "safety"
      ],
      "id": 267
    },
    {
      "name": "CompassJudger",
      "one_line_profile": "All-in-one Judge Models for LLM evaluation",
      "detailed_description": "A collection of specialized 'Judge Models' developed by OpenCompass to perform automated evaluation of other Large Language Models, facilitating scalable and consistent assessment.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "llm_as_a_judge",
        "model_evaluation"
      ],
      "application_level": "solver",
      "primary_language": null,
      "repo_url": "https://github.com/open-compass/CompassJudger",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "judge-model",
        "opencompass",
        "llm-eval"
      ],
      "id": 268
    },
    {
      "name": "VLMEvalKit",
      "one_line_profile": "Evaluation toolkit for Large Multi-modality Models",
      "detailed_description": "An open-source toolkit for evaluating Large Multi-modality Models (LMMs), supporting over 200 models and 80 benchmarks, enabling comprehensive assessment of vision-language capabilities.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "multimodal_eval",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/open-compass/VLMEvalKit",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "vlm",
        "lmm",
        "evaluation",
        "benchmark"
      ],
      "id": 269
    },
    {
      "name": "OpenCompass",
      "one_line_profile": "Comprehensive LLM evaluation platform",
      "detailed_description": "A unified platform for evaluating Large Language Models across a wide range of datasets and capabilities, supporting distributed evaluation and various model architectures.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/open-compass/opencompass",
      "help_website": [
        "https://opencompass.org.cn/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm-eval",
        "benchmark",
        "platform"
      ],
      "id": 270
    },
    {
      "name": "OpenAI Evals",
      "one_line_profile": "Framework for evaluating LLMs and benchmark registry",
      "detailed_description": "A framework for evaluating Large Language Models and LLM systems, providing an open-source registry of benchmarks to test model capabilities and prevent regressions.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/openai/evals",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "openai",
        "evaluation",
        "benchmark"
      ],
      "id": 271
    },
    {
      "name": "Safety Starter Agents",
      "one_line_profile": "Constrained RL agents for benchmarking safe exploration",
      "detailed_description": "A collection of basic constrained Reinforcement Learning agents designed to serve as baselines for benchmarking safe exploration algorithms in deep RL.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "safety_benchmarking",
        "reinforcement_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/openai/safety-starter-agents",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rl",
        "safety",
        "safe-exploration"
      ],
      "id": 272
    },
    {
      "name": "OpenLIT",
      "one_line_profile": "OpenTelemetry-native LLM observability and evaluation platform",
      "detailed_description": "An open-source platform for AI engineering that provides OpenTelemetry-native observability, guardrails, and evaluation capabilities for LLMs and GPUs.",
      "domains": [
        "AI3-04",
        "AI3"
      ],
      "subtask_category": [
        "model_monitoring",
        "model_evaluation",
        "guardrails"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/openlit/openlit",
      "help_website": [
        "https://docs.openlit.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "observability",
        "opentelemetry",
        "llm-eval",
        "guardrails"
      ],
      "id": 273
    },
    {
      "name": "Oumi",
      "one_line_profile": "Platform to fine-tune, evaluate and deploy open source LLMs",
      "detailed_description": "A comprehensive toolchain for fine-tuning, evaluating, and deploying open-source Large Language Models and Vision Language Models, simplifying the lifecycle of AI model development.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_training",
        "model_evaluation",
        "deployment"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/oumi-ai/oumi",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "fine-tuning",
        "evaluation",
        "llm",
        "vlm"
      ],
      "id": 274
    },
    {
      "name": "AdvHat",
      "one_line_profile": "Real-world adversarial attack implementation on Face ID systems",
      "detailed_description": "A tool implementing real-world adversarial attacks against ArcFace Face ID systems using a sticker on a hat, useful for testing the robustness of face recognition models.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_attack",
        "robustness_testing"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/papermsucode/advhat",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "adversarial-attack",
        "face-recognition",
        "robustness"
      ],
      "id": 275
    },
    {
      "name": "Project Mantis",
      "one_line_profile": "Prompt injection tool for defense against LLM-driven cyberattacks",
      "detailed_description": "A framework exploring 'hacking back' AI hackers by using prompt injection as a defensive mechanism against LLM-driven cyberattacks, serving as a red-teaming tool.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "prompt_injection",
        "red_teaming",
        "defense"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/pasquini-dario/project_mantis",
      "help_website": [],
      "license": null,
      "tags": [
        "prompt-injection",
        "security",
        "llm"
      ],
      "id": 276
    },
    {
      "name": "VideoHallucer",
      "one_line_profile": "Benchmark for hallucination detection in Large Video-Language Models",
      "detailed_description": "A comprehensive benchmark dataset and evaluation tool designed to detect and assess hallucinations in Large Video-Language Models (LVLMs).",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "hallucination_detection",
        "multimodal_eval"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/patrick-tssn/VideoHallucer",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "video-llm",
        "hallucination",
        "benchmark"
      ],
      "id": 277
    },
    {
      "name": "Deck of Many Prompts",
      "one_line_profile": "Manual prompt injection and red teaming tool",
      "detailed_description": "A collection of prompts and a tool designed for manual red teaming and prompt injection testing against Large Language Models to identify security vulnerabilities.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "red_teaming",
        "prompt_injection"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/peluche/deck-of-many-prompts",
      "help_website": [],
      "license": null,
      "tags": [
        "red-teaming",
        "prompt-injection",
        "security"
      ],
      "id": 278
    },
    {
      "name": "Deep Mahalanobis Detector",
      "one_line_profile": "Framework for detecting out-of-distribution samples and adversarial attacks",
      "detailed_description": "A unified framework implementing the Mahalanobis distance-based method for detecting out-of-distribution (OOD) samples and adversarial attacks in deep neural networks.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "ood_detection",
        "adversarial_defense"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pokaxpoka/deep_Mahalanobis_detector",
      "help_website": [],
      "license": null,
      "tags": [
        "ood-detection",
        "adversarial-defense",
        "robustness"
      ],
      "id": 279
    },
    {
      "name": "SelfCheckGPT",
      "one_line_profile": "Zero-resource black-box hallucination detection for LLMs",
      "detailed_description": "A tool for detecting hallucinations in Generative Large Language Models using a zero-resource, black-box approach that checks the consistency of sampled responses.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "hallucination_detection",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/potsawee/selfcheckgpt",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "hallucination",
        "llm",
        "consistency-check"
      ],
      "id": 280
    },
    {
      "name": "Adversarial Face Attack",
      "one_line_profile": "Black-box adversarial attack tool for face recognition systems",
      "detailed_description": "A tool implementing black-box adversarial attacks against public face recognition systems, useful for evaluating the robustness of biometric security models.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_attack",
        "robustness_testing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ppwwyyxx/Adversarial-Face-Attack",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "adversarial-attack",
        "face-recognition",
        "black-box"
      ],
      "id": 281
    },
    {
      "name": "SecML Malware",
      "one_line_profile": "Adversarial attacks against malware detectors",
      "detailed_description": "A Python library for creating and testing adversarial attacks against machine learning-based Windows malware detectors, evaluating their robustness.",
      "domains": [
        "AI3-04",
        "Cybersecurity"
      ],
      "subtask_category": [
        "adversarial_attack",
        "robustness_testing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pralab/secml_malware",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "adversarial-ml",
        "malware-detection",
        "security"
      ],
      "id": 282
    },
    {
      "name": "Prometheus Eval",
      "one_line_profile": "Evaluate LLM responses with Prometheus and GPT-4",
      "detailed_description": "A tool for evaluating Large Language Model responses using the Prometheus model or GPT-4, providing a structured way to assess generation quality.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "llm_as_a_judge"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/prometheus-eval/prometheus-eval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-eval",
        "prometheus",
        "judge-model"
      ],
      "id": 283
    },
    {
      "name": "Promptfoo",
      "one_line_profile": "CLI tool for testing, red teaming, and evaluating LLMs",
      "detailed_description": "A CLI tool and library for testing prompts, agents, and RAG systems, supporting red teaming, vulnerability scanning, and performance comparison across multiple LLM providers.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "red_teaming",
        "model_evaluation",
        "prompt_testing"
      ],
      "application_level": "solver",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/promptfoo/promptfoo",
      "help_website": [
        "https://www.promptfoo.dev/"
      ],
      "license": "MIT",
      "tags": [
        "red-teaming",
        "prompt-engineering",
        "evaluation"
      ],
      "id": 284
    },
    {
      "name": "Rebuff",
      "one_line_profile": "LLM Prompt Injection Detector",
      "detailed_description": "A security tool designed to detect and prevent prompt injection attacks in Large Language Model applications, enhancing the safety and robustness of AI systems.",
      "domains": [
        "AI3-04"
      ],
      "subtask_category": [
        "prompt_injection",
        "defense",
        "security"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/protectai/rebuff",
      "help_website": [
        "https://rebuff.ai/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "prompt-injection",
        "security",
        "defense"
      ],
      "id": 285
    },
    {
      "name": "RagaAI-Catalyst",
      "one_line_profile": "Comprehensive framework for Agent AI observability, monitoring, and evaluation",
      "detailed_description": "A Python SDK designed for the evaluation and debugging of Agentic AI systems. It provides features for tracing agents, LLMs, and tools, debugging multi-agent systems, and visualizing execution graphs. It supports the assessment of reliability and performance in AI applications.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "observability",
        "debugging"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/raga-ai-hub/RagaAI-Catalyst",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-evaluation",
        "agent-observability",
        "debugging"
      ],
      "id": 286
    },
    {
      "name": "texture-vs-shape",
      "one_line_profile": "Evaluation artifacts for analyzing texture vs. shape bias in CNNs",
      "detailed_description": "Provides pre-trained models, datasets, and code to evaluate the inductive biases of Convolutional Neural Networks (CNNs), specifically measuring the trade-off between texture bias and shape bias. This tool helps in understanding model robustness and interpretability.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "robustness_analysis",
        "interpretability"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/rgeirhos/texture-vs-shape",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "cnn-bias",
        "robustness",
        "computer-vision"
      ],
      "id": 287
    },
    {
      "name": "auto-evaluator",
      "one_line_profile": "Automated evaluation tool for LLM QA chains",
      "detailed_description": "A lightweight tool designed to evaluate the performance of Question Answering (QA) chains powered by Large Language Models. It automates the process of judging response quality, facilitating rapid iteration and testing of RAG (Retrieval-Augmented Generation) systems.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "qa_testing"
      ],
      "application_level": "tool",
      "primary_language": "Python",
      "repo_url": "https://github.com/rlancemartin/auto-evaluator",
      "help_website": [],
      "license": null,
      "tags": [
        "llm-eval",
        "rag",
        "qa-evaluation"
      ],
      "id": 288
    },
    {
      "name": "winogender-schemas",
      "one_line_profile": "Benchmark dataset for evaluating gender bias in coreference resolution",
      "detailed_description": "A collection of Winograd-schema-style sentences designed to test for the presence of gender bias in automated coreference resolution systems. It serves as a diagnostic tool for evaluating the fairness and robustness of NLP models.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "bias_evaluation",
        "fairness_testing"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/rudinger/winogender-schemas",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gender-bias",
        "nlp-evaluation",
        "coreference-resolution"
      ],
      "id": 289
    },
    {
      "name": "PsiloQA",
      "one_line_profile": "Pipeline for constructing hallucination detection datasets",
      "detailed_description": "Automates the creation of multilingual, span-level hallucination detection datasets with context. This pipeline aids in the evaluation of Large Language Models by generating data to test their faithfulness and detect hallucinations.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "dataset_generation",
        "hallucination_detection"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/s-nlp/PsiloQA",
      "help_website": [],
      "license": null,
      "tags": [
        "hallucination",
        "dataset-construction",
        "llm-evaluation"
      ],
      "id": 290
    },
    {
      "name": "AuditNLG",
      "one_line_profile": "Library for auditing trustworthiness in Natural Language Generation",
      "detailed_description": "A Python library developed by Salesforce for auditing and evaluating the trustworthiness of Generative AI language models. It provides metrics and utilities to assess safety, bias, and quality in NLG outputs.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_auditing",
        "trustworthiness_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/salesforce/AuditNLG",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "nlg-evaluation",
        "trustworthy-ai",
        "auditing"
      ],
      "id": 291
    },
    {
      "name": "YESciEval",
      "one_line_profile": "Robust LLM-as-a-Judge for Scientific Question Answering",
      "detailed_description": "An evaluation framework designed to assess the performance of Large Language Models in scientific question answering tasks. It employs a robust 'LLM-as-a-Judge' methodology to provide reliable metrics for scientific reasoning capabilities.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "scientific_qa"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/sciknoworg/YESciEval",
      "help_website": [
        "https://pypi.org/project/YESciEval/"
      ],
      "license": "MIT",
      "tags": [
        "llm-as-a-judge",
        "scientific-qa",
        "evaluation"
      ],
      "id": 292
    },
    {
      "name": "frai",
      "one_line_profile": "Open-source toolkit for responsible AI governance and evaluation",
      "detailed_description": "A CLI and SDK toolkit designed to facilitate responsible AI practices. It includes features to scan code, collect evidence, generate model cards, manage risk files, and run evaluations, supporting the governance and documentation of AI systems.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "responsible_ai",
        "model_governance",
        "evaluation"
      ],
      "application_level": "tool",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/sebuzdugan/frai",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "responsible-ai",
        "model-cards",
        "governance"
      ],
      "id": 293
    },
    {
      "name": "ST-WebAgentBench",
      "one_line_profile": "Benchmark for evaluating safety and trustworthiness in web agents",
      "detailed_description": "A benchmark suite specifically designed to evaluate the safety and trustworthiness of AI agents operating in web environments, focusing on enterprise scenarios. It helps identify risks and validate agent behavior.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "agent_evaluation",
        "safety_benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/segev-shlomov/ST-WebAgentBench",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "web-agents",
        "safety-benchmark",
        "trustworthiness"
      ],
      "id": 294
    },
    {
      "name": "robust-physical-attack",
      "one_line_profile": "Physical adversarial attack tool for object detectors",
      "detailed_description": "Implements physical adversarial attacks designed to fool object detection models like Faster R-CNN. This tool is used for evaluating the robustness of computer vision models against real-world physical perturbations.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_attack",
        "robustness_testing"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/shangtse/robust-physical-attack",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "adversarial-attack",
        "object-detection",
        "robustness"
      ],
      "id": 295
    },
    {
      "name": "prompt-injection",
      "one_line_profile": "Assessment tool for prompt injection risks in GPTs",
      "detailed_description": "A repository containing code and data to assess and demonstrate prompt injection risks in user-designed GPTs. It serves as a resource for red teaming and evaluating the security of LLM applications.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "red_teaming",
        "security_evaluation"
      ],
      "application_level": "tool",
      "primary_language": null,
      "repo_url": "https://github.com/sherdencooper/prompt-injection",
      "help_website": [],
      "license": null,
      "tags": [
        "prompt-injection",
        "llm-security",
        "red-teaming"
      ],
      "id": 296
    },
    {
      "name": "ad_examples",
      "one_line_profile": "Collection of anomaly detection methods and adversarial attacks",
      "detailed_description": "A comprehensive library implementing various anomaly detection algorithms (point-based, graph, time series) and adversarial attacks (e.g., on Graph Convolutional Networks). It serves as a toolkit for researching and evaluating anomaly detection robustness.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "anomaly_detection",
        "adversarial_attack",
        "algorithm_library"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/shubhomoydas/ad_examples",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "anomaly-detection",
        "adversarial-learning",
        "graph-neural-networks"
      ],
      "id": 297
    },
    {
      "name": "llm-security-prompt-injection",
      "one_line_profile": "Framework for detecting malicious prompts in LLMs",
      "detailed_description": "Investigates Large Language Model security by implementing binary classification of input prompts to detect malicious prompt injection attacks. It includes approaches using classical ML and fine-tuned LLMs for security evaluation.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "security_evaluation",
        "prompt_injection_detection"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/sinanw/llm-security-prompt-injection",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm-security",
        "prompt-injection",
        "classification"
      ],
      "id": 298
    },
    {
      "name": "local-llm-judge",
      "one_line_profile": "Tool for using local LLMs as search relevance judges",
      "detailed_description": "A utility that enables the use of locally hosted Large Language Models to evaluate search relevance. It facilitates the 'LLM-as-a-Judge' pattern for offline or privacy-preserving evaluation workflows.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "search_relevance"
      ],
      "application_level": "tool",
      "primary_language": "Python",
      "repo_url": "https://github.com/softwaredoug/local-llm-judge",
      "help_website": [],
      "license": null,
      "tags": [
        "llm-as-a-judge",
        "search-evaluation",
        "local-llm"
      ],
      "id": 299
    },
    {
      "name": "AIR-Bench 2024",
      "one_line_profile": "Safety benchmark aligning with AI regulations and policies",
      "detailed_description": "A safety evaluation benchmark designed to align with emerging government regulations and corporate policies for AI. It provides a standardized way to assess the compliance and safety risks of foundation models.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "safety_benchmarking",
        "compliance_testing"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/stanford-crfm/air-bench-2024",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ai-safety",
        "benchmark",
        "regulation"
      ],
      "id": 300
    },
    {
      "name": "HELM",
      "one_line_profile": "Holistic Evaluation of Language Models framework",
      "detailed_description": "A comprehensive framework developed by Stanford CRFM for the holistic, reproducible, and transparent evaluation of foundation models. It covers a wide range of metrics including accuracy, robustness, fairness, and bias across diverse scenarios.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/stanford-crfm/helm",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-evaluation",
        "foundation-models",
        "benchmarking"
      ],
      "id": 301
    },
    {
      "name": "PhiSat-2 Trustworthy AI",
      "one_line_profile": "Trustworthy AI pipeline for onboard satellite Earth Observation data processing",
      "detailed_description": "A toolchain for deploying trustworthy AI on satellite hardware, featuring quantization (PyTorch to ONNX to INT8), calibration, and telemetry for Earth Observation tasks.",
      "domains": [
        "AI3",
        "AI3-04",
        "Earth Science"
      ],
      "subtask_category": [
        "model_deployment",
        "trustworthiness_calibration",
        "onboard_inference"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/sylvesterkaczmarek/phisat2-trustworthy-onboard-ai",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "satellite-ai",
        "edge-computing",
        "trustworthy-ai",
        "earth-observation"
      ],
      "id": 302
    },
    {
      "name": "AutoTrust",
      "one_line_profile": "Benchmark for assessing trustworthiness of Vision-Language Models in autonomous driving",
      "detailed_description": "A benchmark suite designed to evaluate the trustworthiness of DriveVLMs across critical safety dimensions, ensuring reliable operation in autonomous driving scenarios.",
      "domains": [
        "AI3",
        "AI3-04",
        "Autonomous Systems"
      ],
      "subtask_category": [
        "safety_benchmarking",
        "trustworthiness_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/taco-group/AutoTrust",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "autonomous-driving",
        "vlm",
        "safety-benchmark",
        "trustworthiness"
      ],
      "id": 303
    },
    {
      "name": "AISafetyLab",
      "one_line_profile": "Comprehensive framework for AI safety attack, defense, and evaluation",
      "detailed_description": "A framework integrating various methods for AI safety research, including adversarial attacks, defense mechanisms, and safety evaluation metrics for machine learning models.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_attack",
        "safety_defense",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/thu-coai/AISafetyLab",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ai-safety",
        "adversarial-ml",
        "robustness"
      ],
      "id": 304
    },
    {
      "name": "DiaSafety",
      "one_line_profile": "Benchmark and dataset for evaluating safety of conversational models",
      "detailed_description": "A repository providing a taxonomy, dataset, and benchmark suite for assessing the safety of conversational AI models, focusing on dialogue safety risks.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "safety_benchmarking",
        "dialogue_safety"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/thu-coai/DiaSafety",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "dialogue-safety",
        "benchmark",
        "llm-safety"
      ],
      "id": 305
    },
    {
      "name": "Safety-Prompts",
      "one_line_profile": "Chinese safety prompts dataset for evaluating LLM safety",
      "detailed_description": "A collection of Chinese safety prompts designed to evaluate and improve the safety of Large Language Models, covering various risk categories.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "safety_evaluation",
        "red_teaming"
      ],
      "application_level": "dataset",
      "primary_language": null,
      "repo_url": "https://github.com/thu-coai/Safety-Prompts",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "safety-prompts",
        "llm-evaluation",
        "chinese-llm"
      ],
      "id": 306
    },
    {
      "name": "SafetyBench",
      "one_line_profile": "Comprehensive benchmark for evaluating LLM safety",
      "detailed_description": "A comprehensive benchmark suite for evaluating the safety of Large Language Models across multiple languages and safety dimensions, supporting automated evaluation.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "safety_benchmarking",
        "automated_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/thu-coai/SafetyBench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm-safety",
        "benchmark",
        "evaluation-framework"
      ],
      "id": 307
    },
    {
      "name": "MLA-Trust",
      "one_line_profile": "Toolbox for benchmarking Multimodal LLM Agents trustworthiness",
      "detailed_description": "A benchmarking toolbox designed to assess the trustworthiness of Multimodal LLM Agents across dimensions such as truthfulness, controllability, safety, and privacy.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "agent_evaluation",
        "trustworthiness_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/thu-ml/MLA-Trust",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "multimodal-agents",
        "trustworthiness",
        "benchmark"
      ],
      "id": 308
    },
    {
      "name": "MMTrustEval",
      "one_line_profile": "Toolbox for benchmarking trustworthiness of multimodal LLMs",
      "detailed_description": "A comprehensive toolbox for evaluating the trustworthiness of Multimodal Large Language Models, covering safety, hallucination, and robustness aspects.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "multimodal_evaluation",
        "trustworthiness_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/thu-ml/MMTrustEval",
      "help_website": [],
      "license": "CC-BY-SA-4.0",
      "tags": [
        "multimodal-llm",
        "trustworthiness",
        "evaluation-toolbox"
      ],
      "id": 309
    },
    {
      "name": "OpenAttack",
      "one_line_profile": "Open-source package for textual adversarial attacks",
      "detailed_description": "A Python library for generating textual adversarial examples, supporting various attack models, evaluation metrics, and victim models for NLP robustness research.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_attack",
        "robustness_testing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/thunlp/OpenAttack",
      "help_website": [
        "https://openattack.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "adversarial-nlp",
        "text-attack",
        "robustness"
      ],
      "id": 310
    },
    {
      "name": "Tiger",
      "one_line_profile": "Toolkit for building trustworthy LLM applications",
      "detailed_description": "A toolkit comprising TigerArmor for AI safety, TigerRAG for retrieval-augmented generation, and TigerTune for fine-tuning, aiming to build secure and reliable LLM systems.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "safety_guardrails",
        "rag_optimization",
        "fine_tuning"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/tigerlab-ai/tiger",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ai-safety",
        "rag",
        "llm-toolkit"
      ],
      "id": 311
    },
    {
      "name": "OS-Harm",
      "one_line_profile": "Benchmark for measuring safety of computer use agents",
      "detailed_description": "A benchmark designed to evaluate the safety of agents that interact with operating systems (OS-use agents), identifying potential risks in automated computer control.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "agent_safety",
        "safety_benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/tml-epfl/os-harm",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "agent-safety",
        "benchmark",
        "os-agents"
      ],
      "id": 312
    },
    {
      "name": "Anamorpher",
      "one_line_profile": "Image scaling attacks for multi-modal prompt injection",
      "detailed_description": "A tool for generating adversarial images using scaling attacks to perform prompt injection in multi-modal models, bypassing visual safety filters.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_attack",
        "prompt_injection",
        "multimodal_security"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/trailofbits/anamorpher",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "adversarial-images",
        "prompt-injection",
        "security"
      ],
      "id": 313
    },
    {
      "name": "TransformerLab",
      "one_line_profile": "Platform for LLM training, fine-tuning, and evaluation",
      "detailed_description": "A local application platform that provides a GUI for interacting with, training, fine-tuning, and evaluating Large Language Models and Diffusion models.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "fine_tuning",
        "experiment_management"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/transformerlab/transformerlab-app",
      "help_website": [
        "https://transformerlab.ai"
      ],
      "license": "AGPL-3.0",
      "tags": [
        "llm-platform",
        "fine-tuning",
        "evaluation-gui"
      ],
      "id": 314
    },
    {
      "name": "TruLens",
      "one_line_profile": "Evaluation and tracking for LLM experiments and agents",
      "detailed_description": "A library for evaluating and tracking the performance of LLM applications and agents, providing feedback functions (RAG triad) and experiment management.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "experiment_tracking",
        "rag_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/truera/trulens",
      "help_website": [
        "https://www.trulens.org"
      ],
      "license": "MIT",
      "tags": [
        "llm-evaluation",
        "observability",
        "rag"
      ],
      "id": 315
    },
    {
      "name": "SafeBench",
      "one_line_profile": "Benchmark for evaluating autonomous vehicles in safety-critical scenarios",
      "detailed_description": "A platform and benchmark for evaluating the safety of autonomous driving algorithms in critical scenarios, supporting simulation and adversarial testing.",
      "domains": [
        "AI3",
        "AI3-04",
        "Autonomous Systems"
      ],
      "subtask_category": [
        "safety_benchmarking",
        "simulation_testing"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/trust-ai/SafeBench",
      "help_website": [
        "https://trust-ai.github.io/SafeBench/"
      ],
      "license": "MIT",
      "tags": [
        "autonomous-driving",
        "safety-benchmark",
        "simulation"
      ],
      "id": 316
    },
    {
      "name": "Folly",
      "one_line_profile": "Playground for LLM prompt injection and jailbreaking",
      "detailed_description": "An open-source playground tool designed for testing and experimenting with prompt injection and jailbreaking techniques on Large Language Models.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "red_teaming",
        "jailbreak_testing"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/user1342/Folly",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "prompt-injection",
        "jailbreak",
        "playground"
      ],
      "id": 317
    },
    {
      "name": "PyTorch CNN Adversarial Attacks",
      "one_line_profile": "PyTorch implementation of CNN adversarial attack techniques",
      "detailed_description": "A collection of implementations for various adversarial attack algorithms on Convolutional Neural Networks using PyTorch, serving as a reference library for robustness research.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_attack",
        "robustness_research"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/utkuozbulak/pytorch-cnn-adversarial-attacks",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "adversarial-attacks",
        "pytorch",
        "cnn-robustness"
      ],
      "id": 318
    },
    {
      "name": "Ragas",
      "one_line_profile": "Framework for evaluating Retrieval Augmented Generation (RAG) pipelines",
      "detailed_description": "A framework designed to evaluate RAG pipelines using metrics like faithfulness, answer relevance, and context precision, enabling automated assessment of LLM applications.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "rag_evaluation",
        "automated_metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/vibrantlabsai/ragas",
      "help_website": [
        "https://docs.ragas.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "rag-evaluation",
        "llm-metrics",
        "automated-testing"
      ],
      "id": 319
    },
    {
      "name": "caption-eval",
      "one_line_profile": "Automated metrics for sentence and image caption evaluation",
      "detailed_description": "A Python library providing standard evaluation metrics (BLEU, METEOR, ROUGE, CIDEr, SPICE) for assessing the quality of image captions and sentence generation models.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "natural_language_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/vsubhashini/caption-eval",
      "help_website": [],
      "license": null,
      "tags": [
        "evaluation-metrics",
        "nlp",
        "image-captioning",
        "bleu",
        "cider"
      ],
      "id": 320
    },
    {
      "name": "Adversarial Box",
      "one_line_profile": "PyTorch library for adversarial attacks and robust training",
      "detailed_description": "A toolbox for generating adversarial examples and performing adversarial training with PyTorch, supporting various attack methods to evaluate and improve model robustness.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "adversarial_attack",
        "adversarial_training",
        "robustness"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/wanglouis49/pytorch-adversarial_box",
      "help_website": [],
      "license": null,
      "tags": [
        "pytorch",
        "adversarial-attacks",
        "robustness",
        "security"
      ],
      "id": 321
    },
    {
      "name": "Circle Guard Bench",
      "one_line_profile": "Benchmark for evaluating LLM guardrail systems",
      "detailed_description": "A benchmark suite designed to evaluate the protection capabilities and effectiveness of Large Language Model (LLM) guard systems, including guardrails and safeguards against various attacks.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "safety_evaluation",
        "guardrail_benchmarking",
        "red_teaming"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/whitecircle-ai/circle-guard-bench",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-safety",
        "benchmark",
        "guardrails",
        "red-teaming"
      ],
      "id": 322
    },
    {
      "name": "whylogs",
      "one_line_profile": "Data logging and quality monitoring library for ML/AI pipelines",
      "detailed_description": "An open-source library for logging data profiles, monitoring data quality, and tracking model performance over time. It enables privacy-preserving data collection and robustness checks for machine learning workflows.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "data_quality",
        "model_monitoring",
        "drift_detection"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/whylabs/whylogs",
      "help_website": [
        "https://whylabs.ai/whylogs"
      ],
      "license": "Apache-2.0",
      "tags": [
        "data-logging",
        "mlops",
        "data-quality",
        "model-monitoring"
      ],
      "id": 323
    },
    {
      "name": "RiOSWorld",
      "one_line_profile": "Benchmark for assessing risks of multimodal computer-use agents",
      "detailed_description": "A benchmark environment and dataset for evaluating the safety and risks associated with multimodal agents that operate computer interfaces, focusing on robustness and safety in agentic workflows.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "agent_evaluation",
        "safety_benchmarking",
        "multimodal_agents"
      ],
      "application_level": "dataset",
      "primary_language": "HTML",
      "repo_url": "https://github.com/yjyddq/RiOSWorld",
      "help_website": [],
      "license": null,
      "tags": [
        "benchmark",
        "multimodal-agents",
        "ai-safety",
        "neurips-2025"
      ],
      "id": 324
    },
    {
      "name": "EasyLM",
      "one_line_profile": "JAX/Flax library for training, fine-tuning, and serving LLMs",
      "detailed_description": "A comprehensive solution for Large Language Model (LLM) pre-training, fine-tuning, evaluation, and serving using JAX and Flax. It simplifies scaling LLMs on TPU/GPU clusters.",
      "domains": [
        "AI3",
        "AI3-01",
        "AI3-02"
      ],
      "subtask_category": [
        "model_training",
        "model_serving",
        "fine_tuning"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/young-geng/EasyLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "jax",
        "flax",
        "llm",
        "distributed-training"
      ],
      "id": 325
    },
    {
      "name": "MT-Consistency",
      "one_line_profile": "Framework for evaluating LLM acquiescence bias and consistency",
      "detailed_description": "A research framework investigating Large Language Models' tendency for acquiescence bias in sequential QA. It includes evaluation methods, datasets, and benchmarks to assess conversational consistency and robustness.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "consistency_evaluation",
        "bias_evaluation",
        "robustness"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/yubol-bobo/MT-Consistency",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm-bias",
        "consistency",
        "evaluation",
        "qa"
      ],
      "id": 326
    },
    {
      "name": "GLM-ASR",
      "one_line_profile": "Robust open-source speech recognition model",
      "detailed_description": "A robust 1.5B parameter speech recognition model (GLM-ASR-Nano) designed for high-quality automatic speech recognition tasks, serving as a tool for audio data analysis.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "speech_recognition",
        "audio_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/zai-org/GLM-ASR",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "asr",
        "speech-recognition",
        "glm",
        "audio-processing"
      ],
      "id": 327
    },
    {
      "name": "PE-RLHF",
      "one_line_profile": "RLHF framework incorporating physics knowledge for safe autonomous driving",
      "detailed_description": "Implementation of Reinforcement Learning with Human Feedback (RLHF) augmented with physics knowledge, designed to improve the safety and trustworthiness of autonomous driving agents.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "safety_alignment",
        "autonomous_driving"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/zilin-huang/PE-RLHF",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rlhf",
        "autonomous-driving",
        "physics-informed",
        "safety"
      ],
      "id": 328
    },
    {
      "name": "ChineseHarm-bench",
      "one_line_profile": "Benchmark for detecting harmful content in Chinese LLMs",
      "detailed_description": "A benchmark dataset and evaluation suite for detecting harmful content in Chinese Large Language Models, facilitating safety alignment and red teaming research.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "safety_evaluation",
        "harmful_content_detection"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/zjunlp/ChineseHarm-bench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "benchmark",
        "chinese-llm",
        "safety",
        "harm-detection"
      ],
      "id": 329
    },
    {
      "name": "EasyDetect",
      "one_line_profile": "Framework for detecting hallucinations in LLMs",
      "detailed_description": "An easy-to-use framework for detecting hallucinations in Large Language Models, providing tools to evaluate factual consistency and reliability of model outputs.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "hallucination_detection",
        "fact_checking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/zjunlp/EasyDetect",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "hallucination",
        "llm-evaluation",
        "factuality"
      ],
      "id": 330
    },
    {
      "name": "FactCHD",
      "one_line_profile": "Benchmark for fact-conflicting hallucination detection",
      "detailed_description": "A benchmark specifically designed to evaluate the detection of fact-conflicting hallucinations in Large Language Models, supporting research in model faithfulness.",
      "domains": [
        "AI3",
        "AI3-04"
      ],
      "subtask_category": [
        "hallucination_detection",
        "benchmark"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/zjunlp/FactCHD",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "hallucination",
        "benchmark",
        "fact-checking"
      ],
      "id": 331
    }
  ]
}
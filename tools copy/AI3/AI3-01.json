{
  "generated_at": "2025-12-16T05:24:02.628830+08:00",
  "metadata": {
    "leaf_cluster": {
      "leaf_cluster_id": "AI3",
      "leaf_cluster_name": "科研领域模型训练与微调生态",
      "domain": "AI Toolchain",
      "typical_objects": "domain corpora",
      "task_chain": "数据→训练→微调→对齐→评测→发布",
      "tool_form": "训练栈 + 数据管线 + serving"
    },
    "unit": {
      "unit_id": "AI3-01",
      "unit_name": "训练框架与分布式/加速",
      "target_scale": "250–600",
      "coverage_tools": "training stacks、distributed"
    },
    "search": {
      "target_candidates": 600,
      "queries": [
        "[GH] Apex",
        "[GH] FSDP",
        "[GH] Ray Train",
        "[GH] PyTorch Lightning",
        "[GH] Accelerate",
        "[GH] Horovod",
        "[GH] ColossalAI",
        "[GH] Megatron-LM",
        "[GH] DeepSpeed",
        "[GH] distributed training",
        "[GH] llm training framework",
        "[GH] model parallelism",
        "[GH] pipeline parallelism",
        "[GH] tensor parallelism",
        "[GH] mixed precision training",
        "[GH] zero redundancy optimizer",
        "[GH] fsdp",
        "[GH] multi-gpu training",
        "[GH] flash attention",
        "[GH] distributed deep learning",
        "[GH] gradient checkpointing",
        "[GH] training stack",
        "[GH] gpu acceleration",
        "[WEB] distributed deep learning framework github",
        "[WEB] llm training stack github",
        "[WEB] pytorch distributed data parallel github",
        "[WEB] large model training acceleration github",
        "[WEB] gpu memory optimization training github"
      ],
      "total_candidates": 1196,
      "tool_candidates": 882,
      "final_tools": 356
    }
  },
  "tools": [
    {
      "name": "AdaptiveCpp",
      "one_line_profile": "Independent open-source compiler for C++-based heterogeneous programming models (SYCL, CUDA, HIP)",
      "detailed_description": "AdaptiveCpp (formerly hipSYCL) is a community-driven compiler infrastructure that enables C++ applications to run on hardware from all major vendors (NVIDIA, AMD, Intel) using standard parallelism models like SYCL and C++17/20 parallel algorithms. It serves as a foundational tool for high-performance scientific computing and AI acceleration.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "acceleration",
        "compilation",
        "heterogeneous_computing"
      ],
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/AdaptiveCpp/AdaptiveCpp",
      "help_website": [],
      "license": "BSD-2-Clause",
      "tags": [
        "sycl",
        "compiler",
        "gpu-acceleration",
        "hpc"
      ],
      "id": 1
    },
    {
      "name": "AgentFly",
      "one_line_profile": "Scalable and extensible reinforcement learning framework for Large Language Model agents",
      "detailed_description": "AgentFly is a research framework designed for training and evaluating LLM agents using reinforcement learning. It supports scalable training pipelines and provides extensibility for developing new agent-based RL algorithms.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "agent_training",
        "model_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Agent-One-Lab/AgentFly",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "reinforcement-learning",
        "llm-agents",
        "training-framework"
      ],
      "id": 2
    },
    {
      "name": "AgileRL",
      "one_line_profile": "Reinforcement learning framework with evolutionary hyperparameter optimization",
      "detailed_description": "AgileRL is a deep reinforcement learning library focused on RLOps and efficiency. It implements evolutionary algorithms for hyperparameter optimization during training, allowing for faster convergence and better model performance in scientific and control tasks.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "hyperparameter_optimization",
        "training_framework"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/AgileRL/AgileRL",
      "help_website": [
        "https://docs.agilerl.com"
      ],
      "license": "Apache-2.0",
      "tags": [
        "reinforcement-learning",
        "evolutionary-algorithms",
        "rlops"
      ],
      "id": 3
    },
    {
      "name": "fsdp_qlora",
      "one_line_profile": "Utility for training LLMs using QLoRA and FSDP",
      "detailed_description": "A specialized training utility that combines Fully Sharded Data Parallel (FSDP) with Quantized LoRA (QLoRA) to enable efficient fine-tuning of large language models on limited GPU resources. It serves as a practical solver for memory-constrained model training.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "fine_tuning",
        "memory_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/AnswerDotAI/fsdp_qlora",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "fsdp",
        "qlora",
        "llm-training",
        "quantization"
      ],
      "id": 4
    },
    {
      "name": "distribuuuu",
      "one_line_profile": "Lightweight PyTorch distributed training framework",
      "detailed_description": "A pure and concise distributed training framework for PyTorch designed to simplify the setup and execution of multi-GPU training tasks. It provides abstractions for distributed data parallel (DDP) and other training strategies.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "training_framework"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/BIGBALLON/distribuuuu",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pytorch",
        "distributed-training",
        "ddp"
      ],
      "id": 5
    },
    {
      "name": "PySNN",
      "one_line_profile": "Efficient Spiking Neural Network framework based on PyTorch",
      "detailed_description": "PySNN is a framework for simulating and training Spiking Neural Networks (SNNs) with GPU acceleration. It extends PyTorch to support neuromorphic computing models, enabling research into biologically plausible neural networks.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "neuromorphic_computing",
        "snn_simulation",
        "model_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/BasBuller/PySNN",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "snn",
        "spiking-neural-networks",
        "neuromorphic"
      ],
      "id": 6
    },
    {
      "name": "Bluefog",
      "one_line_profile": "Distributed and decentralized training framework for PyTorch over graphs",
      "detailed_description": "Bluefog is a high-performance distributed training framework for PyTorch that implements decentralized optimization algorithms over virtual topologies (graphs). It is designed for large-scale deep learning training on heterogeneous or bandwidth-constrained networks.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "decentralized_optimization",
        "training_framework"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Bluefog-Lib/bluefog",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "distributed-training",
        "decentralized-optimization",
        "pytorch"
      ],
      "id": 7
    },
    {
      "name": "VeOmni",
      "one_line_profile": "Model-centric distributed training framework for multi-modality models",
      "detailed_description": "VeOmni is a distributed training framework designed to scale model training across various modalities. It provides a collection of distributed recipes and optimizations to facilitate the training of large-scale foundation models.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "large_scale_training",
        "multimodal_learning"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/ByteDance-Seed/VeOmni",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "distributed-training",
        "foundation-models",
        "multimodal"
      ],
      "id": 8
    },
    {
      "name": "Verbalized Sampling",
      "one_line_profile": "Training-free prompting framework to mitigate mode collapse in LLMs",
      "detailed_description": "Verbalized Sampling is a framework and CLI tool that implements a training-free prompting strategy to improve the diversity of Large Language Model (LLM) outputs. It is used for synthetic data generation, dialogue simulation, and mitigating mode collapse in scientific and creative text generation tasks.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "inference_control",
        "synthetic_data_generation",
        "prompt_engineering"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/CHATS-lab/verbalized-sampling",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "llm",
        "sampling-strategy",
        "inference"
      ],
      "id": 9
    },
    {
      "name": "CV-CUDA",
      "one_line_profile": "GPU-accelerated library for cloud-scale image processing and computer vision",
      "detailed_description": "An open-source, GPU-accelerated library designed for building efficient, cloud-scale image processing and computer vision pipelines. It provides specialized kernels for pre-processing and post-processing tasks in AI workflows.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "image_processing",
        "computer_vision",
        "acceleration"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/CVCUDA/CV-CUDA",
      "help_website": [
        "https://github.com/CVCUDA/CV-CUDA"
      ],
      "license": "NOASSERTION",
      "tags": [
        "gpu-acceleration",
        "computer-vision",
        "image-processing",
        "cuda"
      ],
      "id": 10
    },
    {
      "name": "trlx",
      "one_line_profile": "Distributed training framework for RLHF on language models",
      "detailed_description": "A library for distributed training of large language models using Reinforcement Learning via Human Feedback (RLHF). It supports various RL algorithms and integrates with distributed training backends.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "rlhf",
        "distributed_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/CarperAI/trlx",
      "help_website": [
        "https://github.com/CarperAI/trlx"
      ],
      "license": "MIT",
      "tags": [
        "rlhf",
        "distributed-training",
        "llm",
        "reinforcement-learning"
      ],
      "id": 11
    },
    {
      "name": "Mist",
      "one_line_profile": "Efficient distributed training system for LLMs via memory-parallelism co-optimization",
      "detailed_description": "A system for efficient distributed training of Large Language Models (LLMs) that optimizes memory usage and parallelism strategies. It implements techniques described in the EuroSys'25 paper.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "memory_optimization",
        "llm_training"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/CentML/Mist",
      "help_website": [
        "https://github.com/CentML/Mist"
      ],
      "license": "Apache-2.0",
      "tags": [
        "distributed-training",
        "llm",
        "memory-optimization"
      ],
      "id": 12
    },
    {
      "name": "wolpertinger_ddpg",
      "one_line_profile": "Implementation of Wolpertinger Training with DDPG for discrete action spaces",
      "detailed_description": "A PyTorch implementation of the Wolpertinger policy for Deep Reinforcement Learning in large discrete action spaces, compatible with Multi-GPU/CPU setups.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "model_training"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ChangyWen/wolpertinger_ddpg",
      "help_website": [
        "https://github.com/ChangyWen/wolpertinger_ddpg"
      ],
      "license": null,
      "tags": [
        "reinforcement-learning",
        "ddpg",
        "pytorch"
      ],
      "id": 13
    },
    {
      "name": "llm-rk3588",
      "one_line_profile": "GPU-accelerated LLM inference on RK3588 edge devices",
      "detailed_description": "A tool to run Large Language Models on Rockchip RK3588 platforms with GPU acceleration, enabling edge AI inference.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "inference_acceleration",
        "edge_computing"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/Chrisz236/llm-rk3588",
      "help_website": [
        "https://github.com/Chrisz236/llm-rk3588"
      ],
      "license": "Apache-2.0",
      "tags": [
        "edge-ai",
        "rk3588",
        "llm-inference",
        "gpu-acceleration"
      ],
      "id": 14
    },
    {
      "name": "OpenGPT",
      "one_line_profile": "Framework for creating grounded instruction datasets and training domain expert LLMs",
      "detailed_description": "A framework designed to create grounded instruction-based datasets and train conversational domain expert Large Language Models, facilitating the development of specialized AI agents.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "dataset_creation",
        "model_training",
        "domain_adaptation"
      ],
      "application_level": "framework",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/CogStack/OpenGPT",
      "help_website": [
        "https://github.com/CogStack/OpenGPT"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "dataset-generation",
        "domain-expert"
      ],
      "id": 15
    },
    {
      "name": "gdGPT",
      "one_line_profile": "LLM training tool using DeepSpeed pipeline mode",
      "detailed_description": "A tool for training large language models (BLOOM, LLaMA, Baichuan, ChatGLM) using DeepSpeed's pipeline parallelism mode, optimized for speed compared to Zero/FSDP.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "distributed_training",
        "pipeline_parallelism"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/CoinCheung/gdGPT",
      "help_website": [
        "https://github.com/CoinCheung/gdGPT"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm-training",
        "deepspeed",
        "pipeline-parallelism"
      ],
      "id": 16
    },
    {
      "name": "net2net",
      "one_line_profile": "Network-to-Network Translation with Conditional Invertible Neural Networks",
      "detailed_description": "A library implementing Network-to-Network translation using Conditional Invertible Neural Networks (cINNs), useful for generative modeling and domain translation tasks.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "generative_modeling",
        "network_translation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/CompVis/net2net",
      "help_website": [
        "https://github.com/CompVis/net2net"
      ],
      "license": null,
      "tags": [
        "generative-models",
        "invertible-neural-networks",
        "deep-learning"
      ],
      "id": 17
    },
    {
      "name": "MPP-LLaVA",
      "one_line_profile": "Multimodal Pipeline Parallel training framework for LLaVA-like models",
      "detailed_description": "A project enabling Multimodal Pipeline Parallel (MPP) training for Qwen-based MLLMs, allowing the training of large multimodal models on consumer-grade GPUs (e.g., RTX 3090/4090).",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "multimodal_learning",
        "pipeline_parallelism"
      ],
      "application_level": "framework",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/Coobiw/MPP-LLaVA",
      "help_website": [
        "https://github.com/Coobiw/MPP-LLaVA"
      ],
      "license": null,
      "tags": [
        "multimodal",
        "pipeline-parallelism",
        "llava",
        "qwen"
      ],
      "id": 18
    },
    {
      "name": "flash-attention",
      "one_line_profile": "Fast and memory-efficient exact attention mechanism",
      "detailed_description": "A library providing fast and memory-efficient implementations of exact attention (FlashAttention), a critical primitive for accelerating Transformer-based model training and inference.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "acceleration",
        "model_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Dao-AILab/flash-attention",
      "help_website": [
        "https://github.com/Dao-AILab/flash-attention"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "attention-mechanism",
        "gpu-acceleration",
        "cuda",
        "transformer"
      ],
      "id": 19
    },
    {
      "name": "mixed_precision_for_JAX",
      "one_line_profile": "Mixed Precision Training utilities for JAX based on Equinox",
      "detailed_description": "A repository providing tools and utilities for implementing Mixed Precision Training in JAX, specifically built upon the Equinox library.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "mixed_precision",
        "optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Data-Science-in-Mechanical-Engineering/mixed_precision_for_JAX",
      "help_website": [
        "https://github.com/Data-Science-in-Mechanical-Engineering/mixed_precision_for_JAX"
      ],
      "license": "MIT",
      "tags": [
        "jax",
        "mixed-precision",
        "equinox",
        "training"
      ],
      "id": 20
    },
    {
      "name": "gpuRIR",
      "one_line_profile": "GPU-accelerated Room Impulse Response (RIR) simulation library",
      "detailed_description": "A Python library for simulating Room Impulse Responses (RIR) with GPU acceleration, used for generating acoustic data for audio processing and machine learning tasks.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "simulation",
        "data_generation",
        "acoustics"
      ],
      "application_level": "library",
      "primary_language": "Cuda",
      "repo_url": "https://github.com/DavidDiazGuerra/gpuRIR",
      "help_website": [
        "https://github.com/DavidDiazGuerra/gpuRIR"
      ],
      "license": "AGPL-3.0",
      "tags": [
        "simulation",
        "acoustics",
        "gpu-acceleration",
        "rir"
      ],
      "id": 21
    },
    {
      "name": "horizonml",
      "one_line_profile": "Hybrid Model Parallelism Framework for Distributed Training on Edge Devices",
      "detailed_description": "A framework enabling efficient distributed training of machine learning models across heterogeneous edge devices using hybrid model parallelism.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "edge_computing",
        "model_parallelism"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/Deeptanshu-sankhwar/horizonml",
      "help_website": [
        "https://github.com/Deeptanshu-sankhwar/horizonml"
      ],
      "license": "MIT",
      "tags": [
        "edge-ai",
        "distributed-training",
        "model-parallelism"
      ],
      "id": 22
    },
    {
      "name": "Insanely-Fast-Transcription",
      "one_line_profile": "GPU-accelerated audio transcription utility using Whisper",
      "detailed_description": "A utility for rapid audio transcription leveraging GPU acceleration (CUDA/MPS) and the Whisper model, optimized for high-performance speech-to-text conversion.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "data_processing",
        "speech_recognition",
        "acceleration"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Doriandarko/Insanely-Fast-Transcription",
      "help_website": [
        "https://github.com/Doriandarko/Insanely-Fast-Transcription"
      ],
      "license": null,
      "tags": [
        "whisper",
        "transcription",
        "gpu-acceleration",
        "audio-processing"
      ],
      "id": 23
    },
    {
      "name": "nemesyst",
      "one_line_profile": "Hybrid-parallelism, database-based deep learning framework",
      "detailed_description": "A generalised and highly customisable deep learning framework that employs hybrid parallelism and a database-based approach for training.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "distributed_training",
        "framework"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/DreamingRaven/nemesyst",
      "help_website": [
        "https://github.com/DreamingRaven/nemesyst"
      ],
      "license": "MIT",
      "tags": [
        "deep-learning",
        "hybrid-parallelism",
        "framework"
      ],
      "id": 24
    },
    {
      "name": "xshinnosuke",
      "one_line_profile": "Pure Numpy deep learning framework with GPU acceleration support",
      "detailed_description": "A deep learning framework implemented purely in Numpy, supporting both dynamic and static graphs, with optional GPU acceleration.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "framework",
        "educational"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/E1eveNn/xshinnosuke",
      "help_website": [
        "https://github.com/E1eveNn/xshinnosuke"
      ],
      "license": "MIT",
      "tags": [
        "deep-learning-framework",
        "numpy",
        "gpu-acceleration"
      ],
      "id": 25
    },
    {
      "name": "sheeprl",
      "one_line_profile": "Distributed Reinforcement Learning framework accelerated by Lightning Fabric",
      "detailed_description": "A distributed Reinforcement Learning framework that leverages Lightning Fabric for acceleration, enabling scalable RL training.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "distributed_training",
        "acceleration"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/Eclectic-Sheep/sheeprl",
      "help_website": [
        "https://github.com/Eclectic-Sheep/sheeprl"
      ],
      "license": "Apache-2.0",
      "tags": [
        "reinforcement-learning",
        "distributed-training",
        "pytorch-lightning"
      ],
      "id": 26
    },
    {
      "name": "gpt-neox",
      "one_line_profile": "Library for model-parallel training of autoregressive transformers on GPUs",
      "detailed_description": "An implementation of model-parallel autoregressive transformers on GPUs, built on Megatron-LM and DeepSpeed, designed for training massive language models.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "model_parallelism",
        "llm_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/EleutherAI/gpt-neox",
      "help_website": [
        "https://github.com/EleutherAI/gpt-neox"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "distributed-training",
        "megatron-lm",
        "deepspeed"
      ],
      "id": 27
    },
    {
      "name": "pytorch_tempest",
      "one_line_profile": "Template/Scaffold for training neural nets with PyTorch Lightning and Hydra",
      "detailed_description": "A structured template and workflow tool for training neural networks, integrating PyTorch Lightning for training loops and Hydra for configuration management.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "workflow_management",
        "model_training"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/Erlemar/pytorch_tempest",
      "help_website": [
        "https://github.com/Erlemar/pytorch_tempest"
      ],
      "license": "MIT",
      "tags": [
        "pytorch-lightning",
        "hydra",
        "template",
        "training-workflow"
      ],
      "id": 28
    },
    {
      "name": "llm_parallelisms.c",
      "one_line_profile": "Pure C implementation of LLM training parallelisms",
      "detailed_description": "A minimal, educational implementation of various LLM training parallelism strategies (Data Parallelism, FSDP, Tensor Parallelism, Pipeline Parallelism) in pure C.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "parallelism",
        "educational"
      ],
      "application_level": "library",
      "primary_language": "C",
      "repo_url": "https://github.com/EugenHotaj/llm_parallelisms.c",
      "help_website": [
        "https://github.com/EugenHotaj/llm_parallelisms.c"
      ],
      "license": "MIT",
      "tags": [
        "llm",
        "parallelism",
        "c",
        "distributed-training"
      ],
      "id": 29
    },
    {
      "name": "LLaVA-OneVision-1.5",
      "one_line_profile": "Open framework for democratized multimodal model training",
      "detailed_description": "A fully open framework designed to democratize the training of multimodal models, specifically focusing on the LLaVA-OneVision architecture.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "multimodal_learning",
        "framework"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/EvolvingLMMs-Lab/LLaVA-OneVision-1.5",
      "help_website": [
        "https://github.com/EvolvingLMMs-Lab/LLaVA-OneVision-1.5"
      ],
      "license": "Apache-2.0",
      "tags": [
        "multimodal",
        "llava",
        "training-framework"
      ],
      "id": 30
    },
    {
      "name": "celldetection",
      "one_line_profile": "Scalable Instance Segmentation library for bioimage analysis",
      "detailed_description": "A library for scalable instance segmentation using PyTorch and PyTorch Lightning, specifically tailored for cell detection and bioimage analysis tasks.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "image_segmentation",
        "bioimage_analysis",
        "model_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/FZJ-INM1-BDA/celldetection",
      "help_website": [
        "https://github.com/FZJ-INM1-BDA/celldetection"
      ],
      "license": "Apache-2.0",
      "tags": [
        "cell-detection",
        "instance-segmentation",
        "bioimage",
        "pytorch"
      ],
      "id": 31
    },
    {
      "name": "Medusa",
      "one_line_profile": "Framework for accelerating LLM generation with multiple decoding heads",
      "detailed_description": "A simple framework that accelerates Large Language Model (LLM) generation by employing multiple decoding heads, improving inference speed.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "inference_acceleration",
        "model_optimization"
      ],
      "application_level": "framework",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/FasterDecoding/Medusa",
      "help_website": [
        "https://github.com/FasterDecoding/Medusa"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm-acceleration",
        "decoding",
        "inference"
      ],
      "id": 32
    },
    {
      "name": "FedML",
      "one_line_profile": "Unified library for distributed training and federated learning",
      "detailed_description": "A scalable machine learning library for large-scale distributed training, model serving, and federated learning, supporting cross-cloud scheduling.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "federated_learning",
        "model_serving"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/FedML-AI/FedML",
      "help_website": [
        "https://doc.fedml.ai"
      ],
      "license": "Apache-2.0",
      "tags": [
        "federated-learning",
        "distributed-training",
        "edge-ai"
      ],
      "id": 33
    },
    {
      "name": "FlexQ",
      "one_line_profile": "Post-training INT6 quantization framework for LLM inference",
      "detailed_description": "A post-training quantization framework specifically tailored for Large Language Model (LLM) inference, utilizing INT6 quantization to improve efficiency.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_quantization",
        "inference_optimization"
      ],
      "application_level": "framework",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/FlyFoxPlayer/FlexQ",
      "help_website": [
        "https://github.com/FlyFoxPlayer/FlexQ"
      ],
      "license": "Apache-2.0",
      "tags": [
        "quantization",
        "llm",
        "inference"
      ],
      "id": 34
    },
    {
      "name": "LightningFSL",
      "one_line_profile": "PyTorch Lightning implementations of Few-Shot Learning models",
      "detailed_description": "A library providing PyTorch Lightning implementations of various Few-Shot Learning algorithms and models.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "few_shot_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Frankluox/LightningFSL",
      "help_website": [
        "https://github.com/Frankluox/LightningFSL"
      ],
      "license": "MIT",
      "tags": [
        "few-shot-learning",
        "pytorch-lightning",
        "meta-learning"
      ],
      "id": 35
    },
    {
      "name": "CosyVoice",
      "one_line_profile": "Multi-lingual large voice generation model framework",
      "detailed_description": "A full-stack framework for multi-lingual large voice generation models, providing capabilities for inference, training, and deployment.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "inference",
        "voice_generation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/FunAudioLLM/CosyVoice",
      "help_website": [
        "https://github.com/FunAudioLLM/CosyVoice"
      ],
      "license": "Apache-2.0",
      "tags": [
        "voice-generation",
        "tts",
        "model-training"
      ],
      "id": 36
    },
    {
      "name": "edm2",
      "one_line_profile": "Multi-GPU implementation of EDM2 diffusion model training",
      "detailed_description": "A minimal multi-GPU implementation of the EDM2 (Analyzing and Improving the Training Dynamics of Diffusion Models) framework.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "diffusion_models",
        "distributed_training"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/FutureXiang/edm2",
      "help_website": [
        "https://github.com/FutureXiang/edm2"
      ],
      "license": null,
      "tags": [
        "diffusion-models",
        "multi-gpu",
        "training"
      ],
      "id": 37
    },
    {
      "name": "Rust-SSP",
      "one_line_profile": "Structured Stream Parallelism library for Rust",
      "detailed_description": "A Rust library implementing Structured Stream Parallelism, providing high-performance parallel computing capabilities suitable for scientific data processing streams.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "parallel_computing",
        "data_processing"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/GMAP/Rust-SSP",
      "help_website": [
        "https://github.com/GMAP/Rust-SSP"
      ],
      "license": "MIT",
      "tags": [
        "rust",
        "parallelism",
        "stream-processing"
      ],
      "id": 38
    },
    {
      "name": "RadeonRays_SDK",
      "one_line_profile": "Ray intersection acceleration library for CPU and GPU",
      "detailed_description": "An acceleration library for ray intersection calculations, supporting hardware and software multiplatforms using CPU and GPU, useful for physics simulation and rendering.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "acceleration",
        "simulation",
        "ray_tracing"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/GPUOpen-LibrariesAndSDKs/RadeonRays_SDK",
      "help_website": [
        "https://github.com/GPUOpen-LibrariesAndSDKs/RadeonRays_SDK"
      ],
      "license": "MIT",
      "tags": [
        "ray-tracing",
        "acceleration",
        "gpu",
        "simulation"
      ],
      "id": 39
    },
    {
      "name": "MG-GCN",
      "one_line_profile": "Scalable Multi-GPU Graph Convolutional Network Training Framework",
      "detailed_description": "A framework for scalable training of Graph Convolutional Networks (GCNs) across multiple GPUs, enabling the processing of large-scale graph data.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "graph_neural_networks"
      ],
      "application_level": "framework",
      "primary_language": "C++",
      "repo_url": "https://github.com/GT-TDAlab/MG-GCN",
      "help_website": [
        "https://github.com/GT-TDAlab/MG-GCN"
      ],
      "license": "NOASSERTION",
      "tags": [
        "gcn",
        "multi-gpu",
        "distributed-training",
        "graph-learning"
      ],
      "id": 40
    },
    {
      "name": "nvidia-nemo-on-gke",
      "one_line_profile": "Infrastructure code for training NVIDIA NeMo LLMs on GKE",
      "detailed_description": "A set of configurations and scripts (Terraform/HCL) for deploying and training NVIDIA NeMo Megatron Large Language Models on Google Kubernetes Engine (GKE).",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "infrastructure_deployment",
        "llm_training"
      ],
      "application_level": "workflow",
      "primary_language": "HCL",
      "repo_url": "https://github.com/GoogleCloudPlatform/nvidia-nemo-on-gke",
      "help_website": [
        "https://github.com/GoogleCloudPlatform/nvidia-nemo-on-gke"
      ],
      "license": "Apache-2.0",
      "tags": [
        "kubernetes",
        "gke",
        "nemo",
        "llm-training"
      ],
      "id": 41
    },
    {
      "name": "relora",
      "one_line_profile": "Implementation of ReLoRA for high-rank training through low-rank updates",
      "detailed_description": "The official implementation of ReLoRA, a method for efficient high-rank training of neural networks using low-rank updates, reducing memory and compute requirements.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "optimization",
        "parameter_efficient_tuning"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/Guitaricet/relora",
      "help_website": [
        "https://github.com/Guitaricet/relora"
      ],
      "license": "Apache-2.0",
      "tags": [
        "peft",
        "training-optimization",
        "low-rank-updates"
      ],
      "id": 42
    },
    {
      "name": "hedgehog-lab",
      "one_line_profile": "Browser-based scientific computing and data visualization environment",
      "detailed_description": "An open-source scientific computing environment that runs entirely in the browser, offering matrix operations with GPU acceleration, TeX support, and data visualization.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "scientific_computing",
        "visualization",
        "data_analysis"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/Hedgehog-Computing/hedgehog-lab",
      "help_website": [
        "https://hedgehog-lab.github.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "scientific-computing",
        "visualization",
        "web-based",
        "gpu-acceleration"
      ],
      "id": 43
    },
    {
      "name": "llm-trainer",
      "one_line_profile": "Framework for training Large Language Models from scratch",
      "detailed_description": "A complete framework designed to facilitate the training of Large Language Models (LLMs) from scratch, providing necessary tools and abstractions.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "llm"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/HelpingAI/llm-trainer",
      "help_website": [
        "https://github.com/HelpingAI/llm-trainer"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm-training",
        "framework",
        "deep-learning"
      ],
      "id": 44
    },
    {
      "name": "revlib",
      "one_line_profile": "Memory-efficient reversible network library for PyTorch",
      "detailed_description": "A lightweight library implementing reversible neural networks in PyTorch, enabling significant memory savings during training by reconstructing activations during the backward pass. Supports DeepSpeed and XLA.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "memory_optimization",
        "model_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/HomebrewML/revlib",
      "help_website": [],
      "license": "BSD-2-Clause",
      "tags": [
        "pytorch",
        "reversible-networks",
        "memory-efficient",
        "deepspeed"
      ],
      "id": 45
    },
    {
      "name": "Hetu",
      "one_line_profile": "High-performance distributed deep learning system",
      "detailed_description": "A distributed deep learning system targeting large-scale and automated distributed training, developed by PKU-DAIR. It optimizes communication and computation for efficient model training.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "system_optimization"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/Hsword/Hetu",
      "help_website": [
        "https://github.com/PKU-DAIR/Hetu"
      ],
      "license": "Apache-2.0",
      "tags": [
        "distributed-systems",
        "deep-learning",
        "training-framework"
      ],
      "id": 46
    },
    {
      "name": "transpeeder",
      "one_line_profile": "Efficient LLaMA training tool using Pipeline Parallelism",
      "detailed_description": "A tool designed to train LLaMA models on limited hardware (e.g., single A100) by leveraging Hugging Face Transformers and DeepSpeed Pipeline Parallelism.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "llm_training",
        "distributed_training"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/HuangLK/transpeeder",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llama",
        "deepspeed",
        "pipeline-parallelism",
        "training-optimization"
      ],
      "id": 47
    },
    {
      "name": "DeepMath",
      "one_line_profile": "Framework for training math reasoning agents",
      "detailed_description": "A framework from Intel Labs for training and evaluating mathematical reasoning agents using local models, GRPO, and vLLM. Facilitates research in AI for mathematics.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "math_reasoning",
        "agent_training"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/IntelLabs/DeepMath",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "math-ai",
        "reasoning",
        "training-framework",
        "intel-labs"
      ],
      "id": 48
    },
    {
      "name": "lmdeploy",
      "one_line_profile": "Toolkit for compressing and serving LLMs",
      "detailed_description": "A comprehensive toolkit for compressing, deploying, and serving Large Language Models. It supports high-performance inference and quantization, essential for the LLM lifecycle.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_compression",
        "inference_serving"
      ],
      "application_level": "tool",
      "primary_language": "Python",
      "repo_url": "https://github.com/InternLM/lmdeploy",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "deployment",
        "quantization",
        "inference"
      ],
      "id": 49
    },
    {
      "name": "fast-reid",
      "one_line_profile": "SOTA Re-identification Toolbox",
      "detailed_description": "A software library for object re-identification research and development. It provides state-of-the-art methods and efficient training pipelines for computer vision tasks.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "computer_vision",
        "re_identification"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/JDAI-CV/fast-reid",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "reid",
        "computer-vision",
        "pytorch",
        "toolbox"
      ],
      "id": 50
    },
    {
      "name": "Surge",
      "one_line_profile": "High-performance matrix math library for Swift",
      "detailed_description": "A Swift library leveraging the Accelerate framework to provide high-performance functions for matrix mathematics, digital signal processing (DSP), and image manipulation, serving as a scientific computing foundation for the Swift ecosystem.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "matrix_computation",
        "signal_processing"
      ],
      "application_level": "library",
      "primary_language": "Swift",
      "repo_url": "https://github.com/Jounce/Surge",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "swift",
        "linear-algebra",
        "dsp",
        "accelerate"
      ],
      "id": 51
    },
    {
      "name": "bert-squeeze",
      "one_line_profile": "Toolbox for Transformer model compression",
      "detailed_description": "A set of tools for compressing Transformer models using techniques like distillation, quantization, and pruning, built on PyTorch Lightning.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_compression",
        "distillation"
      ],
      "application_level": "tool",
      "primary_language": "Python",
      "repo_url": "https://github.com/JulesBelveze/bert-squeeze",
      "help_website": [],
      "license": null,
      "tags": [
        "transformers",
        "compression",
        "pytorch-lightning",
        "distillation"
      ],
      "id": 52
    },
    {
      "name": "demucs_lightning",
      "one_line_profile": "PyTorch Lightning implementation of Demucs",
      "detailed_description": "A PyTorch Lightning wrapper for the Demucs music source separation model, facilitating training with features like Hydra configuration and Tensorboard logging.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "audio_processing",
        "source_separation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/KinWaiCheuk/demucs_lightning",
      "help_website": [],
      "license": null,
      "tags": [
        "audio",
        "demucs",
        "pytorch-lightning",
        "music-separation"
      ],
      "id": 53
    },
    {
      "name": "FlashDeBERTa",
      "one_line_profile": "Flash attention implementation for DeBERTa",
      "detailed_description": "An optimized implementation of the DeBERTa disentangled attention mechanism using Flash Attention, designed to accelerate training and inference of DeBERTa models.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_acceleration",
        "attention_mechanism"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Knowledgator/FlashDeBERTa",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "deberta",
        "flash-attention",
        "optimization",
        "nlp"
      ],
      "id": 54
    },
    {
      "name": "libROM",
      "one_line_profile": "Library for Reduced Order Modeling",
      "detailed_description": "A C++ library for data-driven model reduction with an emphasis on large-scale parallelism and linear subspace methods, developed by LLNL for physical simulations.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_reduction",
        "physics_simulation"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/LLNL/libROM",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "model-reduction",
        "hpc",
        "physics",
        "linear-algebra"
      ],
      "id": 55
    },
    {
      "name": "lightning-thunder",
      "one_line_profile": "PyTorch compiler for training acceleration",
      "detailed_description": "A source-to-source compiler for PyTorch that accelerates training and inference by optimizing performance, memory usage, and parallelism.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "compilation",
        "model_acceleration"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/Lightning-AI/lightning-thunder",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "compiler",
        "pytorch",
        "optimization",
        "acceleration"
      ],
      "id": 56
    },
    {
      "name": "lit-llama",
      "one_line_profile": "Hackable implementation of LLaMA",
      "detailed_description": "A clean, hackable implementation of the LLaMA language model based on nanoGPT. Supports pre-training, fine-tuning (LoRA, Adapter), and quantization, serving as a research workbench for LLMs.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "llm_training",
        "fine_tuning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Lightning-AI/lit-llama",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llama",
        "llm",
        "finetuning",
        "nanogpt"
      ],
      "id": 57
    },
    {
      "name": "pytorch-lightning",
      "one_line_profile": "High-level framework for PyTorch training",
      "detailed_description": "A lightweight PyTorch wrapper that decouples science code from engineering, enabling easy scaling of model training across multiple GPUs and TPUs.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "training_framework",
        "distributed_training"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/Lightning-AI/pytorch-lightning",
      "help_website": [
        "https://lightning.ai/docs/pytorch/stable/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "pytorch",
        "deep-learning",
        "framework",
        "distributed"
      ],
      "id": 58
    },
    {
      "name": "lightning-ColossalAI",
      "one_line_profile": "ColossalAI strategy for PyTorch Lightning",
      "detailed_description": "An integration library that brings ColossalAI's large-scale distributed model training strategies to the PyTorch Lightning ecosystem.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "strategy_integration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Lightning-Universe/lightning-ColossalAI",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "colossalai",
        "distributed-training",
        "pytorch-lightning"
      ],
      "id": 59
    },
    {
      "name": "lightning-bolts",
      "one_line_profile": "Toolbox of models and callbacks for Lightning",
      "detailed_description": "A collection of pre-built models, callbacks, and datasets for PyTorch Lightning, designed to accelerate research prototyping and experimentation.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_library",
        "prototyping"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Lightning-Universe/lightning-bolts",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "models",
        "callbacks",
        "research-tools",
        "pytorch-lightning"
      ],
      "id": 60
    },
    {
      "name": "lightning-flash",
      "one_line_profile": "Task-based AI framework",
      "detailed_description": "A high-level framework built on PyTorch Lightning that provides ready-to-use tasks for various domains (text, image, audio), simplifying the configuration and running of complex AI recipes.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "task_framework",
        "automl"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/Lightning-Universe/lightning-flash",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "automl",
        "tasks",
        "pytorch-lightning",
        "high-level-api"
      ],
      "id": 61
    },
    {
      "name": "lightning-transformers",
      "one_line_profile": "Transformers integration for Lightning",
      "detailed_description": "A library that provides flexible components to pair Hugging Face Transformers with PyTorch Lightning, facilitating efficient training and fine-tuning of Transformer models.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_integration",
        "nlp_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Lightning-Universe/lightning-transformers",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "transformers",
        "huggingface",
        "pytorch-lightning",
        "nlp"
      ],
      "id": 62
    },
    {
      "name": "naifu",
      "one_line_profile": "Generative model training tool",
      "detailed_description": "A tool for training generative models (specifically diffusion models) using PyTorch Lightning, often used for fine-tuning image generation models.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "generative_models",
        "diffusion_training"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Mikubill/naifu",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "diffusion",
        "generative-ai",
        "training-tool",
        "pytorch-lightning"
      ],
      "id": 63
    },
    {
      "name": "EasyLLM",
      "one_line_profile": "Usability-focused LLM training framework",
      "detailed_description": "A framework built upon Megatron-Deepspeed and HuggingFace Trainer that reorganizes code logic to improve usability and training efficiency for Large Language Models.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "llm_training",
        "framework_wrapper"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/ModelTC/EasyLLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "megatron",
        "deepspeed",
        "training-framework"
      ],
      "id": 64
    },
    {
      "name": "MyCaffe/NCCL",
      "one_line_profile": "Windows port of NVIDIA's NCCL library for multi-GPU communication",
      "detailed_description": "A Windows adaptation of the NVIDIA Collective Communications Library (NCCL), enabling multi-GPU and multi-node collective communication primitives for deep learning frameworks on Windows systems.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "communication_primitives"
      ],
      "application_level": "library",
      "primary_language": "Cuda",
      "repo_url": "https://github.com/MyCaffe/NCCL",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "nccl",
        "windows",
        "distributed-training",
        "gpu"
      ],
      "id": 65
    },
    {
      "name": "LARS-ImageNet-PyTorch",
      "one_line_profile": "PyTorch implementation of LARS optimizer for large batch training",
      "detailed_description": "A library implementing the Layer-wise Adaptive Rate Scaling (LARS) optimizer, designed for large batch training of deep neural networks (e.g., ResNet on ImageNet) with support for Horovod distributed training.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_optimization",
        "training_optimizer"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NUS-HPC-AI-Lab/LARS-ImageNet-PyTorch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "lars",
        "optimizer",
        "large-batch",
        "pytorch"
      ],
      "id": 66
    },
    {
      "name": "NVIDIA NeMo Automodel",
      "one_line_profile": "Distributed training library for LLMs and VLMs with Hugging Face support",
      "detailed_description": "A PyTorch Distributed native training library designed for Large Language Models (LLMs) and Vision Language Models (VLMs), offering out-of-the-box integration with Hugging Face models and optimized for NVIDIA hardware.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "llm_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NVIDIA-NeMo/Automodel",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "distributed-training",
        "pytorch",
        "nemo"
      ],
      "id": 67
    },
    {
      "name": "NVIDIA DALI",
      "one_line_profile": "GPU-accelerated data loading and augmentation library",
      "detailed_description": "A library containing highly optimized building blocks and an execution engine for data processing to accelerate deep learning training and inference applications, focusing on removing CPU bottlenecks.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "data_processing",
        "data_loading",
        "augmentation"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/NVIDIA/DALI",
      "help_website": [
        "https://docs.nvidia.com/deeplearning/dali"
      ],
      "license": "Apache-2.0",
      "tags": [
        "data-loading",
        "gpu-acceleration",
        "augmentation",
        "pipeline"
      ],
      "id": 68
    },
    {
      "name": "Megatron-LM",
      "one_line_profile": "Framework for large-scale distributed training of Transformer models",
      "detailed_description": "A highly optimized library for training massive Transformer language models at scale, implementing efficient model parallelism (tensor and pipeline) and data parallelism techniques.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "model_parallelism"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NVIDIA/Megatron-LM",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "transformer",
        "distributed-training",
        "llm",
        "parallelism"
      ],
      "id": 69
    },
    {
      "name": "Transformer Engine",
      "one_line_profile": "Library for accelerating Transformer models with FP8/FP4 precision",
      "detailed_description": "A library for accelerating Transformer models on NVIDIA GPUs, enabling 8-bit and 4-bit floating point (FP8 and FP4) precision for better performance and lower memory utilization in training and inference.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "acceleration",
        "mixed_precision"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NVIDIA/TransformerEngine",
      "help_website": [
        "https://docs.nvidia.com/deeplearning/transformer-engine"
      ],
      "license": "Apache-2.0",
      "tags": [
        "fp8",
        "transformer",
        "acceleration",
        "hopper"
      ],
      "id": 70
    },
    {
      "name": "Video Processing Framework",
      "one_line_profile": "Hardware-accelerated video processing library for Python",
      "detailed_description": "A set of Python bindings to C++ libraries providing full hardware acceleration for video decoding, encoding, and GPU-accelerated color space/pixel format conversions, useful for scientific video data pipelines.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "data_processing",
        "video_decoding"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/NVIDIA/VideoProcessingFramework",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "video-processing",
        "gpu-acceleration",
        "decoding",
        "encoding"
      ],
      "id": 71
    },
    {
      "name": "NVIDIA Apex",
      "one_line_profile": "PyTorch extension for mixed precision and distributed training",
      "detailed_description": "A library providing tools for easy mixed precision (AMP) and distributed training in PyTorch, including optimized fused optimizers and layers.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "acceleration",
        "mixed_precision",
        "distributed_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NVIDIA/apex",
      "help_website": [
        "https://nvidia.github.io/apex/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "mixed-precision",
        "pytorch",
        "distributed-training",
        "amp"
      ],
      "id": 72
    },
    {
      "name": "JaxPP",
      "one_line_profile": "JAX library for flexible MPMD pipeline parallelism",
      "detailed_description": "A library for JAX that enables flexible MPMD (Multiple Program Multiple Data) pipeline parallelism, specifically designed for large-scale LLM training workflows.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "pipeline_parallelism"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NVIDIA/jaxpp",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "jax",
        "pipeline-parallelism",
        "llm",
        "distributed"
      ],
      "id": 73
    },
    {
      "name": "nvidia-dlfw-inspect",
      "one_line_profile": "Debugging tool for LLM training convergence issues",
      "detailed_description": "A tool designed to facilitate debugging of convergence issues and testing of new algorithms for training LLMs using NVIDIA libraries like Transformer Engine, Megatron-LM, and NeMo.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "debugging",
        "model_convergence"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/NVIDIA/nvidia-dlfw-inspect",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "debugging",
        "llm",
        "convergence",
        "training-tools"
      ],
      "id": 74
    },
    {
      "name": "NVIDIA Warp",
      "one_line_profile": "Python framework for high-performance simulation and spatial computing",
      "detailed_description": "A Python framework that compiles Python functions to efficient kernel code for accelerated simulation, geometry processing, and data generation tasks on CPU and GPU.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "simulation",
        "data_generation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NVIDIA/warp",
      "help_website": [
        "https://nvidia.github.io/warp/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "simulation",
        "spatial-computing",
        "cuda",
        "physics"
      ],
      "id": 75
    },
    {
      "name": "Kaolin",
      "one_line_profile": "PyTorch library for 3D deep learning research",
      "detailed_description": "A PyTorch library aimed at accelerating 3D deep learning research, providing implementations of 3D modules, differentiable rendering, and data processing for 3D structures.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "scientific_modeling",
        "3d_deep_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NVIDIAGameWorks/kaolin",
      "help_website": [
        "https://kaolin.readthedocs.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "3d-vision",
        "pytorch",
        "differentiable-rendering",
        "geometry"
      ],
      "id": 76
    },
    {
      "name": "Fast-dLLM",
      "one_line_profile": "Acceleration library for Diffusion LLMs via KV cache",
      "detailed_description": "A training-free acceleration framework for Diffusion LLMs that enables KV cache and parallel decoding to improve inference speed and efficiency.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "acceleration",
        "inference_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NVlabs/Fast-dLLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "diffusion-models",
        "llm",
        "acceleration",
        "kv-cache"
      ],
      "id": 77
    },
    {
      "name": "tiny-cuda-nn",
      "one_line_profile": "High-performance C++/CUDA neural network framework",
      "detailed_description": "A lightning-fast C++/CUDA neural network framework, particularly optimized for multi-resolution hash encodings and NeRF applications.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "acceleration"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/NVlabs/tiny-cuda-nn",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "cuda",
        "neural-network",
        "nerf",
        "optimization"
      ],
      "id": 78
    },
    {
      "name": "mlstm_kernels",
      "one_line_profile": "Optimized kernels for mLSTM and Flash Linear Attention",
      "detailed_description": "A library providing Tiled Flash Linear Attention kernels for fast and efficient training and inference of mLSTM (matrix LSTM) models.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "acceleration",
        "kernel_optimization"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/NX-AI/mlstm_kernels",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "lstm",
        "attention",
        "cuda-kernels",
        "acceleration"
      ],
      "id": 79
    },
    {
      "name": "NoteDance Note",
      "one_line_profile": "Lightweight distributed training and machine learning library",
      "detailed_description": "A machine learning library supporting distributed training, deep learning, and reinforcement learning, compatible with TensorFlow and PyTorch.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "model_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NoteDance/Note",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "distributed-training",
        "reinforcement-learning",
        "deep-learning"
      ],
      "id": 80
    },
    {
      "name": "DisTrO",
      "one_line_profile": "Framework for decentralized distributed training over the internet",
      "detailed_description": "A framework enabling Distributed Training Over-The-Internet, allowing for decentralized model training across geographically distributed resources with optimized communication.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "decentralized_learning"
      ],
      "application_level": "library",
      "primary_language": null,
      "repo_url": "https://github.com/NousResearch/DisTrO",
      "help_website": [],
      "license": null,
      "tags": [
        "distributed-training",
        "decentralized",
        "internet-scale"
      ],
      "id": 81
    },
    {
      "name": "LiBai",
      "one_line_profile": "Toolbox for large-scale distributed parallel training based on OneFlow",
      "detailed_description": "A toolbox designed for large-scale distributed parallel training of deep learning models, built on top of the OneFlow framework.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "model_parallelism"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Oneflow-Inc/libai",
      "help_website": [
        "https://libai.readthedocs.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "distributed-training",
        "oneflow",
        "large-scale",
        "parallelism"
      ],
      "id": 82
    },
    {
      "name": "OneFlow",
      "one_line_profile": "Scalable and efficient distributed deep learning framework",
      "detailed_description": "A deep learning framework designed to be user-friendly, scalable, and efficient, with a strong focus on distributed training and performance optimization.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "distributed_training"
      ],
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/Oneflow-Inc/oneflow",
      "help_website": [
        "https://docs.oneflow.org"
      ],
      "license": "Apache-2.0",
      "tags": [
        "deep-learning-framework",
        "distributed-training",
        "compiler"
      ],
      "id": 83
    },
    {
      "name": "llm-finetune",
      "one_line_profile": "Framework for fine-tuning large language models",
      "detailed_description": "A framework for training and fine-tuning large language models, supporting LoRA and full parameter fine-tuning with YAML-based configuration.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_finetuning",
        "llm_training"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenCSGs/llm-finetune",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "finetuning",
        "lora",
        "training-framework"
      ],
      "id": 84
    },
    {
      "name": "FlashVSR",
      "one_line_profile": "Diffusion-based framework for real-time video super-resolution",
      "detailed_description": "An efficient one-step diffusion framework for streaming video super-resolution (VSR) utilizing locality-constrained sparse attention.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "data_processing",
        "super_resolution"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenImagingLab/FlashVSR",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "video-super-resolution",
        "diffusion-models",
        "image-processing"
      ],
      "id": 85
    },
    {
      "name": "CoLLiE",
      "one_line_profile": "Library for efficient collaborative training of LLMs",
      "detailed_description": "Collaborative Training of Large Language Models in an Efficient Way (CoLLiE) is a library designed to facilitate efficient and collaborative training processes for LLMs.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "llm_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenMOSS/CoLLiE",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "collaborative-training",
        "efficient-training"
      ],
      "id": 86
    },
    {
      "name": "MegatronApp",
      "one_line_profile": "Toolchain and utilities for distributed training with Megatron-LM",
      "detailed_description": "A toolchain built around Megatron-LM to facilitate distributed training workflows, providing additional utilities and ease of use.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "workflow_management"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenSQZ/MegatronApp",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "megatron-lm",
        "distributed-training",
        "toolchain"
      ],
      "id": 87
    },
    {
      "name": "Safe-RLHF",
      "one_line_profile": "Framework for constrained value alignment via Safe RLHF",
      "detailed_description": "A library for Safe Reinforcement Learning from Human Feedback (RLHF), focusing on constrained value alignment to ensure safety in LLM training.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_alignment",
        "rlhf"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/PKU-Alignment/safe-rlhf",
      "help_website": [
        "https://safe-rlhf.readthedocs.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "rlhf",
        "alignment",
        "safety",
        "llm"
      ],
      "id": 88
    },
    {
      "name": "LLM-boost-recognition",
      "one_line_profile": "OCR and voice recognition module with LLM-based correction",
      "detailed_description": "A module for converting documents and audio into text using OCR and voice recognition, enhanced with LLM-based correction and GPU acceleration, suitable for scientific data digitization.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "data_processing",
        "ocr",
        "speech_recognition"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/PStarH/LLM-boost-recognition",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "ocr",
        "voice-recognition",
        "llm-correction",
        "data-ingestion"
      ],
      "id": 89
    },
    {
      "name": "PARL",
      "one_line_profile": "High-performance distributed training framework for Reinforcement Learning",
      "detailed_description": "A flexible and high-performance framework for reinforcement learning (RL) training, supporting distributed architecture and massive parallel environment simulation.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "distributed_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/PaddlePaddle/PARL",
      "help_website": [
        "https://parl.readthedocs.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "reinforcement-learning",
        "distributed-training",
        "paddlepaddle"
      ],
      "id": 90
    },
    {
      "name": "PaddlePaddle",
      "one_line_profile": "Industrial-grade distributed deep learning framework",
      "detailed_description": "PArallel Distributed Deep LEarning (PaddlePaddle) is a comprehensive machine learning framework supporting high-performance distributed training and cross-platform deployment.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "distributed_training"
      ],
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/PaddlePaddle/Paddle",
      "help_website": [
        "https://www.paddlepaddle.org.cn/documentation/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "deep-learning-framework",
        "distributed-training",
        "industrial"
      ],
      "id": 91
    },
    {
      "name": "Chimera",
      "one_line_profile": "Library for bidirectional pipeline parallelism in large-scale training",
      "detailed_description": "An implementation of Chimera, a bidirectional pipeline parallelism scheme for efficiently training large-scale models with improved resource utilization.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "pipeline_parallelism"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ParCIS/Chimera",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "pipeline-parallelism",
        "distributed-training",
        "efficiency"
      ],
      "id": 92
    },
    {
      "name": "PERSIA",
      "one_line_profile": "Distributed training framework for deep learning recommendation models",
      "detailed_description": "A high-performance distributed framework specifically designed for training deep learning recommendation models, leveraging hybrid data/model parallelism.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "recommendation_systems"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/PersiaML/PERSIA",
      "help_website": [
        "https://persiaml-tutorials.readthedocs.io"
      ],
      "license": "MIT",
      "tags": [
        "recommendation-models",
        "distributed-training",
        "rust",
        "pytorch"
      ],
      "id": 93
    },
    {
      "name": "Search-R1",
      "one_line_profile": "RL training framework for reasoning and search engine interleaved LLMs",
      "detailed_description": "An efficient and scalable Reinforcement Learning (RL) training framework designed for Large Language Models that interleave reasoning with search engine calls.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "reinforcement_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/PeterGriffinJin/Search-R1",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rl",
        "llm",
        "reasoning",
        "search-engine"
      ],
      "id": 94
    },
    {
      "name": "NeuralSolvers",
      "one_line_profile": "PyTorch library for solving PDEs and inverse problems using neural networks",
      "detailed_description": "A library implementing physics-informed neural networks (PINNs) and other neural solvers for partial differential equations (PDEs) and inverse problems.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "scientific_modeling",
        "pde_solver"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Photon-AI-Research/NeuralSolvers",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pinn",
        "pde",
        "physics-informed",
        "inverse-problems"
      ],
      "id": 95
    },
    {
      "name": "sparse-frontier",
      "one_line_profile": "Framework for evaluating training-free sparse attention in LLMs",
      "detailed_description": "An evaluation framework designed to analyze and benchmark training-free sparse attention mechanisms in Large Language Models.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "attention_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/PiotrNawrot/sparse-frontier",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "sparse-attention",
        "llm",
        "evaluation",
        "benchmarking"
      ],
      "id": 96
    },
    {
      "name": "OpenDiloco",
      "one_line_profile": "Framework for globally distributed low-communication model training",
      "detailed_description": "An open-source framework implementing the DiLoCo algorithm for globally distributed training with low communication overhead, enabling training across disconnected clusters.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "decentralized_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/PrimeIntellect-ai/OpenDiloco",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "distributed-training",
        "low-bandwidth",
        "diloco"
      ],
      "id": 97
    },
    {
      "name": "Prime DiLoCo",
      "one_line_profile": "Framework for globally distributed AI model training over the internet",
      "detailed_description": "A framework designed for efficient, globally distributed training of AI models across geographically dispersed devices connected via the internet, enabling decentralized computing resources to collaborate on model training.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "decentralized_computing"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/PrimeIntellect-ai/prime-diloco",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "distributed-training",
        "decentralized-ai",
        "diloco"
      ],
      "id": 98
    },
    {
      "name": "Prime Iroh",
      "one_line_profile": "Asynchronous P2P communication backend for decentralized pipeline parallelism",
      "detailed_description": "A Rust-based asynchronous peer-to-peer (P2P) communication backend designed to support decentralized pipeline parallelism in distributed AI training workflows.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_communication",
        "pipeline_parallelism"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/PrimeIntellect-ai/prime-iroh",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "p2p",
        "distributed-training",
        "communication-backend"
      ],
      "id": 99
    },
    {
      "name": "Prime vLLM",
      "one_line_profile": "Modified vLLM for pipeline parallelism over public networks",
      "detailed_description": "A modification of the vLLM library tailored to execute pipeline parallelism across public networks, facilitating distributed inference and training setups in decentralized environments.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "inference_optimization",
        "pipeline_parallelism"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/PrimeIntellect-ai/prime-vllm",
      "help_website": [],
      "license": null,
      "tags": [
        "vllm",
        "pipeline-parallelism",
        "distributed-inference"
      ],
      "id": 100
    },
    {
      "name": "MAPLE",
      "one_line_profile": "Hardware-software co-design for asynchronous memory parallelism",
      "detailed_description": "A hardware-software co-design framework that enables programs to perform long-latency memory accesses asynchronously from the core, reducing pipeline stalls and increasing memory level parallelism (MLP).",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "memory_optimization",
        "hardware_acceleration"
      ],
      "application_level": "solver",
      "primary_language": "C",
      "repo_url": "https://github.com/PrincetonUniversity/maple",
      "help_website": [],
      "license": null,
      "tags": [
        "memory-parallelism",
        "hardware-software-codesign",
        "performance-optimization"
      ],
      "id": 101
    },
    {
      "name": "TensorNet",
      "one_line_profile": "Distributed training framework optimized for large-scale sparse data",
      "detailed_description": "A C++ based distributed training framework built on TensorFlow, specifically optimized for handling large-scale sparse data typical in recommendation systems and advertising scenarios.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "sparse_data_processing"
      ],
      "application_level": "framework",
      "primary_language": "C++",
      "repo_url": "https://github.com/Qihoo360/tensornet",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "distributed-training",
        "sparse-data",
        "tensorflow"
      ],
      "id": 102
    },
    {
      "name": "QizNLP",
      "one_line_profile": "TensorFlow framework for rapid NLP task execution",
      "detailed_description": "A TensorFlow-based framework designed for quickly running various Natural Language Processing (NLP) tasks such as classification, sequence labeling, matching, and generation, with support for distributed training.",
      "domains": [
        "AI3"
      ],
      "subtask_category": [
        "nlp_tasks",
        "model_training"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/Qznan/QizNLP",
      "help_website": [],
      "license": "MPL-2.0",
      "tags": [
        "nlp",
        "tensorflow",
        "distributed-training"
      ],
      "id": 103
    },
    {
      "name": "Reinforce-Ada",
      "one_line_profile": "Adaptive sampling framework for Reinforce-style LLM post-training",
      "detailed_description": "A framework implementing adaptive sampling strategies for Reinforce-style post-training of Large Language Models (LLMs), aiming to improve alignment and performance efficiency.",
      "domains": [
        "AI3"
      ],
      "subtask_category": [
        "post_training",
        "rlhf",
        "adaptive_sampling"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/RLHFlow/Reinforce-Ada",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "reinforcement-learning",
        "post-training"
      ],
      "id": 104
    },
    {
      "name": "Flash-Sparse-Attention",
      "one_line_profile": "Efficient implementations of Native Sparse Attention",
      "detailed_description": "A library providing efficient implementations of Native Sparse Attention mechanisms, designed to accelerate Transformer models by reducing computational complexity and memory usage.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "attention_mechanism",
        "model_acceleration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Relaxed-System-Lab/Flash-Sparse-Attention",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "sparse-attention",
        "optimization",
        "transformer"
      ],
      "id": 105
    },
    {
      "name": "Flash Attention v2 RDNA3",
      "one_line_profile": "Flash Attention v2 implementation for ROCm/RDNA3 GPUs",
      "detailed_description": "A minimal implementation of Flash Attention v2 optimized for AMD RDNA3 GPUs using ROCm, enabling accelerated inference and training for models like Stable Diffusion in specific hardware environments.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "hardware_acceleration",
        "attention_mechanism"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Repeerc/flash-attention-v2-RDNA3-minimal",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rocm",
        "flash-attention",
        "rdna3"
      ],
      "id": 106
    },
    {
      "name": "AI Attention Acceleration",
      "one_line_profile": "Pre-compiled attention acceleration packages for Windows AI workflows",
      "detailed_description": "A utility repository providing pre-compiled acceleration libraries (xformers, Flash Attention, SageAttention) to enhance the efficiency of AI workflows like ComfyUI and Fooocus on Windows systems.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "workflow_optimization",
        "hardware_acceleration"
      ],
      "application_level": "utility",
      "primary_language": "Python",
      "repo_url": "https://github.com/Rogala/AI_Attention",
      "help_website": [],
      "license": "Unlicense",
      "tags": [
        "windows",
        "acceleration",
        "xformers"
      ],
      "id": 107
    },
    {
      "name": "FastCkpt",
      "one_line_profile": "Rematerialization-aware gradient checkpointing for memory efficiency",
      "detailed_description": "A Python package implementing rematerialization-aware gradient checkpointing, optimizing memory usage during the training of deep learning models by selectively recomputing activations.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "memory_optimization",
        "gradient_checkpointing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/RulinShao/FastCkpt",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "gradient-checkpointing",
        "memory-optimization",
        "training"
      ],
      "id": 108
    },
    {
      "name": "LightSeq (DistFlashAttn)",
      "one_line_profile": "Distributed memory-efficient attention for long-context LLM training",
      "detailed_description": "The official repository for DistFlashAttn, providing distributed memory-efficient attention mechanisms to support the training of Large Language Models (LLMs) with long context windows.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "attention_mechanism",
        "distributed_training",
        "long_context"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/RulinShao/LightSeq",
      "help_website": [],
      "license": null,
      "tags": [
        "distributed-attention",
        "llm",
        "long-context"
      ],
      "id": 109
    },
    {
      "name": "MagiAttention",
      "one_line_profile": "Distributed attention for linear scalability in ultra-long context training",
      "detailed_description": "A distributed attention mechanism designed to achieve linear scalability, enabling the training of models with ultra-long contexts and heterogeneous data.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "attention_mechanism",
        "distributed_training",
        "scalability"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/SandAI-org/MagiAttention",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "distributed-attention",
        "long-context",
        "linear-scalability"
      ],
      "id": 110
    },
    {
      "name": "DiffEqGPU.jl",
      "one_line_profile": "GPU-acceleration routines for DifferentialEquations.jl",
      "detailed_description": "A library providing GPU-acceleration routines for the DifferentialEquations.jl package, facilitating high-performance scientific machine learning (SciML) and differential equation solving on GPUs.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "scientific_computing",
        "differential_equations",
        "gpu_acceleration"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/SciML/DiffEqGPU.jl",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "sciml",
        "differential-equations",
        "gpu",
        "julia"
      ],
      "id": 111
    },
    {
      "name": "Fast-LLM",
      "one_line_profile": "Framework for accelerating LLM training",
      "detailed_description": "A framework developed by ServiceNow Research to accelerate the training of Large Language Models (LLMs), optimizing performance and resource utilization.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "performance_optimization"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/ServiceNow/Fast-LLM",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "llm",
        "training-acceleration",
        "servicenow"
      ],
      "id": 112
    },
    {
      "name": "LLM-Training",
      "one_line_profile": "Distributed training framework for LLMs powered by Lightning",
      "detailed_description": "A distributed training framework built on PyTorch Lightning, designed to facilitate the training of Large Language Models (LLMs) with distributed computing capabilities.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "llm"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/ShinoharaHare/LLM-Training",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "pytorch-lightning",
        "distributed-training",
        "llm"
      ],
      "id": 113
    },
    {
      "name": "simpleT5",
      "one_line_profile": "Wrapper for quick T5 model training using PyTorch Lightning",
      "detailed_description": "A simplified wrapper library built on top of PyTorch Lightning and Hugging Face Transformers, designed to streamline and accelerate the training process for T5 (Text-to-Text Transfer Transformer) models.",
      "domains": [
        "AI3"
      ],
      "subtask_category": [
        "model_training",
        "nlp"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Shivanandroy/simpleT5",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "t5",
        "pytorch-lightning",
        "transformers",
        "nlp"
      ],
      "id": 114
    },
    {
      "name": "DRPO (Dynamic Alignment Optimization)",
      "one_line_profile": "Tuning-free approach for self-alignment with prompt optimization",
      "detailed_description": "An implementation of Dynamic Rewarding with Prompt Optimization (DRPO), a tuning-free framework that enables Large Language Models (LLMs) to iteratively self-improve and design optimal alignment instructions without additional training.",
      "domains": [
        "AI3"
      ],
      "subtask_category": [
        "model_alignment",
        "prompt_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Singla17/dynamic-alignment-optimization",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "alignment",
        "prompt-optimization",
        "llm"
      ],
      "id": 115
    },
    {
      "name": "Tiresias",
      "one_line_profile": "GPU cluster manager for distributed deep learning training",
      "detailed_description": "A GPU cluster scheduling and management system designed specifically for distributed deep learning training workloads, optimizing resource allocation and job scheduling.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "cluster_management",
        "resource_scheduling"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/SymbioticLab/Tiresias",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "gpu-scheduling",
        "distributed-training",
        "cluster-manager"
      ],
      "id": 116
    },
    {
      "name": "Aparapi",
      "one_line_profile": "Framework for executing native Java and Scala code on the GPU",
      "detailed_description": "A framework that allows developers to write native Java or Scala code and execute it on the GPU by converting bytecode to OpenCL, facilitating parallel computing and acceleration.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "parallel_computing",
        "gpu_acceleration"
      ],
      "application_level": "framework",
      "primary_language": "Java",
      "repo_url": "https://github.com/Syncleus/aparapi",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "gpu",
        "java",
        "opencl",
        "parallel-computing"
      ],
      "id": 117
    },
    {
      "name": "Slime",
      "one_line_profile": "LLM post-training framework for RL Scaling",
      "detailed_description": "A post-training framework for Large Language Models (LLMs) focused on Reinforcement Learning (RL) scaling, providing tools and methods to enhance model performance through RL techniques.",
      "domains": [
        "AI3"
      ],
      "subtask_category": [
        "post_training",
        "reinforcement_learning",
        "rlhf"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/THUDM/slime",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "rlhf",
        "post-training"
      ],
      "id": 118
    },
    {
      "name": "MARTI",
      "one_line_profile": "LLM-based Multi-Agent Reinforced Training and Inference Framework",
      "detailed_description": "A framework designed for multi-agent reinforced training and inference using Large Language Models (LLMs), developed by Tsinghua University C3I.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "training_framework",
        "multi_agent_reinforcement_learning"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/TsinghuaC3I/MARTI",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "multi-agent",
        "reinforcement-learning"
      ],
      "id": 119
    },
    {
      "name": "Mandheling",
      "one_line_profile": "Mixed-Precision On-Device DNN Training with DSP Offloading",
      "detailed_description": "An open-source implementation of the Mandheling system (MobiCom'2022) for efficient on-device deep neural network training using DSP offloading and mixed-precision techniques.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "on_device_training",
        "acceleration"
      ],
      "application_level": "solver",
      "primary_language": "C",
      "repo_url": "https://github.com/UbiquitousLearning/Mandheling-DSP-Training",
      "help_website": [],
      "license": null,
      "tags": [
        "on-device-learning",
        "dsp-offloading",
        "mixed-precision"
      ],
      "id": 120
    },
    {
      "name": "PP-Schedule-Visualization",
      "one_line_profile": "Pipeline Parallelism Emulation and Visualization Tool",
      "detailed_description": "A tool for emulating and visualizing the scheduling of pipeline parallelism in distributed deep learning training, aiding in performance analysis and optimization.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "visualization",
        "performance_analysis"
      ],
      "application_level": "tool",
      "primary_language": "Python",
      "repo_url": "https://github.com/Victarry/PP-Schedule-Visualization",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pipeline-parallelism",
        "visualization",
        "distributed-training"
      ],
      "id": 121
    },
    {
      "name": "blitzdg",
      "one_line_profile": "Parallel Discontinuous Galerkin (DG) Solver",
      "detailed_description": "An open-source parallel discontinuous Galerkin (DG) solver implementation for partial differential equations, utilizing blitz++ for tensor manipulation and MPI for distributed parallelism.",
      "domains": [
        "Scientific Computing"
      ],
      "subtask_category": [
        "pde_solver",
        "simulation"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/WQCG/blitzdg",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "pde",
        "discontinuous-galerkin",
        "mpi"
      ],
      "id": 122
    },
    {
      "name": "SRUM",
      "one_line_profile": "Fine-Grained Self-Rewarding Framework for Multimodal Models",
      "detailed_description": "A post-training framework that creates a cost-effective, self-iterative optimization loop for unified multimodal models, implementing fine-grained self-rewarding mechanisms.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "post_training",
        "optimization"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/WayneJin0918/SRUM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "multimodal",
        "self-rewarding",
        "post-training"
      ],
      "id": 123
    },
    {
      "name": "tf-recsys",
      "one_line_profile": "TensorFlow-based Collaborative Filtering Library",
      "detailed_description": "A library implementing collaborative filtering models (SVD, SVD++) using TensorFlow to utilize GPU acceleration for recommendation system tasks.",
      "domains": [
        "AI3"
      ],
      "subtask_category": [
        "recommendation",
        "model_implementation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/WindQAQ/tf-recsys",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "recsys",
        "tensorflow",
        "collaborative-filtering"
      ],
      "id": 124
    },
    {
      "name": "Mochi-Full-Finetuner",
      "one_line_profile": "Full Finetuning Tool for Mochi Model",
      "detailed_description": "A specialized tool for performing full parameter finetuning of the Mochi video generation model using FSDP (Fully Sharded Data Parallel) and Context Parallelism.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_finetuning",
        "video_generation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Yaofang-Liu/Mochi-Full-Finetuner",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "finetuning",
        "fsdp",
        "mochi"
      ],
      "id": 125
    },
    {
      "name": "ray-skorch",
      "one_line_profile": "Distributed Skorch on Ray Train",
      "detailed_description": "A library that integrates Skorch (a scikit-learn compatible neural network library) with Ray Train to enable distributed training capabilities.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "framework_integration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Yard1/ray-skorch",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ray",
        "skorch",
        "distributed"
      ],
      "id": 126
    },
    {
      "name": "OptimalShardedDataParallel",
      "one_line_profile": "Automated Parallel Training System (OSDP)",
      "detailed_description": "An automated parallel training system that combines the advantages of data and model parallelism, optimizing sharded data parallel strategies (IJCAI 2023).",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "parallelism_optimization"
      ],
      "application_level": "system",
      "primary_language": "Python",
      "repo_url": "https://github.com/Youhe-Jiang/IJCAI2023-OptimalShardedDataParallel",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "distributed-training",
        "sharding",
        "parallelism"
      ],
      "id": 127
    },
    {
      "name": "GNNAdvisor",
      "one_line_profile": "Adaptive Runtime System for GNN Acceleration",
      "detailed_description": "An adaptive and efficient runtime system designed for accelerating Graph Neural Networks (GNNs) on GPUs (OSDI'21 Artifact).",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "gnn_acceleration",
        "runtime_system"
      ],
      "application_level": "system",
      "primary_language": "Cuda",
      "repo_url": "https://github.com/YukeWang96/GNNAdvisor_OSDI21",
      "help_website": [],
      "license": null,
      "tags": [
        "gnn",
        "gpu-acceleration",
        "runtime"
      ],
      "id": 128
    },
    {
      "name": "train-CLIP",
      "one_line_profile": "PyTorch Lightning Solution for Training CLIP",
      "detailed_description": "A comprehensive PyTorch Lightning-based framework for training OpenAI's CLIP models from scratch, providing a reusable training pipeline.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "multimodal"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/Zasder3/train-CLIP",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "clip",
        "pytorch-lightning",
        "training-pipeline"
      ],
      "id": 129
    },
    {
      "name": "TSDS",
      "one_line_profile": "Task-Specific Data Selection Framework",
      "detailed_description": "An optimal-transport based framework for selecting domain-specific and task-specific training data to improve LLM finetuning and instruction tuning efficiency.",
      "domains": [
        "AI3"
      ],
      "subtask_category": [
        "data_selection",
        "finetuning_optimization"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/ZifanL/TSDS",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "data-selection",
        "llm",
        "optimal-transport"
      ],
      "id": 130
    },
    {
      "name": "ProteinWorkshop",
      "one_line_profile": "Protein Representation Learning Benchmarking Framework",
      "detailed_description": "A comprehensive framework for benchmarking protein representation learning, including datasets, pre-training models, and downstream task utilities (ICLR 2024).",
      "domains": [
        "AI4S",
        "Biology"
      ],
      "subtask_category": [
        "protein_modeling",
        "benchmarking"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/a-r-j/ProteinWorkshop",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "protein-structure",
        "representation-learning",
        "benchmark"
      ],
      "id": 131
    },
    {
      "name": "MinimalGPT",
      "one_line_profile": "Minimalist GPT Training and Inference Framework",
      "detailed_description": "A concise and adaptable framework implemented in Keras/TensorFlow for constructing, training, and finetuning GPT models, serving as a lightweight tool for research and education.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "inference"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/abhaskumarsinha/MinimalGPT",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gpt",
        "keras",
        "educational-framework"
      ],
      "id": 132
    },
    {
      "name": "synth.",
      "one_line_profile": "Synthetic Instruction Generation Framework",
      "detailed_description": "A framework designed for generating synthetic instructions to enhance Large Language Model (LLM) training datasets.",
      "domains": [
        "AI3"
      ],
      "subtask_category": [
        "data_generation",
        "synthetic_data"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/aboros98/synth",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "synthetic-data",
        "instruction-tuning",
        "llm"
      ],
      "id": 133
    },
    {
      "name": "AidLearning",
      "one_line_profile": "Mobile AIOT Development and Inference Platform",
      "detailed_description": "A powerful AIOT development platform supporting Linux on Android, enabling CPU+GPU+NPU accelerated inference and development directly on mobile devices.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "edge_inference",
        "mobile_ai"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/aidlearning/AidLearning-FrameWork",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "mobile-ai",
        "edge-computing",
        "inference-acceleration"
      ],
      "id": 134
    },
    {
      "name": "minPP",
      "one_line_profile": "Minimalist Pipeline Parallelism Library",
      "detailed_description": "A lightweight implementation of pipeline parallelism for distributed deep learning training, designed for simplicity and ease of integration.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "pipeline_parallelism"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ailzhang/minPP",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pipeline-parallelism",
        "distributed-training",
        "minimalist"
      ],
      "id": 135
    },
    {
      "name": "rllib-fast-serve",
      "one_line_profile": "Lightweight Inference Tool for Ray RLlib Policies",
      "detailed_description": "A set of tools to export policies trained with Ray RLlib for lightweight and fast inference, decoupling inference from the heavy Ray dependencies.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "inference_serving",
        "reinforcement_learning"
      ],
      "application_level": "tool",
      "primary_language": "Python",
      "repo_url": "https://github.com/airboxlab/rllib-fast-serve",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rllib",
        "inference",
        "serving"
      ],
      "id": 136
    },
    {
      "name": "IceVision",
      "one_line_profile": "Agnostic Computer Vision Framework",
      "detailed_description": "A computer vision framework that is agnostic to the underlying training library, allowing pluggable integration with Fastai, PyTorch Lightning, and others.",
      "domains": [
        "AI3"
      ],
      "subtask_category": [
        "computer_vision",
        "training_framework"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/airctic/icevision",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "computer-vision",
        "framework-agnostic",
        "object-detection"
      ],
      "id": 137
    },
    {
      "name": "full_stack_transformer",
      "one_line_profile": "End-to-End Transformer Training and Serving Library",
      "detailed_description": "A PyTorch library designed for the complete lifecycle of transformer models, including training, inference, and serving.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "serving"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/alexeykarnachev/full_stack_transformer",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "transformer",
        "serving",
        "pytorch"
      ],
      "id": 138
    },
    {
      "name": "flashattention2-custom-mask",
      "one_line_profile": "FlashAttention2 with Custom Mask Support",
      "detailed_description": "A Triton-based implementation of FlashAttention2 that extends the original kernel to support custom attention masks, useful for specialized attention patterns.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "acceleration_kernel",
        "attention_mechanism"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/alexzhang13/flashattention2-custom-mask",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "flash-attention",
        "triton",
        "custom-mask"
      ],
      "id": 139
    },
    {
      "name": "EasyParallelLibrary",
      "one_line_profile": "Distributed Model Training Framework (EPL)",
      "detailed_description": "A general and efficient deep learning framework developed by Alibaba for distributed model training, simplifying the implementation of parallel strategies.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "parallelism"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/alibaba/EasyParallelLibrary",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "distributed-training",
        "deep-learning",
        "alibaba"
      ],
      "id": 140
    },
    {
      "name": "Euler",
      "one_line_profile": "Distributed Graph Deep Learning Framework",
      "detailed_description": "A large-scale distributed graph learning framework developed by Alibaba, designed to handle massive graph data for deep learning tasks.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "graph_learning",
        "distributed_training"
      ],
      "application_level": "framework",
      "primary_language": "C++",
      "repo_url": "https://github.com/alibaba/euler",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "graph-neural-networks",
        "distributed-system",
        "graph-learning"
      ],
      "id": 141
    },
    {
      "name": "oss-connector-for-ai-ml",
      "one_line_profile": "OSS Storage Connector for AI/ML Frameworks",
      "detailed_description": "A high-performance Python library for connecting major AI/ML frameworks (like PyTorch, TensorFlow) directly with Alibaba Cloud OSS storage for efficient data loading.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "data_io",
        "storage_integration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aliyun/oss-connector-for-ai-ml",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "data-loading",
        "oss",
        "cloud-storage"
      ],
      "id": 142
    },
    {
      "name": "AlpaServe",
      "one_line_profile": "Statistical Multiplexing for Deep Learning Serving",
      "detailed_description": "A system for deep learning serving that utilizes statistical multiplexing with model parallelism to improve efficiency and throughput (OSDI 23).",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_serving",
        "distributed_inference"
      ],
      "application_level": "system",
      "primary_language": "Python",
      "repo_url": "https://github.com/alpa-projects/mms",
      "help_website": [],
      "license": null,
      "tags": [
        "serving",
        "model-parallelism",
        "multiplexing"
      ],
      "id": 143
    },
    {
      "name": "alpaka",
      "one_line_profile": "Abstraction Library for Parallel Kernel Acceleration",
      "detailed_description": "A C++ header-only library that provides an abstraction layer for parallel kernel acceleration across various hardware backends (CUDA, HIP, OpenMP, etc.).",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "acceleration",
        "hpc"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/alpaka-group/alpaka",
      "help_website": [],
      "license": "MPL-2.0",
      "tags": [
        "hpc",
        "kernel-acceleration",
        "portability"
      ],
      "id": 144
    },
    {
      "name": "jvp_flash_attention",
      "one_line_profile": "Flash Attention with Second-Order Derivative Support",
      "detailed_description": "A Flash Attention Triton kernel implementation that supports Jacobian-Vector Products (JVP), enabling second-order derivative computations.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "acceleration_kernel",
        "differentiation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/amorehead/jvp_flash_attention",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "flash-attention",
        "triton",
        "second-order-derivatives"
      ],
      "id": 145
    },
    {
      "name": "Galaxy_simulation",
      "one_line_profile": "GPU-Accelerated N-Body Galaxy Simulation",
      "detailed_description": "An N-body simulation tool utilizing GPU acceleration to simulate galaxies, galaxy collisions, and expanding universes.",
      "domains": [
        "Scientific Computing",
        "Astrophysics"
      ],
      "subtask_category": [
        "simulation",
        "n_body"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/angeluriot/Galaxy_simulation",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "n-body",
        "galaxy-simulation",
        "gpu"
      ],
      "id": 146
    },
    {
      "name": "LLMOPT",
      "one_line_profile": "Comprehensive resources and framework for LLM training and inference optimization",
      "detailed_description": "A project offering a model, dataset, training framework, and inference code to enable users to utilize and optimize Large Language Models (LLMs), specifically focusing on instruction tuning and optimization.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": "model_training",
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/antgroup/LLMOPT",
      "help_website": [],
      "license": null,
      "tags": [
        "llm",
        "training-framework",
        "optimization",
        "instruction-tuning"
      ],
      "id": 147
    },
    {
      "name": "GLake",
      "one_line_profile": "GPU memory management and IO transmission optimization library",
      "detailed_description": "A library designed to optimize GPU memory management and I/O transmission for deep learning training, aiming to improve efficiency and performance in distributed environments.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": "infrastructure_optimization",
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/antgroup/glake",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "gpu-optimization",
        "memory-management",
        "distributed-training"
      ],
      "id": 148
    },
    {
      "name": "Apache MXNet",
      "one_line_profile": "Flexible and efficient distributed deep learning framework",
      "detailed_description": "A lightweight, portable, and flexible distributed deep learning framework that supports dynamic, mutation-aware dataflow dependency scheduling. It supports multiple languages including Python, R, and Julia.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": "model_training",
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/apache/mxnet",
      "help_website": [
        "https://mxnet.apache.org"
      ],
      "license": "Apache-2.0",
      "tags": [
        "deep-learning",
        "distributed-training",
        "framework"
      ],
      "id": 149
    },
    {
      "name": "Apache SINGA",
      "one_line_profile": "Distributed deep learning platform",
      "detailed_description": "A distributed deep learning platform for training big deep learning models over large datasets, designed to be intuitive and usable.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": "model_training",
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/apache/singa",
      "help_website": [
        "http://singa.apache.org"
      ],
      "license": "Apache-2.0",
      "tags": [
        "deep-learning",
        "distributed-system",
        "training-platform"
      ],
      "id": 150
    },
    {
      "name": "TensorFlow for macOS",
      "one_line_profile": "TensorFlow accelerated for macOS using ML Compute",
      "detailed_description": "A version of TensorFlow optimized for macOS 11.0+ that leverages Apple's ML Compute framework for hardware-accelerated training and inference on Mac devices.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": "model_training",
      "application_level": "solver",
      "primary_language": "Shell",
      "repo_url": "https://github.com/apple/tensorflow_macos",
      "help_website": [],
      "license": null,
      "tags": [
        "tensorflow",
        "macos",
        "acceleration",
        "ml-compute"
      ],
      "id": 151
    },
    {
      "name": "LLM-Inference-Bench",
      "one_line_profile": "Benchmark suite for LLM inference performance",
      "detailed_description": "A benchmarking tool designed to evaluate the inference performance of Large Language Models, likely developed by Argonne National Laboratory.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": "performance_analysis",
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/argonne-lcf/LLM-Inference-Bench",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "llm",
        "benchmarking",
        "inference",
        "hpc"
      ],
      "id": 152
    },
    {
      "name": "Arkalos",
      "one_line_profile": "Python framework for data analysis and LLM training",
      "detailed_description": "A Python framework designed to simplify data analysis, building data apps, and training Large Language Models (LLMs) with an elegant syntax.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": "model_training",
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/arkaloscom/arkalos",
      "help_website": [],
      "license": null,
      "tags": [
        "llm-training",
        "data-analysis",
        "framework"
      ],
      "id": 153
    },
    {
      "name": "SiLLM",
      "one_line_profile": "LLM training and inference framework for Apple Silicon",
      "detailed_description": "A framework that simplifies the process of training and running Large Language Models (LLMs) on Apple Silicon devices by leveraging the MLX framework.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": "model_training",
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/armbues/SiLLM",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "apple-silicon",
        "mlx",
        "llm",
        "training"
      ],
      "id": 154
    },
    {
      "name": "StringZilla",
      "one_line_profile": "High-performance string processing library leveraging SIMD",
      "detailed_description": "A library providing up to 100x faster string operations (search, hashing, sorting, edit distances) for C, C++, Python, and other languages by leveraging NEON, AVX2, AVX-512, and other hardware acceleration. Useful for bioinformatics and text processing.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": "data_processing",
      "application_level": "library",
      "primary_language": "C",
      "repo_url": "https://github.com/ashvardanian/StringZilla",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "simd",
        "string-processing",
        "hpc",
        "acceleration"
      ],
      "id": 155
    },
    {
      "name": "Fine-Tune Codebase",
      "one_line_profile": "Tool for fine-tuning LLMs on codebases",
      "detailed_description": "A scalable and efficient tool for fine-tuning large language models (LLMs) specifically on codebases. It supports LoRA, mixed precision training, and quantization.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": "model_training",
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ayminovitch/fine-tune-codebase",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "fine-tuning",
        "lora",
        "code-llm"
      ],
      "id": 156
    },
    {
      "name": "Piper Plus",
      "one_line_profile": "Enhanced Piper TTS training framework",
      "detailed_description": "An enhanced version of Piper TTS supporting multi-GPU training, Japanese language support, and quality improvements. It serves as a training framework for text-to-speech models.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": "model_training",
      "application_level": "framework",
      "primary_language": "C++",
      "repo_url": "https://github.com/ayutaz/piper-plus",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "tts",
        "training-framework",
        "multi-gpu"
      ],
      "id": 157
    },
    {
      "name": "Distributed Llama",
      "one_line_profile": "Distributed LLM inference on home devices",
      "detailed_description": "A tool for distributed LLM inference that allows connecting multiple devices into a cluster to accelerate inference, effectively creating a distributed computing environment for LLMs.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": "inference",
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/b4rtaz/distributed-llama",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "distributed-inference",
        "llm",
        "edge-computing"
      ],
      "id": 158
    },
    {
      "name": "tf2-gradient-checkpointing",
      "one_line_profile": "Gradient checkpointing implementation for TensorFlow 2 eager execution",
      "detailed_description": "A Python library providing gradient checkpointing functionality for TensorFlow 2 in eager mode, enabling the training of larger models by trading compute for memory.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "memory_optimization",
        "model_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/davisyoshida/tf2-gradient-checkpointing",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "tensorflow",
        "gradient-checkpointing",
        "memory-optimization"
      ],
      "id": 159
    },
    {
      "name": "AdaSplash",
      "one_line_profile": "Adaptive Sparse Flash Attention implementation",
      "detailed_description": "A library implementing Adaptive Sparse Flash Attention (Flash Entmax Attention), designed to optimize attention mechanisms in transformer models for better efficiency.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "attention_mechanism",
        "model_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/deep-spin/adasplash",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "attention",
        "sparse-attention",
        "transformer"
      ],
      "id": 160
    },
    {
      "name": "3FS",
      "one_line_profile": "High-performance distributed file system for AI training",
      "detailed_description": "A high-performance distributed file system specifically designed to address the I/O challenges of large-scale AI training and inference workloads, providing high throughput and low latency.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "data_storage",
        "distributed_training"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/deepseek-ai/3FS",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "file-system",
        "distributed-storage",
        "ai-infrastructure"
      ],
      "id": 161
    },
    {
      "name": "DualPipe",
      "one_line_profile": "Bidirectional pipeline parallelism for LLM training",
      "detailed_description": "A bidirectional pipeline parallelism algorithm designed to overlap computation and communication during the training of large language models like DeepSeek V3/R1.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "pipeline_parallelism"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/deepseek-ai/DualPipe",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pipeline-parallelism",
        "llm-training",
        "distributed-computing"
      ],
      "id": 162
    },
    {
      "name": "FlashMLA",
      "one_line_profile": "Efficient Multi-head Latent Attention Kernels",
      "detailed_description": "A library providing optimized kernels for Multi-head Latent Attention (MLA), designed to accelerate inference and training of large language models on GPUs.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "kernel_optimization",
        "attention_mechanism"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/deepseek-ai/FlashMLA",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "cuda-kernels",
        "attention",
        "optimization"
      ],
      "id": 163
    },
    {
      "name": "DeepSpeed",
      "one_line_profile": "Deep learning optimization library for distributed training",
      "detailed_description": "A deep learning optimization library that enables massive-scale distributed training and inference with high efficiency, ease of use, and effectiveness, supporting techniques like ZeRO, pipeline parallelism, and 3D parallelism.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "model_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/deepspeedai/DeepSpeed",
      "help_website": [
        "https://www.deepspeed.ai/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "distributed-training",
        "zero-redundancy",
        "optimization"
      ],
      "id": 164
    },
    {
      "name": "DeepSpeed-MII",
      "one_line_profile": "Low-latency and high-throughput inference library",
      "detailed_description": "Model Implementations for Inference (MII) is a library powered by DeepSpeed that provides low-latency and high-throughput inference for deep learning models.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "inference_optimization",
        "model_serving"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/deepspeedai/DeepSpeed-MII",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "inference",
        "latency-optimization",
        "deepspeed"
      ],
      "id": 165
    },
    {
      "name": "Determined",
      "one_line_profile": "Open-source deep learning training platform",
      "detailed_description": "A platform that simplifies distributed training, hyperparameter tuning, experiment tracking, and resource management for deep learning models, supporting PyTorch and TensorFlow.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "training_platform",
        "experiment_management",
        "hyperparameter_tuning"
      ],
      "application_level": "platform",
      "primary_language": "Go",
      "repo_url": "https://github.com/determined-ai/determined",
      "help_website": [
        "https://docs.determined.ai/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "mlops",
        "distributed-training",
        "experiment-tracking"
      ],
      "id": 166
    },
    {
      "name": "DeepDist",
      "one_line_profile": "Distributed Deep Learning on Apache Spark",
      "detailed_description": "A tool that enables distributed deep learning training on Apache Spark clusters, allowing users to leverage Spark's data processing capabilities for deep learning workflows.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "spark_integration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/dirkneumann/deepdist",
      "help_website": [],
      "license": null,
      "tags": [
        "spark",
        "distributed-learning",
        "deep-learning"
      ],
      "id": 167
    },
    {
      "name": "Paracel",
      "one_line_profile": "Distributed training framework with parameter server",
      "detailed_description": "A distributed training framework that implements the parameter server architecture to scale machine learning model training across multiple nodes.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "parameter_server"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/douban/paracel",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "parameter-server",
        "distributed-ml",
        "framework"
      ],
      "id": 168
    },
    {
      "name": "Dragonfly",
      "one_line_profile": "P2P-based data distribution and acceleration system",
      "detailed_description": "An intelligent P2P based image and file distribution system that provides efficient data distribution and acceleration for cloud native and AI workloads.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "data_distribution",
        "infrastructure_acceleration"
      ],
      "application_level": "solver",
      "primary_language": "Go",
      "repo_url": "https://github.com/dragonflyoss/dragonfly",
      "help_website": [
        "https://d7y.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "p2p",
        "file-distribution",
        "container-acceleration"
      ],
      "id": 169
    },
    {
      "name": "VisDrone-dataset-python-toolkit",
      "one_line_profile": "Toolkit for VisDrone aerial object detection dataset",
      "detailed_description": "A PyTorch toolkit designed for the VisDrone dataset, providing training pipelines, inference tools, and format converters for aerial object detection tasks.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "data_processing",
        "object_detection",
        "training_pipeline"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/dronefreak/VisDrone-dataset-python-toolkit",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "visdrone",
        "object-detection",
        "aerial-imagery"
      ],
      "id": 170
    },
    {
      "name": "js-pytorch",
      "one_line_profile": "JavaScript deep learning library with GPU acceleration",
      "detailed_description": "A JavaScript library that provides a PyTorch-like API for deep learning, supporting GPU acceleration for training and inference in JavaScript environments.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "inference"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/eduardoleao052/js-pytorch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "javascript",
        "deep-learning",
        "gpu-acceleration"
      ],
      "id": 171
    },
    {
      "name": "jax-flash-attn2",
      "one_line_profile": "Flash Attention 2.0 implementation for JAX",
      "detailed_description": "A flexible and efficient implementation of Flash Attention 2.0 for JAX, supporting multiple backends (GPU/TPU) and enabling efficient attention computation in JAX-based models.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "attention_mechanism",
        "kernel_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/erfanzar/jax-flash-attn2",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "jax",
        "flash-attention",
        "optimization"
      ],
      "id": 172
    },
    {
      "name": "fsdp_optimizers",
      "one_line_profile": "Optimizer support for PyTorch FSDP",
      "detailed_description": "A library that provides support for using various optimizers with PyTorch's Fully Sharded Data Parallel (FSDP) training, facilitating distributed training workflows.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ethansmith2000/fsdp_optimizers",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "fsdp",
        "pytorch",
        "distributed-training"
      ],
      "id": 173
    },
    {
      "name": "nvblox_ros1",
      "one_line_profile": "ROS1 wrappers for nvblox GPU volumetric mapping",
      "detailed_description": "ROS1 wrappers for nvblox, enabling GPU-accelerated volumetric mapping for robotics and perception tasks, facilitating real-time 3D reconstruction.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "volumetric_mapping",
        "robotics_perception"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/ethz-asl/nvblox_ros1",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ros",
        "mapping",
        "gpu-acceleration"
      ],
      "id": 174
    },
    {
      "name": "Expert Kit",
      "one_line_profile": "Expert Parallelism foundation for MoE inference",
      "detailed_description": "A library providing efficient implementations of Expert Parallelism (EP) for Mixture-of-Experts (MoE) model inference on heterogeneous hardware.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "inference_optimization",
        "moe_parallelism"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/expert-kit/expert-kit",
      "help_website": [],
      "license": "LGPL-3.0",
      "tags": [
        "moe",
        "inference",
        "parallelism"
      ],
      "id": 175
    },
    {
      "name": "spaCy",
      "one_line_profile": "Industrial-strength Natural Language Processing library",
      "detailed_description": "A comprehensive library for Natural Language Processing (NLP) in Python, featuring pre-trained models, efficient tokenization, and support for deep learning integration.",
      "domains": [
        "AI3"
      ],
      "subtask_category": [
        "nlp",
        "text_processing",
        "model_inference"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/explosion/spaCy",
      "help_website": [
        "https://spacy.io/"
      ],
      "license": "MIT",
      "tags": [
        "nlp",
        "text-processing",
        "linguistics"
      ],
      "id": 176
    },
    {
      "name": "Dynolog",
      "one_line_profile": "Telemetry daemon for AI performance monitoring",
      "detailed_description": "A telemetry daemon for performance monitoring and tracing of AI workloads, capable of exporting metrics from system components and integrating with PyTorch for distributed training analysis.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "performance_monitoring",
        "profiling"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/facebookincubator/dynolog",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "telemetry",
        "profiling",
        "gpu-monitoring"
      ],
      "id": 177
    },
    {
      "name": "Flashy",
      "one_line_profile": "Framework for deep learning training loops",
      "detailed_description": "A lightweight framework for writing deep learning training loops that handles checkpointing, logging, and distributed training setup, allowing researchers to focus on model design.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "training_framework",
        "experiment_management"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/flashy",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "training-loop",
        "pytorch",
        "boilerplate"
      ],
      "id": 178
    },
    {
      "name": "moolib",
      "one_line_profile": "Library for distributed ML training with PyTorch",
      "detailed_description": "A C++ and Python library designed to facilitate distributed machine learning training with PyTorch, offering efficient communication primitives and data loading.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "communication_primitives"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/facebookresearch/moolib",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "distributed-ml",
        "rpc",
        "pytorch"
      ],
      "id": 179
    },
    {
      "name": "Stochastic Gradient Push",
      "one_line_profile": "Stochastic Gradient Push algorithm for distributed learning",
      "detailed_description": "An implementation of the Stochastic Gradient Push algorithm, enabling decentralized distributed deep learning on directed graphs.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "optimization_algorithm"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/stochastic_gradient_push",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "distributed-learning",
        "gossip-algorithm",
        "decentralized"
      ],
      "id": 180
    },
    {
      "name": "flash-bidirectional-linear-attention",
      "one_line_profile": "Triton implementation of bi-directional linear attention",
      "detailed_description": "A library providing efficient Triton-based implementations of bi-directional (non-causal) linear attention mechanisms for transformer models.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "attention_mechanism",
        "kernel_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/fla-org/flash-bidirectional-linear-attention",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "linear-attention",
        "triton",
        "optimization"
      ],
      "id": 181
    },
    {
      "name": "flash-linear-attention",
      "one_line_profile": "Efficient linear attention model implementations",
      "detailed_description": "A library containing efficient implementations of state-of-the-art linear attention models, optimized for speed and memory usage in sequence modeling tasks.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "attention_mechanism",
        "model_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/fla-org/flash-linear-attention",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "linear-attention",
        "efficient-transformers",
        "cuda"
      ],
      "id": 182
    },
    {
      "name": "Flash Sparse Attention",
      "one_line_profile": "Fast and memory-efficient sparse attention mechanism for Transformer training",
      "detailed_description": "A library providing trainable, fast, and memory-efficient sparse attention kernels, designed to accelerate the training of large-scale Transformer models by optimizing GPU memory usage and computation speed.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_acceleration",
        "training_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/flash-algo/flash-sparse-attention",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "sparse-attention",
        "transformer",
        "acceleration",
        "gpu-kernels"
      ],
      "id": 183
    },
    {
      "name": "FlashInfer",
      "one_line_profile": "High-performance kernel library for LLM serving and inference acceleration",
      "detailed_description": "A kernel library designed to accelerate Large Language Model (LLM) serving. It provides optimized CUDA kernels for attention mechanisms and other operations critical for efficient inference in scientific and general AI applications.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "inference_acceleration",
        "model_serving"
      ],
      "application_level": "library",
      "primary_language": "Cuda",
      "repo_url": "https://github.com/flashinfer-ai/flashinfer",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-serving",
        "cuda-kernels",
        "inference",
        "acceleration"
      ],
      "id": 184
    },
    {
      "name": "FlexFlow",
      "one_line_profile": "Deep learning framework for automatic distributed training parallelization",
      "detailed_description": "A deep learning framework that automatically discovers fast parallelization strategies for distributed deep neural network training, optimizing performance across heterogeneous cluster environments.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "parallelization_strategy"
      ],
      "application_level": "framework",
      "primary_language": "C++",
      "repo_url": "https://github.com/flexflow/flexflow-train",
      "help_website": [
        "https://flexflow.ai"
      ],
      "license": "Apache-2.0",
      "tags": [
        "distributed-training",
        "parallelization",
        "deep-learning"
      ],
      "id": 185
    },
    {
      "name": "Deep Learning on Flink",
      "one_line_profile": "Integration of deep learning frameworks with Apache Flink for distributed training",
      "detailed_description": "A framework that integrates Flink with deep learning libraries (TensorFlow, PyTorch) to enable distributed deep learning training and inference workflows on Flink clusters.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "workflow_integration"
      ],
      "application_level": "framework",
      "primary_language": "Java",
      "repo_url": "https://github.com/flink-extended/dl-on-flink",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "flink",
        "distributed-training",
        "tensorflow",
        "pytorch"
      ],
      "id": 186
    },
    {
      "name": "FMS Acceleration",
      "one_line_profile": "Acceleration libraries for fine-tuning foundation models",
      "detailed_description": "A collection of libraries designed to work with fms-hf-tuning to accelerate the fine-tuning and training processes of large foundation models.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_acceleration",
        "fine_tuning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/foundation-model-stack/fms-acceleration",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "fine-tuning",
        "acceleration",
        "foundation-models"
      ],
      "id": 187
    },
    {
      "name": "FMS FSDP",
      "one_line_profile": "Efficient foundation model training using PyTorch FSDP",
      "detailed_description": "A library for efficiently pre-training and training foundation models leveraging native PyTorch features, specifically Fully Sharded Data Parallel (FSDP) and Flash Attention v2.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "fsdp"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/foundation-model-stack/fms-fsdp",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "fsdp",
        "distributed-training",
        "pytorch"
      ],
      "id": 188
    },
    {
      "name": "FMS HF Tuning",
      "one_line_profile": "Tuning recipes for foundation models with HuggingFace and FSDP",
      "detailed_description": "A collection of tuning recipes and utilities integrating HuggingFace SFTTrainer with PyTorch FSDP for efficient model fine-tuning.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "fine_tuning",
        "training_recipes"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/foundation-model-stack/fms-hf-tuning",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "fine-tuning",
        "huggingface",
        "fsdp"
      ],
      "id": 189
    },
    {
      "name": "Foundation Model Stack",
      "one_line_profile": "Comprehensive stack for foundation model development and training",
      "detailed_description": "A collection of components for the development, training, tuning, and inference of foundation models, leveraging native PyTorch components for modularity and performance.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_development",
        "training_stack"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/foundation-model-stack/foundation-model-stack",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "foundation-models",
        "training-stack",
        "pytorch"
      ],
      "id": 190
    },
    {
      "name": "SkipPipe",
      "one_line_profile": "Framework for pipelined training of LLMs in heterogeneous networks",
      "detailed_description": "A framework implementing partial and reordered pipelining strategies to optimize the training of Large Language Models (LLMs) across heterogeneous network environments.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "pipeline_parallelism"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/gensyn-ai/skippipe",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pipeline-parallelism",
        "llm-training",
        "heterogeneous-computing"
      ],
      "id": 191
    },
    {
      "name": "Knowledge Distillation Toolkit",
      "one_line_profile": "Toolkit for knowledge distillation based on PyTorch Lightning",
      "detailed_description": "A toolkit designed to facilitate knowledge distillation experiments and implementation, built on top of PyTorch and PyTorch Lightning.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "knowledge_distillation",
        "model_compression"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/georgian-io/Knowledge-Distillation-Toolkit",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "knowledge-distillation",
        "pytorch-lightning"
      ],
      "id": 192
    },
    {
      "name": "Llama2 LoRA Fine-tuning",
      "one_line_profile": "Scripts for fine-tuning Llama2 using DeepSpeed and LoRA",
      "detailed_description": "A utility repository providing scripts and configurations for fine-tuning Llama2 models using DeepSpeed for distributed training and LoRA for parameter-efficient adaptation.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "fine_tuning",
        "parameter_efficient_tuning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/git-cloner/llama2-lora-fine-tuning",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llama2",
        "lora",
        "deepspeed",
        "fine-tuning"
      ],
      "id": 193
    },
    {
      "name": "Pax",
      "one_line_profile": "Jax-based framework for training large scale machine learning models",
      "detailed_description": "A machine learning framework built on Jax, designed for advanced configuration and parallelization to train large-scale models with high flop utilization.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "training_framework",
        "large_scale_training"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/google/paxml",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "jax",
        "large-scale-training",
        "pax"
      ],
      "id": 194
    },
    {
      "name": "GPGPU-Sim",
      "one_line_profile": "Cycle-level simulator for NVIDIA GPUs",
      "detailed_description": "A detailed simulation model of contemporary NVIDIA GPUs running CUDA and/or OpenCL workloads, used for computer architecture research and performance analysis of GPU-accelerated applications.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "hardware_simulation",
        "performance_analysis"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/gpgpu-sim/gpgpu-sim_distribution",
      "help_website": [
        "http://www.gpgpu-sim.org/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "gpu-simulator",
        "cuda",
        "computer-architecture"
      ],
      "id": 195
    },
    {
      "name": "PyGraphistry",
      "one_line_profile": "GPU-accelerated library for big graph visualization and analysis",
      "detailed_description": "A Python library to load, shape, embed, and explore large graphs using GPU acceleration, facilitating visual graph analysis for scientific and data science applications.",
      "domains": [
        "AI3"
      ],
      "subtask_category": [
        "scientific_visualization",
        "graph_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/graphistry/pygraphistry",
      "help_website": [
        "https://github.com/graphistry/pygraphistry"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "graph-visualization",
        "gpu-acceleration",
        "data-analysis"
      ],
      "id": 196
    },
    {
      "name": "Koifish",
      "one_line_profile": "C++ framework for efficient LLM training and fine-tuning",
      "detailed_description": "A C++ framework designed for the efficient training and fine-tuning of Large Language Models (LLMs), offering performance optimizations for model development.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "training_framework",
        "fine_tuning"
      ],
      "application_level": "framework",
      "primary_language": "C++",
      "repo_url": "https://github.com/gruai/koifish",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-training",
        "c++",
        "fine-tuning"
      ],
      "id": 197
    },
    {
      "name": "gLLM",
      "one_line_profile": "Global balanced pipeline parallelism system for distributed LLM serving",
      "detailed_description": "A system for distributed Large Language Model (LLM) serving that implements global balanced pipeline parallelism and token throttling to optimize throughput and latency.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_serving",
        "pipeline_parallelism"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/gty111/gLLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-serving",
        "distributed-system",
        "pipeline-parallelism"
      ],
      "id": 198
    },
    {
      "name": "BERT Pre-training",
      "one_line_profile": "Multi-GPU BERT pre-training implementation",
      "detailed_description": "A utility implementation for multi-GPU pre-training of BERT models on a single machine without requiring Horovod, facilitating model training workflows.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "pre_training"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/guotong1988/BERT-pre-training",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "bert",
        "pre-training",
        "multi-gpu"
      ],
      "id": 199
    },
    {
      "name": "BERT Classifier",
      "one_line_profile": "General text classifier based on BERT with multi-GPU support",
      "detailed_description": "A general-purpose text classification tool based on BERT, supporting multi-process data processing and multi-GPU parallel training.",
      "domains": [
        "AI3"
      ],
      "subtask_category": [
        "text_classification",
        "model_training"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/guoyaohua/BERT-Classifier",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "bert",
        "text-classification",
        "nlp"
      ],
      "id": 200
    },
    {
      "name": "LvLLM",
      "one_line_profile": "NUMA-aware extension of vLLM for efficient CPU/GPU hybrid inference",
      "detailed_description": "A specialized extension of vLLM that optimizes for NUMA architectures, enabling efficient use of CPU and memory resources alongside GPUs for hybrid inference of large models, particularly MOEs.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "inference_acceleration",
        "hybrid_inference"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/guqiong96/Lvllm",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "vllm",
        "numa",
        "inference",
        "moe"
      ],
      "id": 201
    },
    {
      "name": "H2O-3",
      "one_line_profile": "Distributed and scalable open-source machine learning platform",
      "detailed_description": "A distributed, in-memory machine learning platform that provides scalable implementations of various algorithms (GBM, GLM, Deep Learning, etc.) and AutoML capabilities for data analysis and modeling.",
      "domains": [
        "AI3"
      ],
      "subtask_category": [
        "machine_learning_platform",
        "automl"
      ],
      "application_level": "platform",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/h2oai/h2o-3",
      "help_website": [
        "http://docs.h2o.ai"
      ],
      "license": "Apache-2.0",
      "tags": [
        "machine-learning",
        "distributed-computing",
        "automl"
      ],
      "id": 202
    },
    {
      "name": "APPy",
      "one_line_profile": "Annotated Parallelism for Python to GPU compiler",
      "detailed_description": "A framework that enables users to annotate Python loops and tensor expressions with compiler directives to automatically generate and compile optimized GPU kernels.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "code_acceleration",
        "compilation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/habanero-lab/APPy",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gpu-compilation",
        "parallelism",
        "python"
      ],
      "id": 203
    },
    {
      "name": "XReflection",
      "one_line_profile": "Toolbox for single-image reflection removal",
      "detailed_description": "A toolbox providing state-of-the-art solutions for single-image reflection removal (SIRR), including training and inference pipelines with multi-GPU/TPU support.",
      "domains": [
        "AI3"
      ],
      "subtask_category": [
        "image_processing",
        "image_restoration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/hainuo-wang/XReflection",
      "help_website": [],
      "license": null,
      "tags": [
        "reflection-removal",
        "image-processing",
        "computer-vision"
      ],
      "id": 204
    },
    {
      "name": "EfficientNetV2-pytorch",
      "one_line_profile": "PyTorch implementation of EfficientNetV2 with pretrained models",
      "detailed_description": "A PyTorch implementation of the EfficientNetV2 architecture, including pretrained models, facilitating its use in computer vision tasks.",
      "domains": [
        "AI3"
      ],
      "subtask_category": [
        "model_implementation",
        "computer_vision"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/hankyul2/EfficientNetV2-pytorch",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "efficientnetv2",
        "pytorch",
        "computer-vision"
      ],
      "id": 205
    },
    {
      "name": "FastVideo",
      "one_line_profile": "Framework for accelerated video generation inference and post-training",
      "detailed_description": "A unified framework designed to accelerate the inference and post-training processes for video generation models.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "video_generation",
        "inference_acceleration"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/hao-ai-lab/FastVideo",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "video-generation",
        "acceleration",
        "inference"
      ],
      "id": 206
    },
    {
      "name": "HeAT",
      "one_line_profile": "Distributed tensor and machine learning framework with GPU/MPI acceleration",
      "detailed_description": "A Python framework for distributed tensor processing and machine learning, leveraging GPU and MPI acceleration to handle large-scale scientific data analysis.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_computing",
        "tensor_processing"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/helmholtz-analytics/heat",
      "help_website": [
        "https://heat.readthedocs.io"
      ],
      "license": "MIT",
      "tags": [
        "distributed-tensors",
        "mpi",
        "gpu",
        "scientific-computing"
      ],
      "id": 207
    },
    {
      "name": "VodaScheduler",
      "one_line_profile": "GPU scheduler for elastic distributed deep learning workloads",
      "detailed_description": "A GPU scheduler designed for Kubernetes clusters to manage elastic and distributed deep learning workloads efficiently.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "resource_scheduling",
        "cluster_management"
      ],
      "application_level": "platform",
      "primary_language": "Go",
      "repo_url": "https://github.com/heyfey/vodascheduler",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "gpu-scheduler",
        "kubernetes",
        "distributed-training"
      ],
      "id": 208
    },
    {
      "name": "Higgsfield",
      "one_line_profile": "Fault-tolerant GPU orchestration and training framework for large-scale models",
      "detailed_description": "Higgsfield is a machine learning framework and GPU orchestration platform designed for training models with billions to trillions of parameters. It focuses on fault tolerance and high scalability, enabling efficient distributed training on large clusters.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "orchestration"
      ],
      "application_level": "platform",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/higgsfield-ai/higgsfield",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "gpu-orchestration",
        "large-model-training",
        "fault-tolerance"
      ],
      "id": 209
    },
    {
      "name": "EasyR1",
      "one_line_profile": "Efficient multi-modality reinforcement learning training framework",
      "detailed_description": "EasyR1 is a scalable and efficient reinforcement learning (RL) training framework based on veRL. It supports multi-modality training, making it suitable for complex RLHF (Reinforcement Learning from Human Feedback) tasks and large model fine-tuning.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "fine_tuning"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/hiyouga/EasyR1",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rlhf",
        "multi-modality",
        "verl"
      ],
      "id": 210
    },
    {
      "name": "mipnerf_pl",
      "one_line_profile": "PyTorch Lightning implementation of Mip-NeRF for 3D reconstruction",
      "detailed_description": "An unofficial but widely used PyTorch Lightning implementation of Mip-NeRF, a method for high-quality 3D reconstruction and rendering from 2D images. It serves as a solver for inverse rendering tasks.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "3d_reconstruction",
        "rendering"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/hjxwhy/mipnerf_pl",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nerf",
        "pytorch-lightning",
        "3d-rendering"
      ],
      "id": 211
    },
    {
      "name": "ptlflow",
      "one_line_profile": "Unified interface for Optical Flow models using PyTorch Lightning",
      "detailed_description": "Ptlflow is a library that provides a unified interface for various optical flow models, scripts for training and inference, and pretrained weights. It facilitates the application of optical flow estimation in scientific video analysis.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "optical_flow",
        "computer_vision"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/hmorimitsu/ptlflow",
      "help_website": [
        "https://ptlflow.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "optical-flow",
        "pytorch-lightning",
        "motion-estimation"
      ],
      "id": 212
    },
    {
      "name": "Horovod",
      "one_line_profile": "Distributed deep learning training framework",
      "detailed_description": "Horovod is a distributed deep learning training framework for TensorFlow, Keras, PyTorch, and Apache MXNet. It simplifies the process of scaling single-GPU training scripts to run across many GPUs and nodes using MPI.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/horovod/horovod",
      "help_website": [
        "https://horovod.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "distributed-training",
        "mpi",
        "deep-learning"
      ],
      "id": 213
    },
    {
      "name": "CachedEmbedding",
      "one_line_profile": "Memory-efficient embedding solution for DLRM training",
      "detailed_description": "CachedEmbedding is a library designed to optimize memory usage during the training of Deep Learning Recommendation Models (DLRM). It utilizes ColossalAI to enable efficient handling of large embedding tables.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "memory_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/hpcaitech/CachedEmbedding",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "dlrm",
        "embedding",
        "colossalai"
      ],
      "id": 214
    },
    {
      "name": "Colossal-AI",
      "one_line_profile": "Unified deep learning system for large-scale model training and inference",
      "detailed_description": "Colossal-AI is a comprehensive system designed to make large AI models cheaper, faster, and more accessible. It provides a collection of parallel components (data, pipeline, tensor, sequence parallelism) and optimization techniques for distributed training.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "large_model_optimization"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/hpcaitech/ColossalAI",
      "help_website": [
        "https://colossalai.org/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "distributed-system",
        "parallelism",
        "large-models"
      ],
      "id": 215
    },
    {
      "name": "ColossalAI-Benchmark",
      "one_line_profile": "Benchmarking suite for ColossalAI performance",
      "detailed_description": "A utility tool for benchmarking the performance of models trained with ColossalAI, helping researchers and engineers evaluate training throughput and efficiency.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_analysis"
      ],
      "application_level": "utility",
      "primary_language": "Python",
      "repo_url": "https://github.com/hpcaitech/ColossalAI-Benchmark",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "benchmark",
        "performance",
        "colossalai"
      ],
      "id": 216
    },
    {
      "name": "ColossalAI-Platform-CLI",
      "one_line_profile": "Command-line interface for ColossalAI Platform",
      "detailed_description": "The official CLI tool for interacting with the ColossalAI Platform, enabling users to manage jobs, datasets, and compute resources for large-scale AI training.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "job_management",
        "platform_interface"
      ],
      "application_level": "utility",
      "primary_language": "Python",
      "repo_url": "https://github.com/hpcaitech/ColossalAI-Platform-CLI",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "cli",
        "colossalai",
        "cloud-platform"
      ],
      "id": 217
    },
    {
      "name": "Titans",
      "one_line_profile": "Model zoo for ColossalAI",
      "detailed_description": "Titans is a collection of model implementations built with ColossalAI. It serves as a model zoo library, providing ready-to-use model definitions compatible with ColossalAI's distributed training features.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_definition",
        "model_zoo"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/hpcaitech/Titans",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "model-zoo",
        "colossalai",
        "transformers"
      ],
      "id": 218
    },
    {
      "name": "Accelerate",
      "one_line_profile": "Library for abstracting training loops across hardware configurations",
      "detailed_description": "Accelerate is a library that enables the same PyTorch code to run across any distributed configuration (single GPU, multi-GPU, TPU, MPS) without significant code changes. It handles device placement and distributed communication.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "hardware_abstraction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/huggingface/accelerate",
      "help_website": [
        "https://huggingface.co/docs/accelerate/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "pytorch",
        "distributed",
        "mixed-precision"
      ],
      "id": 219
    },
    {
      "name": "Nanotron",
      "one_line_profile": "Minimalistic 3D-parallelism training library for LLMs",
      "detailed_description": "Nanotron is a library designed for training Large Language Models (LLMs) using 3D parallelism (Tensor, Pipeline, and Data Parallelism). It focuses on simplicity and efficiency for scaling model training.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "llm_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/huggingface/nanotron",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "3d-parallelism",
        "llm",
        "training"
      ],
      "id": 220
    },
    {
      "name": "Optimum",
      "one_line_profile": "Hardware optimization toolkit for Transformers",
      "detailed_description": "Optimum is an extension of Hugging Face Transformers that provides tools for training and inference optimization on specific hardware architectures (Intel, ONNX Runtime, AWS Trainium, etc.).",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "training_optimization",
        "inference_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/huggingface/optimum",
      "help_website": [
        "https://huggingface.co/docs/optimum/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "hardware-acceleration",
        "optimization",
        "transformers"
      ],
      "id": 221
    },
    {
      "name": "Transformers",
      "one_line_profile": "State-of-the-art machine learning model library",
      "detailed_description": "Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models. It supports a wide range of tasks across text, vision, and audio, serving as a foundational library for AI4S modeling.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "model_definition"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/huggingface/transformers",
      "help_website": [
        "https://huggingface.co/docs/transformers/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "nlp",
        "computer-vision",
        "pretrained-models"
      ],
      "id": 222
    },
    {
      "name": "Machin",
      "one_line_profile": "Reinforcement learning library for PyTorch",
      "detailed_description": "Machin is a reinforcement learning library designed for PyTorch, implementing various algorithms like DQN, DDPG, PPO, SAC, etc. It provides a framework for developing and testing RL agents.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "reinforcement_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/iffiX/machin",
      "help_website": [
        "https://machin.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "reinforcement-learning",
        "pytorch",
        "rl-algorithms"
      ],
      "id": 223
    },
    {
      "name": "self_supervised",
      "one_line_profile": "PyTorch Lightning implementation of self-supervised algorithms",
      "detailed_description": "A library providing implementations of various self-supervised learning algorithms (SimCLR, BYOL, SwAV, etc.) using PyTorch Lightning, facilitating representation learning research.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "self_supervised_learning",
        "representation_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/imbue-ai/self_supervised",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "self-supervised",
        "contrastive-learning",
        "pytorch-lightning"
      ],
      "id": 224
    },
    {
      "name": "Multipack",
      "one_line_profile": "Distributed sampler for padding-free LLM training",
      "detailed_description": "Multipack is a specialized distributed sampler designed to optimize Large Language Model (LLM) training by enabling fast, padding-free data loading, improving training efficiency.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "data_loading",
        "training_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/imoneoi/multipack",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "data-sampler",
        "efficiency"
      ],
      "id": 225
    },
    {
      "name": "Asystem-Awex",
      "one_line_profile": "High-performance RL training-inference synchronization framework",
      "detailed_description": "A framework designed for high-performance Reinforcement Learning (RL) workflows, specifically focusing on the rapid synchronization of weights between training and inference processes.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "distributed_system"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/inclusionAI/asystem-awex",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "reinforcement-learning",
        "synchronization",
        "inference"
      ],
      "id": 226
    },
    {
      "name": "Intel MLSL",
      "one_line_profile": "Intel Machine Learning Scaling Library",
      "detailed_description": "Intel MLSL is a library providing efficient implementations of communication patterns used in deep learning, designed to scale training across multiple nodes and clusters.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "communication_optimization"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/intel/MLSL",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "hpc",
        "distributed-learning",
        "intel"
      ],
      "id": 227
    },
    {
      "name": "Intel Extension for DeepSpeed",
      "one_line_profile": "Intel GPU (XPU) support for DeepSpeed",
      "detailed_description": "An extension that enables DeepSpeed features on Intel GPU (XPU) devices, allowing for optimized distributed training on Intel hardware architectures.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "hardware_acceleration",
        "distributed_training"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/intel/intel-extension-for-deepspeed",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "deepspeed",
        "intel-xpu",
        "acceleration"
      ],
      "id": 228
    },
    {
      "name": "Intel Technology Enabling for OpenShift",
      "one_line_profile": "Full-stack AI solution for OpenShift on Intel hardware",
      "detailed_description": "A comprehensive solution for provisioning and managing Intel AI accelerators and software stacks on the OpenShift platform, facilitating enterprise AI workloads like LLM training and inference.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "infrastructure_provisioning",
        "platform_deployment"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/intel/intel-technology-enabling-for-openshift",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "openshift",
        "intel",
        "ai-infrastructure"
      ],
      "id": 229
    },
    {
      "name": "IPEX-LLM",
      "one_line_profile": "LLM inference and finetuning acceleration on Intel XPU",
      "detailed_description": "IPEX-LLM is a library for accelerating local Large Language Model (LLM) inference and fine-tuning on Intel hardware (CPUs, GPUs, NPUs). It integrates with popular ecosystems like HuggingFace, vLLM, and LlamaIndex.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "inference_acceleration",
        "fine_tuning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/intel/ipex-llm",
      "help_website": [
        "https://ipex-llm.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "intel",
        "llm",
        "acceleration"
      ],
      "id": 230
    },
    {
      "name": "Nauta",
      "one_line_profile": "Distributed deep learning platform for Intel Xeon systems",
      "detailed_description": "Nauta is a multi-user, distributed computing environment designed for running deep learning model training experiments on Intel Xeon Scalable processor-based systems using Kubernetes.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "cluster_management",
        "job_scheduling"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/intel/nauta",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "kubernetes",
        "deep-learning-platform",
        "intel"
      ],
      "id": 231
    },
    {
      "name": "DLRover",
      "one_line_profile": "Automatic distributed deep learning system",
      "detailed_description": "DLRover is an automatic distributed deep learning system that provides fault tolerance, auto-scaling, and resource optimization for training jobs on Kubernetes clusters.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "resource_management"
      ],
      "application_level": "system",
      "primary_language": "Python",
      "repo_url": "https://github.com/intelligent-machine-learning/dlrover",
      "help_website": [
        "https://dlrover.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "distributed-training",
        "kubernetes",
        "auto-scaling"
      ],
      "id": 232
    },
    {
      "name": "NeuralNetworks",
      "one_line_profile": "Java deep learning library with GPU acceleration",
      "detailed_description": "A Java-based library for implementing deep learning algorithms and neural networks, featuring GPU acceleration support. It serves as a solver/library for Java-based scientific computing environments.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "deep_learning"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/ivan-vasilev/neuralnetworks",
      "help_website": [],
      "license": "Unknown",
      "tags": [
        "java",
        "deep-learning",
        "gpu"
      ],
      "id": 233
    },
    {
      "name": "Alpaca-LoRA-RLHF-PyTorch",
      "one_line_profile": "Pipeline for finetuning Alpaca with LoRA and RLHF",
      "detailed_description": "A complete pipeline implementation for fine-tuning Alpaca-style Large Language Models using Low-Rank Adaptation (LoRA) and Reinforcement Learning with Human Feedback (RLHF) on consumer hardware.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "fine_tuning",
        "rlhf"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/jackaduma/Alpaca-LoRA-RLHF-PyTorch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "lora",
        "rlhf",
        "alpaca"
      ],
      "id": 234
    },
    {
      "name": "ChatGLM-LoRA-RLHF-PyTorch",
      "one_line_profile": "Pipeline for finetuning ChatGLM with LoRA and RLHF",
      "detailed_description": "A complete pipeline implementation for fine-tuning ChatGLM models using LoRA and RLHF. It provides the necessary workflows to adapt ChatGLM for specific tasks using reinforcement learning.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "fine_tuning",
        "rlhf"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/jackaduma/ChatGLM-LoRA-RLHF-PyTorch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "chatglm",
        "lora",
        "rlhf"
      ],
      "id": 235
    },
    {
      "name": "robo-gym",
      "one_line_profile": "Open source toolkit for Distributed Deep Reinforcement Learning on real and simulated robots",
      "detailed_description": "A toolkit that enables distributed Deep Reinforcement Learning on both real and simulated robots, facilitating the development and training of robotic control policies.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "robotics_simulation"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/jr-robotics/robo-gym",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "robotics",
        "reinforcement-learning",
        "simulation",
        "distributed-training"
      ],
      "id": 236
    },
    {
      "name": "torch_ACA",
      "one_line_profile": "Adaptive Checkpoint Adjoint (ACA) method for gradient estimation in neural ODEs",
      "detailed_description": "Implementation of the Adaptive Checkpoint Adjoint method to improve gradient estimation accuracy and memory efficiency in Neural Ordinary Differential Equations (Neural ODEs).",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "gradient_estimation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/juntang-zhuang/torch_ACA",
      "help_website": [],
      "license": null,
      "tags": [
        "neural-ode",
        "gradient-estimation",
        "pytorch"
      ],
      "id": 237
    },
    {
      "name": "pykaldi2",
      "one_line_profile": "Speech processing toolkit based on Kaldi and PyTorch",
      "detailed_description": "A toolkit for speech recognition and processing that integrates Kaldi's efficiency with PyTorch's flexibility, supporting various speech modeling tasks.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "speech_processing",
        "model_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/jzlianglu/pykaldi2",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "speech-recognition",
        "kaldi",
        "pytorch"
      ],
      "id": 238
    },
    {
      "name": "point2vec",
      "one_line_profile": "Self-Supervised Representation Learning on Point Clouds",
      "detailed_description": "A library for self-supervised representation learning on 3D point clouds, enabling effective feature extraction for downstream 3D vision tasks.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "representation_learning",
        "3d_vision"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/kabouzeid/point2vec",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "point-cloud",
        "self-supervised-learning",
        "3d-vision"
      ],
      "id": 239
    },
    {
      "name": "FlashAttention.C",
      "one_line_profile": "Raw CUDA C implementation of Flash Attention",
      "detailed_description": "A highly optimized CUDA C implementation of the Flash Attention algorithm, designed to accelerate attention mechanisms in transformer models.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "acceleration",
        "kernel_optimization"
      ],
      "application_level": "library",
      "primary_language": "Cuda",
      "repo_url": "https://github.com/kilianhae/FlashAttention.C",
      "help_website": [],
      "license": null,
      "tags": [
        "cuda",
        "flash-attention",
        "optimization"
      ],
      "id": 240
    },
    {
      "name": "hydra",
      "one_line_profile": "Execution framework for multi-task model parallelism",
      "detailed_description": "A framework enabling the training of large models with multi-task model parallelism, offering linear speedups for multi-GPU execution.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "model_parallelism"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/knagrecha/hydra",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "distributed-training",
        "model-parallelism",
        "multi-task"
      ],
      "id": 241
    },
    {
      "name": "Unity_GPUNearestNeighbor",
      "one_line_profile": "GPU-accelerated Spatial Hashing Algorithm for Unity",
      "detailed_description": "An implementation of the Spatial Hashing algorithm using GPU acceleration in Unity, useful for particle simulations and neighbor search in scientific visualization or simulation.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "simulation",
        "spatial_analysis"
      ],
      "application_level": "library",
      "primary_language": "C#",
      "repo_url": "https://github.com/kodai100/Unity_GPUNearestNeighbor",
      "help_website": [],
      "license": null,
      "tags": [
        "unity",
        "gpu-acceleration",
        "spatial-hashing",
        "simulation"
      ],
      "id": 242
    },
    {
      "name": "raylight",
      "one_line_profile": "Multi-GPU distributed inference/training for ComfyUI using Ray",
      "detailed_description": "Enables distributed multi-GPU capabilities in ComfyUI workflows using XDiT, XFuser, and FSDP managed by Ray, facilitating large-scale image generation or processing.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_inference",
        "image_generation"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/komikndr/raylight",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "comfyui",
        "ray",
        "distributed-computing",
        "fsdp"
      ],
      "id": 243
    },
    {
      "name": "perceiver-io",
      "one_line_profile": "PyTorch implementation of Perceiver architectures",
      "detailed_description": "A PyTorch implementation of Perceiver, Perceiver IO, and Perceiver AR architectures, including PyTorch Lightning scripts for distributed training.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_implementation",
        "distributed_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/krasserm/perceiver-io",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "perceiver",
        "pytorch",
        "pytorch-lightning"
      ],
      "id": 244
    },
    {
      "name": "mpi-operator",
      "one_line_profile": "Kubernetes Operator for MPI-based applications",
      "detailed_description": "A Kubernetes Operator to manage MPI-based applications, essential for distributed training and HPC workloads on Kubernetes clusters.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "hpc_infrastructure"
      ],
      "application_level": "platform",
      "primary_language": "Go",
      "repo_url": "https://github.com/kubeflow/mpi-operator",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "kubernetes",
        "mpi",
        "distributed-training",
        "hpc"
      ],
      "id": 245
    },
    {
      "name": "kubeflow-trainer",
      "one_line_profile": "Distributed AI Model Training on Kubernetes",
      "detailed_description": "A component of Kubeflow for managing distributed AI model training and fine-tuning jobs on Kubernetes infrastructure.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "mlops"
      ],
      "application_level": "platform",
      "primary_language": "Go",
      "repo_url": "https://github.com/kubeflow/trainer",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "kubeflow",
        "kubernetes",
        "distributed-training"
      ],
      "id": 246
    },
    {
      "name": "jobset",
      "one_line_profile": "Kubernetes native API for distributed ML training and HPC",
      "detailed_description": "A Kubernetes native API designed to manage distributed machine learning training and HPC workloads as a set of related jobs.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "hpc_infrastructure"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/kubernetes-sigs/jobset",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "kubernetes",
        "hpc",
        "distributed-ml"
      ],
      "id": 247
    },
    {
      "name": "keras_multi_gpu",
      "one_line_profile": "Multi-GPU training utility for Keras",
      "detailed_description": "A utility library to enable multi-GPU training for Keras models, facilitating parallel processing.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "acceleration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/kuixu/keras_multi_gpu",
      "help_website": [],
      "license": null,
      "tags": [
        "keras",
        "multi-gpu",
        "parallel-training"
      ],
      "id": 248
    },
    {
      "name": "CasMVSNet_pl",
      "one_line_profile": "Cascade Cost Volume for Multi-View Stereo using PyTorch Lightning",
      "detailed_description": "Implementation of Cascade Cost Volume for High-Resolution Multi-View Stereo (MVS) and Stereo Matching, utilizing PyTorch Lightning for training.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "3d_reconstruction",
        "stereo_matching"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/kwea123/CasMVSNet_pl",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "mvs",
        "3d-reconstruction",
        "pytorch-lightning"
      ],
      "id": 249
    },
    {
      "name": "nerf_pl",
      "one_line_profile": "NeRF implementation using PyTorch Lightning",
      "detailed_description": "A PyTorch Lightning implementation of Neural Radiance Fields (NeRF) and NeRF in the Wild, facilitating 3D scene synthesis and training.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "neural_rendering",
        "3d_synthesis"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/kwea123/nerf_pl",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nerf",
        "neural-rendering",
        "pytorch-lightning"
      ],
      "id": 250
    },
    {
      "name": "ngp_pl",
      "one_line_profile": "Instant-NGP implementation in PyTorch",
      "detailed_description": "A high-performance implementation of Instant Neural Graphics Primitives (Instant-NGP) using PyTorch and CUDA, trained with PyTorch Lightning.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "neural_rendering",
        "acceleration"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/kwea123/ngp_pl",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "instant-ngp",
        "nerf",
        "cuda"
      ],
      "id": 251
    },
    {
      "name": "nsff_pl",
      "one_line_profile": "Neural Scene Flow Fields using PyTorch Lightning",
      "detailed_description": "Implementation of Neural Scene Flow Fields for dynamic 3D scene reconstruction and flow estimation.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "3d_reconstruction",
        "scene_flow"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/kwea123/nsff_pl",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "scene-flow",
        "nerf",
        "dynamic-scene"
      ],
      "id": 252
    },
    {
      "name": "FlashAttention20",
      "one_line_profile": "FlashAttention 2.0 implementation in PyTorch",
      "detailed_description": "A PyTorch implementation of the FlashAttention 2.0 algorithm, providing accelerated attention mechanisms without complex custom CUDA kernels.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "acceleration",
        "model_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/kyegomez/FlashAttention20",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "flash-attention",
        "pytorch",
        "optimization"
      ],
      "id": 253
    },
    {
      "name": "FlashAttention20Triton",
      "one_line_profile": "Triton implementation of Flash Attention 2.0",
      "detailed_description": "An implementation of Flash Attention 2.0 using OpenAI Triton, enabling high-performance attention computation on GPUs.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "acceleration",
        "kernel_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/kyegomez/FlashAttention20Triton",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "triton",
        "flash-attention",
        "gpu-acceleration"
      ],
      "id": 254
    },
    {
      "name": "kymatio",
      "one_line_profile": "Wavelet scattering transforms with GPU acceleration",
      "detailed_description": "A library for computing wavelet scattering transforms in Python, featuring GPU acceleration and compatibility with major deep learning frameworks.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "signal_processing",
        "feature_extraction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/kymatio/kymatio",
      "help_website": [
        "https://www.kymat.io/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "wavelet-scattering",
        "signal-processing",
        "gpu"
      ],
      "id": 255
    },
    {
      "name": "alpaca-rlhf",
      "one_line_profile": "RLHF Fine-tuning for LLaMA based on DeepSpeed Chat",
      "detailed_description": "A tool for fine-tuning LLaMA models using Reinforcement Learning with Human Feedback (RLHF), built upon the DeepSpeed Chat framework.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_finetuning",
        "rlhf"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/l294265421/alpaca-rlhf",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "rlhf",
        "deepspeed",
        "finetuning"
      ],
      "id": 256
    },
    {
      "name": "Chatglm_lora_multi-gpu",
      "one_line_profile": "Multi-GPU LoRA fine-tuning for ChatGLM",
      "detailed_description": "A tool for performing multi-GPU fine-tuning of ChatGLM models using LoRA and DeepSpeed, enabling efficient training of large language models.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_finetuning",
        "distributed_training"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/liangwq/Chatglm_lora_multi-gpu",
      "help_website": [],
      "license": null,
      "tags": [
        "chatglm",
        "lora",
        "deepspeed",
        "multi-gpu"
      ],
      "id": 257
    },
    {
      "name": "Lightning-UQ-Box",
      "one_line_profile": "Uncertainty Quantification for Neural Networks with PyTorch Lightning",
      "detailed_description": "A comprehensive library for Uncertainty Quantification (UQ) in neural networks, leveraging PyTorch and PyTorch Lightning to provide various UQ methods.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "uncertainty_quantification",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/lightning-uq-box/lightning-uq-box",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "uncertainty-quantification",
        "pytorch-lightning",
        "uq"
      ],
      "id": 258
    },
    {
      "name": "isolation-forest",
      "one_line_profile": "Distributed Isolation Forest implementation for Spark/Scala",
      "detailed_description": "A distributed implementation of the Isolation Forest algorithm for unsupervised outlier detection on Spark, supporting scalable training and ONNX export.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "anomaly_detection",
        "data_analysis"
      ],
      "application_level": "library",
      "primary_language": "Scala",
      "repo_url": "https://github.com/linkedin/isolation-forest",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "spark",
        "isolation-forest",
        "anomaly-detection",
        "distributed-computing"
      ],
      "id": 259
    },
    {
      "name": "Crossbow",
      "one_line_profile": "Multi-GPU Deep Learning System for Training with Small Batch Sizes",
      "detailed_description": "Crossbow is a deep learning system designed for multi-GPU training, specifically optimized for small batch sizes to improve hardware efficiency and training speed.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "model_training"
      ],
      "application_level": "platform",
      "primary_language": "Java",
      "repo_url": "https://github.com/lsds/Crossbow",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "deep-learning",
        "distributed-systems",
        "gpu-acceleration"
      ],
      "id": 260
    },
    {
      "name": "mu_transformer",
      "one_line_profile": "Transformer with Mu-Parameterization implemented in Jax/Flax",
      "detailed_description": "A library implementing Transformers with Mu-Parameterization (Maximal Update Parameterization) in Jax/Flax, supporting Fully Sharded Data Parallel (FSDP) on TPU pods.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "modeling",
        "distributed_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/lucaslingle/mu_transformer",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "jax",
        "flax",
        "transformer",
        "fsdp",
        "tpu"
      ],
      "id": 261
    },
    {
      "name": "flash-attention-jax",
      "one_line_profile": "JAX implementation of Flash Attention",
      "detailed_description": "A JAX implementation of the Flash Attention algorithm, providing memory-efficient and fast attention mechanisms for deep learning models.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "acceleration",
        "model_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/lucidrains/flash-attention-jax",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "jax",
        "flash-attention",
        "optimization"
      ],
      "id": 262
    },
    {
      "name": "flash-cosine-sim-attention",
      "one_line_profile": "Fused cosine similarity attention implementation",
      "detailed_description": "An implementation of fused cosine similarity attention, designed in the style of Flash Attention to optimize attention mechanisms in neural networks.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "acceleration",
        "modeling"
      ],
      "application_level": "library",
      "primary_language": "Cuda",
      "repo_url": "https://github.com/lucidrains/flash-cosine-sim-attention",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "attention-mechanism",
        "cuda",
        "optimization"
      ],
      "id": 263
    },
    {
      "name": "flash-genomics-model",
      "one_line_profile": "Long context genomics model using Flash Attention",
      "detailed_description": "A deep learning model designed for genomics, leveraging long context attention modeling techniques like Flash Attention to process genomic sequences.",
      "domains": [
        "AI3",
        "Bioinformatics"
      ],
      "subtask_category": [
        "modeling",
        "genomics_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/lucidrains/flash-genomics-model",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "genomics",
        "deep-learning",
        "long-context"
      ],
      "id": 264
    },
    {
      "name": "JaxLightning",
      "one_line_profile": "Integration of JAX with PyTorch Lightning",
      "detailed_description": "A library that enables running JAX models and operations within the PyTorch Lightning training framework, facilitating mixed-framework workflows.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "interoperability"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ludwigwinkler/JaxLightning",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "jax",
        "pytorch-lightning",
        "training-framework"
      ],
      "id": 265
    },
    {
      "name": "cute-flash-attention",
      "one_line_profile": "Flash Attention implementation using CuTe",
      "detailed_description": "An implementation of the Flash Attention algorithm utilizing the CuTe layout and algebra library for CUDA C++, aimed at high-performance GPU computing.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "acceleration",
        "optimization"
      ],
      "application_level": "library",
      "primary_language": "Cuda",
      "repo_url": "https://github.com/luliyucoordinate/cute-flash-attention",
      "help_website": [],
      "license": null,
      "tags": [
        "cuda",
        "flash-attention",
        "cute"
      ],
      "id": 266
    },
    {
      "name": "magix",
      "one_line_profile": "Model parallelism for Hugging Face Transformers",
      "detailed_description": "A tool to enable model parallelism for Hugging Face Transformers, allowing the training and inference of large models across multiple devices.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "model_parallelism"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/luyug/magix",
      "help_website": [],
      "license": null,
      "tags": [
        "huggingface",
        "transformers",
        "parallel-computing"
      ],
      "id": 267
    },
    {
      "name": "llama-tune",
      "one_line_profile": "LLaMa tuning workflow with DeepSpeed",
      "detailed_description": "A workflow tool for fine-tuning LLaMa models using the Stanford Alpaca dataset, leveraging DeepSpeed and Hugging Face Transformers for efficiency.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "fine_tuning",
        "model_training"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/lxe/llama-tune",
      "help_website": [],
      "license": null,
      "tags": [
        "llm",
        "fine-tuning",
        "deepspeed"
      ],
      "id": 268
    },
    {
      "name": "Elephas",
      "one_line_profile": "Distributed Deep Learning with Keras & Spark",
      "detailed_description": "Elephas is an extension of Keras, which allows you to run distributed deep learning models at scale with Apache Spark.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "model_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/maxpumperla/elephas",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "keras",
        "spark",
        "distributed-deep-learning"
      ],
      "id": 269
    },
    {
      "name": "torcheval",
      "one_line_profile": "PyTorch model metrics and evaluation toolkit",
      "detailed_description": "A library providing a rich collection of performant PyTorch model metrics and tools to facilitate metric computation in distributed training and model evaluation.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "metrics_computation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/meta-pytorch/torcheval",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "pytorch",
        "metrics",
        "evaluation"
      ],
      "id": 270
    },
    {
      "name": "torchft",
      "one_line_profile": "Fault tolerance library for PyTorch",
      "detailed_description": "A library providing fault tolerance mechanisms for PyTorch distributed training, including implementations for HSDP, LocalSGD, and DiLoCo.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "fault_tolerance"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/meta-pytorch/torchft",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "pytorch",
        "fault-tolerance",
        "distributed-training"
      ],
      "id": 271
    },
    {
      "name": "CNTK",
      "one_line_profile": "Microsoft Cognitive Toolkit for deep learning",
      "detailed_description": "An open-source deep-learning toolkit by Microsoft that describes neural networks as a series of computational steps via a directed graph.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "deep_learning"
      ],
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/microsoft/CNTK",
      "help_website": [
        "https://docs.microsoft.com/en-us/cognitive-toolkit/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "deep-learning",
        "neural-networks",
        "toolkit"
      ],
      "id": 272
    },
    {
      "name": "DirectML",
      "one_line_profile": "Hardware-accelerated DirectX 12 library for machine learning",
      "detailed_description": "A high-performance, hardware-accelerated DirectX 12 library for machine learning, providing GPU acceleration across a broad range of hardware.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "acceleration",
        "inference"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/microsoft/DirectML",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "directx",
        "machine-learning",
        "gpu-acceleration"
      ],
      "id": 273
    },
    {
      "name": "ONNX Runtime",
      "one_line_profile": "Cross-platform ML inferencing and training accelerator",
      "detailed_description": "A cross-platform, high-performance engine for machine learning inference and training, supporting models from PyTorch, TensorFlow, and other frameworks via ONNX.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "inference",
        "model_training",
        "acceleration"
      ],
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/microsoft/onnxruntime",
      "help_website": [
        "https://onnxruntime.ai"
      ],
      "license": "MIT",
      "tags": [
        "onnx",
        "inference",
        "acceleration"
      ],
      "id": 274
    },
    {
      "name": "WebDNN",
      "one_line_profile": "Fast DNN execution framework for web browsers",
      "detailed_description": "A framework for running deep neural networks (DNN) on web browsers with high performance, utilizing WebGPU and WebAssembly.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "inference",
        "web_deployment"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/mil-tokyo/webdnn",
      "help_website": [
        "https://mil-tokyo.github.io/webdnn/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "web-browser",
        "deep-learning",
        "inference"
      ],
      "id": 275
    },
    {
      "name": "DistriFusion",
      "one_line_profile": "Distributed Parallel Inference for High-Resolution Diffusion Models",
      "detailed_description": "A framework for distributed parallel inference of high-resolution diffusion models, enabling efficient generation across multiple devices.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "inference",
        "distributed_computing"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/mit-han-lab/distrifuser",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "diffusion-models",
        "distributed-inference",
        "parallel-computing"
      ],
      "id": 276
    },
    {
      "name": "TorchSparse",
      "one_line_profile": "Efficient Training and Inference for Sparse Convolution",
      "detailed_description": "A high-performance framework for training and inference of sparse convolutional neural networks on GPUs.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "inference",
        "acceleration"
      ],
      "application_level": "library",
      "primary_language": "Cuda",
      "repo_url": "https://github.com/mit-han-lab/torchsparse",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "sparse-convolution",
        "gpu",
        "optimization"
      ],
      "id": 277
    },
    {
      "name": "LiteAttention",
      "one_line_profile": "Optimized attention mechanism implementation over FlashAttention-3",
      "detailed_description": "A lightweight and efficient implementation of attention mechanisms built on top of FlashAttention-3, designed to accelerate Transformer model training and inference.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_acceleration",
        "attention_mechanism"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/moonmath-ai/LiteAttention",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "flash-attention",
        "transformer",
        "optimization"
      ],
      "id": 278
    },
    {
      "name": "mptorch",
      "one_line_profile": "Mixed-precision arithmetic simulation wrapper for PyTorch",
      "detailed_description": "A wrapper framework built atop PyTorch to simulate the use of custom and mixed precision arithmetic in Deep Neural Network (DNN) training and inference workloads, aiding in hardware-aware algorithm development.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "training_simulation",
        "mixed_precision"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/mptorch/mptorch",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "pytorch",
        "mixed-precision",
        "simulation"
      ],
      "id": 279
    },
    {
      "name": "QuIP",
      "one_line_profile": "Interactive environment for scientific computing and psychophysics",
      "detailed_description": "An interactive environment for computing, presenting images/sequences, and general scientific computing with built-in support for psychophysical experimentation and GPU acceleration.",
      "domains": [
        "Scientific Computing",
        "Psychophysics"
      ],
      "subtask_category": [
        "scientific_visualization",
        "experiment_control"
      ],
      "application_level": "platform",
      "primary_language": "C",
      "repo_url": "https://github.com/nasa/QuIP",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "scientific-computing",
        "psychophysics",
        "visualization"
      ],
      "id": 280
    },
    {
      "name": "distributed_rl",
      "one_line_profile": "Distributed deep reinforcement learning library for PyTorch",
      "detailed_description": "A PyTorch-based implementation of distributed deep reinforcement learning algorithms, enabling scalable training of RL agents.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "distributed_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/neka-nat/distributed_rl",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "reinforcement-learning",
        "distributed",
        "pytorch"
      ],
      "id": 281
    },
    {
      "name": "gsplat",
      "one_line_profile": "CUDA accelerated rasterization for Gaussian Splatting",
      "detailed_description": "A library providing CUDA-accelerated rasterization for Gaussian Splatting, a technique used in 3D reconstruction and novel view synthesis research.",
      "domains": [
        "Computer Vision",
        "AI3-01"
      ],
      "subtask_category": [
        "rendering",
        "3d_reconstruction"
      ],
      "application_level": "library",
      "primary_language": "Cuda",
      "repo_url": "https://github.com/nerfstudio-project/gsplat",
      "help_website": [
        "https://docs.nerf.studio/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "gaussian-splatting",
        "cuda",
        "nerf"
      ],
      "id": 282
    },
    {
      "name": "Newton",
      "one_line_profile": "GPU-accelerated physics simulation engine for robotics",
      "detailed_description": "An open-source, GPU-accelerated physics simulation engine built upon NVIDIA Warp, specifically targeting roboticists and simulation researchers for generating physical data.",
      "domains": [
        "Robotics",
        "Physics Simulation"
      ],
      "subtask_category": [
        "physics_simulation",
        "data_generation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/newton-physics/newton",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "physics-engine",
        "robotics",
        "simulation"
      ],
      "id": 283
    },
    {
      "name": "NexRL",
      "one_line_profile": "Framework for LLM post-training and RLHF",
      "detailed_description": "An ultra-loosely-coupled framework designed for Large Language Model (LLM) post-training, including Reinforcement Learning from Human Feedback (RLHF).",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "llm_training",
        "rlhf"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/nex-agi/NexRL",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "rlhf",
        "post-training"
      ],
      "id": 284
    },
    {
      "name": "disent",
      "one_line_profile": "Modular VAE disentanglement framework",
      "detailed_description": "A modular framework for Variational Autoencoder (VAE) disentanglement research, built with PyTorch Lightning, including metrics, datasets, and various supervision methods.",
      "domains": [
        "AI3",
        "Machine Learning"
      ],
      "subtask_category": [
        "representation_learning",
        "model_training"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/nmichlo/disent",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "vae",
        "disentanglement",
        "pytorch-lightning"
      ],
      "id": 285
    },
    {
      "name": "flash_attn_jax",
      "one_line_profile": "JAX bindings for Flash Attention v2",
      "detailed_description": "Provides JAX bindings for Flash Attention v2, enabling high-performance attention mechanism computation in JAX-based scientific machine learning workflows.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_acceleration",
        "attention_mechanism"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/nshepperd/flash_attn_jax",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "jax",
        "flash-attention",
        "acceleration"
      ],
      "id": 286
    },
    {
      "name": "DI-star",
      "one_line_profile": "Distributed training platform for StarCraft II AI",
      "detailed_description": "An artificial intelligence platform for StarCraft II enabling large-scale distributed training of grand-master agents, used in reinforcement learning research.",
      "domains": [
        "AI3",
        "Game AI"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "distributed_training"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/opendilab/DI-star",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "starcraft-ii",
        "reinforcement-learning",
        "distributed"
      ],
      "id": 287
    },
    {
      "name": "ReaLHF",
      "one_line_profile": "Efficient RLHF training framework for LLMs",
      "detailed_description": "A framework for super-efficient Reinforcement Learning from Human Feedback (RLHF) training of Large Language Models (LLMs) featuring parameter reallocation techniques.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "rlhf",
        "llm_training"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/openpsi-project/ReaLHF",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rlhf",
        "llm",
        "optimization"
      ],
      "id": 288
    },
    {
      "name": "openspeech",
      "one_line_profile": "Open-source toolkit for end-to-end speech recognition",
      "detailed_description": "A comprehensive toolkit for End-to-End Speech Recognition leveraging PyTorch-Lightning and Hydra, supporting research and development of ASR models.",
      "domains": [
        "AI3",
        "Speech Processing"
      ],
      "subtask_category": [
        "speech_recognition",
        "model_training"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/openspeech-team/openspeech",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "asr",
        "speech-recognition",
        "pytorch-lightning"
      ],
      "id": 289
    },
    {
      "name": "EE-LLM",
      "one_line_profile": "Framework for early-exit LLM training and inference",
      "detailed_description": "A framework designed for large-scale training and inference of early-exit (EE) Large Language Models (LLMs), optimizing efficiency.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "llm_training",
        "inference_optimization"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/pan-x-c/EE-LLM",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "llm",
        "early-exit",
        "training-framework"
      ],
      "id": 290
    },
    {
      "name": "AutoDist",
      "one_line_profile": "Distributed Deep Learning framework for TensorFlow",
      "detailed_description": "A framework for simple distributed deep learning on TensorFlow, automating the distribution strategy for model training.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "model_parallelism"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/petuum/autodist",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "tensorflow",
        "distributed-learning",
        "automation"
      ],
      "id": 291
    },
    {
      "name": "metal-flash-attention",
      "one_line_profile": "FlashAttention implementation for Apple Metal",
      "detailed_description": "A port of FlashAttention to Apple's Metal API, enabling accelerated attention mechanism computation on Apple Silicon GPUs for AI research.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_acceleration",
        "attention_mechanism"
      ],
      "application_level": "library",
      "primary_language": "Swift",
      "repo_url": "https://github.com/philipturner/metal-flash-attention",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "metal",
        "flash-attention",
        "apple-silicon"
      ],
      "id": 292
    },
    {
      "name": "energizer",
      "one_line_profile": "Active learning library for PyTorch",
      "detailed_description": "An active learning library for PyTorch based on Lightning-Fabric, facilitating the development of active learning loops for scientific data analysis.",
      "domains": [
        "AI3",
        "Machine Learning"
      ],
      "subtask_category": [
        "active_learning",
        "data_selection"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pietrolesci/energizer",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "active-learning",
        "pytorch",
        "lightning-fabric"
      ],
      "id": 293
    },
    {
      "name": "VINS-Fusion-gpu",
      "one_line_profile": "GPU-accelerated VINS-Fusion for SLAM",
      "detailed_description": "A GPU-accelerated version of VINS-Fusion, a Visual-Inertial State Estimator, enabling real-time SLAM on embedded devices like Nvidia TX2.",
      "domains": [
        "Robotics",
        "Computer Vision"
      ],
      "subtask_category": [
        "slam",
        "state_estimation"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/pjrambo/VINS-Fusion-gpu",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "slam",
        "vins-fusion",
        "gpu-acceleration"
      ],
      "id": 294
    },
    {
      "name": "libdnn",
      "one_line_profile": "Lightweight C++ library for deep neural networks with GPU acceleration",
      "detailed_description": "A lightweight and user-friendly C++ library designed for implementing deep and convolutional neural networks with GPU acceleration support. It provides low-level primitives for building and training neural networks efficiently.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "acceleration"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/poweic/libdnn",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "deep-learning",
        "gpu-acceleration",
        "cpp",
        "neural-networks"
      ],
      "id": 295
    },
    {
      "name": "DinkyTrain",
      "one_line_profile": "Pre-training library based on fairseq with DeepSpeed integration",
      "detailed_description": "A pre-training library developed by Princeton NLP, built upon fairseq and integrated with DeepSpeed kernels. It is designed to facilitate efficient training of large language models.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "pretraining"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/princeton-nlp/DinkyTrain",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nlp",
        "pre-training",
        "deepspeed",
        "fairseq"
      ],
      "id": 296
    },
    {
      "name": "PiPPy",
      "one_line_profile": "Pipeline Parallelism library for PyTorch",
      "detailed_description": "A library that provides pipeline parallelism capabilities for PyTorch, enabling the training of large models that do not fit into a single GPU memory by splitting them across multiple devices.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "pipeline_parallelism"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pytorch/PiPPy",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "pytorch",
        "pipeline-parallelism",
        "distributed-training"
      ],
      "id": 297
    },
    {
      "name": "PyTorch",
      "one_line_profile": "Open source machine learning framework",
      "detailed_description": "A comprehensive open-source machine learning framework that accelerates the path from research prototyping to production deployment. It provides tensor computation with strong GPU acceleration and deep neural networks built on a tape-based autograd system.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "deep_learning_framework"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/pytorch/pytorch",
      "help_website": [
        "https://pytorch.org"
      ],
      "license": "NOASSERTION",
      "tags": [
        "deep-learning",
        "tensor",
        "autograd",
        "gpu-acceleration"
      ],
      "id": 298
    },
    {
      "name": "Ray",
      "one_line_profile": "Unified framework for scaling AI and Python applications",
      "detailed_description": "An open-source unified compute framework that makes it easy to scale AI and Python workloads. It provides a core distributed runtime and a set of AI libraries for accelerating ML workloads, including training, tuning, and serving.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_computing",
        "scaling"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/ray-project/ray",
      "help_website": [
        "https://docs.ray.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "distributed-systems",
        "scaling",
        "machine-learning",
        "parallel-computing"
      ],
      "id": 299
    },
    {
      "name": "Ray Lightning",
      "one_line_profile": "PyTorch Lightning Distributed Accelerators using Ray",
      "detailed_description": "A library that integrates PyTorch Lightning with Ray, allowing users to leverage Ray's distributed computing capabilities for training PyTorch Lightning models across clusters.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "acceleration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ray-project/ray_lightning",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "pytorch-lightning",
        "ray",
        "distributed-training"
      ],
      "id": 300
    },
    {
      "name": "RobotPerf Benchmarks",
      "one_line_profile": "Benchmarking suite for robotics computing performance",
      "detailed_description": "A vendor-neutral benchmarking suite designed to evaluate robotics computing performance using grey-box and black-box approaches. It helps in assessing the efficiency of hardware and software stacks for robotics and AI workloads.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/robotperf/benchmarks",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "robotics",
        "benchmarking",
        "performance",
        "ros2"
      ],
      "id": 301
    },
    {
      "name": "KubeTorch",
      "one_line_profile": "Distributed AI workloads on Kubernetes for Python",
      "detailed_description": "A tool that allows users to distribute and run AI workloads on Kubernetes clusters seamlessly using Python, acting as an infrastructure layer for ML workflows similar to PyTorch's abstraction for tensors.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "infrastructure_management"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/run-house/kubetorch",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "kubernetes",
        "distributed-training",
        "mlops",
        "infrastructure"
      ],
      "id": 302
    },
    {
      "name": "l2hmc-qcd",
      "one_line_profile": "L2HMC algorithm for Lattice QCD simulations",
      "detailed_description": "An implementation of the L2HMC (Learn to Hamiltonian Monte Carlo) algorithm applied to simulations in Lattice Quantum Chromodynamics (QCD). It serves as a scientific simulation tool for physics research.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "simulation",
        "scientific_modeling"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/saforem2/l2hmc-qcd",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "lattice-qcd",
        "physics",
        "mcmc",
        "generative-models"
      ],
      "id": 303
    },
    {
      "name": "PMLS-Caffe",
      "one_line_profile": "Distributed Deep Learning Framework based on Caffe",
      "detailed_description": "A distributed deep learning framework designed for Parallel Machine Learning Systems (PMLS). It extends Caffe to support distributed training, enabling efficient scaling of deep learning models.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "distributed_training"
      ],
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/sailing-pmls/pmls-caffe",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "caffe",
        "distributed-learning",
        "deep-learning",
        "framework"
      ],
      "id": 304
    },
    {
      "name": "GRACE",
      "one_line_profile": "Gradient Compression for distributed deep learning",
      "detailed_description": "A library for gradient compression in distributed deep learning. It aims to reduce communication overhead during distributed training by compressing gradients, thereby accelerating the training process.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "gradient_compression"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/sands-lab/grace",
      "help_website": [],
      "license": "BSD-2-Clause",
      "tags": [
        "distributed-learning",
        "compression",
        "optimization",
        "gradient-compression"
      ],
      "id": 305
    },
    {
      "name": "FullLLM",
      "one_line_profile": "Full-stack library for LLM pre-training, fine-tuning, RLHF, and inference",
      "detailed_description": "A comprehensive toolkit for the Large Language Model lifecycle, supporting pre-training, supervised fine-tuning (SFT), reinforcement learning from human feedback (RLHF/PPO), inference, and quantization.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "finetuning",
        "rlhf"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/schinger/FullLLM",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "rlhf",
        "finetuning",
        "quantization"
      ],
      "id": 306
    },
    {
      "name": "SpecForge",
      "one_line_profile": "Framework for training speculative decoding models",
      "detailed_description": "A tool designed to effortlessly train speculative decoding models and port them to SGLang serving, optimizing inference speed for large language models.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "acceleration",
        "inference_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/sgl-project/SpecForge",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "speculative-decoding",
        "llm",
        "inference-acceleration"
      ],
      "id": 307
    },
    {
      "name": "FlashAttention-PyTorch",
      "one_line_profile": "PyTorch implementation of FlashAttention for efficient transformer training",
      "detailed_description": "A PyTorch implementation of the FlashAttention algorithm, designed to accelerate attention computation and reduce memory usage in transformer models.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "acceleration",
        "model_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/shreyansh26/FlashAttention-PyTorch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "flash-attention",
        "pytorch",
        "acceleration"
      ],
      "id": 308
    },
    {
      "name": "ShallowSpeed",
      "one_line_profile": "Lightweight distributed training library for deep learning",
      "detailed_description": "A small-scale distributed training library built on Numpy and MPI, designed for educational purposes and training sequential deep learning models.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "model_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/siboehm/ShallowSpeed",
      "help_website": [],
      "license": null,
      "tags": [
        "distributed-training",
        "mpi",
        "numpy"
      ],
      "id": 309
    },
    {
      "name": "gpumonitor",
      "one_line_profile": "GPU monitoring callbacks for TensorFlow and PyTorch Lightning",
      "detailed_description": "A utility library providing callbacks to monitor GPU usage during model training with TensorFlow 2.x and PyTorch Lightning.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "monitoring",
        "resource_management"
      ],
      "application_level": "utility",
      "primary_language": "Python",
      "repo_url": "https://github.com/sicara/gpumonitor",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gpu-monitoring",
        "tensorflow",
        "pytorch-lightning"
      ],
      "id": 310
    },
    {
      "name": "ArcticTraining",
      "one_line_profile": "Framework for post-training large language models",
      "detailed_description": "A framework designed by Snowflake to simplify and accelerate the post-training process (fine-tuning, alignment) for large language models.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "finetuning"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/snowflakedb/ArcticTraining",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "post-training",
        "finetuning"
      ],
      "id": 311
    },
    {
      "name": "parallax",
      "one_line_profile": "Automatic parallelization tool for deep learning training",
      "detailed_description": "A tool that automates the parallelization of deep learning training across distributed multi-GPU environments.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "parallelization"
      ],
      "application_level": "tool",
      "primary_language": "Python",
      "repo_url": "https://github.com/snuspl/parallax",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "distributed-training",
        "multi-gpu",
        "parallelization"
      ],
      "id": 312
    },
    {
      "name": "Flash-Attention-Softmax-N",
      "one_line_profile": "Flash Attention implementation with SoftmaxN",
      "detailed_description": "CUDA and Triton implementations of Flash Attention incorporating SoftmaxN, aimed at optimizing attention mechanisms in transformer models.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "acceleration",
        "model_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/softmax1/Flash-Attention-Softmax-N",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "flash-attention",
        "cuda",
        "triton"
      ],
      "id": 313
    },
    {
      "name": "coom",
      "one_line_profile": "Large-scale language model training framework",
      "detailed_description": "A training framework based on Megatron-Core, designed to efficiently handle extensive model training, inspired by Deepseek's HAI-LLM optimizations.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "distributed_training"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/soketlabs/coom",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "llm",
        "megatron-core",
        "distributed-training"
      ],
      "id": 314
    },
    {
      "name": "llms_tool",
      "one_line_profile": "Tool for LLM training, testing, and quantization",
      "detailed_description": "A tool based on HuggingFace for training, testing, and deploying large language models, supporting pre-training, SFT, RLHF, DPO, and quantization.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "finetuning",
        "quantization"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/stanleylsx/llms_tool",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "sft",
        "rlhf",
        "quantization"
      ],
      "id": 315
    },
    {
      "name": "flash-attention-windows",
      "one_line_profile": "Pre-built Flash Attention wheels for Windows",
      "detailed_description": "Provides pre-built binary wheels for Flash Attention 2 on Windows, enabling researchers to use accelerated attention mechanisms on Windows platforms without complex build setups.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "acceleration",
        "environment_setup"
      ],
      "application_level": "utility",
      "primary_language": null,
      "repo_url": "https://github.com/sunsetcoder/flash-attention-windows",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "flash-attention",
        "windows",
        "wheels"
      ],
      "id": 316
    },
    {
      "name": "RLHF",
      "one_line_profile": "Implementation of RLHF pipeline for Chinese ChatGPT",
      "detailed_description": "An implementation of the Reinforcement Learning from Human Feedback (RLHF) pipeline, specifically tailored for training Chinese language models similar to ChatGPT.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "rlhf"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/sunzeyeah/RLHF",
      "help_website": [],
      "license": null,
      "tags": [
        "rlhf",
        "llm",
        "chatgpt"
      ],
      "id": 317
    },
    {
      "name": "mixed-precision-pytorch",
      "one_line_profile": "Library for mixed precision training in PyTorch",
      "detailed_description": "A library/implementation for training deep learning models using FP16 weights (mixed precision) in PyTorch to accelerate training and reduce memory usage.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "acceleration",
        "model_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/suvojit-0x55aa/mixed-precision-pytorch",
      "help_website": [],
      "license": "WTFPL",
      "tags": [
        "mixed-precision",
        "fp16",
        "pytorch"
      ],
      "id": 318
    },
    {
      "name": "Silice",
      "one_line_profile": "Hardware description language for FPGA design",
      "detailed_description": "A hardware description language that simplifies designing hardware algorithms with parallelism and pipelines, useful for creating custom hardware accelerators for scientific computing.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "hardware_design",
        "acceleration"
      ],
      "application_level": "language",
      "primary_language": "C++",
      "repo_url": "https://github.com/sylefeb/Silice",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "fpga",
        "hdl",
        "hardware-acceleration"
      ],
      "id": 319
    },
    {
      "name": "deep-gradient-compression",
      "one_line_profile": "Deep Gradient Compression for distributed training",
      "detailed_description": "Implementation of Deep Gradient Compression (DGC) to reduce communication bandwidth requirements during distributed training of deep learning models.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "compression"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/synxlin/deep-gradient-compression",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "distributed-training",
        "gradient-compression",
        "bandwidth-optimization"
      ],
      "id": 320
    },
    {
      "name": "pytorch-distributed",
      "one_line_profile": "Benchmark and quickstart for PyTorch distributed training",
      "detailed_description": "A repository providing benchmarks and quickstart guides for setting up and evaluating distributed training environments with PyTorch.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "benchmarking"
      ],
      "application_level": "utility",
      "primary_language": "Python",
      "repo_url": "https://github.com/tczhangzhi/pytorch-distributed",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "distributed-training",
        "pytorch",
        "benchmark"
      ],
      "id": 321
    },
    {
      "name": "Qwen2-Audio-finetune",
      "one_line_profile": "Fine-tuning toolkit for Qwen2-Audio models",
      "detailed_description": "A repository specifically designed for fine-tuning the Qwen2-Audio model, supporting Distributed Data Parallel (DDP) and DeepSpeed for efficient training.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "finetuning"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/teamtee/Qwen2-Audio-finetune",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "qwen2-audio",
        "finetuning",
        "deepspeed"
      ],
      "id": 322
    },
    {
      "name": "cube-studio",
      "one_line_profile": "Cloud-native one-stop MLOps and AI platform",
      "detailed_description": "An open-source, cloud-native machine learning platform supporting the full MLOps lifecycle, including distributed training, hyperparameter search, inference serving, and LLM fine-tuning.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "platform",
        "model_training",
        "mlops"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/tencentmusic/cube-studio",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "mlops",
        "distributed-training",
        "llm-platform"
      ],
      "id": 323
    },
    {
      "name": "TensorDiffEq",
      "one_line_profile": "Physics-Informed Deep Learning framework on TensorFlow",
      "detailed_description": "A library for Efficient and Scalable Physics-Informed Deep Learning (PINNs) and Scientific Machine Learning built on top of TensorFlow, supporting multi-worker distributed computing.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "scientific_modeling",
        "pinn_solver",
        "differential_equations"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/tensordiffeq/TensorDiffEq",
      "help_website": [],
      "license": null,
      "tags": [
        "physics-informed-neural-networks",
        "scientific-machine-learning",
        "tensorflow",
        "distributed-computing"
      ],
      "id": 324
    },
    {
      "name": "Mesh TensorFlow",
      "one_line_profile": "Distributed deep learning library for model parallelism",
      "detailed_description": "A language for distributed deep learning capable of specifying a class of distributed tensor computations, enabling model parallelism for training very large models.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "model_parallelism"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/tensorflow/mesh",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "distributed-training",
        "model-parallelism",
        "tensorflow"
      ],
      "id": 325
    },
    {
      "name": "Tensor2Tensor",
      "one_line_profile": "Library of deep learning models and datasets",
      "detailed_description": "A library of deep learning models and datasets designed to make deep learning more accessible and accelerate ML research, featuring implementations of Transformers and other architectures.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "deep_learning_models"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/tensorflow/tensor2tensor",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "deep-learning",
        "transformer",
        "datasets"
      ],
      "id": 326
    },
    {
      "name": "ScalarLM",
      "one_line_profile": "Unified training and inference stack for LLMs",
      "detailed_description": "A unified stack for training and inference of Large Language Models, designed to streamline the lifecycle of LLM development.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "llm_training",
        "inference"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/tensorwavecloud/ScalarLM",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "llm",
        "training-stack",
        "inference"
      ],
      "id": 327
    },
    {
      "name": "ReMoE",
      "one_line_profile": "Fully Differentiable Mixture-of-Experts implementation",
      "detailed_description": "Codebase for ReMoE (Fully Differentiable Mixture-of-Experts with ReLU Routing), built on Megatron-LM, providing a specialized architecture for efficient large model training.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "mixture_of_experts"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/thu-ml/ReMoE",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "mixture-of-experts",
        "megatron-lm",
        "llm-training"
      ],
      "id": 328
    },
    {
      "name": "libflash_attn",
      "one_line_profile": "Standalone Flash Attention v2 kernel",
      "detailed_description": "A standalone Flash Attention v2 kernel implementation without libtorch dependency, providing optimized attention mechanisms for transformer models.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "acceleration",
        "attention_kernel"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/tlc-pack/libflash_attn",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "flash-attention",
        "cuda",
        "optimization"
      ],
      "id": 329
    },
    {
      "name": "keypoint-detection",
      "one_line_profile": "2D keypoint detection solver",
      "detailed_description": "A tool for 2D keypoint detection utilizing PyTorch Lightning and Weights & Biases for experiment tracking.",
      "domains": [
        "AI3"
      ],
      "subtask_category": [
        "keypoint_detection",
        "computer_vision"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/tlpss/keypoint-detection",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "keypoint-detection",
        "pytorch-lightning",
        "computer-vision"
      ],
      "id": 330
    },
    {
      "name": "TonY",
      "one_line_profile": "Deep learning framework on Apache Hadoop",
      "detailed_description": "TonY is a framework to natively run deep learning frameworks (like TensorFlow and PyTorch) on Apache Hadoop, enabling distributed training on big data clusters.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "resource_scheduling"
      ],
      "application_level": "platform",
      "primary_language": "Java",
      "repo_url": "https://github.com/tony-framework/TonY",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "hadoop",
        "distributed-deep-learning",
        "yarn"
      ],
      "id": 331
    },
    {
      "name": "go-metal",
      "one_line_profile": "Deep learning library for Go on Apple Silicon",
      "detailed_description": "A high-performance deep learning library for the Go programming language that leverages Apple's Metal API for GPU acceleration on Apple Silicon devices.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_training",
        "acceleration"
      ],
      "application_level": "library",
      "primary_language": "Go",
      "repo_url": "https://github.com/tsawler/go-metal",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "deep-learning",
        "go",
        "metal",
        "gpu-acceleration"
      ],
      "id": 332
    },
    {
      "name": "cuSNN",
      "one_line_profile": "GPU-accelerated Spiking Neural Networks library",
      "detailed_description": "A library for simulating Spiking Neural Networks (SNNs) in C++ with strong GPU acceleration through CUDA, suitable for neuromorphic computing research.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "neuromorphic_computing",
        "snn_simulation"
      ],
      "application_level": "library",
      "primary_language": "Cuda",
      "repo_url": "https://github.com/tudelft/cuSNN",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "spiking-neural-networks",
        "cuda",
        "neuromorphic"
      ],
      "id": 333
    },
    {
      "name": "Cekirdekler",
      "one_line_profile": "Multi-device OpenCL kernel load balancer",
      "detailed_description": "A multi-device OpenCL kernel load balancer and pipeliner API for C#, designed to distribute workloads across multiple GPUs using a shared-distributed memory model.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "gpu_acceleration",
        "load_balancing"
      ],
      "application_level": "library",
      "primary_language": "C#",
      "repo_url": "https://github.com/tugrul512bit/Cekirdekler",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "opencl",
        "load-balancing",
        "gpu-computing"
      ],
      "id": 334
    },
    {
      "name": "Petastorm",
      "one_line_profile": "Data loading library for deep learning from Parquet",
      "detailed_description": "A library enabling single machine or distributed training and evaluation of deep learning models directly from datasets in Apache Parquet format, supporting TensorFlow, PyTorch, and PySpark.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "data_loading",
        "distributed_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/uber/petastorm",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "parquet",
        "data-loading",
        "distributed-training"
      ],
      "id": 335
    },
    {
      "name": "Detoxify",
      "one_line_profile": "Toxic comment detection models and library",
      "detailed_description": "A library providing trained models and code to predict toxic comments, useful for dataset filtering and quality control in NLP research.",
      "domains": [
        "AI3"
      ],
      "subtask_category": [
        "text_classification",
        "data_filtering"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/unitaryai/detoxify",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "nlp",
        "toxicity-detection",
        "data-cleaning"
      ],
      "id": 336
    },
    {
      "name": "PipeEdge",
      "one_line_profile": "Pipeline parallelism for edge inference",
      "detailed_description": "A framework for pipeline parallelism designed to enable large-scale model inference on heterogeneous edge devices.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_inference",
        "pipeline_parallelism"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/usc-isi/PipeEdge",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "edge-computing",
        "pipeline-parallelism",
        "inference"
      ],
      "id": 337
    },
    {
      "name": "OnnxStream",
      "one_line_profile": "Lightweight ONNX inference library",
      "detailed_description": "A lightweight inference library for ONNX files written in C++, capable of running large models like Stable Diffusion and Mistral on low-resource devices.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_inference",
        "edge_computing"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/vitoplantamura/OnnxStream",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "onnx",
        "inference",
        "edge-ai"
      ],
      "id": 338
    },
    {
      "name": "veScale",
      "one_line_profile": "PyTorch Distributed framework for hyperscale training",
      "detailed_description": "A distributed training framework based on PyTorch, optimized for hyperscale training of Large Language Models (LLMs) and Reinforcement Learning (RL).",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "llm_training"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/volcengine/veScale",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "distributed-training",
        "pytorch",
        "llm"
      ],
      "id": 339
    },
    {
      "name": "PytorchAutoDrive",
      "one_line_profile": "Segmentation and lane detection models library",
      "detailed_description": "A library providing implementations of various segmentation and lane detection models based on PyTorch, including tools for training, visualization, and benchmarking.",
      "domains": [
        "AI3"
      ],
      "subtask_category": [
        "image_segmentation",
        "object_detection"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/voldemortX/pytorch-auto-drive",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "autonomous-driving",
        "segmentation",
        "lane-detection"
      ],
      "id": 340
    },
    {
      "name": "solo-learn",
      "one_line_profile": "Self-supervised visual representation learning library",
      "detailed_description": "A library of self-supervised methods for visual representation learning, powered by PyTorch Lightning, facilitating research in unsupervised learning.",
      "domains": [
        "AI3"
      ],
      "subtask_category": [
        "self_supervised_learning",
        "representation_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/vturrisi/solo-learn",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "self-supervised-learning",
        "computer-vision",
        "pytorch-lightning"
      ],
      "id": 341
    },
    {
      "name": "modelparallel_pytorch",
      "one_line_profile": "Model parallelism library for PyTorch",
      "detailed_description": "A library enabling model parallelism for training multiple networks across multiple GPUs using PyTorch.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_parallelism",
        "distributed_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/waitwaitforget/modelparallel_pytorch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pytorch",
        "model-parallelism",
        "multi-gpu"
      ],
      "id": 342
    },
    {
      "name": "llm.scala",
      "one_line_profile": "LLM training framework in Scala",
      "detailed_description": "An extensible implementation of a Language Model (LLM) training framework written in Scala.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "llm_training"
      ],
      "application_level": "library",
      "primary_language": "Scala",
      "repo_url": "https://github.com/wassemgtk/llm.scala",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "scala",
        "llm",
        "training-framework"
      ],
      "id": 343
    },
    {
      "name": "cutlass_flash_atten_fp8",
      "one_line_profile": "FP8 Flash Attention kernel implementation",
      "detailed_description": "An implementation of Flash Attention using FP8 precision on Ada architecture GPUs via Cutlass, serving as an acceleration kernel.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "acceleration",
        "attention_kernel"
      ],
      "application_level": "library",
      "primary_language": "Cuda",
      "repo_url": "https://github.com/weishengying/cutlass_flash_atten_fp8",
      "help_website": [],
      "license": null,
      "tags": [
        "fp8",
        "flash-attention",
        "cutlass",
        "cuda"
      ],
      "id": 344
    },
    {
      "name": "TernGrad",
      "one_line_profile": "Ternary gradients implementation for reducing communication in distributed deep learning",
      "detailed_description": "An implementation of Ternary Gradients (TernGrad) to reduce communication overhead in distributed deep learning training, specifically for TensorFlow.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "gradient_compression"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/wenwei202/terngrad",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "distributed-training",
        "gradient-compression",
        "tensorflow"
      ],
      "id": 345
    },
    {
      "name": "FlashVGGT",
      "one_line_profile": "Efficient descriptor-based global attention acceleration for VGGT",
      "detailed_description": "A tool to accelerate VGGT (Visual Geometry Grounded Transformer) using efficient descriptor-based global attention mechanisms.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "model_acceleration",
        "attention_mechanism"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/wzpscott/flashvggt",
      "help_website": [],
      "license": null,
      "tags": [
        "attention",
        "acceleration",
        "transformer"
      ],
      "id": 346
    },
    {
      "name": "PipeGoose",
      "one_line_profile": "Large scale 4D parallelism pre-training library for transformers",
      "detailed_description": "A library for large-scale 4D parallelism pre-training of Transformer models, specifically targeting Mixture of Experts (MoE) architectures.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "parallelism"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/xrsrke/pipegoose",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "distributed-training",
        "parallelism",
        "moe",
        "transformers"
      ],
      "id": 347
    },
    {
      "name": "CaffeOnSpark",
      "one_line_profile": "Distributed deep learning framework on Hadoop and Spark clusters",
      "detailed_description": "A framework that enables distributed deep learning on Hadoop and Spark clusters, allowing for training of deep learning models on big data infrastructure.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "deep_learning"
      ],
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/yahoo/CaffeOnSpark",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "distributed-learning",
        "spark",
        "hadoop",
        "caffe"
      ],
      "id": 348
    },
    {
      "name": "DeDLOC",
      "one_line_profile": "Distributed Deep Learning in Open Collaborations",
      "detailed_description": "Implementation of methods for distributed deep learning in open collaborations, enabling decentralized training workflows.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "decentralized_learning"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/yandex-research/DeDLOC",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "distributed-training",
        "collaborative-learning"
      ],
      "id": 349
    },
    {
      "name": "SWARM",
      "one_line_profile": "Communication-efficient SWARM parallelism for training large models",
      "detailed_description": "A library implementing SWARM parallelism to enable communication-efficient training of large models across unreliable or heterogeneous devices.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "parallelism"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/yandex-research/swarm",
      "help_website": [],
      "license": null,
      "tags": [
        "distributed-training",
        "swarm-parallelism"
      ],
      "id": 350
    },
    {
      "name": "YaFSDP",
      "one_line_profile": "Optimized Fully Sharded Data Parallel implementation",
      "detailed_description": "Yet another Fully Sharded Data Parallel (YaFSDP) implementation, providing optimized memory and communication efficiency for training large language models.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "memory_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/yandex/YaFSDP",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "fsdp",
        "distributed-training",
        "llm"
      ],
      "id": 351
    },
    {
      "name": "LLM-SFT",
      "one_line_profile": "Framework for Supervised Fine-Tuning of Large Language Models",
      "detailed_description": "A framework for Supervised Fine-Tuning (SFT) of Chinese and other Large Language Models, supporting LoRA, QLoRA, DeepSpeed, and various model architectures.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "fine_tuning",
        "model_training"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/yongzhuo/LLM-SFT",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "sft",
        "fine-tuning",
        "lora"
      ],
      "id": 352
    },
    {
      "name": "Realtime Semantic Segmentation PyTorch",
      "one_line_profile": "PyTorch implementation of realtime semantic segmentation models",
      "detailed_description": "A comprehensive library providing PyTorch implementations for over 30 realtime semantic segmentation models, supporting distributed training and knowledge distillation.",
      "domains": [
        "AI3"
      ],
      "subtask_category": [
        "semantic_segmentation",
        "model_implementation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/zh320/realtime-semantic-segmentation-pytorch",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "semantic-segmentation",
        "pytorch",
        "computer-vision"
      ],
      "id": 353
    },
    {
      "name": "yolort",
      "one_line_profile": "Runtime stack for YOLOv5 on specialized accelerators",
      "detailed_description": "A runtime stack designed to accelerate YOLOv5 inference on specialized hardware accelerators such as TensorRT, LibTorch, ONNX Runtime, TVM, and NCNN.",
      "domains": [
        "AI3"
      ],
      "subtask_category": [
        "inference_acceleration",
        "model_deployment"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/zhiqwang/yolort",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "inference",
        "acceleration",
        "yolo",
        "tensorrt"
      ],
      "id": 354
    },
    {
      "name": "Ring Flash Attention",
      "one_line_profile": "Ring attention implementation with Flash Attention",
      "detailed_description": "An implementation of Ring Attention combined with Flash Attention to enable training with extremely long context windows in a distributed setting.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "distributed_training",
        "attention_mechanism"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/zhuzilin/ring-flash-attention",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "flash-attention",
        "ring-attention",
        "distributed-training"
      ],
      "id": 355
    },
    {
      "name": "KnowLM",
      "one_line_profile": "Knowledgeable Large Language Model Framework",
      "detailed_description": "An open-source framework for training and utilizing Knowledgeable Large Language Models (KnowLM), focusing on knowledge extraction and reasoning.",
      "domains": [
        "AI3",
        "AI3-01"
      ],
      "subtask_category": [
        "llm_training",
        "knowledge_extraction"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/zjunlp/KnowLM",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "knowledge-graph",
        "framework"
      ],
      "id": 356
    }
  ]
}
{
  "leaf_cluster_name": "科研评测-基准/指标/复现实验生态",
  "domain": "AI Toolchain",
  "typical_objects": "datasets/tasks",
  "task_chain": "任务→指标→自动评测→leaderboard→回归",
  "tool_form": "benchmark + eval harness",
  "total_tools": 1219,
  "tools": [
    {
      "name": "SLM4Mol",
      "one_line_profile": "Quantitative benchmark and analysis suite for molecular large language models",
      "detailed_description": "A comprehensive benchmark and analysis framework designed for Molecular Large Language Models (MolLLMs). It provides quantitative evaluation metrics and datasets to assess the performance of LLMs on molecular tasks, facilitating the development of AI for molecular science.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "molecular_modeling"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/AI-HPC-Research-Team/SLM4Mol",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "molecular-llm",
        "benchmark",
        "chemistry",
        "ai4science"
      ],
      "id": 1
    },
    {
      "name": "AISBench Benchmark",
      "one_line_profile": "Model evaluation tool extending OpenCompass for service-based models",
      "detailed_description": "An AI model evaluation tool built upon the OpenCompass framework. It is compatible with OpenCompass's configuration system and dataset structure but extends capabilities to support service-based models, facilitating comprehensive benchmarking of AI systems.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/AISBench/benchmark",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "benchmarking",
        "model-evaluation",
        "opencompass",
        "ai-infrastructure"
      ],
      "id": 2
    },
    {
      "name": "ThinkingAgent",
      "one_line_profile": "Evaluation framework for rating overthinking behavior in LLMs",
      "detailed_description": "A systematic evaluation framework designed to automatically rate and analyze 'overthinking' behavior in Large Language Models. It serves as a tool for behavioral analysis and interpretability of LLM reasoning processes.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "interpretability"
      ],
      "application_level": "solver",
      "primary_language": "Shell",
      "repo_url": "https://github.com/AlexCuadron/ThinkingAgent",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm-evaluation",
        "reasoning",
        "behavioral-analysis"
      ],
      "id": 3
    },
    {
      "name": "DocGenome",
      "one_line_profile": "Large-scale scientific document benchmark for multi-modal large models",
      "detailed_description": "An open, large-scale benchmark specifically designed for training and testing Multi-modal Large Models (MLLMs) on scientific documents. It facilitates the evaluation of AI models' capabilities in understanding and processing complex scientific literature.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "scientific_literature_mining"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/Alpha-Innovator/DocGenome",
      "help_website": [],
      "license": "CC-BY-4.0",
      "tags": [
        "scientific-documents",
        "multimodal-llm",
        "benchmark",
        "dataset"
      ],
      "id": 4
    },
    {
      "name": "FedHeteroBench",
      "one_line_profile": "Benchmark framework for data heterogeneity in federated learning",
      "detailed_description": "A repository of frameworks and unified implementations for handling data heterogeneity in federated learning (Non-IID settings). It provides reproducible experiments and benchmarks to evaluate FL algorithms under heterogeneous data conditions.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "federated_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/AntonioZC666/FedHeteroBench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "federated-learning",
        "benchmark",
        "non-iid",
        "data-heterogeneity"
      ],
      "id": 5
    },
    {
      "name": "EmotionCircuits-LLM",
      "one_line_profile": "Framework for discovering and controlling emotion circuits in LLMs",
      "detailed_description": "A complete and reproducible framework for analyzing, discovering, and controlling 'emotion circuits' within Large Language Models. It provides tools for interpretability and mechanistic analysis of LLM internal states.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "interpretability",
        "model_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Aurora-cx/EmotionCircuits-LLM",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "llm-interpretability",
        "mechanistic-interpretability",
        "emotion-analysis"
      ],
      "id": 6
    },
    {
      "name": "ai4sci-2021-denovo-benchmarks",
      "one_line_profile": "Benchmarks for de novo molecular design from NeurIPS AI4Sci workshop",
      "detailed_description": "Code and benchmark definitions for the 'A Fresh Look at De Novo Molecular Design Benchmarks' paper presented at the NeurIPS 2021 AI for Science Workshop. It provides a standardized environment for evaluating molecular generation models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "molecular_design"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/AustinT/ai4sci-2021-denovo-benchmarks",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "molecular-design",
        "benchmark",
        "neurips-workshop",
        "drug-discovery"
      ],
      "id": 7
    },
    {
      "name": "BigVectorBench",
      "one_line_profile": "Benchmark suite for vector database embedding performance",
      "detailed_description": "A benchmark suite designed to evaluate the embedding performance of vector databases. It supports heterogeneous data and abstracts compound queries (multimodal or single-modal) to assess vector search capabilities critical for AI applications.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "vector_search"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/BenchCouncil/BigVectorBench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "vector-database",
        "benchmark",
        "embedding",
        "ai-infrastructure"
      ],
      "id": 8
    },
    {
      "name": "ko-lm-evaluation-harness",
      "one_line_profile": "Evaluation harness for Korean language models",
      "detailed_description": "A specialized fork of the lm-evaluation-harness adapted for Korean Language Models. It provides a framework for few-shot evaluation of autoregressive language models on Korean datasets.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "nlp_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Beomi/ko-lm-evaluation-harness",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm-evaluation",
        "korean-nlp",
        "benchmark"
      ],
      "id": 9
    },
    {
      "name": "MolGenBench",
      "one_line_profile": "Benchmark for molecular generative models from de novo design to lead optimization",
      "detailed_description": "A codebase and benchmark suite for evaluating the real-world applicability of molecular generative models. It covers tasks ranging from de novo design to lead optimization, providing metrics for chemical validity and optimization success.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "molecular_generation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/CAODH/MolGenBench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "molecular-generation",
        "drug-design",
        "benchmark",
        "lead-optimization"
      ],
      "id": 10
    },
    {
      "name": "ctf4science",
      "one_line_profile": "Benchmarking framework for modeling dynamic systems (ODEs/PDEs)",
      "detailed_description": "A modular and extensible framework for benchmarking modeling methods on dynamic systems. It supports the evaluation of models for ordinary differential equations (ODEs) and partial differential equations (PDEs) using standardized datasets and metrics.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "dynamic_systems_modeling"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/CTF-for-Science/ctf4science",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pde-solving",
        "ode-solving",
        "benchmark",
        "scientific-modeling"
      ],
      "id": 11
    },
    {
      "name": "Matcha (variationanalysis)",
      "one_line_profile": "Framework for training and evaluating deep learning models for genomic variation calling",
      "detailed_description": "The Matcha framework, part of the variationanalysis repository, is designed to help train and evaluate deep learning models for calling genomic variations. It supports semi-simulation workflows for generating training data and assessing model performance.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "variant_calling"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/CampagneLaboratory/variationanalysis",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "genomics",
        "variant-calling",
        "deep-learning",
        "bioinformatics"
      ],
      "id": 12
    },
    {
      "name": "PoseX",
      "one_line_profile": "Benchmark for molecular docking algorithms",
      "detailed_description": "A benchmark suite specifically designed for evaluating molecular docking methods. It provides datasets and metrics to assess the accuracy of pose prediction and binding affinity estimation in drug discovery contexts.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "molecular_docking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/CataAI/PoseX",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "molecular-docking",
        "benchmark",
        "drug-discovery",
        "pose-prediction"
      ],
      "id": 13
    },
    {
      "name": "jetson-orin-matmul-analysis",
      "one_line_profile": "Scientific CUDA benchmarking framework for matrix multiplication on Jetson Orin",
      "detailed_description": "A scientific benchmarking framework for evaluating CUDA matrix multiplication performance on Jetson Orin edge devices. It includes implementations for different power modes and matrix sizes, providing performance (GFLOPS) and power efficiency analysis.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "hpc_performance"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Cre4T3Tiv3/jetson-orin-matmul-analysis",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "cuda",
        "benchmarking",
        "edge-ai",
        "matrix-multiplication"
      ],
      "id": 14
    },
    {
      "name": "Benchmark-Gemma-Models",
      "one_line_profile": "Customizable Python suite for evaluating Gemma and LLaMA-based LLMs",
      "detailed_description": "A lightweight and customizable framework designed for benchmarking Large Language Models (LLMs) like Gemma and LLaMA. It supports custom scripts for models, datasets, tasks, and metrics, facilitating reproducible evaluation experiments.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/D0men1c0/Benchmark-Gemma-Models",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "evaluation",
        "gemma",
        "llama"
      ],
      "id": 15
    },
    {
      "name": "ABC (Annotated Beethoven Corpus)",
      "one_line_profile": "Dataset of harmonic analyses for Beethoven's string quartets",
      "detailed_description": "The Annotated Beethoven Corpus (ABC) provides expert harmonic analyses of Beethoven's string quartets. It serves as a benchmark dataset for symbolic music analysis and computational musicology tasks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "music_analysis",
        "harmonic_analysis"
      ],
      "application_level": "dataset",
      "primary_language": "None",
      "repo_url": "https://github.com/DCMLab/ABC",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "musicology",
        "dataset",
        "beethoven",
        "harmonics"
      ],
      "id": 16
    },
    {
      "name": "DeepDIVA",
      "one_line_profile": "Framework for reproducible deep learning experiments in document image analysis",
      "detailed_description": "A Python framework designed to enable quick and reproducible experiments in Deep Learning, specifically for Document Image Analysis (DIA). Although deprecated, it represents a tool for managing experimental workflows and hyperparameters.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "experiment_reproduction",
        "document_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/DIVA-DIA/DeepDIVA",
      "help_website": [
        "https://diva-dia.github.io/DeepDIVA/"
      ],
      "license": "LGPL-3.0",
      "tags": [
        "reproducibility",
        "deep-learning",
        "document-analysis"
      ],
      "id": 17
    },
    {
      "name": "DMind-Benchmark",
      "one_line_profile": "Evaluation framework for LLMs on blockchain and Web3 knowledge",
      "detailed_description": "A comprehensive benchmark framework designed to evaluate Large Language Models (LLMs) on domain-specific knowledge related to blockchain, cryptocurrency, and Web3 technologies.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "domain_specific_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/DMindAI/DMind-Benchmark",
      "help_website": [],
      "license": "None",
      "tags": [
        "llm",
        "blockchain",
        "benchmark",
        "web3"
      ],
      "id": 18
    },
    {
      "name": "DDRL4NAV",
      "one_line_profile": "Reproducible reinforcement learning training framework based on OpenAI Five",
      "detailed_description": "A lightweight reinforcement learning training framework that reproduces the architecture used in OpenAI Five. It separates Forward, Backward, and Env modules to facilitate RL research and reproduction of navigation tasks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "reproduction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/DRL-Navigation/DDRL4NAV",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "reinforcement-learning",
        "openai-five",
        "navigation"
      ],
      "id": 19
    },
    {
      "name": "DeepSpark",
      "one_line_profile": "Evaluation platform for industrial AI algorithms and models",
      "detailed_description": "An open platform that provides a multi-dimensional evaluation system for open-source application algorithms and models. It focuses on industrial applications and supports mainstream frameworks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "industrial_ai"
      ],
      "application_level": "platform",
      "primary_language": "None",
      "repo_url": "https://github.com/Deep-Spark/DeepSpark",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "evaluation",
        "industrial-ai",
        "benchmark"
      ],
      "id": 20
    },
    {
      "name": "SPDEBench",
      "one_line_profile": "Benchmark for learning regular and singular stochastic PDEs",
      "detailed_description": "SPDEBench is an extensive benchmark suite designed for evaluating machine learning models on regular and singular Stochastic Partial Differential Equations (SPDEs), facilitating research in scientific machine learning (SciML).",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "pde_solving",
        "sciml_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/DeepIntoStreams/SPDE_hackathon",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "spde",
        "pde",
        "benchmark",
        "sciml"
      ],
      "id": 21
    },
    {
      "name": "Khmer OCR Benchmark Dataset",
      "one_line_profile": "Standardized benchmark dataset for Khmer Optical Character Recognition",
      "detailed_description": "A standardized dataset designed to benchmark Optical Character Recognition (OCR) engines specifically for the Khmer language, facilitating evaluation and comparison of OCR models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "ocr_evaluation",
        "dataset"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/EKYCSolutions/khmer-ocr-benchmark-dataset",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ocr",
        "khmer",
        "benchmark",
        "dataset"
      ],
      "id": 22
    },
    {
      "name": "EvoxBench",
      "one_line_profile": "Benchmark suite for evolutionary neural architecture search",
      "detailed_description": "A benchmark suite that transforms Neural Architecture Search (NAS) into multi-objective optimization problems, designed for testing evolutionary algorithms in deep learning contexts.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "neural_architecture_search",
        "evolutionary_algorithms"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/EMI-Group/evoxbench",
      "help_website": [
        "https://evoxbench.readthedocs.io/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "nas",
        "evolutionary-computation",
        "benchmark"
      ],
      "id": 23
    },
    {
      "name": "LM Evaluation Harness",
      "one_line_profile": "Framework for few-shot evaluation of language models",
      "detailed_description": "A widely used framework for evaluating autoregressive language models (LLMs) on a large number of tasks. It provides a unified interface for few-shot evaluation and benchmarking.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "nlp_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/EleutherAI/lm-evaluation-harness",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "evaluation",
        "few-shot",
        "benchmark"
      ],
      "id": 24
    },
    {
      "name": "JamAIBase",
      "one_line_profile": "Collaborative spreadsheet platform for LLM experimentation and evaluation",
      "detailed_description": "A platform that combines a spreadsheet interface with LLM capabilities, allowing users to chain cells into pipelines, experiment with prompts, and evaluate LLM responses in real-time.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "prompt_engineering",
        "model_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/EmbeddedLLM/JamAIBase",
      "help_website": [
        "https://jamaibase.com/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "spreadsheet",
        "no-code",
        "evaluation"
      ],
      "id": 25
    },
    {
      "name": "MoSQITo",
      "one_line_profile": "Modular framework for sound quality metrics and psychoacoustics",
      "detailed_description": "A unified and modular development framework for calculating key sound quality metrics. It supports reproducible science in acoustics and psychoacoustics by providing standardized implementations of metrics.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "signal_processing",
        "psychoacoustics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Eomys/MoSQITo",
      "help_website": [
        "https://github.com/Eomys/MoSQITo"
      ],
      "license": "Apache-2.0",
      "tags": [
        "acoustics",
        "sound-quality",
        "metrics"
      ],
      "id": 26
    },
    {
      "name": "WEHUB (Water Environmental Hub)",
      "one_line_profile": "Cloud-based platform for accessing and sharing environmental data",
      "detailed_description": "An open-source web platform designed to facilitate the search, access, and sharing of water and environmental data. It supports international data standards and provides tools for data translation and cataloging.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "data_management",
        "environmental_science"
      ],
      "application_level": "platform",
      "primary_language": "Ruby",
      "repo_url": "https://github.com/ExplorusDataSolutions/WaterEnvironmentalHub-WEHUB",
      "help_website": [],
      "license": "None",
      "tags": [
        "environmental-data",
        "water",
        "data-sharing"
      ],
      "id": 27
    },
    {
      "name": "openFDA",
      "one_line_profile": "API and data access service for FDA public datasets",
      "detailed_description": "A project providing open APIs, raw data downloads, and documentation for a large collection of FDA public datasets, enabling researchers and developers to access health and drug-related data programmatically.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "data_access",
        "public_health"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/FDA/openfda",
      "help_website": [
        "https://open.fda.gov/"
      ],
      "license": "CC0-1.0",
      "tags": [
        "fda",
        "health-data",
        "api"
      ],
      "id": 28
    },
    {
      "name": "FluxBench.jl",
      "one_line_profile": "Benchmark suite for the FluxML ecosystem and scientific machine learning",
      "detailed_description": "A benchmarking tool for the FluxML ecosystem, covering deep learning, scientific machine learning (SciML), differentiable programming, and CUDA-accelerated workloads in Julia.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "sciml"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/FluxML/FluxBench.jl",
      "help_website": [],
      "license": "None",
      "tags": [
        "julia",
        "fluxml",
        "benchmark",
        "sciml"
      ],
      "id": 29
    },
    {
      "name": "LLM Zoo",
      "one_line_profile": "Repository of data, models, and benchmarks for LLMs",
      "detailed_description": "A project providing a collection of instruction-tuning data, models, and evaluation benchmarks for Large Language Models, facilitating the development and assessment of LLMs.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "dataset"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/FreedomIntelligence/LLMZoo",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "benchmark",
        "instruction-tuning"
      ],
      "id": 30
    },
    {
      "name": "LAB-Bench",
      "one_line_profile": "Benchmark dataset for AI capabilities in biological research",
      "detailed_description": "An evaluation dataset designed to benchmark AI systems on capabilities foundational to scientific research in biology, such as literature retrieval, protocol planning, and data interpretation.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "biology_evaluation",
        "scientific_reasoning"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/Future-House/LAB-Bench",
      "help_website": [],
      "license": "CC-BY-SA-4.0",
      "tags": [
        "biology",
        "benchmark",
        "ai4science"
      ],
      "id": 31
    },
    {
      "name": "MAYE",
      "one_line_profile": "Framework for evaluating RL scaling in Vision Language Models",
      "detailed_description": "A framework and comprehensive evaluation scheme for Reinforcement Learning (RL) scaling in Vision Language Models (VLMs), providing tools for transparent and from-scratch training and assessment.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "rl_evaluation",
        "vlm_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/GAIR-NLP/MAYE",
      "help_website": [],
      "license": "None",
      "tags": [
        "vlm",
        "reinforcement-learning",
        "evaluation"
      ],
      "id": 32
    },
    {
      "name": "Molecules Dataset Collection",
      "one_line_profile": "Collection of molecular datasets for property inference validation",
      "detailed_description": "A curated collection of datasets containing molecular structures and properties, intended for the validation and benchmarking of molecular property inference models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "molecular_property_prediction",
        "dataset"
      ],
      "application_level": "dataset",
      "primary_language": "None",
      "repo_url": "https://github.com/GLambard/Molecules_Dataset_Collection",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "molecules",
        "dataset",
        "chemistry"
      ],
      "id": 33
    },
    {
      "name": "GPT-Fathom",
      "one_line_profile": "Reproducible evaluation suite for LLMs on curated benchmarks",
      "detailed_description": "An open-source LLM evaluation suite that benchmarks leading open-source and closed-source models on over 20 curated benchmarks under aligned settings to ensure reproducibility.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/GPT-Fathom/GPT-Fathom",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "evaluation",
        "reproducibility"
      ],
      "id": 34
    },
    {
      "name": "MolCap-Arena",
      "one_line_profile": "Benchmark for language-enhanced molecular property prediction",
      "detailed_description": "A comprehensive captioning benchmark designed for evaluating language-enhanced molecular property prediction models, facilitating research at the intersection of chemistry and NLP.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "molecular_captioning",
        "property_prediction"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/Genentech/molcap-arena",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "molecular-captioning",
        "benchmark",
        "chemistry"
      ],
      "id": 35
    },
    {
      "name": "MultimodalHugs",
      "one_line_profile": "Framework for training and evaluating multimodal AI models",
      "detailed_description": "An extension of Hugging Face Transformers that provides a generalized framework for training, evaluating, and using multimodal AI models with unified code interfaces.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "multimodal_training",
        "evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/GerrySant/multimodalhugs",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "multimodal",
        "huggingface",
        "framework"
      ],
      "id": 36
    },
    {
      "name": "Giskard",
      "one_line_profile": "Evaluation and testing library for LLM agents and AI models",
      "detailed_description": "An open-source library for testing and evaluating AI models and LLM agents. It helps detect vulnerabilities, hallucinations, and performance issues through automated tests.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_testing",
        "quality_assurance"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Giskard-AI/giskard-oss",
      "help_website": [
        "https://docs.giskard.ai/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "testing",
        "llm",
        "evaluation",
        "qa"
      ],
      "id": 37
    },
    {
      "name": "GeSS",
      "one_line_profile": "Benchmark for Geometric Deep Learning under distribution shifts",
      "detailed_description": "A benchmarking suite for Geometric Deep Learning (GDL) focusing on scientific applications and robustness against distribution shifts in geometric data.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "geometric_deep_learning",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Graph-COM/GESS",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gdl",
        "benchmark",
        "scientific-applications"
      ],
      "id": 38
    },
    {
      "name": "SciKnowEval",
      "one_line_profile": "Benchmark for evaluating scientific knowledge of LLMs",
      "detailed_description": "A benchmark designed to evaluate the multi-level scientific knowledge capabilities of Large Language Models, assessing their proficiency in various scientific domains.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "scientific_knowledge_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/HICAI-ZJU/SciKnowEval",
      "help_website": [],
      "license": "None",
      "tags": [
        "llm",
        "science",
        "evaluation"
      ],
      "id": 39
    },
    {
      "name": "NewtonBench",
      "one_line_profile": "Benchmark for scientific law discovery in LLM agents",
      "detailed_description": "A benchmark suite for evaluating the ability of LLM-based agents to discover generalizable scientific laws, testing their reasoning and discovery capabilities.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "scientific_discovery",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/HKUST-KnowComp/NewtonBench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "scientific-discovery",
        "llm-agent",
        "benchmark"
      ],
      "id": 40
    },
    {
      "name": "MD-Bench",
      "one_line_profile": "Prototyping harness for Molecular Dynamics algorithms",
      "detailed_description": "A performance-oriented benchmarking harness for developing and testing state-of-the-art Molecular Dynamics (MD) algorithms, focusing on computational efficiency.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "molecular_dynamics",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "C",
      "repo_url": "https://github.com/HPC-Dwarfs/MD-Bench",
      "help_website": [],
      "license": "LGPL-3.0",
      "tags": [
        "molecular-dynamics",
        "hpc",
        "benchmark"
      ],
      "id": 41
    },
    {
      "name": "BubbleML",
      "one_line_profile": "Dataset and benchmarks for multiphase multiphysics SciML",
      "detailed_description": "A dataset and benchmark suite for scientific machine learning (SciML) focused on multiphase multiphysics simulations, providing data for training and evaluating physics-informed models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "multiphysics_simulation",
        "sciml_benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/HPCForge/BubbleML",
      "help_website": [],
      "license": "None",
      "tags": [
        "sciml",
        "multiphysics",
        "dataset"
      ],
      "id": 42
    },
    {
      "name": "GMNS Plus Dataset",
      "one_line_profile": "Standardized transportation network dataset collection",
      "detailed_description": "A standardized collection of transportation network datasets based on the General Modeling Network Specification (GMNS), supporting reproducible research in transportation modeling and planning.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "transportation_modeling",
        "dataset"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/HanZhengIntelliTransport/GMNS_Plus_Dataset",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "transportation",
        "gmns",
        "dataset"
      ],
      "id": 43
    },
    {
      "name": "Helicone",
      "one_line_profile": "Observability and evaluation platform for LLM applications",
      "detailed_description": "An open-source platform for monitoring, evaluating, and experimenting with Large Language Models (LLMs). It provides tools for logging, caching, and analyzing model performance.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "llm_observability",
        "model_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/Helicone/helicone",
      "help_website": [
        "https://docs.helicone.ai/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "observability",
        "llm",
        "evaluation"
      ],
      "id": 44
    },
    {
      "name": "DL-based-MI-EEG-models",
      "one_line_profile": "Deep learning models and leaderboard for Motor Imagery EEG analysis",
      "detailed_description": "A repository collecting source code of representative deep learning-based Motor Imagery EEG (MI-EEG) models and running a leaderboard for fair comparison.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "eeg_analysis",
        "model_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Henrywang621/DL-based-MI-EEG-models",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "eeg",
        "brain-computer-interface",
        "deep-learning"
      ],
      "id": 45
    },
    {
      "name": "SDE-Harness",
      "one_line_profile": "Framework for evaluating scientific discovery capabilities of agents",
      "detailed_description": "SDE-Harness (Scientific Discovery Evaluation Framework) is designed to evaluate AI agents on scientific discovery tasks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "scientific_discovery_evaluation",
        "agent_benchmarking"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/HowieHwong/sde-harness",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "scientific-discovery",
        "evaluation-framework",
        "agents"
      ],
      "id": 46
    },
    {
      "name": "Multimodal 3D Image Segmentation",
      "one_line_profile": "Frameworks for multimodal 3D medical image segmentation",
      "detailed_description": "Source code for multimodal image segmentation frameworks, providing network architectures and training procedures for 3D medical imaging analysis.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "medical_image_segmentation",
        "3d_imaging"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/IBM/multimodal-3d-image-segmentation",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "medical-imaging",
        "segmentation",
        "multimodal"
      ],
      "id": 47
    },
    {
      "name": "VLM4Bio",
      "one_line_profile": "Benchmark dataset of scientific QA pairs for biological images",
      "detailed_description": "A benchmark dataset of scientific question-answer pairs used to evaluate pretrained Vision-Language Models (VLMs) for trait discovery from biological images.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "biological_image_analysis",
        "visual_question_answering"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/Imageomics/VLM4Bio",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "biology",
        "vlm",
        "benchmark"
      ],
      "id": 48
    },
    {
      "name": "InternManip",
      "one_line_profile": "Robot manipulation learning suite for policy training and evaluation",
      "detailed_description": "An all-in-one robot manipulation learning suite for policy models training and evaluation on various datasets and benchmarks, relevant to embodied AI in science.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "robotic_manipulation",
        "policy_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/InternRobotics/InternManip",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "robotics",
        "manipulation",
        "embodied-ai"
      ],
      "id": 49
    },
    {
      "name": "SGI-Bench",
      "one_line_profile": "Benchmark defining and evaluating Scientific General Intelligence",
      "detailed_description": "A benchmark suite designed to define and evaluate Scientific General Intelligence capabilities in AI models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "scientific_reasoning_evaluation",
        "general_intelligence"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/InternScience/SGI-Bench",
      "help_website": [],
      "license": null,
      "tags": [
        "scientific-intelligence",
        "benchmark",
        "reasoning"
      ],
      "id": 50
    },
    {
      "name": "Computer-Generated-Hologram",
      "one_line_profile": "Simulation framework for digital holography and computer-generated holograms",
      "detailed_description": "A computational framework for simulating the production process of computer holography, recording, and reproducing holograms using MATLAB and Python.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "holography_simulation",
        "optical_computing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/JackHCC/Computer-Generated-Hologram",
      "help_website": [],
      "license": null,
      "tags": [
        "holography",
        "optics",
        "simulation"
      ],
      "id": 51
    },
    {
      "name": "Science-T2I",
      "one_line_profile": "Benchmark for addressing scientific illusions in text-to-image synthesis",
      "detailed_description": "A benchmark designed to evaluate and address scientific illusions in image synthesis models, ensuring scientific accuracy in generated imagery.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "scientific_image_synthesis",
        "hallucination_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/Jialuo-Li/Science-T2I",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "image-synthesis",
        "scientific-accuracy",
        "benchmark"
      ],
      "id": 52
    },
    {
      "name": "FAERS Data Toolkit",
      "one_line_profile": "Tools for processing FDA Adverse Event Reporting System datasets",
      "detailed_description": "Script tools for downloading, data preprocessing, data merging, and standardizing the FDA Adverse Event Reporting System (FAERS) dataset for pharmacovigilance research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "pharmacovigilance",
        "data_preprocessing"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/Judenpech/FAERS-data-toolkit",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "faers",
        "pharmacovigilance",
        "medical-data"
      ],
      "id": 53
    },
    {
      "name": "LLM-MAP",
      "one_line_profile": "Bimanual robot task planning using Large Language Models",
      "detailed_description": "Implementation of LLM+MAP for bimanual robot task planning using LLMs and PDDL, relevant to lab automation and embodied AI.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "robot_task_planning",
        "embodied_ai"
      ],
      "application_level": "library",
      "primary_language": null,
      "repo_url": "https://github.com/Kchu/LLM-MAP",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "robotics",
        "task-planning",
        "llm"
      ],
      "id": 54
    },
    {
      "name": "MolPuzzle",
      "one_line_profile": "Multimodal benchmark for molecular structure elucidation using LLMs",
      "detailed_description": "A multimodal benchmark designed to evaluate the capability of Large Language Models in solving molecule puzzles and elucidating molecular structures.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "molecular_structure_prediction",
        "chemistry_reasoning"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/KehanGuo2/MolPuzzle",
      "help_website": [],
      "license": null,
      "tags": [
        "chemistry",
        "molecular-structure",
        "benchmark"
      ],
      "id": 55
    },
    {
      "name": "Guaranteed-Non-Local-Molecular-Dataset",
      "one_line_profile": "Dataset for benchmarking non-local capabilities of geometric ML models",
      "detailed_description": "A dataset specifically created to benchmark the non-local capabilities of geometric machine learning models in molecular contexts.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "molecular_modeling",
        "geometric_deep_learning"
      ],
      "application_level": "dataset",
      "primary_language": null,
      "repo_url": "https://github.com/LarsSchaaf/Guaranteed-Non-Local-Molecular-Dataset",
      "help_website": [],
      "license": null,
      "tags": [
        "molecular-dataset",
        "geometric-ml",
        "benchmark"
      ],
      "id": 56
    },
    {
      "name": "Spyglass",
      "one_line_profile": "Neuroscience data analysis framework for reproducible research",
      "detailed_description": "A neuroscience data analysis framework built for reproducible research, facilitating the management and analysis of complex neurophysiological data.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "neuroscience_data_analysis",
        "reproducible_research"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/LorenFrankLab/spyglass",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "neuroscience",
        "data-analysis",
        "framework"
      ],
      "id": 57
    },
    {
      "name": "MolData",
      "one_line_profile": "Molecular benchmark for disease and target-based machine learning",
      "detailed_description": "A benchmark suite for evaluating machine learning models on molecular datasets focused on diseases and targets.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "drug_discovery",
        "molecular_property_prediction"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/LumosBio/MolData",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "molecular-benchmark",
        "drug-discovery",
        "machine-learning"
      ],
      "id": 58
    },
    {
      "name": "Proteomics Lab Agent",
      "one_line_profile": "Multimodal agentic AI framework for automating proteomics laboratory work",
      "detailed_description": "A multimodal, agentic AI framework that links written instructions to real-world laboratory work in proteomics, using video analysis for documentation and guidance.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "lab_automation",
        "proteomics"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/MannLabs/proteomics_lab_agent",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "proteomics",
        "lab-automation",
        "agentic-ai"
      ],
      "id": 59
    },
    {
      "name": "MedMNIST",
      "one_line_profile": "Collection of standardized datasets for biomedical image classification",
      "detailed_description": "A large-scale MNIST-like collection of standardized biomedical images, including 2D and 3D datasets, designed for lightweight benchmarking of biomedical image analysis models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "biomedical_image_classification",
        "benchmark_suite"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/MedMNIST/MedMNIST",
      "help_website": [
        "https://medmnist.com/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "biomedical-imaging",
        "classification",
        "dataset"
      ],
      "id": 60
    },
    {
      "name": "PaperArena",
      "one_line_profile": "Benchmark for tool-augmented agentic reasoning on scientific literature",
      "detailed_description": "An evaluation benchmark designed to assess the performance of tool-augmented agents in reasoning over scientific literature.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "scientific_literature_mining",
        "agent_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/Melmaphother/PaperArena",
      "help_website": [],
      "license": null,
      "tags": [
        "scientific-literature",
        "agents",
        "benchmark"
      ],
      "id": 61
    },
    {
      "name": "MobiSurvStd",
      "one_line_profile": "Standardization tool for French mobility survey datasets",
      "detailed_description": "A Python tool designed to standardize various French mobility survey datasets (such as EMC², EGT, EMP) into a unified format to facilitate transportation research and social science analysis.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "data_standardization",
        "data_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/MobiSurvStd/MobiSurvStd",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mobility-surveys",
        "data-standardization",
        "transportation-science"
      ],
      "id": 62
    },
    {
      "name": "Modalities",
      "one_line_profile": "Distributed and reproducible foundation model training framework",
      "detailed_description": "A PyTorch-native framework designed for the distributed and reproducible training of foundation models, emphasizing scientific rigor in model development experiments.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_training",
        "reproducibility"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/Modalities/modalities",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "foundation-models",
        "distributed-training",
        "reproducibility"
      ],
      "id": 63
    },
    {
      "name": "smiles-featurizers",
      "one_line_profile": "Molecular SMILES embedding extractor using language models",
      "detailed_description": "A tool to extract molecular embeddings from SMILES strings using various pre-trained language model architectures, facilitating downstream cheminformatics and drug discovery tasks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "feature_extraction",
        "molecular_representation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/MoleculeTransformers/smiles-featurizers",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "smiles",
        "molecular-embeddings",
        "cheminformatics"
      ],
      "id": 64
    },
    {
      "name": "MolScore",
      "one_line_profile": "Automated scoring function for de novo molecular design",
      "detailed_description": "An automated scoring framework to facilitate and standardize the evaluation of goal-directed generative models for de novo molecular design, supporting various objective functions.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "molecular_design"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/MorganCThomas/MolScore",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "molecular-design",
        "generative-models",
        "scoring-function"
      ],
      "id": 65
    },
    {
      "name": "BiomedSQL",
      "one_line_profile": "Text-to-SQL benchmark for biomedical scientific reasoning",
      "detailed_description": "A benchmark dataset and task definition for evaluating Text-to-SQL capabilities specifically within the domain of biomedical scientific reasoning.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "scientific_reasoning"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/NIH-CARD/biomedsql",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "text-to-sql",
        "biomedical",
        "benchmark"
      ],
      "id": 66
    },
    {
      "name": "ComputeEval",
      "one_line_profile": "Evaluation framework for CUDA code generation models",
      "detailed_description": "A framework designed to generate and evaluate CUDA code from Large Language Models, targeting high-performance computing and scientific code generation tasks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "code_generation",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NVIDIA/compute-eval",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "cuda",
        "llm-evaluation",
        "hpc"
      ],
      "id": 67
    },
    {
      "name": "Framework Reproducibility",
      "one_line_profile": "Tools for ensuring reproducibility in deep learning frameworks",
      "detailed_description": "A collection of tools and methodologies to ensure deterministic and reproducible results across deep learning frameworks, essential for scientific validity of AI experiments.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "reproducibility",
        "experiment_control"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NVIDIA/framework-reproducibility",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "reproducibility",
        "deep-learning",
        "determinism"
      ],
      "id": 68
    },
    {
      "name": "Rapidae",
      "one_line_profile": "Framework for developing and comparing autoencoder models",
      "detailed_description": "A back-end agnostic framework to explore, compare, and develop autoencoder models, facilitating research into representation learning and generative modeling.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_development",
        "autoencoders"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NahuelCostaCortez/rapidae",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "autoencoder",
        "model-comparison",
        "deep-learning"
      ],
      "id": 69
    },
    {
      "name": "CanarySEFI",
      "one_line_profile": "Robustness evaluation framework for image recognition models",
      "detailed_description": "A comprehensive framework for evaluating the robustness of deep learning-based image recognition models against adversarial attacks, including metrics for attack and defense effectiveness.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "adversarial_testing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NeoSunJZ/Canary_Master",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "robustness",
        "adversarial-attacks",
        "model-evaluation"
      ],
      "id": 70
    },
    {
      "name": "Atropos",
      "one_line_profile": "LLM reinforcement learning environment and evaluation framework",
      "detailed_description": "A framework for collecting and evaluating Large Language Model trajectories through diverse environments, supporting reinforcement learning research and agent evaluation.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "agent_evaluation",
        "reinforcement_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NousResearch/atropos",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "reinforcement-learning",
        "evaluation"
      ],
      "id": 71
    },
    {
      "name": "ScienceBoard",
      "one_line_profile": "Benchmark for multimodal autonomous agents in scientific workflows",
      "detailed_description": "A benchmark suite and environment designed to evaluate the performance of multimodal autonomous agents in realistic scientific workflows and tasks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "agent_benchmarking",
        "scientific_workflow"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/OS-Copilot/ScienceBoard",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "autonomous-agents",
        "scientific-benchmark",
        "multimodal"
      ],
      "id": 72
    },
    {
      "name": "OneIG-Bench",
      "one_line_profile": "Fine-grained evaluation benchmark for Text-to-Image models",
      "detailed_description": "A comprehensive benchmark framework for evaluating Text-to-Image models across dimensions like subject-element alignment, reasoning, and scientific/technical accuracy in generation.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "text-to-image"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/OneIG-Bench/OneIG-Benchmark",
      "help_website": [],
      "license": null,
      "tags": [
        "text-to-image",
        "benchmark",
        "generative-ai"
      ],
      "id": 73
    },
    {
      "name": "OlympiadBench",
      "one_line_profile": "Benchmark for Olympiad-level scientific problems",
      "detailed_description": "A challenging benchmark dataset and evaluation framework featuring Olympiad-level bilingual multimodal scientific problems to promote AGI research in scientific reasoning.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "scientific_reasoning",
        "benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenBMB/OlympiadBench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "scientific-reasoning",
        "olympiad",
        "multimodal"
      ],
      "id": 74
    },
    {
      "name": "UltraEval",
      "one_line_profile": "Open source framework for evaluating foundation models",
      "detailed_description": "A comprehensive framework for evaluating foundation models across various capabilities, providing a standardized interface for model assessment.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenBMB/UltraEval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "foundation-models",
        "evaluation-framework",
        "llm"
      ],
      "id": 75
    },
    {
      "name": "OpenBioLink",
      "one_line_profile": "Evaluation framework for biomedical link prediction",
      "detailed_description": "A resource and evaluation framework designed for assessing link prediction models on heterogeneous biomedical graph data, facilitating network biology research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "link_prediction",
        "graph_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenBioLink/OpenBioLink",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "biomedical-graph",
        "link-prediction",
        "benchmark"
      ],
      "id": 76
    },
    {
      "name": "BioSimSpace",
      "one_line_profile": "Interoperable framework for biomolecular simulation",
      "detailed_description": "A Python framework that provides an interoperable interface for various biomolecular simulation tools, facilitating the setup, execution, and analysis of molecular dynamics simulations.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "molecular_simulation",
        "workflow_management"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenBioSim/biosimspace",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "molecular-dynamics",
        "simulation",
        "interoperability"
      ],
      "id": 77
    },
    {
      "name": "MULTI-Benchmark",
      "one_line_profile": "Multimodal understanding leaderboard with text and images",
      "detailed_description": "A benchmark suite for evaluating multimodal AI models on their understanding of complex text and image data.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "multimodal_evaluation",
        "benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenDFM/MULTI-Benchmark",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "multimodal",
        "benchmark",
        "leaderboard"
      ],
      "id": 78
    },
    {
      "name": "SciEval",
      "one_line_profile": "Multi-level LLM evaluation benchmark for scientific research",
      "detailed_description": "A benchmark designed to evaluate Large Language Models specifically on their capability to perform scientific research tasks across multiple levels of complexity.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "scientific_evaluation",
        "benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenDFM/SciEval",
      "help_website": [],
      "license": null,
      "tags": [
        "scientific-research",
        "llm-evaluation",
        "benchmark"
      ],
      "id": 79
    },
    {
      "name": "OpenHands Benchmarks",
      "one_line_profile": "Evaluation harness for OpenHands agents",
      "detailed_description": "The evaluation harness and benchmark suite for OpenHands, enabling the assessment of autonomous coding and task-solving agents.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "agent_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenHands/benchmarks",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "autonomous-agents",
        "evaluation",
        "harness"
      ],
      "id": 80
    },
    {
      "name": "GAOKAO-Bench",
      "one_line_profile": "LLM evaluation framework using GAOKAO questions",
      "detailed_description": "An evaluation framework that utilizes questions from the Chinese National College Entrance Examination (GAOKAO) to assess the knowledge and reasoning capabilities of Large Language Models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "knowledge_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenLMLab/GAOKAO-Bench",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "gaokao",
        "llm-evaluation",
        "reasoning"
      ],
      "id": 81
    },
    {
      "name": "OG-Core",
      "one_line_profile": "Overlapping generations model framework for fiscal policy evaluation",
      "detailed_description": "A Python framework for modeling overlapping generations (OG) to evaluate the economic effects of fiscal policies, serving as a tool for economic simulation and analysis.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "economic_modeling",
        "policy_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/PSLmodels/OG-Core",
      "help_website": [],
      "license": "CC0-1.0",
      "tags": [
        "economics",
        "simulation",
        "fiscal-policy"
      ],
      "id": 82
    },
    {
      "name": "GBBS",
      "one_line_profile": "Graph Based Benchmark Suite",
      "detailed_description": "A comprehensive benchmark suite for evaluating the performance of graph algorithms and systems, essential for research in high-performance graph processing.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "graph_benchmarking",
        "algorithm_evaluation"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/ParAlg/gbbs",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "graph-algorithms",
        "benchmark",
        "hpc"
      ],
      "id": 83
    },
    {
      "name": "SkillMetricsToolbox",
      "one_line_profile": "Matlab toolbox for model skill assessment",
      "detailed_description": "A collection of Matlab functions for calculating statistical metrics (like Taylor diagrams) to evaluate the skill of model predictions against observational data.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_verification",
        "statistical_analysis"
      ],
      "application_level": "library",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/PeterRochford/SkillMetricsToolbox",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "model-evaluation",
        "statistics",
        "matlab"
      ],
      "id": 84
    },
    {
      "name": "The Well",
      "one_line_profile": "Physics simulation dataset collection and access tools",
      "detailed_description": "A large-scale collection of physics simulation datasets accompanied by tools/loaders to facilitate research in physics-informed machine learning and simulation surrogates.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "physics_simulation",
        "dataset_access"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/PolymathicAI/the_well",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "physics",
        "simulation",
        "dataset"
      ],
      "id": 85
    },
    {
      "name": "LIKWID",
      "one_line_profile": "Performance monitoring and benchmarking suite for HPC",
      "detailed_description": "A tool suite for performance oriented programmers to measure and benchmark hardware performance counters, critical for optimizing scientific computing applications.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "performance_benchmarking",
        "hpc_optimization"
      ],
      "application_level": "solver",
      "primary_language": "C",
      "repo_url": "https://github.com/RRZE-HPC/likwid",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "hpc",
        "performance-monitoring",
        "benchmarking"
      ],
      "id": 86
    },
    {
      "name": "LLMBox",
      "one_line_profile": "Unified library for LLM training and evaluation",
      "detailed_description": "A comprehensive library for implementing Large Language Models, featuring a unified pipeline for training and rigorous model evaluation.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_training",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/RUCAIBox/LLMBox",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "training-pipeline",
        "evaluation"
      ],
      "id": 87
    },
    {
      "name": "STAVER",
      "one_line_profile": "Algorithm for variation reduction in DIA MS data",
      "detailed_description": "A standardized dataset-based algorithm designed to efficiently reduce variation in large-scale Data-Independent Acquisition (DIA) Mass Spectrometry data.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "data_processing",
        "mass_spectrometry"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/Ran485/STAVER",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "proteomics",
        "mass-spectrometry",
        "data-normalization"
      ],
      "id": 88
    },
    {
      "name": "RobustBench",
      "one_line_profile": "Standardized adversarial robustness benchmark",
      "detailed_description": "A standardized benchmark and library for evaluating the adversarial robustness of image classification models, providing a leaderboard and model zoo.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/RobustBench/robustbench",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "adversarial-robustness",
        "benchmark",
        "computer-vision"
      ],
      "id": 89
    },
    {
      "name": "RouterArena",
      "one_line_profile": "Evaluation framework for LLM routers",
      "detailed_description": "An open framework for evaluating LLM routing strategies with standardized datasets and metrics, facilitating the optimization of compound AI systems.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_routing",
        "system_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/RouteWorks/RouterArena",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-routing",
        "evaluation",
        "benchmark"
      ],
      "id": 90
    },
    {
      "name": "RAG Evaluation Harnesses",
      "one_line_profile": "Evaluation suite for Retrieval-Augmented Generation",
      "detailed_description": "A comprehensive evaluation suite designed to assess the performance of Retrieval-Augmented Generation (RAG) systems across various metrics.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "rag_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/RulinShao/RAG-evaluation-harnesses",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "evaluation",
        "retrieval"
      ],
      "id": 91
    },
    {
      "name": "ServerlessBench",
      "one_line_profile": "Benchmark suite for serverless computing",
      "detailed_description": "A benchmark suite designed to evaluate the performance and characteristics of serverless computing platforms, aiding systems research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "system_benchmarking",
        "serverless"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/SJTU-IPADS/ServerlessBench",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "serverless",
        "benchmark",
        "systems-research"
      ],
      "id": 92
    },
    {
      "name": "Guardians MT Eval",
      "one_line_profile": "Machine Translation meta-evaluation metrics",
      "detailed_description": "Implementation of metrics for the meta-evaluation of Machine Translation systems, as proposed in the ACL 2024 paper 'Guardians of the Machine Translation Meta-Evaluation'.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "mt_evaluation",
        "meta_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/SapienzaNLP/guardians-mt-eval",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "machine-translation",
        "evaluation-metrics",
        "nlp"
      ],
      "id": 93
    },
    {
      "name": "SceMQA",
      "one_line_profile": "Benchmark for scientific multimodal question answering",
      "detailed_description": "A scientific college entrance level multimodal question answering benchmark designed to evaluate the reasoning capabilities of multimodal models in scientific domains.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "question_answering",
        "scientific_reasoning"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/SceMQA/SceMQA",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "multimodal",
        "qa",
        "scientific-benchmark"
      ],
      "id": 94
    },
    {
      "name": "DiffEqDevTools.jl",
      "one_line_profile": "Benchmarking and testing tools for differential equations",
      "detailed_description": "A Julia library providing tools for benchmarking, testing, and developing solvers for differential equations and scientific machine learning (SciML).",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "differential_equations"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/SciML/DiffEqDevTools.jl",
      "help_website": [
        "https://docs.sciml.ai/DiffEqDevTools/stable/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "sciml",
        "differential-equations",
        "benchmarking"
      ],
      "id": 95
    },
    {
      "name": "SciMLBenchmarks.jl",
      "one_line_profile": "Scientific machine learning and differential equation solver benchmarks",
      "detailed_description": "A comprehensive benchmark suite for scientific machine learning (SciML) and differential equation solvers, covering Julia, Python, MATLAB, and R implementations.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "solver_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/SciML/SciMLBenchmarks.jl",
      "help_website": [
        "https://benchmarks.sciml.ai/"
      ],
      "license": "MIT",
      "tags": [
        "sciml",
        "benchmarks",
        "ode-solvers"
      ],
      "id": 96
    },
    {
      "name": "AstroVisBench",
      "one_line_profile": "Benchmark for astronomy scientific computing and visualization",
      "detailed_description": "The first benchmark designed to evaluate both scientific computing and visualization capabilities specifically within the astronomy domain.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "visualization",
        "scientific_computing"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/SebaJoe/AstroVisBench",
      "help_website": [],
      "license": null,
      "tags": [
        "astronomy",
        "visualization",
        "benchmark"
      ],
      "id": 97
    },
    {
      "name": "nonbonded",
      "one_line_profile": "Framework for optimizing and benchmarking molecular force fields",
      "detailed_description": "A management system designed for optimizing and benchmarking molecular force fields against physical property data, facilitating computational chemistry research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "force_field_optimization",
        "molecular_modeling"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/SimonBoothroyd/nonbonded",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "molecular-dynamics",
        "force-fields",
        "chemistry"
      ],
      "id": 98
    },
    {
      "name": "CSVQA",
      "one_line_profile": "Benchmark for scientific reasoning in VLMs",
      "detailed_description": "A multimodal benchmark specifically designed to evaluate the scientific reasoning capabilities of Vision-Language Models (VLMs).",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "scientific_reasoning",
        "multimodal_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/SkyworkAI/CSVQA",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "vlm",
        "scientific-reasoning",
        "benchmark"
      ],
      "id": 99
    },
    {
      "name": "EmoLLM",
      "one_line_profile": "LLM framework for mental health evaluation and therapy",
      "detailed_description": "A comprehensive framework including datasets, evaluation, and deployment tools for Large Language Models focused on mental health and psychology applications.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "mental_health_analysis",
        "psychological_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/SmartFlowAI/EmoLLM",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mental-health",
        "llm",
        "psychology"
      ],
      "id": 100
    },
    {
      "name": "OmixBench",
      "one_line_profile": "Evaluation framework for LLMs in multi-omics analysis",
      "detailed_description": "A systematic evaluation framework designed to assess the performance of Large Language Models in the context of multi-omics data analysis.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "multi_omics",
        "bioinformatics_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "R",
      "repo_url": "https://github.com/SolvingLab/OmixBench",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "multi-omics",
        "llm-evaluation",
        "bioinformatics"
      ],
      "id": 101
    },
    {
      "name": "StreetView-NatureVisibility",
      "one_line_profile": "Framework for greenness visibility modelling using street view data",
      "detailed_description": "A scalable and reproducible framework for utilising Mapillary street view data to model nature visibility, supporting environmental and urban science research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "environmental_modelling",
        "urban_science"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/Spatial-Data-Science-and-GEO-AI-Lab/StreetView-NatureVisibility",
      "help_website": [],
      "license": null,
      "tags": [
        "geo-ai",
        "environmental-science",
        "street-view"
      ],
      "id": 102
    },
    {
      "name": "SMDG-19",
      "one_line_profile": "Standardized multi-channel dataset for glaucoma",
      "detailed_description": "A collection and standardization of 19 public full-fundus glaucoma images and associated metadata, serving as a benchmark for medical imaging analysis.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "medical_imaging",
        "disease_diagnosis"
      ],
      "application_level": "dataset",
      "primary_language": null,
      "repo_url": "https://github.com/TheBeastCoding/standardized-multichannel-dataset-glaucoma",
      "help_website": [],
      "license": null,
      "tags": [
        "medical-imaging",
        "glaucoma",
        "dataset"
      ],
      "id": 103
    },
    {
      "name": "TSB-UAD",
      "one_line_profile": "Benchmark suite for univariate time-series anomaly detection",
      "detailed_description": "An end-to-end benchmark suite for univariate time-series anomaly detection, including datasets from scientific and engineering domains (e.g., NASA spacecraft, water treatment).",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "anomaly_detection",
        "time_series_analysis"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/TheDatumOrg/TSB-UAD",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "time-series",
        "anomaly-detection",
        "scientific-data"
      ],
      "id": 104
    },
    {
      "name": "AgML-CY-Bench",
      "one_line_profile": "Crop yield forecasting benchmark dataset",
      "detailed_description": "A comprehensive dataset and benchmark for forecasting crop yields at the subnational level, harmonizing public yield statistics with relevant predictors for agricultural research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "crop_yield_forecasting",
        "agricultural_modeling"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/WUR-AI/AgML-CY-Bench",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "agriculture",
        "crop-yield",
        "benchmark"
      ],
      "id": 105
    },
    {
      "name": "SciTab",
      "one_line_profile": "Benchmark for reasoning on scientific tables",
      "detailed_description": "A challenging benchmark designed for compositional reasoning and claim verification specifically on scientific tables.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "scientific_reasoning",
        "table_verification"
      ],
      "application_level": "dataset",
      "primary_language": null,
      "repo_url": "https://github.com/XinyuanLu00/SciTab",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "scientific-tables",
        "reasoning",
        "benchmark"
      ],
      "id": 106
    },
    {
      "name": "EHRStruct",
      "one_line_profile": "Benchmark for LLMs on structured electronic health records",
      "detailed_description": "A comprehensive benchmark framework for evaluating Large Language Models on tasks involving structured Electronic Health Records (EHR).",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "ehr_analysis",
        "medical_informatics"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/YXNTU/EHRStruct",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "ehr",
        "medical-benchmark",
        "llm"
      ],
      "id": 107
    },
    {
      "name": "mimic3-benchmarks",
      "one_line_profile": "Benchmark datasets from MIMIC-III clinical database",
      "detailed_description": "A Python suite to construct benchmark machine learning datasets from the MIMIC-III clinical database for healthcare research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "clinical_prediction",
        "medical_dataset_construction"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/YerevaNN/mimic3-benchmarks",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mimic-iii",
        "clinical-data",
        "benchmark"
      ],
      "id": 108
    },
    {
      "name": "MUBen",
      "one_line_profile": "Benchmark for uncertainty of molecular representation models",
      "detailed_description": "A benchmark suite for evaluating the uncertainty estimation capabilities of molecular representation models in cheminformatics.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "molecular_representation",
        "uncertainty_estimation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Yinghao-Li/MUBen",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "molecular-modeling",
        "uncertainty",
        "benchmark"
      ],
      "id": 109
    },
    {
      "name": "math-evaluation-harness",
      "one_line_profile": "Toolkit for benchmarking LLMs on mathematical reasoning tasks",
      "detailed_description": "A specialized evaluation framework designed to assess the performance of Large Language Models (LLMs) on mathematical reasoning problems, providing a standardized harness for metrics calculation and result comparison.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "math_reasoning",
        "model_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ZubinGou/math-evaluation-harness",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "benchmark",
        "mathematics",
        "reasoning"
      ],
      "id": 110
    },
    {
      "name": "AidanBench",
      "one_line_profile": "Benchmark suite for measuring specific behavioral metrics in LLMs",
      "detailed_description": "A benchmarking tool aimed at measuring 'big_model_smell' and other behavioral characteristics in Large Language Models, contributing to the evaluation ecology of AI models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "behavioral_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/aidanmclaughlin/AidanBench",
      "help_website": [],
      "license": null,
      "tags": [
        "llm",
        "benchmark",
        "evaluation"
      ],
      "id": 111
    },
    {
      "name": "PeerRead",
      "one_line_profile": "Dataset and code for analyzing scientific peer reviews",
      "detailed_description": "A dataset and accompanying code for the analysis of scientific peer reviews, enabling NLP research into scientific literature assessment and review processes.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "scientific_literature_mining",
        "nlp_dataset"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/allenai/PeerRead",
      "help_website": [],
      "license": null,
      "tags": [
        "nlp",
        "peer-review",
        "scientific-literature"
      ],
      "id": 112
    },
    {
      "name": "tau2-bench-verified",
      "one_line_profile": "Verified benchmark for evaluating AI agents on database tasks",
      "detailed_description": "A corrected and verified version of the τ²-bench benchmark, providing reliable task definitions and evaluation criteria for assessing the performance of AI agents in database interaction scenarios.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "agent_evaluation",
        "database_interaction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/amazon-agi/tau2-bench-verified",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "benchmark",
        "agents",
        "database"
      ],
      "id": 113
    },
    {
      "name": "SARosPerceptionKitti",
      "one_line_profile": "ROS package for KITTI vision benchmark perception tasks",
      "detailed_description": "A ROS-based framework for executing and evaluating perception tasks (detection, tracking) using the KITTI Vision Benchmark Suite, facilitating robotics and computer vision research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "object_detection",
        "tracking",
        "benchmark_implementation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/appinho/SARosPerceptionKitti",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ros",
        "kitti",
        "computer-vision",
        "robotics"
      ],
      "id": 114
    },
    {
      "name": "AD-ML",
      "one_line_profile": "Framework for reproducible classification of Alzheimer's disease",
      "detailed_description": "A machine learning framework designed for the reproducible classification of Alzheimer's disease using neuroimaging data, serving as a benchmark for medical AI methods.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "disease_classification",
        "medical_imaging"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/aramis-lab/AD-ML",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "alzheimers",
        "machine-learning",
        "medical-imaging"
      ],
      "id": 115
    },
    {
      "name": "clinicadl",
      "one_line_profile": "Deep learning framework for neuroimaging data processing",
      "detailed_description": "A framework for reproducible processing and analysis of neuroimaging data using deep learning, supporting tasks like classification, reconstruction, and segmentation in medical research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "medical_imaging",
        "deep_learning_framework"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aramis-lab/clinicadl",
      "help_website": [
        "https://clinicadl.readthedocs.io"
      ],
      "license": "MIT",
      "tags": [
        "neuroimaging",
        "deep-learning",
        "medical-research"
      ],
      "id": 116
    },
    {
      "name": "BigCodeBench-X",
      "one_line_profile": "Multilingual programming task benchmark for LLMs",
      "detailed_description": "A benchmark suite for evaluating Large Language Models on programming tasks across multiple programming languages, assessing code generation and reasoning capabilities.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "code_generation",
        "model_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/arjunguha/BigCodeBench-X",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "benchmark",
        "code-generation"
      ],
      "id": 117
    },
    {
      "name": "Tartarus",
      "one_line_profile": "Benchmark platform for inverse molecular design",
      "detailed_description": "A benchmarking platform designed for realistic and practical inverse molecular design, evaluating generative models in chemistry and materials science contexts.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "molecular_design",
        "generative_chemistry"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/aspuru-guzik-group/Tartarus",
      "help_website": [],
      "license": null,
      "tags": [
        "molecular-design",
        "chemistry",
        "benchmark"
      ],
      "id": 118
    },
    {
      "name": "File-Format-Testing",
      "one_line_profile": "Benchmark for scientific file formats (HDF5, netCDF4, Zarr)",
      "detailed_description": "A configurable benchmarking tool for comparing the performance of common scientific file formats (HDF5, netCDF4, Zarr) across various I/O operations.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "io_benchmarking",
        "scientific_data_management"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/asriniket/File-Format-Testing",
      "help_website": [],
      "license": null,
      "tags": [
        "hdf5",
        "netcdf",
        "zarr",
        "benchmark"
      ],
      "id": 119
    },
    {
      "name": "adaptrapezoid_benchmark",
      "one_line_profile": "Benchmark for adaptive trapezoid numeric integration algorithms",
      "detailed_description": "A benchmarking tool to evaluate the performance of different programming languages for scientific computing tasks, specifically using the adaptive trapezoid numeric integration algorithm.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "numeric_integration",
        "performance_benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Scala",
      "repo_url": "https://github.com/astrojhgu/adaptrapezoid_benchmark",
      "help_website": [],
      "license": null,
      "tags": [
        "numeric-integration",
        "scientific-computing",
        "benchmark"
      ],
      "id": 120
    },
    {
      "name": "TerjamaBench",
      "one_line_profile": "Evaluation code for the TerjamaBench dataset",
      "detailed_description": "Code repository for running evaluations on the TerjamaBench dataset, facilitating benchmarking of NLP models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "nlp_benchmark"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/atlasia-ma/TerjamaBench",
      "help_website": [],
      "license": null,
      "tags": [
        "nlp",
        "benchmark",
        "evaluation"
      ],
      "id": 121
    },
    {
      "name": "fm-leaderboarder",
      "one_line_profile": "Tool for creating custom LLM evaluation leaderboards",
      "detailed_description": "A utility to create custom leaderboards for evaluating Large Language Models (LLMs) based on specific data, tasks, and prompts, enabling tailored model selection for business or research use cases.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "leaderboard_generation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/aws-samples/fm-leaderboarder",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "leaderboard",
        "evaluation"
      ],
      "id": 122
    },
    {
      "name": "RT-CUDA",
      "one_line_profile": "Optimizing compiler for CUDA-based scientific computing",
      "detailed_description": "A restructuring compiler and optimization tool designed to bridge high-level languages and CUDA for scientific applications, supporting linear algebra solvers and simulations (e.g., reservoir, molecular dynamics).",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "scientific_computing",
        "hpc_optimization",
        "linear_algebra"
      ],
      "application_level": "solver",
      "primary_language": "C",
      "repo_url": "https://github.com/ayazhassan/RT-CUDA-GUI-Development",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "cuda",
        "hpc",
        "compiler",
        "scientific-simulation"
      ],
      "id": 123
    },
    {
      "name": "MolNet Geometric Lightning",
      "one_line_profile": "Benchmarking framework for molecular property prediction using PyTorch Lightning and Torch Geometric",
      "detailed_description": "A repository implementing MoleculeNet benchmarks specifically adapted for Graph Neural Networks using PyTorch Lightning and Torch Geometric, facilitating reproducible evaluation of molecular machine learning models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "molecular_property_prediction",
        "graph_representation_learning"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/bayer-science-for-a-better-life/molnet-geometric-lightning",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "molecular-graphs",
        "drug-discovery",
        "benchmark"
      ],
      "id": 124
    },
    {
      "name": "BEIR",
      "one_line_profile": "Heterogeneous benchmark for zero-shot information retrieval including scientific corpora",
      "detailed_description": "A heterogeneous benchmark for information retrieval that includes diverse datasets, significantly covering scientific domains (BioASQ, TREC-COVID, NFCorpus, SciFact), enabling the evaluation of retrieval models for scientific literature mining.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "information_retrieval",
        "scientific_literature_mining"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/beir-cellar/beir",
      "help_website": [
        "https://github.com/beir-cellar/beir/wiki"
      ],
      "license": "Apache-2.0",
      "tags": [
        "information-retrieval",
        "biomedical-ir",
        "benchmark"
      ],
      "id": 125
    },
    {
      "name": "CiteME",
      "one_line_profile": "Benchmark for evaluating citation recommendation in scientific texts",
      "detailed_description": "A benchmark designed to test the abilities of language models in identifying and retrieving papers that are cited in scientific texts, supporting the development of AI assistants for scientific writing.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "citation_recommendation",
        "scientific_literature_mining"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/bethgelab/CiteME",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "nlp-for-science",
        "citation-analysis",
        "benchmark"
      ],
      "id": 126
    },
    {
      "name": "MDBenchmark",
      "one_line_profile": "Tool for generating and analyzing molecular dynamics simulation benchmarks",
      "detailed_description": "A command-line tool and Python library to quickly generate, start, and analyze benchmarks for molecular dynamics simulations (e.g., GROMACS, NAMD), helping researchers optimize simulation performance on HPC systems.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "molecular_dynamics_simulation",
        "performance_benchmarking"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/bio-phys/MDBenchmark",
      "help_website": [
        "https://mdbenchmark.readthedocs.io/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "molecular-dynamics",
        "hpc",
        "benchmarking"
      ],
      "id": 127
    },
    {
      "name": "Brain-Score Vision",
      "one_line_profile": "Framework for evaluating vision models against brain and behavioral data",
      "detailed_description": "A framework that evaluates artificial neural networks on their alignment with biological brain measurements (neural recordings) and behavioral data, serving as a key benchmark in computational neuroscience and vision science.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "neural_alignment_evaluation",
        "computational_neuroscience"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/brain-score/vision",
      "help_website": [
        "http://www.brain-score.org/"
      ],
      "license": "MIT",
      "tags": [
        "neuroscience",
        "computer-vision",
        "brain-alignment"
      ],
      "id": 128
    },
    {
      "name": "ConvolutionalNeuralOperator",
      "one_line_profile": "Implementation of Convolutional Neural Operators for PDE solving",
      "detailed_description": "Official implementation of Convolutional Neural Operators (CNO), a deep learning framework for robust and accurate learning of Partial Differential Equations (PDEs), used in physics simulations and fluid dynamics.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "pde_solving",
        "fluid_dynamics_simulation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/camlab-ethz/ConvolutionalNeuralOperator",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pde",
        "neural-operators",
        "scientific-machine-learning"
      ],
      "id": 129
    },
    {
      "name": "gee_brazil_sv",
      "one_line_profile": "Benchmark maps and code for secondary forest age in Brazil",
      "detailed_description": "Code repository and dataset for generating benchmark maps of secondary forest age in Brazil using Google Earth Engine, supporting ecological research and carbon stock estimation.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "forest_age_mapping",
        "remote_sensing"
      ],
      "application_level": "dataset",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/celsohlsj/gee_brazil_sv",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "ecology",
        "remote-sensing",
        "google-earth-engine"
      ],
      "id": 130
    },
    {
      "name": "PDF Text Extraction Benchmark",
      "one_line_profile": "Benchmark for evaluating PDF text extraction tools on scientific articles",
      "detailed_description": "A benchmarking suite designed to evaluate the semantic capabilities of PDF extraction tools specifically on scientific articles, addressing the challenge of parsing complex scientific document layouts.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "document_parsing",
        "scientific_literature_mining"
      ],
      "application_level": "library",
      "primary_language": "TeX",
      "repo_url": "https://github.com/ckorzen/pdf-text-extraction-benchmark",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pdf-extraction",
        "scientific-documents",
        "benchmark"
      ],
      "id": 131
    },
    {
      "name": "Quantum-PDE-Benchmark",
      "one_line_profile": "Benchmark for near-term quantum algorithms solving PDEs",
      "detailed_description": "A repository for benchmarking near-term quantum algorithms designed to solve Partial Differential Equations (PDEs), facilitating the evaluation of quantum computing applications in computational physics.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "quantum_algorithm_benchmarking",
        "pde_solving"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/comp-physics/Quantum-PDE-Benchmark",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "quantum-computing",
        "pde",
        "computational-physics"
      ],
      "id": 132
    },
    {
      "name": "DIEF_BTS",
      "one_line_profile": "Building TimeSeries dataset with Brick schema standardization",
      "detailed_description": "A dataset containing time-series data from buildings, standardized using the Brick schema, designed for benchmarking algorithms in building energy modeling and smart building applications.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "building_energy_modeling",
        "time_series_forecasting"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/cruiseresearchgroup/DIEF_BTS",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "smart-buildings",
        "energy-data",
        "brick-schema"
      ],
      "id": 133
    },
    {
      "name": "GBM_Benchmarking",
      "one_line_profile": "Benchmarking Gradient Boosting for molecular property prediction",
      "detailed_description": "Scripts and guidelines for reproducing benchmarks of Gradient Boosting models on molecular property prediction tasks, providing a baseline for chemoinformatics research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "molecular_property_prediction",
        "model_benchmarking"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/dahvida/GBM_Benchmarking",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "chemoinformatics",
        "gradient-boosting",
        "drug-discovery"
      ],
      "id": 134
    },
    {
      "name": "FixaTons",
      "one_line_profile": "Datasets and metrics for evaluating human fixation scanpath similarity",
      "detailed_description": "A collection of datasets and metrics specifically designed to calculate and evaluate the similarity of human eye-fixation scanpaths, supporting research in visual attention and saliency modeling.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "metric_calculation",
        "evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/dariozanca/FixaTons",
      "help_website": [],
      "license": null,
      "tags": [
        "scanpath",
        "eye-tracking",
        "metrics",
        "saliency"
      ],
      "id": 135
    },
    {
      "name": "zk-benchmark-r1cs",
      "one_line_profile": "Tools for benchmarking Zero-Knowledge provers using R1CS",
      "detailed_description": "A set of tools to serialize R1CS (Rank-1 Constraint Systems) and witness representations into standardized datasets, enabling the benchmarking of different Zero-Knowledge provers.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "data_generation"
      ],
      "application_level": "tool",
      "primary_language": "Rust",
      "repo_url": "https://github.com/dcbuild3r/zk-benchmark-r1cs",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "zero-knowledge",
        "cryptography",
        "benchmarking",
        "r1cs"
      ],
      "id": 136
    },
    {
      "name": "WEFE",
      "one_line_profile": "Word Embeddings Fairness Evaluation Framework",
      "detailed_description": "An open source framework for measuring and mitigating bias in word embedding models, providing standardized metrics for fairness evaluation in NLP.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "fairness_evaluation",
        "bias_measurement"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/dccuchile/wefe",
      "help_website": [
        "https://wefe.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "nlp",
        "fairness",
        "bias",
        "word-embeddings"
      ],
      "id": 137
    },
    {
      "name": "FunctionBench",
      "one_line_profile": "Workload suite for benchmarking serverless cloud functions",
      "detailed_description": "A suite of workloads designed to benchmark serverless cloud function services, evaluating performance across different providers and configurations for systems research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_evaluation"
      ],
      "application_level": "benchmark_suite",
      "primary_language": "Python",
      "repo_url": "https://github.com/ddps-lab/serverless-faas-workbench",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "serverless",
        "cloud-computing",
        "benchmark",
        "faas"
      ],
      "id": 138
    },
    {
      "name": "LLM-SRBench",
      "one_line_profile": "Benchmark for Scientific Equation Discovery with LLMs",
      "detailed_description": "A benchmark designed to evaluate Large Language Models on the task of scientific equation discovery, assessing their ability to recover symbolic relationships from data.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "equation_discovery"
      ],
      "application_level": "benchmark_suite",
      "primary_language": "Python",
      "repo_url": "https://github.com/deep-symbolic-mathematics/llm-srbench",
      "help_website": [],
      "license": null,
      "tags": [
        "symbolic-regression",
        "llm",
        "scientific-discovery",
        "benchmark"
      ],
      "id": 139
    },
    {
      "name": "SciAssess",
      "one_line_profile": "Benchmark for LLMs in scientific literature analysis",
      "detailed_description": "A comprehensive benchmark for evaluating Large Language Models' proficiency in scientific literature analysis, covering memorization, comprehension, and analysis across various scientific fields.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "literature_analysis"
      ],
      "application_level": "benchmark_suite",
      "primary_language": "Python",
      "repo_url": "https://github.com/deepmodeling/SciAssess",
      "help_website": [],
      "license": "LGPL-3.0",
      "tags": [
        "llm",
        "scientific-literature",
        "benchmark",
        "evaluation"
      ],
      "id": 140
    },
    {
      "name": "DeathStarBench",
      "one_line_profile": "Benchmark suite for cloud microservices",
      "detailed_description": "An open-source benchmark suite for cloud microservices, designed to evaluate the performance and implications of microservices architectures in systems research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "systems_evaluation"
      ],
      "application_level": "benchmark_suite",
      "primary_language": "Lua",
      "repo_url": "https://github.com/delimitrou/DeathStarBench",
      "help_website": [
        "http://microservices.csl.cornell.edu/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "microservices",
        "cloud",
        "benchmark",
        "systems"
      ],
      "id": 141
    },
    {
      "name": "pyhpc-benchmarks",
      "one_line_profile": "Benchmarks for Python high-performance computing libraries",
      "detailed_description": "A suite of benchmarks for evaluating the CPU and GPU performance of popular high-performance computing libraries in the Python ecosystem, aiding in library selection for scientific computing.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_profiling"
      ],
      "application_level": "benchmark_suite",
      "primary_language": "Python",
      "repo_url": "https://github.com/dionhaefner/pyhpc-benchmarks",
      "help_website": [],
      "license": "Unlicense",
      "tags": [
        "hpc",
        "python",
        "benchmark",
        "gpu",
        "cpu"
      ],
      "id": 142
    },
    {
      "name": "AL4PDE",
      "one_line_profile": "Benchmark for Active Learning in Neural PDE Solvers",
      "detailed_description": "A benchmark suite specifically designed for evaluating active learning strategies applied to neural partial differential equation (PDE) solvers.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "active_learning",
        "pde_solving"
      ],
      "application_level": "benchmark_suite",
      "primary_language": "Python",
      "repo_url": "https://github.com/dmusekamp/al4pde",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "pde",
        "active-learning",
        "neural-solver",
        "benchmark"
      ],
      "id": 143
    },
    {
      "name": "dockstring",
      "one_line_profile": "Dataset and benchmark tasks for molecular docking",
      "detailed_description": "A package providing a curated dataset and realistic benchmark tasks for molecular docking, facilitating the evaluation of machine learning models in drug discovery.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "molecular_docking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/dockstring/dockstring",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "molecular-docking",
        "drug-discovery",
        "benchmark",
        "dataset-loader"
      ],
      "id": 144
    },
    {
      "name": "docling-eval",
      "one_line_profile": "Evaluation framework for document processing models",
      "detailed_description": "A framework for evaluating document processing models and services, providing metrics and tools to assess performance on document understanding tasks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "document_processing"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/docling-project/docling-eval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "document-processing",
        "evaluation",
        "metrics"
      ],
      "id": 145
    },
    {
      "name": "ADaM Standard Code",
      "one_line_profile": "Standardized code for creating ADaM clinical datasets",
      "detailed_description": "A repository of standardized SAS and R code designed to create ADaM (Analysis Data Model) datasets from SDTM data, supporting clinical trial data analysis and submission.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "data_processing",
        "standardization"
      ],
      "application_level": "library",
      "primary_language": "SAS",
      "repo_url": "https://github.com/dominodatalab/ADaM_Standard_Code",
      "help_website": [],
      "license": null,
      "tags": [
        "clinical-trials",
        "adam",
        "sdtm",
        "sas",
        "r"
      ],
      "id": 146
    },
    {
      "name": "dpbench",
      "one_line_profile": "Dataplane benchmarking suite",
      "detailed_description": "A benchmarking suite for evaluating dataplane performance, useful for systems research and network optimization.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_evaluation"
      ],
      "application_level": "benchmark_suite",
      "primary_language": "Shell",
      "repo_url": "https://github.com/dpbench/dpbench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "dataplane",
        "networking",
        "benchmark",
        "systems"
      ],
      "id": 147
    },
    {
      "name": "Blind Image Quality Toolbox",
      "one_line_profile": "Collection of blind image quality metrics",
      "detailed_description": "A MATLAB toolbox containing various blind (no-reference) image quality metrics, used for evaluating image processing algorithms and quality assessment.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "metric_calculation",
        "image_quality_assessment"
      ],
      "application_level": "library",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/dsoellinger/blind_image_quality_toolbox",
      "help_website": [],
      "license": null,
      "tags": [
        "image-quality",
        "matlab",
        "metrics",
        "blind-assessment"
      ],
      "id": 148
    },
    {
      "name": "SAR-ShipDet-Dataset-Processor",
      "one_line_profile": "Processing tool for SAR ship detection datasets",
      "detailed_description": "A unified tool for processing various Synthetic Aperture Radar (SAR) ship detection datasets (HRSID, SSDD, etc.) into standardized formats for research and model training.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "data_processing",
        "normalization"
      ],
      "application_level": "tool",
      "primary_language": "Python",
      "repo_url": "https://github.com/egshkim/SAR-ShipDet-Dataset-Processor",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "sar",
        "remote-sensing",
        "dataset-processing",
        "ship-detection"
      ],
      "id": 149
    },
    {
      "name": "AdaTime",
      "one_line_profile": "Benchmarking suite for domain adaptation on time series",
      "detailed_description": "A benchmarking suite designed to evaluate domain adaptation algorithms specifically on time series data, facilitating reproducible research in temporal distribution shifts.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "domain_adaptation"
      ],
      "application_level": "benchmark_suite",
      "primary_language": "Python",
      "repo_url": "https://github.com/emadeldeen24/AdaTime",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "time-series",
        "domain-adaptation",
        "benchmark",
        "machine-learning"
      ],
      "id": 150
    },
    {
      "name": "repurpose",
      "one_line_profile": "Framework for drug repurposing classifiers",
      "detailed_description": "A Python-based framework for building and evaluating drug-disease association classifiers, supporting reproducible research in drug repurposing.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "modeling",
        "drug_repurposing"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/emreg00/repurpose",
      "help_website": [],
      "license": null,
      "tags": [
        "drug-repurposing",
        "bioinformatics",
        "machine-learning",
        "drug-discovery"
      ],
      "id": 151
    },
    {
      "name": "text-to-image-eval",
      "one_line_profile": "Evaluation metrics for text-to-image models",
      "detailed_description": "A tool to evaluate custom and HuggingFace text-to-image and zero-shot image classification models using metrics like Zero-shot accuracy, Linear Probe, and Image retrieval.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "metric_calculation"
      ],
      "application_level": "tool",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/encord-team/text-to-image-eval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "text-to-image",
        "evaluation",
        "metrics",
        "computer-vision"
      ],
      "id": 152
    },
    {
      "name": "Kurtis",
      "one_line_profile": "Fine-tuning and evaluation tool for Small Language Models",
      "detailed_description": "A tool designed for fine-tuning, inference, and evaluation of Small Language Models (SLMs), streamlining the development lifecycle for smaller AI models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "fine_tuning"
      ],
      "application_level": "tool",
      "primary_language": "Python",
      "repo_url": "https://github.com/ethicalabs-ai/kurtis",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "slm",
        "fine-tuning",
        "evaluation",
        "inference"
      ],
      "id": 153
    },
    {
      "name": "EvalPlus",
      "one_line_profile": "Rigorous evaluation framework for LLM-synthesized code",
      "detailed_description": "A framework for rigorously evaluating code synthesized by Large Language Models, providing enhanced test generation and benchmarking capabilities.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "code_generation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/evalplus/evalplus",
      "help_website": [
        "https://evalplus.github.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "code-generation",
        "evaluation",
        "benchmark"
      ],
      "id": 154
    },
    {
      "name": "lammps-bulk-benchmark",
      "one_line_profile": "Benchmark for LAMMPS molecular dynamics simulations",
      "detailed_description": "A benchmarking tool for evaluating CPU and GPU performance by running bulk molecular dynamics simulations using LAMMPS.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_evaluation"
      ],
      "application_level": "benchmark_suite",
      "primary_language": "Shell",
      "repo_url": "https://github.com/evenmn/lammps-bulk-benchmark",
      "help_website": [],
      "license": null,
      "tags": [
        "lammps",
        "molecular-dynamics",
        "benchmark",
        "hpc"
      ],
      "id": 155
    },
    {
      "name": "Evidently",
      "one_line_profile": "ML and LLM observability and evaluation framework",
      "detailed_description": "An open-source framework to evaluate, test, and monitor ML models and LLMs, providing over 100 metrics for data quality, drift detection, and model performance.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "monitoring",
        "quality_control"
      ],
      "application_level": "framework",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/evidentlyai/evidently",
      "help_website": [
        "https://docs.evidentlyai.com/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "observability",
        "evaluation",
        "mlops",
        "drift-detection"
      ],
      "id": 156
    },
    {
      "name": "HydroNet",
      "one_line_profile": "Benchmark tasks for molecular data modeling",
      "detailed_description": "A set of benchmark tasks designed to evaluate predictive and generative models for molecular data, focusing on preserving long-range interactions and structural motifs.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "molecular_modeling"
      ],
      "application_level": "benchmark_suite",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/exalearn/hydronet",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "molecular-data",
        "benchmark",
        "generative-models",
        "chemistry"
      ],
      "id": 157
    },
    {
      "name": "datamaestro",
      "one_line_profile": "Scripts for automated dataset handling and standardization",
      "detailed_description": "A library to automatize and standardize the handling of datasets for experiments, facilitating reproducible data pipelines.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "data_management",
        "workflow_automation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/experimaestro/datamaestro",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "dataset-management",
        "reproducibility",
        "workflow"
      ],
      "id": 158
    },
    {
      "name": "BenchMARL",
      "one_line_profile": "Benchmarking library for Multi-Agent Reinforcement Learning",
      "detailed_description": "A library for benchmarking Multi-Agent Reinforcement Learning (MARL) algorithms, enabling standardized comparison of tasks, models, and algorithms.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "reinforcement_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/BenchMARL",
      "help_website": [
        "https://benchmarl.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "marl",
        "reinforcement-learning",
        "benchmark",
        "torchrl"
      ],
      "id": 159
    },
    {
      "name": "DCPerf",
      "one_line_profile": "Benchmark suite for hyperscale cloud applications",
      "detailed_description": "A benchmark suite designed to evaluate the performance of hyperscale cloud applications and datacenter workloads, useful for systems research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "systems_evaluation"
      ],
      "application_level": "benchmark_suite",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/DCPerf",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "cloud",
        "benchmark",
        "datacenter",
        "performance"
      ],
      "id": 160
    },
    {
      "name": "ParlAI",
      "one_line_profile": "A unified framework for training and evaluating dialogue models",
      "detailed_description": "A python framework for sharing, training and testing dialogue models, from open-domain chitchat to task-oriented dialogue. It provides access to many popular datasets and a wide set of reference models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "dialogue_system",
        "dataset_access"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/ParlAI",
      "help_website": [
        "https://parl.ai/"
      ],
      "license": "MIT",
      "tags": [
        "nlp",
        "dialogue",
        "evaluation-framework",
        "chatbot"
      ],
      "id": 161
    },
    {
      "name": "video-transformers",
      "one_line_profile": "Streamlined fine-tuning interface for video classification models",
      "detailed_description": "A wrapper tool designed to simplify the fine-tuning process of HuggingFace video classification models, providing an accessible interface for model training and experimentation.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_training",
        "fine_tuning",
        "video_classification"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/fcakyon/video-transformers",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "video-classification",
        "transformers",
        "fine-tuning"
      ],
      "id": 162
    },
    {
      "name": "transfer-nlp",
      "one_line_profile": "Experiment management library for reproducible NLP research",
      "detailed_description": "A library designed to manage NLP experiments, ensuring reproducibility and easing the process of transfer learning and model configuration.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "experiment_management",
        "reproducibility",
        "nlp"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/feedly/transfer-nlp",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nlp",
        "experiment-tracking",
        "reproducibility"
      ],
      "id": 163
    },
    {
      "name": "FlagEvalMM",
      "one_line_profile": "Comprehensive evaluation framework for multimodal models",
      "detailed_description": "A flexible framework designed to evaluate multimodal AI models across various dimensions, supporting the assessment of model capabilities in processing diverse data types.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "multimodal_learning",
        "benchmarking"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/flageval-baai/FlagEvalMM",
      "help_website": [],
      "license": null,
      "tags": [
        "multimodal",
        "evaluation",
        "benchmark"
      ],
      "id": 164
    },
    {
      "name": "FluidML",
      "one_line_profile": "Lightweight framework for developing machine learning pipelines",
      "detailed_description": "A framework for building ML pipelines that supports caching, parallel execution, and experiment tracking, facilitating efficient model development and scientific experimentation.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "pipeline_orchestration",
        "experiment_management"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/fluidml/fluidml",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "pipeline",
        "machine-learning",
        "experimentation"
      ],
      "id": 165
    },
    {
      "name": "frictionless-js",
      "one_line_profile": "Library for standardized access to tabular scientific data",
      "detailed_description": "A JavaScript library implementing the Frictionless Data specifications, providing standardized methods to access, validate, and process tabular datasets (CSV, Excel) often used in open science.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "data_access",
        "data_validation",
        "interoperability"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/frictionlessdata/frictionless-js",
      "help_website": [
        "https://frictionlessdata.io/"
      ],
      "license": null,
      "tags": [
        "data-standard",
        "csv",
        "open-science"
      ],
      "id": 166
    },
    {
      "name": "pLitter",
      "one_line_profile": "Dataset and model for plastic litter detection",
      "detailed_description": "A standardized dataset and pre-trained deep learning model specifically designed for the detection of plastic litter in environmental settings, aiding in environmental science research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "object_detection",
        "environmental_monitoring",
        "dataset"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/gicait/pLitter",
      "help_website": [],
      "license": null,
      "tags": [
        "dataset",
        "environmental-science",
        "plastic-detection"
      ],
      "id": 167
    },
    {
      "name": "language-table",
      "one_line_profile": "Benchmark for open vocabulary visuolinguomotor learning",
      "detailed_description": "A suite of datasets and a multi-task continuous control benchmark designed for researching open vocabulary visual-language-motor learning in robotics.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "robotics_benchmark",
        "visuomotor_learning",
        "dataset"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/google-research/language-table",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "robotics",
        "benchmark",
        "multimodal"
      ],
      "id": 168
    },
    {
      "name": "realworldrl_suite",
      "one_line_profile": "Benchmark suite for Real-World Reinforcement Learning",
      "detailed_description": "A collection of reinforcement learning tasks designed to capture the challenges of real-world control problems, serving as a benchmark for RL algorithm evaluation.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "benchmarking",
        "control_systems"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/google-research/realworldrl_suite",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "reinforcement-learning",
        "benchmark",
        "real-world-rl"
      ],
      "id": 169
    },
    {
      "name": "BIG-bench",
      "one_line_profile": "Collaborative benchmark for large language models",
      "detailed_description": "The Beyond the Imitation Game Benchmark (BIG-bench) is a collaborative initiative to measure and extrapolate the capabilities of large language models across a diverse range of tasks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "llm_benchmark",
        "task_definition"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/google/BIG-bench",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "benchmark",
        "evaluation"
      ],
      "id": 170
    },
    {
      "name": "Google ADK (Go)",
      "one_line_profile": "Toolkit for building and evaluating AI agents (Go)",
      "detailed_description": "An open-source toolkit for building, evaluating, and deploying sophisticated AI agents, providing frameworks for agentic workflows and evaluation.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "agent_evaluation",
        "agent_development"
      ],
      "application_level": "framework",
      "primary_language": "Go",
      "repo_url": "https://github.com/google/adk-go",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ai-agents",
        "toolkit",
        "evaluation"
      ],
      "id": 171
    },
    {
      "name": "Google ADK (Java)",
      "one_line_profile": "Toolkit for building and evaluating AI agents (Java)",
      "detailed_description": "An open-source toolkit for building, evaluating, and deploying sophisticated AI agents, providing frameworks for agentic workflows and evaluation.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "agent_evaluation",
        "agent_development"
      ],
      "application_level": "framework",
      "primary_language": "Java",
      "repo_url": "https://github.com/google/adk-java",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ai-agents",
        "toolkit",
        "evaluation"
      ],
      "id": 172
    },
    {
      "name": "Google ADK (Python)",
      "one_line_profile": "Toolkit for building and evaluating AI agents (Python)",
      "detailed_description": "An open-source toolkit for building, evaluating, and deploying sophisticated AI agents, providing frameworks for agentic workflows and evaluation.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "agent_evaluation",
        "agent_development"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/google/adk-python",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ai-agents",
        "toolkit",
        "evaluation"
      ],
      "id": 173
    },
    {
      "name": "h2o-LLM-eval",
      "one_line_profile": "LLM evaluation framework with Elo leaderboard",
      "detailed_description": "A framework for evaluating Large Language Models, featuring an Elo rating system and A/B testing capabilities to assess model performance.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "llm_ranking",
        "ab_testing"
      ],
      "application_level": "framework",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/h2oai/h2o-LLM-eval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "evaluation",
        "leaderboard"
      ],
      "id": 174
    },
    {
      "name": "pytorch-worker",
      "one_line_profile": "Framework for training and evaluating PyTorch models",
      "detailed_description": "A lightweight framework designed to streamline the training, evaluation, and testing processes for deep learning models built with PyTorch.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_training",
        "model_evaluation",
        "workflow_automation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/haoxizhong/pytorch-worker",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pytorch",
        "training-framework",
        "evaluation"
      ],
      "id": 175
    },
    {
      "name": "SeisFlowBench",
      "one_line_profile": "Benchmark for seismic wave propagation simulations",
      "detailed_description": "A reproducible scientific project and benchmark suite for seismic wave propagation and fluid flow simulations in porous media, built using Julia.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "seismic_simulation",
        "benchmarking",
        "geophysics"
      ],
      "application_level": "solver",
      "primary_language": "Julia",
      "repo_url": "https://github.com/haoyunl2/SeisFlowBench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "seismology",
        "simulation",
        "julia"
      ],
      "id": 176
    },
    {
      "name": "dlio_benchmark",
      "one_line_profile": "I/O benchmark for scientific deep learning workloads",
      "detailed_description": "A benchmark suite designed to represent and evaluate the I/O patterns and performance of scientific deep learning applications.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "io_benchmarking",
        "scientific_computing",
        "performance_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/hariharan-devarajan/dlio_benchmark",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "hpc",
        "deep-learning",
        "io-benchmark"
      ],
      "id": 177
    },
    {
      "name": "big-ann-benchmarks",
      "one_line_profile": "Benchmark for billion-scale approximate nearest neighbor search",
      "detailed_description": "A framework for evaluating Approximate Nearest Neighbor Search (ANNS) algorithms on billion-scale datasets, critical for vector search in AI applications.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "algorithm_evaluation",
        "vector_search",
        "benchmarking"
      ],
      "application_level": "framework",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/harsha-simhadri/big-ann-benchmarks",
      "help_website": [
        "http://big-ann-benchmarks.com/"
      ],
      "license": "MIT",
      "tags": [
        "ann",
        "vector-search",
        "benchmark"
      ],
      "id": 178
    },
    {
      "name": "EvalView",
      "one_line_profile": "Test harness for AI agents evaluation",
      "detailed_description": "A pytest-style test harness for evaluating AI agents, supporting YAML scenarios, tool-call checks, and reporting on cost, latency, and safety.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "agent_evaluation",
        "testing_framework",
        "safety_eval"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/hidai25/eval-view",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ai-agents",
        "evaluation",
        "testing"
      ],
      "id": 179
    },
    {
      "name": "This-is-not-a-Dataset",
      "one_line_profile": "Dataset for evaluating negation and commonsense in LLMs",
      "detailed_description": "A large semi-automatically generated dataset of descriptive sentences about commonsense knowledge, focusing on negation, used for evaluating Large Language Models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "commonsense_reasoning",
        "dataset"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/hitz-zentroa/This-is-not-a-Dataset",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-evaluation",
        "dataset",
        "commonsense"
      ],
      "id": 180
    },
    {
      "name": "Latxa",
      "one_line_profile": "Language model and evaluation suite for Basque",
      "detailed_description": "An open language model and a comprehensive evaluation suite specifically designed for the Basque language, facilitating NLP research in low-resource languages.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "nlp_benchmark",
        "low_resource_language"
      ],
      "application_level": "framework",
      "primary_language": "Shell",
      "repo_url": "https://github.com/hitz-zentroa/latxa",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nlp",
        "basque",
        "evaluation-suite"
      ],
      "id": 181
    },
    {
      "name": "ColorTransferLib",
      "one_line_profile": "Library for color transfer algorithms and evaluation metrics",
      "detailed_description": "A collection of algorithms for color and style transfer, accompanied by objective evaluation metrics for quantitative assessment of image processing results.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "image_processing",
        "algorithm_evaluation",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/hpotechius/ColorTransferLib",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "color-transfer",
        "image-processing",
        "metrics"
      ],
      "id": 182
    },
    {
      "name": "hf_benchmarks",
      "one_line_profile": "Starter kit for evaluating benchmarks on HuggingFace Hub",
      "detailed_description": "A toolkit designed to facilitate the evaluation of benchmarks hosted on the HuggingFace Hub, providing starter code and utilities for model assessment.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "model_evaluation",
        "huggingface_hub"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/huggingface/hf_benchmarks",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "benchmark",
        "evaluation",
        "huggingface"
      ],
      "id": 183
    },
    {
      "name": "LightEval",
      "one_line_profile": "Toolkit for evaluating LLMs across multiple backends",
      "detailed_description": "A comprehensive toolkit for evaluating Large Language Models, supporting multiple backends and providing a unified interface for running various evaluation tasks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "llm",
        "inference_backend"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/huggingface/lighteval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "evaluation",
        "toolkit"
      ],
      "id": 184
    },
    {
      "name": "ScreenSuite",
      "one_line_profile": "Benchmarking suite for GUI Agents",
      "detailed_description": "A comprehensive benchmarking suite designed to evaluate the performance of GUI Agents, enabling standardized testing of agent interactions with graphical user interfaces.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "agent_evaluation",
        "gui_automation",
        "benchmark"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/huggingface/screensuite",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "gui-agents",
        "benchmark",
        "evaluation"
      ],
      "id": 185
    },
    {
      "name": "PINNacle",
      "one_line_profile": "Benchmark for Physics-Informed Neural Networks",
      "detailed_description": "A comprehensive benchmark suite for Physics-Informed Neural Networks (PINNs) applied to solving Partial Differential Equations (PDEs), facilitating fair comparison of different PINN methods.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "pde_solving",
        "model_benchmarking",
        "scientific_ml"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/i207M/PINNacle",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pinns",
        "pde",
        "benchmark",
        "ai4science"
      ],
      "id": 186
    },
    {
      "name": "transformers-lightning",
      "one_line_profile": "Integration library for PyTorch Lightning and Transformers",
      "detailed_description": "A collection of utilities, metrics, and modules to seamlessly integrate HuggingFace Transformers with PyTorch Lightning, facilitating efficient model training and evaluation workflows.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_training",
        "integration",
        "workflow_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/iKernels/transformers-lightning",
      "help_website": [],
      "license": "GPL-2.0",
      "tags": [
        "pytorch-lightning",
        "transformers",
        "integration"
      ],
      "id": 187
    },
    {
      "name": "ChainForge",
      "one_line_profile": "Visual programming environment for LLM prompt evaluation",
      "detailed_description": "An open-source visual programming environment designed for battle-testing and evaluating prompts for Large Language Models, enabling systematic comparison of model outputs.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "prompt_engineering",
        "model_evaluation",
        "visual_programming"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/ianarawjo/ChainForge",
      "help_website": [
        "https://chainforge.ai/"
      ],
      "license": "MIT",
      "tags": [
        "prompt-engineering",
        "llm",
        "evaluation",
        "visualization"
      ],
      "id": 188
    },
    {
      "name": "coref-data",
      "one_line_profile": "Standardized collection of coreference resolution datasets",
      "detailed_description": "A repository containing a collection of coreference datasets formatted in a standardized way to facilitate training and evaluation of coreference resolution models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "dataset_management",
        "evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/ianporada/coref-data",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "nlp",
        "coreference-resolution",
        "dataset"
      ],
      "id": 189
    },
    {
      "name": "PPTAgent",
      "one_line_profile": "Framework for generating and evaluating presentation slides",
      "detailed_description": "A research framework designed to generate and evaluate presentations, moving beyond simple text-to-slides by incorporating multimodal reasoning and evaluation metrics.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "generation"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/icip-cas/PPTAgent",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "multimodal",
        "evaluation-framework",
        "presentation-generation"
      ],
      "id": 190
    },
    {
      "name": "scaling-resnets",
      "one_line_profile": "Framework investigating scaling limits of ResNets vs Neural ODEs",
      "detailed_description": "A research framework used to investigate the scaling limits of ResNets and compare them to Neural ODEs, tested on synthetic and standardized datasets for scientific modeling analysis.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "modeling",
        "evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/instadeepai/scaling-resnets",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "neural-odes",
        "resnet",
        "scaling-laws"
      ],
      "id": 191
    },
    {
      "name": "itu-p1203-open-dataset",
      "one_line_profile": "Open dataset for ITU-T P.1203 video quality standardization",
      "detailed_description": "An open dataset used for the standardization of ITU-T P.1203, facilitating the evaluation and benchmarking of video quality assessment models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "quality_control"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/itu-p1203/open-dataset",
      "help_website": [],
      "license": null,
      "tags": [
        "video-quality",
        "standardization",
        "dataset"
      ],
      "id": 192
    },
    {
      "name": "mallet",
      "one_line_profile": "Evaluation harness for VLMs controlling robots",
      "detailed_description": "Cloud-based tools and an evaluation harness designed to test and benchmark Vision-Language Models (VLMs) for controlling real-world robots.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "robotics"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/jacobphillips99/mallet",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "vlm",
        "robotics",
        "evaluation-harness"
      ],
      "id": 193
    },
    {
      "name": "matbench-discovery",
      "one_line_profile": "Evaluation framework for high-throughput materials discovery",
      "detailed_description": "An evaluation framework for machine learning models that simulates high-throughput materials discovery, providing metrics and benchmarks for materials science.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "modeling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/janosh/matbench-discovery",
      "help_website": [
        "https://matbench-discovery.materialsproject.org"
      ],
      "license": "MIT",
      "tags": [
        "materials-science",
        "discovery",
        "benchmark"
      ],
      "id": 194
    },
    {
      "name": "41-llms-evaluated-on-19-benchmarks",
      "one_line_profile": "Benchmark suite and results for 41 LLMs across 19 tasks",
      "detailed_description": "A project benchmarking 41 open-source large language models across 19 evaluation tasks using the lm-evaluation-harness library, providing reproducible evaluation scripts and data.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "benchmarking"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/jayminban/41-llms-evaluated-on-19-benchmarks",
      "help_website": [],
      "license": null,
      "tags": [
        "llm",
        "benchmark",
        "evaluation"
      ],
      "id": 195
    },
    {
      "name": "text2sql-data",
      "one_line_profile": "Collection of Text-to-SQL datasets",
      "detailed_description": "A collection of datasets that pair natural language questions with SQL queries, serving as a benchmark resource for semantic parsing and NLP tasks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "dataset_management",
        "evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/jkkummerfeld/text2sql-data",
      "help_website": [
        "https://jkk.name/text2sql-data/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "nlp",
        "text-to-sql",
        "dataset"
      ],
      "id": 196
    },
    {
      "name": "MicroVQA",
      "one_line_profile": "Multimodal reasoning benchmark for microscopy",
      "detailed_description": "Evaluation code and benchmark for MicroVQA, a multimodal reasoning task focused on microscopy-based scientific research, including the RefineBot method.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "image_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/jmhb0/microvqa",
      "help_website": [],
      "license": null,
      "tags": [
        "microscopy",
        "vqa",
        "benchmark"
      ],
      "id": 197
    },
    {
      "name": "julia-pde-benchmark",
      "one_line_profile": "Benchmark for PDE integration algorithms in Julia",
      "detailed_description": "A benchmarking tool for evaluating the performance of simple Partial Differential Equation (PDE) integration algorithms in Julia compared to other languages.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "modeling"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/johnfgibson/julia-pde-benchmark",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pde",
        "julia",
        "benchmark"
      ],
      "id": 198
    },
    {
      "name": "SciFIBench",
      "one_line_profile": "Benchmark for scientific figure interpretation",
      "detailed_description": "A benchmark suite for evaluating Large Multimodal Models (LMMs) on their ability to interpret and reason about scientific figures.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "image_analysis"
      ],
      "application_level": "dataset",
      "primary_language": null,
      "repo_url": "https://github.com/jonathan-roberts1/SciFIBench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "scientific-figures",
        "multimodal",
        "benchmark"
      ],
      "id": 199
    },
    {
      "name": "ScientificComputingBenchmarks.jl",
      "one_line_profile": "Benchmarks for scientific computing in Julia",
      "detailed_description": "A collection of benchmarks designed to evaluate the performance of various scientific computing tasks and algorithms in the Julia programming language.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_analysis"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/jonathanBieler/ScientificComputingBenchmarks.jl",
      "help_website": [],
      "license": null,
      "tags": [
        "scientific-computing",
        "julia",
        "benchmark"
      ],
      "id": 200
    },
    {
      "name": "entity-recognition-datasets",
      "one_line_profile": "Collection of Named Entity Recognition (NER) corpora",
      "detailed_description": "A comprehensive collection of annotated datasets for named entity recognition tasks across various languages and domains, formatted for research use.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "dataset_management",
        "evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/juand-r/entity-recognition-datasets",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ner",
        "nlp",
        "dataset"
      ],
      "id": 201
    },
    {
      "name": "eva",
      "one_line_profile": "Evaluation framework for oncology foundation models",
      "detailed_description": "A framework designed for the evaluation of oncology foundation models, providing metrics and workflows for assessing model performance in cancer research contexts.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "medical_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/kaiko-ai/eva",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "oncology",
        "foundation-models",
        "evaluation"
      ],
      "id": 202
    },
    {
      "name": "MalDataGen",
      "one_line_profile": "Synthetic tabular dataset generation and evaluation framework",
      "detailed_description": "An advanced framework for generating and evaluating synthetic tabular datasets using generative models like diffusion and adversarial architectures, applicable to scientific data augmentation.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "data_generation",
        "evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/kayua/MalDataGen",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "synthetic-data",
        "generative-models",
        "tabular-data"
      ],
      "id": 203
    },
    {
      "name": "Mars-Bench",
      "one_line_profile": "Benchmark for vision models on Martian imagery",
      "detailed_description": "A standardized benchmark for evaluating computer vision models on Martian surface and orbital imagery, covering classification, segmentation, and detection tasks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "image_analysis"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/kerner-lab/Mars-Bench",
      "help_website": [],
      "license": null,
      "tags": [
        "planetary-science",
        "mars",
        "computer-vision"
      ],
      "id": 204
    },
    {
      "name": "DROP-Fixed-Income",
      "one_line_profile": "Java libraries for fixed income analytics and curve construction",
      "detailed_description": "A collection of libraries for quantitative finance, including multi-curve construction, valuation, and stochastic evolution, serving as a scientific modeling tool for financial mathematics.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "modeling",
        "analysis"
      ],
      "application_level": "library",
      "primary_language": "HTML",
      "repo_url": "https://github.com/lakshmiDRIP/DROP-Fixed-Income",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "quantitative-finance",
        "numerical-analysis",
        "modeling"
      ],
      "id": 205
    },
    {
      "name": "deeplearning-benchmark",
      "one_line_profile": "Benchmark suite for deep learning infrastructure",
      "detailed_description": "A suite of scripts and tools for benchmarking the performance of deep learning models on various hardware configurations.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "Shell",
      "repo_url": "https://github.com/lambdal/deeplearning-benchmark",
      "help_website": [],
      "license": null,
      "tags": [
        "deep-learning",
        "benchmark",
        "gpu"
      ],
      "id": 206
    },
    {
      "name": "lamindb",
      "one_line_profile": "Data framework for biology and scientific data management",
      "detailed_description": "A data framework specifically designed for biology, enabling queryable, traceable, and reproducible data management (FAIR principles) integrating lakehouse and lineage tracking.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "data_management",
        "reproducibility"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/laminlabs/lamindb",
      "help_website": [
        "https://lamin.ai"
      ],
      "license": "Apache-2.0",
      "tags": [
        "biology",
        "data-management",
        "fair-data"
      ],
      "id": 207
    },
    {
      "name": "langfuse",
      "one_line_profile": "LLM engineering platform for observability and evaluation",
      "detailed_description": "An open-source platform for LLM engineering that includes tools for observability, metrics collection, dataset management, and model evaluation.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "monitoring"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/langfuse/langfuse",
      "help_website": [
        "https://langfuse.com"
      ],
      "license": "NOASSERTION",
      "tags": [
        "llm-ops",
        "evaluation",
        "observability"
      ],
      "id": 208
    },
    {
      "name": "langwatch",
      "one_line_profile": "LLM Ops platform for traces, analytics, and evaluations",
      "detailed_description": "A platform for LLM operations focusing on traces, analytics, and evaluations, allowing for the optimization of prompts and assessment of model performance.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "analysis"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/langwatch/langwatch",
      "help_website": [
        "https://langwatch.ai"
      ],
      "license": "NOASSERTION",
      "tags": [
        "llm-ops",
        "evaluation",
        "analytics"
      ],
      "id": 209
    },
    {
      "name": "latitude-llm",
      "one_line_profile": "Prompt engineering and evaluation platform",
      "detailed_description": "An open-source platform designed to build, evaluate, and refine prompts for Large Language Models, facilitating systematic prompt engineering and testing.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "prompt_engineering"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/latitude-dev/latitude-llm",
      "help_website": [
        "https://latitude.so"
      ],
      "license": "LGPL-3.0",
      "tags": [
        "prompt-engineering",
        "evaluation",
        "llm"
      ],
      "id": 210
    },
    {
      "name": "BenchmarkDatasetCreator",
      "one_line_profile": "Pipeline for creating standardized bioacoustic datasets",
      "detailed_description": "A standardized pipeline for creating, storing, sharing, and using bioacoustic datasets, designed to facilitate the training and testing of AI models in bioacoustics.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "data_generation",
        "dataset_management"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/leabouffaut/BenchmarkDatasetCreator",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "bioacoustics",
        "dataset-creation",
        "standardization"
      ],
      "id": 211
    },
    {
      "name": "les-audits-affaires-eval-harness",
      "one_line_profile": "Evaluation harness for French business law LLMs",
      "detailed_description": "A lightweight CLI tool for benchmarking French Large Language Models specifically in the domain of business law, evaluating aspects like action, delay, and risk assessment.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/legml-ai/les-audits-affaires-eval-harness",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "legal-tech",
        "llm-evaluation",
        "french-law"
      ],
      "id": 212
    },
    {
      "name": "llm_benchmarks",
      "one_line_profile": "Collection of benchmarks and datasets for evaluating Large Language Models",
      "detailed_description": "A comprehensive collection of benchmarks and datasets designed for the evaluation of Large Language Models (LLMs), facilitating comparative analysis and performance assessment.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/leobeeson/llm_benchmarks",
      "help_website": [],
      "license": null,
      "tags": [
        "llm",
        "benchmark",
        "dataset",
        "evaluation"
      ],
      "id": 213
    },
    {
      "name": "Blades",
      "one_line_profile": "Unified benchmark suite for attacks and defenses in Federated Learning",
      "detailed_description": "A unified benchmark suite designed to evaluate attacks and defenses in Federated Learning systems, providing a standard environment for security and robustness research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "federated_learning",
        "security_evaluation",
        "benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/lishenghui/blades",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "federated-learning",
        "security",
        "benchmark",
        "attacks",
        "defenses"
      ],
      "id": 214
    },
    {
      "name": "llm-jp-eval-mm",
      "one_line_profile": "Lightweight framework for evaluating visual-language models",
      "detailed_description": "A lightweight evaluation framework specifically designed for visual-language models, supporting various metrics and datasets for multimodal performance assessment.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "multimodal_evaluation",
        "vlm_benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/llm-jp/llm-jp-eval-mm",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "vlm",
        "evaluation",
        "multimodal",
        "benchmark"
      ],
      "id": 215
    },
    {
      "name": "RouteLLM",
      "one_line_profile": "Framework for serving and evaluating LLM routers",
      "detailed_description": "A framework designed to serve and evaluate Large Language Model (LLM) routers, enabling researchers to optimize the trade-off between cost and response quality in LLM deployments.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_routing",
        "cost_optimization",
        "evaluation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/lm-sys/RouteLLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-routing",
        "evaluation",
        "inference-optimization"
      ],
      "id": 216
    },
    {
      "name": "SWT-Bench",
      "one_line_profile": "Evaluation harness for benchmarking LLM repository-level test generation",
      "detailed_description": "An evaluation harness for SWT-Bench, a benchmark designed to assess the capabilities of Large Language Models in generating repository-level tests, specifically for software testing tasks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "code_generation",
        "test_generation",
        "benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/logic-star-ai/swt-bench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "software-testing",
        "benchmark",
        "code-generation"
      ],
      "id": 217
    },
    {
      "name": "Loghub",
      "one_line_profile": "Large collection of system log datasets for AI-driven log analytics",
      "detailed_description": "A comprehensive collection of system log datasets designed to support research in AI-driven log analytics, including tasks such as anomaly detection and failure prediction.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "log_analysis",
        "anomaly_detection",
        "dataset"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/logpai/loghub",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "log-analysis",
        "aiops",
        "dataset",
        "anomaly-detection"
      ],
      "id": 218
    },
    {
      "name": "Matbench",
      "one_line_profile": "Benchmarks for materials science property prediction",
      "detailed_description": "A benchmark suite for evaluating machine learning models on materials science property prediction tasks, providing a standardized set of datasets and evaluation metrics.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "materials_science",
        "property_prediction",
        "benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/materialsproject/matbench",
      "help_website": [
        "https://matbench.materialsproject.org/"
      ],
      "license": "MIT",
      "tags": [
        "materials-science",
        "benchmark",
        "property-prediction"
      ],
      "id": 219
    },
    {
      "name": "pysaliency",
      "one_line_profile": "Python Framework for Saliency Modeling and Evaluation",
      "detailed_description": "A Python framework designed for the modeling and evaluation of visual saliency, providing tools to handle datasets, models, and standard metrics in saliency research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "saliency_prediction",
        "model_evaluation",
        "computer_vision"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/matthias-k/pysaliency",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "saliency",
        "evaluation",
        "computer-vision",
        "neuroscience"
      ],
      "id": 220
    },
    {
      "name": "Video-ChatGPT",
      "one_line_profile": "Video conversation model with quantitative evaluation benchmarking",
      "detailed_description": "A video conversation model capable of generating meaningful conversations about videos, accompanied by a rigorous quantitative evaluation benchmark for assessing video-based conversational models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "video_understanding",
        "multimodal_conversation",
        "benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/mbzuai-oryx/Video-ChatGPT",
      "help_website": [],
      "license": "CC-BY-4.0",
      "tags": [
        "video-llm",
        "benchmark",
        "multimodal"
      ],
      "id": 221
    },
    {
      "name": "Torcheval",
      "one_line_profile": "Library for performant PyTorch model metrics and evaluation",
      "detailed_description": "A library providing a rich collection of performant PyTorch model metrics and tools to facilitate metric computation in distributed training and model evaluation.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "metrics_computation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/meta-pytorch/torcheval",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "pytorch",
        "metrics",
        "evaluation"
      ],
      "id": 222
    },
    {
      "name": "micarraylib",
      "one_line_profile": "Software for aggregation and processing of microphone array datasets",
      "detailed_description": "A software tool for the reproducible aggregation, standardization, and signal processing of microphone array datasets, facilitating research in acoustics and audio processing.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "signal_processing",
        "dataset_standardization",
        "acoustics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/micarraylib/micarraylib",
      "help_website": [],
      "license": "CC-BY-4.0",
      "tags": [
        "audio-processing",
        "microphone-array",
        "dataset-tools"
      ],
      "id": 223
    },
    {
      "name": "OpenTTDLab",
      "one_line_profile": "Framework for running reproducible experiments using OpenTTD",
      "detailed_description": "A Python framework designed to run reproducible experiments using the OpenTTD game engine, suitable for research in reinforcement learning and simulation.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "simulation",
        "reinforcement_learning",
        "experiment_management"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/michalc/OpenTTDLab",
      "help_website": [],
      "license": "GPL-2.0",
      "tags": [
        "simulation",
        "reinforcement-learning",
        "openttd"
      ],
      "id": 224
    },
    {
      "name": "FS-Mol",
      "one_line_profile": "Few-Shot Learning Dataset and Benchmark for Molecules",
      "detailed_description": "A dataset and evaluation benchmark for few-shot learning in molecular property prediction, containing molecular compounds with activity measurements against various protein targets.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "molecular_property_prediction",
        "few_shot_learning",
        "benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/FS-Mol",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "drug-discovery",
        "molecules",
        "few-shot-learning",
        "benchmark"
      ],
      "id": 225
    },
    {
      "name": "Eureka ML Insights",
      "one_line_profile": "Framework for standardizing evaluations of large foundation models",
      "detailed_description": "A framework designed to standardize the evaluation of large foundation models, moving beyond single-score reporting to provide deeper insights into model capabilities.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "foundation_models"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/eureka-ml-insights",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "evaluation",
        "foundation-models",
        "metrics"
      ],
      "id": 226
    },
    {
      "name": "PromptBench",
      "one_line_profile": "Unified evaluation framework for large language models",
      "detailed_description": "A unified evaluation framework for Large Language Models (LLMs) that supports various tasks, datasets, and metrics to assess model performance and robustness.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "prompt_engineering",
        "benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/promptbench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "evaluation",
        "benchmark",
        "prompting"
      ],
      "id": 227
    },
    {
      "name": "Clustering-Datasets",
      "one_line_profile": "Collection of datasets for clustering algorithm benchmarking",
      "detailed_description": "A collection of UCI real-life and synthetic datasets, formatted and ready for use in benchmarking and evaluating clustering algorithms.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "clustering",
        "benchmarking",
        "dataset"
      ],
      "application_level": "dataset",
      "primary_language": null,
      "repo_url": "https://github.com/milaan9/Clustering-Datasets",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "clustering",
        "dataset",
        "machine-learning"
      ],
      "id": 228
    },
    {
      "name": "SPECTRA",
      "one_line_profile": "Spectral framework for evaluation of biomedical AI models",
      "detailed_description": "A framework utilizing spectral analysis methods for the evaluation of biomedical AI models, providing specialized metrics for this domain.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "biomedical_ai",
        "model_evaluation",
        "spectral_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/mims-harvard/SPECTRA",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "biomedical",
        "evaluation",
        "spectral-analysis"
      ],
      "id": 229
    },
    {
      "name": "GenAI Energy Leaderboard",
      "one_line_profile": "Benchmark and measurements for Generative AI energy consumption",
      "detailed_description": "A canonical source and benchmark suite for measuring and evaluating the energy consumption of Generative AI models, providing standardized metrics for efficiency analysis.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "energy_efficiency_analysis"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/ml-energy/leaderboard-v2",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "genai",
        "energy-efficiency",
        "benchmark"
      ],
      "id": 230
    },
    {
      "name": "CK-MLOps",
      "one_line_profile": "Portable workflows and automation recipes for MLOps",
      "detailed_description": "A collection of portable workflows, automation recipes, and components for MLOps in the Unified Collective Knowledge (CK) format, facilitating reproducible ML experiments and benchmarking.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "workflow_automation",
        "reproducibility"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/mlcommons/ck-mlops",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "mlops",
        "reproducibility",
        "workflow"
      ],
      "id": 231
    },
    {
      "name": "MLPerf Storage",
      "one_line_profile": "Benchmark suite for ML storage performance",
      "detailed_description": "The MLPerf Storage Benchmark Suite measures the performance of storage systems in the context of machine learning workloads, providing standardized metrics for system evaluation.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "system_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/mlcommons/storage",
      "help_website": [
        "https://mlcommons.org"
      ],
      "license": "Apache-2.0",
      "tags": [
        "mlperf",
        "storage",
        "benchmark"
      ],
      "id": 232
    },
    {
      "name": "MLPerf Tiny",
      "one_line_profile": "ML benchmark suite for low-power embedded systems",
      "detailed_description": "MLPerf Tiny is a machine learning benchmark suite specifically designed for extremely low-power systems such as microcontrollers, evaluating inference performance on constrained hardware.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "inference_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "C",
      "repo_url": "https://github.com/mlcommons/tiny",
      "help_website": [
        "https://mlcommons.org"
      ],
      "license": "Apache-2.0",
      "tags": [
        "tinyml",
        "embedded-systems",
        "benchmark"
      ],
      "id": 233
    },
    {
      "name": "EvalScope",
      "one_line_profile": "Framework for large model evaluation and benchmarking",
      "detailed_description": "A streamlined and customizable framework for efficient evaluation and performance benchmarking of large models (LLM, VLM, AIGC), supporting various datasets and metrics.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/modelscope/evalscope",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-evaluation",
        "benchmark",
        "modelscope"
      ],
      "id": 234
    },
    {
      "name": "MOSES",
      "one_line_profile": "Benchmarking platform for molecular generation models",
      "detailed_description": "Molecular Sets (MOSES) is a benchmarking platform for molecular generation models, providing standard datasets and metrics to evaluate the quality and diversity of generated chemical structures.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "molecular_generation",
        "benchmarking"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/molecularsets/moses",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "drug-discovery",
        "molecular-generation",
        "benchmark"
      ],
      "id": 235
    },
    {
      "name": "eICU Benchmark Updated",
      "one_line_profile": "Benchmark for clinical tasks on eICU dataset",
      "detailed_description": "An updated version of the eICU Benchmark providing problem definitions and evaluation frameworks for clinical tasks such as Length of Stay (LoS) prediction and Decompensation detection.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "clinical_prediction",
        "benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/mostafaalishahi/eICU_Benchmark_updated",
      "help_website": [],
      "license": null,
      "tags": [
        "healthcare",
        "clinical-benchmark",
        "eicu"
      ],
      "id": 236
    },
    {
      "name": "SciREX",
      "one_line_profile": "Benchmark for scientific information extraction",
      "detailed_description": "A benchmark dataset and evaluation framework for document-level information extraction from scientific articles, focusing on identifying entities and relationships in scientific text.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "information_extraction",
        "benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/n0w0f/scirex",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nlp",
        "scientific-extraction",
        "benchmark"
      ],
      "id": 237
    },
    {
      "name": "NASA Prognostic Algorithms",
      "one_line_profile": "Framework for model-based prognostics of engineering systems",
      "detailed_description": "A Python framework for model-based prognostics, providing algorithms for state estimation, uncertainty propagation, and remaining useful life (RUL) computation for engineering systems.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "prognostics",
        "state_estimation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/nasa/prog_algs",
      "help_website": [],
      "license": null,
      "tags": [
        "prognostics",
        "nasa",
        "engineering-systems"
      ],
      "id": 238
    },
    {
      "name": "latrend",
      "one_line_profile": "R package for clustering longitudinal datasets",
      "detailed_description": "An R package designed for clustering longitudinal datasets in a standardized way, providing interfaces to various clustering methods and facilitating method evaluation.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "clustering",
        "longitudinal_analysis"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/niekdt/latrend",
      "help_website": [],
      "license": "GPL-2.0",
      "tags": [
        "r-package",
        "clustering",
        "longitudinal-data"
      ],
      "id": 239
    },
    {
      "name": "ProkEvo",
      "one_line_profile": "Framework for bacterial population genomics analyses",
      "detailed_description": "An automated, reproducible, and scalable framework for high-throughput bacterial population genomics analyses, facilitating large-scale bioinformatics workflows.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "genomics",
        "bioinformatics_workflow"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/npavlovikj/ProkEvo",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "genomics",
        "bioinformatics",
        "pipeline"
      ],
      "id": 240
    },
    {
      "name": "pcapML",
      "one_line_profile": "Tool for standardizing network traffic analysis datasets",
      "detailed_description": "pcapML standardizes network traffic analysis datasets by directly encoding metadata information into raw traffic captures, facilitating machine learning on network data.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "data_standardization",
        "network_analysis"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/nprint/pcapml",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "network-traffic",
        "dataset-creation",
        "pcap"
      ],
      "id": 241
    },
    {
      "name": "Jury",
      "one_line_profile": "Comprehensive NLP evaluation system",
      "detailed_description": "A comprehensive evaluation system for Natural Language Processing (NLP) tasks, providing a unified interface for various metrics to assess model performance.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "nlp_evaluation",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/obss/jury",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nlp",
        "evaluation",
        "metrics"
      ],
      "id": 242
    },
    {
      "name": "mteval",
      "one_line_profile": "Evaluation metrics for machine translation",
      "detailed_description": "A collection of evaluation metrics and algorithms specifically designed for Machine Translation tasks, implemented in C++ for efficiency.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "machine_translation_evaluation",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/odashi/mteval",
      "help_website": [],
      "license": "LGPL-3.0",
      "tags": [
        "machine-translation",
        "evaluation",
        "cpp"
      ],
      "id": 243
    },
    {
      "name": "blurr",
      "one_line_profile": "Integration library for Transformers and fastai",
      "detailed_description": "A library that integrates Hugging Face Transformers with the fastai framework, enabling efficient training, evaluation, and deployment of transformer-based models for scientific and general NLP tasks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_training",
        "integration"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/ohmeow/blurr",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "fastai",
        "transformers",
        "deep-learning"
      ],
      "id": 244
    },
    {
      "name": "SRSD Benchmark",
      "one_line_profile": "Symbolic Regression datasets and benchmarks",
      "detailed_description": "A benchmark suite rethinking Symbolic Regression datasets for scientific discovery, providing standardized tasks to evaluate algorithms that discover mathematical expressions from data.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "symbolic_regression",
        "scientific_discovery"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/omron-sinicx/srsd-benchmark",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "symbolic-regression",
        "scientific-discovery",
        "benchmark"
      ],
      "id": 245
    },
    {
      "name": "GenAIEval",
      "one_line_profile": "Evaluation and benchmark for Generative AI",
      "detailed_description": "A comprehensive evaluation framework, benchmark, and scorecard for Generative AI, targeting performance (throughput/latency), accuracy, safety, and hallucination metrics.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "platform",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/opea-project/GenAIEval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "genai",
        "evaluation",
        "safety"
      ],
      "id": 246
    },
    {
      "name": "ATLAS",
      "one_line_profile": "Benchmark for frontier scientific reasoning",
      "detailed_description": "ATLAS is a high-difficulty, multidisciplinary benchmark designed to evaluate the scientific reasoning capabilities of AI models across various scientific domains.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "scientific_reasoning",
        "benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": null,
      "repo_url": "https://github.com/open-compass/ATLAS",
      "help_website": [],
      "license": null,
      "tags": [
        "scientific-reasoning",
        "benchmark",
        "llm"
      ],
      "id": 247
    },
    {
      "name": "CompassJudger",
      "one_line_profile": "Judge models for automated evaluation",
      "detailed_description": "A collection of 'Judge Models' introduced by OpenCompass, designed to act as automated evaluators for assessing the performance of other large language models on various benchmarks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "automated_evaluation",
        "model_judging"
      ],
      "application_level": "solver",
      "primary_language": null,
      "repo_url": "https://github.com/open-compass/CompassJudger",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-judge",
        "evaluation",
        "opencompass"
      ],
      "id": 248
    },
    {
      "name": "VLMEvalKit",
      "one_line_profile": "Evaluation toolkit for large multi-modality models",
      "detailed_description": "An open-source evaluation toolkit for Large Multi-modality Models (LMMs), supporting over 220 models and 80 benchmarks, facilitating comprehensive performance assessment.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "multimodal_evaluation",
        "benchmarking"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/open-compass/VLMEvalKit",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "vlm",
        "multimodal",
        "evaluation"
      ],
      "id": 249
    },
    {
      "name": "OpenCompass",
      "one_line_profile": "Comprehensive LLM evaluation platform",
      "detailed_description": "OpenCompass is a comprehensive platform for evaluating Large Language Models (LLMs), supporting a wide range of models and over 100 datasets to assess capabilities across various domains.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/open-compass/opencompass",
      "help_website": [
        "https://opencompass.org.cn/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm-evaluation",
        "benchmark",
        "platform"
      ],
      "id": 250
    },
    {
      "name": "OpenAI Evals",
      "one_line_profile": "Framework for evaluating LLMs and registry of benchmarks",
      "detailed_description": "Evals is a framework for evaluating Large Language Models (LLMs) and LLM systems, providing an open-source registry of benchmarks to test model performance on diverse tasks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/openai/evals",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "llm",
        "evaluation",
        "benchmark"
      ],
      "id": 251
    },
    {
      "name": "Oumi",
      "one_line_profile": "Open-source framework for fine-tuning, evaluating, and deploying LLMs/VLMs",
      "detailed_description": "Oumi is a comprehensive platform that simplifies the lifecycle of large language models (LLMs) and vision-language models (VLMs), providing tools for fine-tuning, evaluation, and deployment, facilitating AI research and application development.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_training",
        "model_evaluation",
        "inference"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/oumi-ai/oumi",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "fine-tuning",
        "evaluation",
        "vlm"
      ],
      "id": 252
    },
    {
      "name": "ToR[e]cSys",
      "one_line_profile": "PyTorch framework for recommendation system algorithms",
      "detailed_description": "ToR[e]cSys is a PyTorch-based framework designed to implement, experiment with, and reproduce recommendation system algorithms, including CTR prediction and learning-to-rank models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "modeling",
        "recommendation_system"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/p768lwy3/torecsys",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "recsys",
        "pytorch",
        "ctr-prediction"
      ],
      "id": 253
    },
    {
      "name": "CloudSuite",
      "one_line_profile": "Benchmark suite for cloud services",
      "detailed_description": "CloudSuite is a benchmark suite for cloud services, covering a wide range of applications such as data analytics, web serving, and media streaming, used for evaluating system performance in cloud environments.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "C",
      "repo_url": "https://github.com/parsa-epfl/cloudsuite",
      "help_website": [
        "http://cloudsuite.ch"
      ],
      "license": "NOASSERTION",
      "tags": [
        "cloud-computing",
        "benchmark",
        "systems-research"
      ],
      "id": 254
    },
    {
      "name": "PDEBench",
      "one_line_profile": "Extensive benchmark for Scientific Machine Learning (SciML)",
      "detailed_description": "PDEBench is a comprehensive benchmark suite for Scientific Machine Learning, specifically focused on solving Partial Differential Equations (PDEs). It provides diverse datasets and tasks to evaluate ML models in scientific contexts.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "sciml",
        "pde_solving"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/pdebench/PDEBench",
      "help_website": [
        "https://arxiv.org/abs/2210.07182"
      ],
      "license": "NOASSERTION",
      "tags": [
        "sciml",
        "pde",
        "benchmark",
        "physics"
      ],
      "id": 255
    },
    {
      "name": "Japanese LM Financial Harness",
      "one_line_profile": "Evaluation harness for Japanese financial language models",
      "detailed_description": "A specialized evaluation framework for assessing the performance of Japanese Language Models on financial domain tasks, facilitating domain-specific AI research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "benchmarking",
        "nlp"
      ],
      "application_level": "framework",
      "primary_language": "Shell",
      "repo_url": "https://github.com/pfnet-research/japanese-lm-fin-harness",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "finance",
        "japanese",
        "evaluation"
      ],
      "id": 256
    },
    {
      "name": "Phoronix Test Suite",
      "one_line_profile": "Automated testing and benchmarking software",
      "detailed_description": "The Phoronix Test Suite is a comprehensive, open-source testing and benchmarking platform for Linux, Windows, and macOS, widely used in computer science research for system performance evaluation.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_testing"
      ],
      "application_level": "solver",
      "primary_language": "PHP",
      "repo_url": "https://github.com/phoronix-test-suite/phoronix-test-suite",
      "help_website": [
        "https://www.phoronix-test-suite.com/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "benchmark",
        "hardware",
        "performance"
      ],
      "id": 257
    },
    {
      "name": "OpenCGRA",
      "one_line_profile": "Framework for modeling and evaluating CGRAs",
      "detailed_description": "OpenCGRA is an open-source framework designed for modeling, testing, and evaluating Coarse-Grained Reconfigurable Architectures (CGRAs), supporting computer architecture research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "modeling",
        "architecture_evaluation"
      ],
      "application_level": "framework",
      "primary_language": "Verilog",
      "repo_url": "https://github.com/pnnl/OpenCGRA",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "cgra",
        "computer-architecture",
        "modeling"
      ],
      "id": 258
    },
    {
      "name": "QASMBench",
      "one_line_profile": "Low-level OpenQASM benchmark suite for NISQ evaluation",
      "detailed_description": "QASMBench is a benchmark suite for evaluating Noisy Intermediate-Scale Quantum (NISQ) devices and simulators using OpenQASM, facilitating quantum computing research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "quantum_computing"
      ],
      "application_level": "dataset",
      "primary_language": "OpenQASM",
      "repo_url": "https://github.com/pnnl/QASMBench",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "quantum",
        "benchmark",
        "openqasm"
      ],
      "id": 259
    },
    {
      "name": "LitSearch",
      "one_line_profile": "Retrieval benchmark for scientific literature search",
      "detailed_description": "LitSearch is a benchmark designed to evaluate retrieval systems on the task of scientific literature search, supporting research in NLP and meta-science.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "information_retrieval"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/princeton-nlp/LitSearch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nlp",
        "retrieval",
        "scientific-literature"
      ],
      "id": 260
    },
    {
      "name": "PhysGym",
      "one_line_profile": "Benchmark suite for LLM-based interactive scientific reasoning",
      "detailed_description": "PhysGym is a benchmark suite for evaluating the capabilities of Large Language Models in interactive scientific reasoning tasks, specifically in physics.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "scientific_reasoning"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/principia-ai/PhysGym",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "physics",
        "reasoning",
        "benchmark"
      ],
      "id": 261
    },
    {
      "name": "Etalon",
      "one_line_profile": "LLM serving performance evaluation harness",
      "detailed_description": "Etalon is a framework for evaluating the performance of LLM serving systems, providing metrics for throughput, latency, and cost, essential for systems research in AI.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_evaluation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/project-etalon/etalon",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-serving",
        "performance",
        "benchmark"
      ],
      "id": 262
    },
    {
      "name": "Prometheus-Eval",
      "one_line_profile": "Library for evaluating LLM responses using Prometheus and GPT-4",
      "detailed_description": "Prometheus-Eval is a Python library that facilitates the evaluation of Large Language Model outputs by leveraging the Prometheus model and GPT-4 as judges, supporting research in automated evaluation metrics.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/prometheus-eval/prometheus-eval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-evaluation",
        "metrics",
        "nlp"
      ],
      "id": 263
    },
    {
      "name": "pyperformance",
      "one_line_profile": "Python performance benchmark suite",
      "detailed_description": "pyperformance is the standard benchmark suite for the Python programming language, used to evaluate and track the performance of Python implementations, supporting computer science and programming language research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/python/pyperformance",
      "help_website": [
        "https://pyperformance.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "python",
        "benchmark",
        "performance"
      ],
      "id": 264
    },
    {
      "name": "psychopy_ext",
      "one_line_profile": "Framework for behavioral neuroscience and psychology experiments",
      "detailed_description": "psychopy_ext is a framework built on top of PsychoPy to facilitate rapid, reproducible design, analysis, and plotting of experiments in behavioral neuroscience and psychology.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "experiment_control",
        "data_analysis"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/qbilius/psychopy_ext",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "neuroscience",
        "psychology",
        "experiment-design"
      ],
      "id": 265
    },
    {
      "name": "Quaterion Models",
      "one_line_profile": "Building blocks for fine-tunable metric learning models",
      "detailed_description": "Quaterion Models provides a collection of pre-trained models and building blocks for metric learning, enabling researchers to build and fine-tune models for similarity search and related tasks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "modeling",
        "metric_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/qdrant/quaterion-models",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "metric-learning",
        "similarity-search",
        "embeddings"
      ],
      "id": 266
    },
    {
      "name": "Renaissance",
      "one_line_profile": "Benchmark suite for the JVM",
      "detailed_description": "Renaissance is a modern benchmark suite for the Java Virtual Machine (JVM), focusing on parallelism and concurrency, used in systems research to evaluate JVM performance.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "SMT",
      "repo_url": "https://github.com/renaissance-benchmarks/renaissance",
      "help_website": [
        "http://renaissance.dev"
      ],
      "license": "GPL-3.0",
      "tags": [
        "jvm",
        "benchmark",
        "concurrency"
      ],
      "id": 267
    },
    {
      "name": "Phylociraptor",
      "one_line_profile": "Rapid phylogenomic tree calculator",
      "detailed_description": "Phylociraptor is a highly customizable and reproducible framework for phylogenomic inference, automating the calculation of phylogenetic trees from genomic data.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "inference",
        "phylogenetics"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/reslp/phylociraptor",
      "help_website": [
        "https://phylociraptor.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "phylogenomics",
        "bioinformatics",
        "tree-inference"
      ],
      "id": 268
    },
    {
      "name": "Auto-Evaluator",
      "one_line_profile": "Evaluation tool for LLM QA chains",
      "detailed_description": "Auto-Evaluator is a tool designed to evaluate the quality of Question Answering (QA) chains powered by LLMs, providing automated metrics for assessing model performance.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "evaluation",
        "qa_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/rlancemartin/auto-evaluator",
      "help_website": [],
      "license": null,
      "tags": [
        "llm",
        "evaluation",
        "qa"
      ],
      "id": 269
    },
    {
      "name": "RobotPerf Benchmarks",
      "one_line_profile": "Benchmarking suite for robotics computing performance",
      "detailed_description": "RobotPerf is a vendor-neutral benchmarking suite designed to evaluate robotics computing performance, covering both grey-box and black-box approaches for robotics systems.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "robotics"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/robotperf/benchmarks",
      "help_website": [
        "https://robotperf.org"
      ],
      "license": "Apache-2.0",
      "tags": [
        "robotics",
        "benchmark",
        "performance"
      ],
      "id": 270
    },
    {
      "name": "YAIB",
      "one_line_profile": "Yet Another ICU Benchmark for clinical prediction models",
      "detailed_description": "YAIB is a holistic framework for standardizing clinical prediction model experiments, providing datasets, cohorts, tasks, and preprocessing pipelines for ICU data.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "clinical_prediction"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/rvandewater/YAIB",
      "help_website": [
        "https://arxiv.org/abs/2306.05109"
      ],
      "license": "MIT",
      "tags": [
        "clinical-ml",
        "benchmark",
        "icu"
      ],
      "id": 271
    },
    {
      "name": "RXN Reaction Preprocessing",
      "one_line_profile": "Preprocessing library for chemical reaction datasets",
      "detailed_description": "A Python library for the standardization, filtering, augmentation, and tokenization of chemical reaction datasets, developed by IBM Research for chemical AI tasks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "data_processing",
        "chemistry"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/rxn4chemistry/rxn-reaction-preprocessing",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "chemistry",
        "preprocessing",
        "reaction-prediction"
      ],
      "id": 272
    },
    {
      "name": "DialogStudio",
      "one_line_profile": "Unified dataset collection and instruction-aware models for conversational AI",
      "detailed_description": "A large-scale collection of unified datasets for conversational AI, designed to facilitate research in dialogue systems and instruction tuning.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "dataset_collection",
        "conversational_ai"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/salesforce/DialogStudio",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "nlp",
        "dialogue-systems",
        "dataset"
      ],
      "id": 273
    },
    {
      "name": "RePlay",
      "one_line_profile": "Framework for building and evaluating recommendation systems",
      "detailed_description": "A comprehensive framework for building, training, and evaluating recommendation systems, supporting various state-of-the-art models and data preprocessing pipelines.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "recommendation_system",
        "model_evaluation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/sb-ai-lab/RePlay",
      "help_website": [
        "https://replay.readthedocs.io/en/latest/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "recsys",
        "pytorch",
        "evaluation"
      ],
      "id": 274
    },
    {
      "name": "GAP Benchmark Suite",
      "one_line_profile": "Benchmark suite for graph processing algorithms",
      "detailed_description": "A benchmark suite designed to standardize the evaluation of graph processing algorithms and systems, providing reference implementations and datasets.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "graph_processing",
        "performance_benchmarking"
      ],
      "application_level": "benchmark",
      "primary_language": "C++",
      "repo_url": "https://github.com/sbeamer/gapbs",
      "help_website": [
        "http://gap.cs.berkeley.edu/benchmark.html"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "graph-algorithms",
        "hpc",
        "benchmark"
      ],
      "id": 275
    },
    {
      "name": "SciCode",
      "one_line_profile": "Benchmark for code generation in scientific domains",
      "detailed_description": "A benchmark designed to evaluate the capability of language models to generate code for solving scientific problems across various disciplines.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "code_generation",
        "model_evaluation"
      ],
      "application_level": "benchmark",
      "primary_language": "Python",
      "repo_url": "https://github.com/scicode-bench/SciCode",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "scientific-computing",
        "benchmark"
      ],
      "id": 276
    },
    {
      "name": "TorchSpatial",
      "one_line_profile": "Framework and benchmark for Spatial Representation Learning",
      "detailed_description": "A comprehensive framework and benchmark suite designed to advance Spatial Representation Learning (SRL), providing tools for modeling and evaluating spatial data.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "spatial_representation_learning",
        "model_evaluation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/seai-lab/TorchSpatial",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "spatial-data",
        "representation-learning",
        "benchmark"
      ],
      "id": 277
    },
    {
      "name": "selva86/datasets",
      "one_line_profile": "Collection of standard datasets for machine learning",
      "detailed_description": "A repository hosting a variety of standard datasets commonly used for machine learning problems, facilitating easy access for experimentation and benchmarking.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "dataset_collection"
      ],
      "application_level": "dataset",
      "primary_language": "R",
      "repo_url": "https://github.com/selva86/datasets",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "machine-learning",
        "datasets",
        "statistics"
      ],
      "id": 278
    },
    {
      "name": "numpy-benchmarks",
      "one_line_profile": "Collection of scientific kernels for NumPy benchmarking",
      "detailed_description": "A set of scientific computing kernels implemented using NumPy, designed for benchmarking and performance analysis of numerical operations.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "performance_benchmarking",
        "numerical_computing"
      ],
      "application_level": "benchmark",
      "primary_language": "Python",
      "repo_url": "https://github.com/serge-sans-paille/numpy-benchmarks",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "numpy",
        "hpc",
        "benchmark"
      ],
      "id": 279
    },
    {
      "name": "gnn-benchmark",
      "one_line_profile": "Benchmarking framework for Graph Neural Networks",
      "detailed_description": "A framework for evaluating Graph Neural Network (GNN) models on semi-supervised node classification tasks, ensuring fair and reproducible comparisons.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "graph_neural_networks",
        "model_evaluation"
      ],
      "application_level": "benchmark",
      "primary_language": "Python",
      "repo_url": "https://github.com/shchur/gnn-benchmark",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gnn",
        "graph-learning",
        "benchmark"
      ],
      "id": 280
    },
    {
      "name": "ChemBench",
      "one_line_profile": "Benchmark datasets for molecular machine learning",
      "detailed_description": "A collection of benchmark datasets including MoleculeNet and MolMapNet, designed for evaluating machine learning models in chemistry and drug discovery.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "molecular_modeling",
        "dataset_collection"
      ],
      "application_level": "dataset",
      "primary_language": "HTML",
      "repo_url": "https://github.com/shenwanxiang/ChemBench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "cheminformatics",
        "moleculenet",
        "benchmark"
      ],
      "id": 281
    },
    {
      "name": "DataRec",
      "one_line_profile": "Library for reproducible data management in recommender systems",
      "detailed_description": "A Python library designed to standardize and ensure reproducibility in data management and preprocessing for recommender system research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "data_management",
        "reproducibility"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/sisinflab/DataRec",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "recsys",
        "data-processing",
        "reproducibility"
      ],
      "id": 282
    },
    {
      "name": "Elliot",
      "one_line_profile": "Framework for reproducible recommender systems evaluation",
      "detailed_description": "A comprehensive and rigorous framework for the evaluation of recommender systems, focusing on reproducibility and standardized metrics.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "recommendation_system"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/sisinflab/elliot",
      "help_website": [
        "https://elliot.readthedocs.io/en/latest/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "recsys",
        "evaluation",
        "benchmark"
      ],
      "id": 283
    },
    {
      "name": "Seamless",
      "one_line_profile": "Framework for reproducible and interactive computations",
      "detailed_description": "A framework to set up reproducible computations and visualizations that respond to changes in data or code, suitable for scientific workflows.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "workflow_management",
        "reproducibility"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/sjdv1982/seamless",
      "help_website": [
        "http://sjdv1982.github.io/seamless/"
      ],
      "license": "MIT",
      "tags": [
        "reproducibility",
        "workflow",
        "interactive-computing"
      ],
      "id": 284
    },
    {
      "name": "ZSC-Eval",
      "one_line_profile": "Evaluation toolkit for multi-agent zero-shot coordination",
      "detailed_description": "An evaluation toolkit and benchmark designed for assessing multi-agent reinforcement learning agents in zero-shot coordination scenarios.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "multi_agent_rl",
        "model_evaluation"
      ],
      "application_level": "benchmark",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/sjtu-marl/ZSC-Eval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "marl",
        "reinforcement-learning",
        "benchmark"
      ],
      "id": 285
    },
    {
      "name": "SpeechLLM",
      "one_line_profile": "Training and evaluation framework for SpeechLLM models",
      "detailed_description": "A repository containing code for training, inference, and evaluation of SpeechLLM models, facilitating research in speech-language modeling.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "speech_processing",
        "model_training",
        "evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/skit-ai/SpeechLLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "speech-recognition",
        "llm",
        "audio-processing"
      ],
      "id": 286
    },
    {
      "name": "matbench-genmetrics",
      "one_line_profile": "Metrics for generative materials benchmarking",
      "detailed_description": "A Python library providing benchmarking metrics for generative models in materials science, inspired by Guacamol and CDVAE.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "materials_science",
        "generative_models",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/sparks-baird/matbench-genmetrics",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "materials-informatics",
        "generative-ai",
        "benchmark"
      ],
      "id": 287
    },
    {
      "name": "SeBS",
      "one_line_profile": "Serverless benchmarking suite for performance analysis",
      "detailed_description": "A benchmarking suite for automatic performance analysis of FaaS platforms, useful for systems research and infrastructure evaluation.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "performance_benchmarking",
        "systems_research"
      ],
      "application_level": "benchmark",
      "primary_language": "Python",
      "repo_url": "https://github.com/spcl/serverless-benchmarks",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "serverless",
        "benchmark",
        "faas"
      ],
      "id": 288
    },
    {
      "name": "HELM",
      "one_line_profile": "Holistic Evaluation of Language Models framework",
      "detailed_description": "A framework for holistic, reproducible, and transparent evaluation of foundation models, including LLMs, covering a wide range of metrics and scenarios.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "llm_benchmarking"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/stanford-crfm/helm",
      "help_website": [
        "https://crfm.stanford.edu/helm/latest/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "evaluation",
        "benchmark"
      ],
      "id": 289
    },
    {
      "name": "SciML Bench",
      "one_line_profile": "Benchmarking suite for AI for Science",
      "detailed_description": "A benchmarking suite specifically designed for evaluating AI models and methods in scientific domains (AI for Science).",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "ai4science",
        "model_evaluation"
      ],
      "application_level": "benchmark",
      "primary_language": "Python",
      "repo_url": "https://github.com/stfc-sciml/sciml-bench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "sciml",
        "benchmark",
        "scientific-computing"
      ],
      "id": 290
    },
    {
      "name": "BIG-Bench-Hard",
      "one_line_profile": "Challenging subset of BIG-Bench tasks for LLMs",
      "detailed_description": "A collection of challenging tasks from the BIG-Bench suite, designed to evaluate the reasoning capabilities of large language models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "reasoning"
      ],
      "application_level": "benchmark",
      "primary_language": "Python",
      "repo_url": "https://github.com/suzgunmirac/BIG-Bench-Hard",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "benchmark",
        "reasoning"
      ],
      "id": 291
    },
    {
      "name": "tdoku",
      "one_line_profile": "Fast Sudoku solver and benchmark suite",
      "detailed_description": "A highly optimized Sudoku solver and generator, including a benchmark suite for comparing solver performance, relevant for constraint satisfaction research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "constraint_satisfaction",
        "solver_benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/t-dillon/tdoku",
      "help_website": [],
      "license": "BSD-2-Clause",
      "tags": [
        "sudoku",
        "optimization",
        "benchmark"
      ],
      "id": 292
    },
    {
      "name": "NPF",
      "one_line_profile": "Network Performance Framework for automated experiments",
      "detailed_description": "An experiment manager for network performance testing, providing automated execution, result collection, and graphing for reproducible research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "network_experiments",
        "performance_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/tbarbette/npf",
      "help_website": [
        "https://npf.readthedocs.io/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "networking",
        "reproducibility",
        "benchmark"
      ],
      "id": 293
    },
    {
      "name": "bigint-benchmark-rs",
      "one_line_profile": "Benchmarks for Rust big integer implementations",
      "detailed_description": "A benchmarking suite for comparing the performance of various big integer arithmetic implementations in Rust, useful for cryptography and number theory research.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "numerical_computing",
        "performance_benchmarking"
      ],
      "application_level": "benchmark",
      "primary_language": "Rust",
      "repo_url": "https://github.com/tczajka/bigint-benchmark-rs",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rust",
        "bigint",
        "benchmark"
      ],
      "id": 294
    },
    {
      "name": "GPTeacher",
      "one_line_profile": "Modular datasets generated by GPT-4 for instruction tuning",
      "detailed_description": "A collection of datasets generated by GPT-4, including General-Instruct, Roleplay-Instruct, Code-Instruct, and Toolformer, for training and evaluating models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "dataset_collection",
        "instruction_tuning"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/teknium1/GPTeacher",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "dataset",
        "synthetic-data"
      ],
      "id": 295
    },
    {
      "name": "TensorFlow Datasets",
      "one_line_profile": "Collection of ready-to-use datasets for TensorFlow",
      "detailed_description": "A library of datasets ready to use with TensorFlow, JAX, and other machine learning frameworks, handling downloading and preparing data.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "dataset_access",
        "data_loading"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/tensorflow/datasets",
      "help_website": [
        "https://www.tensorflow.org/datasets"
      ],
      "license": "Apache-2.0",
      "tags": [
        "tensorflow",
        "datasets",
        "machine-learning"
      ],
      "id": 296
    },
    {
      "name": "Safety-Prompts",
      "one_line_profile": "Chinese safety prompts dataset for evaluating LLM safety",
      "detailed_description": "A collection of Chinese safety prompts designed to evaluate and improve the safety of Large Language Models (LLMs), covering various safety categories.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "safety_evaluation",
        "benchmark_dataset"
      ],
      "application_level": "dataset",
      "primary_language": "No Code",
      "repo_url": "https://github.com/thu-coai/Safety-Prompts",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-safety",
        "benchmark",
        "prompts"
      ],
      "id": 297
    },
    {
      "name": "TransformerLab",
      "one_line_profile": "Desktop application for LLM engineering and evaluation",
      "detailed_description": "An open-source application designed for advanced Large Language Model (LLM) engineering, enabling users to interact with, train, fine-tune, and evaluate models locally.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "fine_tuning",
        "training_platform"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/transformerlab/transformerlab-app",
      "help_website": [
        "https://transformerlab.ai"
      ],
      "license": "AGPL-3.0",
      "tags": [
        "llm",
        "evaluation",
        "fine-tuning",
        "desktop-app"
      ],
      "id": 298
    },
    {
      "name": "uci_datasets",
      "one_line_profile": "Python library for loading standardized UCI regression datasets",
      "detailed_description": "A Python package that provides easy access to regression datasets from the UCI Machine Learning Repository with standardized train-test splits for benchmarking.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "dataset_loading",
        "benchmark_data"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/treforevans/uci_datasets",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "uci",
        "datasets",
        "regression",
        "benchmark"
      ],
      "id": 299
    },
    {
      "name": "TruLens",
      "one_line_profile": "Evaluation and tracking library for LLM experiments",
      "detailed_description": "A library for evaluating and tracking Large Language Model (LLM) experiments and AI agents, providing feedback functions to measure performance and quality.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "experiment_tracking",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/truera/trulens",
      "help_website": [
        "https://www.trulens.org"
      ],
      "license": "MIT",
      "tags": [
        "llm-evaluation",
        "observability",
        "metrics"
      ],
      "id": 300
    },
    {
      "name": "ApeBench",
      "one_line_profile": "Benchmark suite for autoregressive neural emulation of PDEs",
      "detailed_description": "A comprehensive benchmark suite for evaluating autoregressive neural emulators for Partial Differential Equations (PDEs), covering 1D, 2D, and 3D problems with differentiable physics metrics.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "pde_solving",
        "neural_operator_benchmark",
        "scientific_machine_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/tum-pbs/apebench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pde",
        "benchmark",
        "neural-operators",
        "physics-ml"
      ],
      "id": 301
    },
    {
      "name": "Petastorm",
      "one_line_profile": "Library for deep learning data access from Apache Parquet",
      "detailed_description": "A library enabling single-machine or distributed training and evaluation of deep learning models directly from datasets in Apache Parquet format, supporting TensorFlow and PyTorch.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "data_loading",
        "training_infrastructure"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/uber/petastorm",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "parquet",
        "data-loading",
        "deep-learning"
      ],
      "id": 302
    },
    {
      "name": "RADIal",
      "one_line_profile": "High-definition radar dataset for multi-task learning",
      "detailed_description": "A raw high-definition radar dataset designed for multi-task learning in autonomous driving, including object detection and segmentation tasks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "autonomous_driving_benchmark",
        "radar_perception"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/valeoai/RADIal",
      "help_website": [],
      "license": "No Assertion",
      "tags": [
        "radar",
        "dataset",
        "autonomous-driving"
      ],
      "id": 303
    },
    {
      "name": "Whisper Finetune",
      "one_line_profile": "Tools for fine-tuning and evaluating Whisper ASR models",
      "detailed_description": "A collection of scripts and tools to fine-tune and evaluate OpenAI's Whisper models for Automatic Speech Recognition (ASR) on custom or Hugging Face datasets.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_finetuning",
        "asr_evaluation"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/vasistalodagala/whisper-finetune",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "whisper",
        "asr",
        "fine-tuning"
      ],
      "id": 304
    },
    {
      "name": "SHOC",
      "one_line_profile": "Scalable Heterogeneous Computing Benchmark Suite",
      "detailed_description": "A benchmark suite designed to test the performance and stability of scalable heterogeneous computing systems, including GPUs and multi-core processors, for scientific workloads.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "hpc_benchmark",
        "system_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Makefile",
      "repo_url": "https://github.com/vetter/shoc",
      "help_website": [],
      "license": "No Assertion",
      "tags": [
        "hpc",
        "benchmark",
        "gpu",
        "opencl",
        "cuda"
      ],
      "id": 305
    },
    {
      "name": "Ragas",
      "one_line_profile": "Evaluation framework for RAG pipelines",
      "detailed_description": "A framework for evaluating Retrieval Augmented Generation (RAG) pipelines, providing metrics to assess the faithfulness, answer relevance, and context retrieval quality of LLM applications.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "rag_evaluation",
        "llm_metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/vibrantlabsai/ragas",
      "help_website": [
        "https://docs.ragas.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "evaluation",
        "llm",
        "metrics"
      ],
      "id": 306
    },
    {
      "name": "Rdatasets",
      "one_line_profile": "Collection of standard datasets from R packages",
      "detailed_description": "A comprehensive collection of datasets originally distributed in R packages, made available as CSV files for use in other data analysis environments and benchmarks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "dataset_collection",
        "benchmark_data"
      ],
      "application_level": "dataset",
      "primary_language": "HTML",
      "repo_url": "https://github.com/vincentarelbundock/Rdatasets",
      "help_website": [
        "https://vincentarelbundock.github.io/Rdatasets"
      ],
      "license": "No Assertion",
      "tags": [
        "datasets",
        "r",
        "csv",
        "benchmark"
      ],
      "id": 307
    },
    {
      "name": "WfCommons",
      "one_line_profile": "Framework for scientific workflow research and trace generation",
      "detailed_description": "A framework for enabling research and development in scientific workflows, providing tools to generate synthetic workflow traces and analyze workflow execution logs.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "workflow_research",
        "trace_generation",
        "simulation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/wfcommons/WfCommons",
      "help_website": [
        "https://wfcommons.org"
      ],
      "license": "LGPL-3.0",
      "tags": [
        "scientific-workflows",
        "simulation",
        "benchmark"
      ],
      "id": 308
    },
    {
      "name": "PhaseLLM",
      "one_line_profile": "Framework for LLM evaluation and workflow management",
      "detailed_description": "A framework designed to streamline the evaluation and workflow management of Large Language Models (LLMs), helping developers build more robust AI applications.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "workflow_management"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/wgryc/phasellm",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "evaluation",
        "workflow"
      ],
      "id": 309
    },
    {
      "name": "whylogs",
      "one_line_profile": "Data logging and quality monitoring library for ML",
      "detailed_description": "An open-source library for logging data profiles, enabling data quality monitoring, model performance tracking, and drift detection in machine learning pipelines.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "data_quality",
        "model_monitoring",
        "logging"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/whylabs/whylogs",
      "help_website": [
        "https://whylabs.ai/whylogs"
      ],
      "license": "Apache-2.0",
      "tags": [
        "data-quality",
        "mlops",
        "monitoring"
      ],
      "id": 310
    },
    {
      "name": "ControlGym",
      "one_line_profile": "Benchmark environments for reinforcement learning in control",
      "detailed_description": "A large-scale benchmark suite of control environments for evaluating Reinforcement Learning algorithms, focusing on control theory and dynamic systems.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "rl_benchmark",
        "control_systems"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/xiangyuan-zhang/controlgym",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "reinforcement-learning",
        "control-theory",
        "benchmark"
      ],
      "id": 311
    },
    {
      "name": "Multiphysics-Bench",
      "one_line_profile": "Benchmark for scientific machine learning on multiphysics PDEs",
      "detailed_description": "A benchmark suite for investigating and evaluating Scientific Machine Learning (SciML) methods for solving multiphysics Partial Differential Equations (PDEs).",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "pde_benchmark",
        "sciml",
        "multiphysics"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/xie-lab-ml/multiphysics-bench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "sciml",
        "pde",
        "benchmark",
        "multiphysics"
      ],
      "id": 312
    },
    {
      "name": "GAN-Metrics",
      "one_line_profile": "Collection of evaluation metrics for GAN models",
      "detailed_description": "A Python library implementing various metrics for evaluating the performance and quality of Generative Adversarial Networks (GANs).",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "gan_evaluation",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/yhlleo/GAN-Metrics",
      "help_website": [],
      "license": "No Assertion",
      "tags": [
        "gan",
        "metrics",
        "evaluation"
      ],
      "id": 313
    },
    {
      "name": "RaLLe",
      "one_line_profile": "Framework for developing and evaluating Retrieval-Augmented Large Language Models (RAG)",
      "detailed_description": "RaLLe is a framework designed to facilitate the development and evaluation of Retrieval-Augmented Generation (RAG) systems for Large Language Models. It provides tools for assessing the performance of RAG pipelines, ensuring the reliability and accuracy of retrieved information in AI-driven workflows.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "rag_framework"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/yhoshi3/RaLLe",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "llm",
        "evaluation",
        "benchmark"
      ],
      "id": 314
    },
    {
      "name": "MGBench",
      "one_line_profile": "Benchmark suite for molecular glue ternary structure prediction methods",
      "detailed_description": "MGBench is a benchmarking tool specifically designed for evaluating cofolding methods used in molecular glue ternary structure prediction. It provides a standardized environment and datasets to assess the accuracy and performance of computational models in drug discovery and structural biology contexts.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "structure_prediction",
        "molecular_modeling"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/yiyanliao/MGBench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "molecular-glue",
        "protein-folding",
        "drug-discovery",
        "benchmark"
      ],
      "id": 315
    },
    {
      "name": "torchdistill",
      "one_line_profile": "Reproducible deep learning framework for knowledge distillation",
      "detailed_description": "torchdistill is a coding-free framework built on PyTorch designed to facilitate reproducible deep learning studies, specifically focusing on knowledge distillation. It offers implementations of numerous state-of-the-art distillation methods and provides configurations to ensure consistent benchmarking and experimental reproduction.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_optimization",
        "reproducibility"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/yoshitomo-matsubara/torchdistill",
      "help_website": [
        "https://torchdistill.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "knowledge-distillation",
        "pytorch",
        "reproducibility",
        "deep-learning"
      ],
      "id": 316
    },
    {
      "name": "EasyLM",
      "one_line_profile": "One-stop solution for pre-training, finetuning, and serving LLMs using JAX/Flax",
      "detailed_description": "EasyLM is a comprehensive library for Large Language Models (LLMs) built on JAX and Flax. It simplifies the process of pre-training, fine-tuning, evaluating, and serving LLMs, providing a scalable and efficient toolchain for AI model development.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_training",
        "inference"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/young-geng/EasyLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "jax",
        "flax",
        "model-serving"
      ],
      "id": 317
    },
    {
      "name": "MT-Consistency",
      "one_line_profile": "Benchmark for evaluating acquiescence bias and consistency in LLMs",
      "detailed_description": "MT-Consistency is a research framework and benchmark designed to investigate and mitigate acquiescence bias in Large Language Models during sequential QA interactions. It includes datasets and evaluation scripts to assess the robustness and conversational consistency of AI models.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "model_evaluation",
        "bias_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/yubol-bobo/MT-Consistency",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "bias",
        "consistency",
        "benchmark"
      ],
      "id": 318
    },
    {
      "name": "fret",
      "one_line_profile": "Framework for Reproducible ExperimenTs in scientific computing",
      "detailed_description": "fret is a lightweight framework designed to simplify the management and reproduction of scientific experiments. It provides tools to configure, run, and track experiments, ensuring that research results are reproducible and manageable.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "experiment_management",
        "reproducibility"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/yxonic/fret",
      "help_website": [
        "https://fret.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "reproducibility",
        "experiment-tracking",
        "workflow"
      ],
      "id": 319
    },
    {
      "name": "MedSegBench",
      "one_line_profile": "Standardized benchmark collection for medical image segmentation",
      "detailed_description": "MedSegBench is a comprehensive benchmarking library that provides access to standardized medical segmentation datasets across various modalities. It facilitates the evaluation and comparison of medical image segmentation models by offering a unified interface for data loading and processing.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "image_segmentation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/zekikus/MedSegBench",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "medical-imaging",
        "segmentation",
        "benchmark",
        "datasets"
      ],
      "id": 320
    },
    {
      "name": "dabestr",
      "one_line_profile": "Data analysis tool using bootstrap estimation for robust statistical inference",
      "detailed_description": "A package for Data Analysis with Bootstrap-coupled ESTimation (DABEST). It provides a user-friendly interface to calculate and visualize effect sizes and their confidence intervals using bootstrap methods, moving away from traditional significance testing.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_analysis",
        "estimation",
        "visualization"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/ACCLAB/dabestr",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "statistics",
        "bootstrap",
        "estimation-plots",
        "r-package"
      ],
      "id": 321
    },
    {
      "name": "continual_rl",
      "one_line_profile": "Baselines and metrics library for continual reinforcement learning",
      "detailed_description": "A repository containing baselines, experiment specifications, and common metrics for continual reinforcement learning. It is designed to be easily extensible for new methods and facilitates reproducible research in continual RL.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "benchmarking",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/AGI-Labs/continual_rl",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "reinforcement-learning",
        "continual-learning",
        "benchmarks",
        "metrics"
      ],
      "id": 322
    },
    {
      "name": "MTT",
      "one_line_profile": "Bayesian multi-target tracking algorithms and evaluation metrics",
      "detailed_description": "Implementation of several Bayesian multi-target tracking algorithms, including Poisson multi-Bernoulli mixture filters. It specifically includes implementations of GOSPA and T-GOSPA metrics for evaluating tracking performance.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "object_tracking",
        "evaluation_metrics",
        "bayesian_filtering"
      ],
      "application_level": "library",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/Agarciafernandez/MTT",
      "help_website": [],
      "license": "BSD-2-Clause",
      "tags": [
        "multi-target-tracking",
        "bayesian-methods",
        "gospa",
        "metrics"
      ],
      "id": 323
    },
    {
      "name": "eval4imagecaption",
      "one_line_profile": "Evaluation metrics toolkit for image captioning tasks",
      "detailed_description": "A collection of evaluation tools for image captioning, implementing standard metrics including BLEU, ROUGE-L, CIDEr, METEOR, and SPICE scores to assess the quality of generated captions.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "image_captioning",
        "evaluation_metrics",
        "nlp_metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Aldenhovel/bleu-rouge-meteor-cider-spice-eval4imagecaption",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "image-captioning",
        "bleu",
        "rouge",
        "cider",
        "meteor"
      ],
      "id": 324
    },
    {
      "name": "GuardBench",
      "one_line_profile": "Evaluation library for guardrail models in AI safety",
      "detailed_description": "A Python library designed for the evaluation of guardrail models, facilitating the assessment of safety and compliance mechanisms in AI systems.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "ai_safety",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/AmenRa/GuardBench",
      "help_website": [],
      "license": "EUPL-1.2",
      "tags": [
        "guardrails",
        "evaluation",
        "ai-safety",
        "benchmarking"
      ],
      "id": 325
    },
    {
      "name": "ttest",
      "one_line_profile": "JavaScript library for performing Student's t hypothesis tests",
      "detailed_description": "A lightweight JavaScript library to perform the Student's t hypothesis test, enabling statistical analysis directly within JavaScript environments.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_testing",
        "hypothesis_testing"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/AndreasMadsen/ttest",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "statistics",
        "t-test",
        "hypothesis-testing",
        "javascript"
      ],
      "id": 326
    },
    {
      "name": "rexmex",
      "one_line_profile": "General purpose recommender systems metrics library",
      "detailed_description": "A general-purpose library for evaluating recommender systems, providing a comprehensive set of metrics to ensure fair and standardized evaluation of recommendation algorithms.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "recommender_systems",
        "evaluation_metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/AstraZeneca/rexmex",
      "help_website": [],
      "license": null,
      "tags": [
        "recommender-systems",
        "metrics",
        "evaluation",
        "data-science"
      ],
      "id": 327
    },
    {
      "name": "SORDI-AI-Evaluation-GUI",
      "one_line_profile": "GUI tool for evaluating computer vision models on SORDI datasets",
      "detailed_description": "A graphical user interface tool developed by BMW Innovation Lab to evaluate trained computer vision models, specifically designed to work with the SORDI dataset ecosystem, providing general information and evaluation metrics.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "computer_vision",
        "metrics_visualization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/BMW-InnovationLab/SORDI-AI-Evaluation-GUI",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "computer-vision",
        "evaluation",
        "gui",
        "sordi"
      ],
      "id": 328
    },
    {
      "name": "scikit-llm",
      "one_line_profile": "Integration of Large Language Models into scikit-learn framework",
      "detailed_description": "A library that seamlessly integrates Large Language Models (LLMs) into the scikit-learn framework, allowing users to utilize LLMs for text analysis tasks (like classification and embedding) using the familiar sklearn API.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "text_analysis",
        "modeling",
        "nlp"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/BeastByteAI/scikit-llm",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "scikit-learn",
        "llm",
        "nlp",
        "text-classification"
      ],
      "id": 329
    },
    {
      "name": "madmom",
      "one_line_profile": "Python audio and music signal processing library",
      "detailed_description": "A comprehensive Python library for audio and music signal processing. It includes tools for feature extraction, beat tracking, onset detection, and other music information retrieval (MIR) tasks.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "signal_processing",
        "audio_analysis",
        "feature_extraction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/CPJKU/madmom",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "audio-processing",
        "music-information-retrieval",
        "signal-processing",
        "mir"
      ],
      "id": 330
    },
    {
      "name": "mAP",
      "one_line_profile": "Evaluation code for object detection neural networks",
      "detailed_description": "A widely used tool to calculate the mean Average Precision (mAP) for object detection models. It provides a standardized way to evaluate the performance of neural networks on detection tasks.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "object_detection",
        "evaluation_metrics",
        "performance_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Cartucho/mAP",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "computer-vision",
        "object-detection",
        "map",
        "metrics"
      ],
      "id": 331
    },
    {
      "name": "pymar",
      "one_line_profile": "Markov Switching Models extension for Statsmodels",
      "detailed_description": "A Python library that implements Markov Switching Models, designed to work with and extend the capabilities of the Statsmodels library for time series analysis.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "time_series_analysis",
        "statistical_modeling",
        "markov_models"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ChadFulton/pymar",
      "help_website": [],
      "license": null,
      "tags": [
        "statsmodels",
        "time-series",
        "markov-switching",
        "statistics"
      ],
      "id": 332
    },
    {
      "name": "metric-learning-divide-and-conquer",
      "one_line_profile": "Implementation of Divide and Conquer the Embedding Space for Metric Learning",
      "detailed_description": "A PyTorch implementation of the 'Divide and Conquer the Embedding Space for Metric Learning' (CVPR 2019) approach, providing a solver for deep metric learning tasks by partitioning the embedding space.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "metric_learning",
        "embedding_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/CompVis/metric-learning-divide-and-conquer",
      "help_website": [],
      "license": "LGPL-3.0",
      "tags": [
        "metric-learning",
        "computer-vision",
        "embedding"
      ],
      "id": 333
    },
    {
      "name": "Deep-Metric-Learning-Baselines",
      "one_line_profile": "PyTorch implementation for Deep Metric Learning pipelines and baselines",
      "detailed_description": "A comprehensive library providing implementations of various deep metric learning algorithms and baselines in PyTorch, facilitating benchmarking and reproduction of metric learning experiments.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "metric_learning",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Confusezius/Deep-Metric-Learning-Baselines",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "deep-metric-learning",
        "pytorch",
        "benchmark"
      ],
      "id": 334
    },
    {
      "name": "Revisiting_Deep_Metric_Learning_PyTorch",
      "one_line_profile": "Code for benchmarking training strategies in Deep Metric Learning",
      "detailed_description": "The official repository for the ICML 2020 paper 'Revisiting Training Strategies and Generalization Performance in Deep Metric Learning', providing a framework for consistent research and evaluation in deep metric learning.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "metric_learning",
        "evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Confusezius/Revisiting_Deep_Metric_Learning_PyTorch",
      "help_website": [
        "https://arxiv.org/abs/2002.08473"
      ],
      "license": "MIT",
      "tags": [
        "metric-learning",
        "generalization",
        "icml-2020"
      ],
      "id": 335
    },
    {
      "name": "Avalanche",
      "one_line_profile": "End-to-End Library for Continual Learning based on PyTorch",
      "detailed_description": "A comprehensive library for Continual Learning (CL) research, providing benchmarks, metrics, and evaluation protocols to assess the performance of CL algorithms.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "continual_learning",
        "evaluation_metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ContinualAI/avalanche",
      "help_website": [
        "https://avalanche.continualai.org"
      ],
      "license": "MIT",
      "tags": [
        "continual-learning",
        "pytorch",
        "benchmarks"
      ],
      "id": 336
    },
    {
      "name": "Yellowbrick",
      "one_line_profile": "Visual analysis and diagnostic tools for machine learning model selection",
      "detailed_description": "A suite of visual analysis and diagnostic tools designed to facilitate machine learning model selection, enabling data scientists to steer the model selection process.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_visualization",
        "model_selection",
        "diagnostics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/DistrictDataLabs/yellowbrick",
      "help_website": [
        "https://www.scikit-yb.org"
      ],
      "license": "Apache-2.0",
      "tags": [
        "visualization",
        "machine-learning",
        "model-selection"
      ],
      "id": 337
    },
    {
      "name": "ESMValTool",
      "one_line_profile": "Community diagnostic and performance metrics tool for Earth system models",
      "detailed_description": "A community diagnostic and performance metrics tool for routine evaluation of Earth system models in CMIP, facilitating the analysis of climate data and model performance.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "climate_model_evaluation",
        "diagnostics",
        "earth_system_modeling"
      ],
      "application_level": "platform",
      "primary_language": "NCL",
      "repo_url": "https://github.com/ESMValGroup/ESMValTool",
      "help_website": [
        "https://esmvaltool.org"
      ],
      "license": "Apache-2.0",
      "tags": [
        "climate-science",
        "cmip",
        "model-evaluation"
      ],
      "id": 338
    },
    {
      "name": "BlonDe",
      "one_line_profile": "Automatic Evaluation Metric for Document-level Machine Translation",
      "detailed_description": "Official implementation of BlonDe, an automatic evaluation metric designed for document-level machine translation, focusing on discourse-related phenomena.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "machine_translation_evaluation",
        "nlp_metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/EleanorJiang/BlonDe",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nlp",
        "metrics",
        "machine-translation"
      ],
      "id": 339
    },
    {
      "name": "TPOT",
      "one_line_profile": "Automated Machine Learning tool using genetic programming",
      "detailed_description": "A Python Automated Machine Learning (AutoML) tool that optimizes machine learning pipelines using genetic programming to automate the process of feature selection, model selection, and parameter tuning.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "automl",
        "pipeline_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/EpistasisLab/tpot",
      "help_website": [
        "http://epistasislab.github.io/tpot/"
      ],
      "license": "LGPL-3.0",
      "tags": [
        "automl",
        "genetic-programming",
        "machine-learning"
      ],
      "id": 340
    },
    {
      "name": "LanguageGuidance_for_DML",
      "one_line_profile": "Integrating Language Guidance into Vision-based Deep Metric Learning",
      "detailed_description": "Implementation of a method for integrating language guidance into vision-based deep metric learning, as presented in CVPR 2022.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "metric_learning",
        "multimodal_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ExplainableML/LanguageGuidance_for_DML",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "deep-metric-learning",
        "vision-language",
        "cvpr-2022"
      ],
      "id": 341
    },
    {
      "name": "Dolmen",
      "one_line_profile": "Library to parse, typecheck, and evaluate automated deduction languages",
      "detailed_description": "A library and binary tool to parse, typecheck, and evaluate various languages used in automated deduction and formal verification.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "automated_deduction",
        "parsing",
        "formal_methods"
      ],
      "application_level": "library",
      "primary_language": "OCaml",
      "repo_url": "https://github.com/Gbury/dolmen",
      "help_website": [],
      "license": "BSD-2-Clause",
      "tags": [
        "automated-deduction",
        "parsing",
        "logic"
      ],
      "id": 342
    },
    {
      "name": "PixTrack",
      "one_line_profile": "Object Tracking using NeRF templates and feature-metric alignment",
      "detailed_description": "A Computer Vision method for Object Tracking which uses NeRF templates and feature-metric alignment to robustly track the 6DoF pose of a known object.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "object_tracking",
        "nerf",
        "pose_estimation"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/GiantAI/pixtrack",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "computer-vision",
        "nerf",
        "tracking"
      ],
      "id": 343
    },
    {
      "name": "TextDescriptives",
      "one_line_profile": "Library for calculating a large variety of metrics from text",
      "detailed_description": "A Python library for calculating a wide range of text metrics, including readability, complexity, and other linguistic features, useful for NLP analysis.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "text_metrics",
        "nlp_analysis",
        "readability"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/HLasse/TextDescriptives",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "nlp",
        "metrics",
        "text-analysis"
      ],
      "id": 344
    },
    {
      "name": "MM-SHAP",
      "one_line_profile": "Metric for Measuring Multimodal Contributions in Vision and Language Models",
      "detailed_description": "Official implementation of MM-SHAP, a performance-agnostic metric for measuring multimodal contributions in vision and language models and tasks.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_interpretability",
        "multimodal_metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Heidelberg-NLP/MM-SHAP",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "shap",
        "multimodal",
        "interpretability"
      ],
      "id": 345
    },
    {
      "name": "Few-shot-via-ensembling-Transformer-with-Mahalanobis-distance",
      "one_line_profile": "Few-Shot Bearing Fault Diagnosis via Ensembling Transformer and Mahalanobis Distance",
      "detailed_description": "Implementation of a few-shot bearing fault diagnosis method using an ensembling Transformer-based model with Mahalanobis distance metric learning.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "fault_diagnosis",
        "metric_learning",
        "few_shot_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/HungVu307/Few-shot-via-ensembling-Transformer-with-Mahalanobis-distance",
      "help_website": [],
      "license": null,
      "tags": [
        "fault-diagnosis",
        "transformer",
        "metric-learning"
      ],
      "id": 346
    },
    {
      "name": "MUTIS",
      "one_line_profile": "Analysis of correlations of light curves and their statistical significance",
      "detailed_description": "A Python package developed by IAA-CSIC for the analysis of correlations of light curves and their statistical significance in astrophysical data.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "time_series_analysis",
        "correlation_analysis",
        "astrophysics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/IAA-CSIC/MUTIS",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "astronomy",
        "statistics",
        "light-curves"
      ],
      "id": 347
    },
    {
      "name": "transition-amr-parser",
      "one_line_profile": "AMR parsing with word-node alignments and Smatch metrics",
      "detailed_description": "State-of-the-Art Abstract Meaning Representation (AMR) parsing tool with word-node alignments, including tools for statistical significance testing using Smatch.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "amr_parsing",
        "nlp_evaluation",
        "smatch_metric"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/IBM/transition-amr-parser",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "nlp",
        "amr",
        "parsing"
      ],
      "id": 348
    },
    {
      "name": "ImagePy",
      "one_line_profile": "Image processing framework based on plugins",
      "detailed_description": "An image processing framework based on a plugin architecture (similar to ImageJ), designed to easily integrate with Scipy, Scikit-image, OpenCV, and other Numpy-based libraries for scientific image analysis.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "image_processing",
        "scientific_imaging"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/Image-Py/imagepy",
      "help_website": [],
      "license": "BSD-4-Clause",
      "tags": [
        "image-processing",
        "microscopy",
        "scientific-imaging"
      ],
      "id": 349
    },
    {
      "name": "TrackEval",
      "one_line_profile": "Evaluation metrics library for Multi-Object Tracking (MOT)",
      "detailed_description": "A Python library providing implementations of various evaluation metrics for Multi-Object Tracking (MOT), including HOTA, CLEAR, and Identity metrics.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "metrics_calculation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/JonathonLuiten/TrackEval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mot",
        "evaluation-metrics",
        "computer-vision"
      ],
      "id": 350
    },
    {
      "name": "Distances.jl",
      "one_line_profile": "Julia package for evaluating distances and metrics between vectors",
      "detailed_description": "A Julia package that provides a collection of distance metrics and divergence measures for evaluating the similarity or dissimilarity between vectors, essential for statistical analysis and machine learning.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "metrics_calculation",
        "statistical_analysis"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/JuliaStats/Distances.jl",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "julia",
        "distance-metrics",
        "statistics"
      ],
      "id": 351
    },
    {
      "name": "HypothesisTests.jl",
      "one_line_profile": "Hypothesis testing library for Julia",
      "detailed_description": "A comprehensive Julia package for conducting various statistical hypothesis tests, including t-tests, chi-squared tests, and non-parametric tests.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_testing",
        "hypothesis_testing"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/JuliaStats/HypothesisTests.jl",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "julia",
        "statistics",
        "hypothesis-testing"
      ],
      "id": 352
    },
    {
      "name": "StatsModels.jl",
      "one_line_profile": "Statistical model specification and fitting in Julia",
      "detailed_description": "A Julia package for specifying, fitting, and evaluating statistical models using a formula-based interface similar to R, facilitating statistical analysis and inference.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_modeling",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/JuliaStats/StatsModels.jl",
      "help_website": [],
      "license": null,
      "tags": [
        "julia",
        "statistical-modeling",
        "data-analysis"
      ],
      "id": 353
    },
    {
      "name": "common_metrics_on_video_quality",
      "one_line_profile": "Video quality evaluation metrics calculator",
      "detailed_description": "A Python toolkit to calculate common video quality metrics such as FVD, PSNR, SSIM, and LPIPS for evaluating generated or predicted videos.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "quality_control",
        "metrics_calculation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/JunyaoHu/common_metrics_on_video_quality",
      "help_website": [],
      "license": null,
      "tags": [
        "video-quality",
        "fvd",
        "psnr",
        "ssim"
      ],
      "id": 354
    },
    {
      "name": "deep-significance",
      "one_line_profile": "Statistical significance testing for deep neural networks",
      "detailed_description": "A Python tool designed to facilitate statistical significance testing specifically for comparing deep neural network models, ensuring robust performance evaluation.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_testing",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Kaleidophon/deep-significance",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "deep-learning",
        "significance-testing",
        "statistics"
      ],
      "id": 355
    },
    {
      "name": "DocumentFeatureSelection",
      "one_line_profile": "Metrics for feature selection from text data",
      "detailed_description": "A Python library providing a set of metrics and methods for selecting features from text data, aiding in NLP model optimization and analysis.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "feature_selection",
        "metrics_calculation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Kensuke-Mitsuzawa/DocumentFeatureSelection",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "nlp",
        "feature-selection",
        "text-mining"
      ],
      "id": 356
    },
    {
      "name": "pytorch-metric-learning",
      "one_line_profile": "Deep metric learning library for PyTorch",
      "detailed_description": "A comprehensive and modular PyTorch library for deep metric learning, providing loss functions, miners, and trainers to learn distance metrics for various applications.",
      "domains": [
        "AI4",
        "AI4-01",
        "AI4-02"
      ],
      "subtask_category": [
        "metric_learning",
        "model_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/KevinMusgrave/pytorch-metric-learning",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pytorch",
        "metric-learning",
        "deep-learning"
      ],
      "id": 357
    },
    {
      "name": "YACHT",
      "one_line_profile": "Hypothesis test for organism presence in metagenomes",
      "detailed_description": "A mathematically characterized hypothesis testing tool for determining the presence or absence of organisms in metagenomic samples, enabling scalable and accurate metagenome profiling.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "hypothesis_testing",
        "metagenomics"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/KoslickiLab/YACHT",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bioinformatics",
        "metagenomics",
        "hypothesis-testing"
      ],
      "id": 358
    },
    {
      "name": "FPChecker",
      "one_line_profile": "Floating-point error detection for HPC applications",
      "detailed_description": "A dynamic analysis tool developed by LLNL to detect floating-point errors and instabilities in High-Performance Computing (HPC) applications.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "quality_control",
        "numerical_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/LLNL/FPChecker",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "hpc",
        "floating-point",
        "debugging"
      ],
      "id": 359
    },
    {
      "name": "torchmetrics",
      "one_line_profile": "Machine learning metrics for PyTorch",
      "detailed_description": "A collection of machine learning metrics for distributed, scalable PyTorch applications, offering implementations of common metrics for classification, regression, and more.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "metrics_calculation",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Lightning-AI/torchmetrics",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "pytorch",
        "metrics",
        "machine-learning"
      ],
      "id": 360
    },
    {
      "name": "MAC-VO",
      "one_line_profile": "Metrics-aware Covariance for Stereo Visual Odometry",
      "detailed_description": "A learning-based Stereo Visual Odometry solver that incorporates metrics-aware covariance estimation to improve trajectory accuracy and reliability.",
      "domains": [
        "AI4",
        "AI4-01",
        "AI4-02"
      ],
      "subtask_category": [
        "visual_odometry",
        "state_estimation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/MAC-VO/MAC-VO",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "visual-odometry",
        "robotics",
        "covariance-estimation"
      ],
      "id": 361
    },
    {
      "name": "SparseOcc",
      "one_line_profile": "Sparse 3D Occupancy Prediction and Evaluation",
      "detailed_description": "A fully sparse 3D occupancy prediction method that includes the RayIoU evaluation metric for assessing geometric accuracy in 3D vision tasks.",
      "domains": [
        "AI4",
        "AI4-01",
        "AI4-02"
      ],
      "subtask_category": [
        "occupancy_prediction",
        "model_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/MCG-NJU/SparseOcc",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "3d-vision",
        "occupancy-prediction",
        "evaluation-metric"
      ],
      "id": 362
    },
    {
      "name": "opinionated",
      "one_line_profile": "Clean stylesheets for scientific plotting",
      "detailed_description": "A Python library providing opinionated, clean stylesheets for matplotlib and seaborn to create publication-quality scientific plots.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "visualization",
        "plotting"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/MNoichl/opinionated",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "visualization",
        "matplotlib",
        "seaborn"
      ],
      "id": 363
    },
    {
      "name": "dcurves",
      "one_line_profile": "Decision Curve Analysis for prediction models",
      "detailed_description": "A Python package for performing Decision Curve Analysis (DCA), a method to evaluate prediction models and diagnostic tests by calculating net benefit.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "decision_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/MSKCC-Epi-Bio/dcurves",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "decision-curve-analysis",
        "statistics",
        "medical-statistics"
      ],
      "id": 364
    },
    {
      "name": "pcr",
      "one_line_profile": "Analysis and statistical testing of qPCR data",
      "detailed_description": "An R package for quality assessment, analysis, and statistical significance testing of real-time quantitative PCR (qPCR) data.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_analysis",
        "bioinformatics"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/MahShaaban/pcr",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "qpcr",
        "bioinformatics",
        "statistics"
      ],
      "id": 365
    },
    {
      "name": "nlg-eval",
      "one_line_profile": "Evaluation metrics for Natural Language Generation",
      "detailed_description": "A Python library containing code for various unsupervised automated metrics for evaluating Natural Language Generation (NLG) systems.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "metrics_calculation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Maluuba/nlg-eval",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "nlg",
        "evaluation-metrics",
        "nlp"
      ],
      "id": 366
    },
    {
      "name": "nervaluate",
      "one_line_profile": "Named Entity Recognition evaluation metrics",
      "detailed_description": "A Python library for full named-entity evaluation metrics based on SemEval-2013 standards, allowing for detailed performance analysis of NER models.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "metrics_calculation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/MantisAI/nervaluate",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ner",
        "evaluation-metrics",
        "nlp"
      ],
      "id": 367
    },
    {
      "name": "localization_evaluation_toolkit",
      "one_line_profile": "Toolkit for localization evaluation in robotics",
      "detailed_description": "A toolkit for evaluating localization algorithms, supporting CSV and ROS 2 bag formats, and providing error analysis and NDT performance metrics.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "robotics_localization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/MapIV/localization_evaluation_toolkit",
      "help_website": [],
      "license": null,
      "tags": [
        "localization",
        "robotics",
        "evaluation"
      ],
      "id": 368
    },
    {
      "name": "volkscv",
      "one_line_profile": "Computer vision research toolbox",
      "detailed_description": "A Python toolbox designed to facilitate computer vision research and projects, providing common utilities and implementations.",
      "domains": [
        "AI4",
        "AI4-01",
        "AI4-02"
      ],
      "subtask_category": [
        "image_processing",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Media-Smart/volkscv",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "computer-vision",
        "toolbox",
        "research"
      ],
      "id": 369
    },
    {
      "name": "evo",
      "one_line_profile": "Evaluation package for odometry and SLAM",
      "detailed_description": "A Python package for the evaluation of odometry and SLAM trajectories, providing tools for trajectory alignment, error calculation, and visualization.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "slam_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/MichaelGrupp/evo",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "slam",
        "odometry",
        "evaluation"
      ],
      "id": 370
    },
    {
      "name": "PySR",
      "one_line_profile": "High-performance symbolic regression",
      "detailed_description": "A high-performance symbolic regression library for Python and Julia, used to discover mathematical expressions that best describe a dataset.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "symbolic_regression",
        "scientific_discovery"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/MilesCranmer/PySR",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "symbolic-regression",
        "scientific-discovery",
        "machine-learning"
      ],
      "id": 371
    },
    {
      "name": "IOHMM",
      "one_line_profile": "Input Output Hidden Markov Models",
      "detailed_description": "A Python library for Input Output Hidden Markov Models (IOHMM), enabling statistical modeling of sequential data with inputs.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "statistical_modeling",
        "sequence_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Mogeng/IOHMM",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "hmm",
        "statistical-modeling",
        "sequence-analysis"
      ],
      "id": 372
    },
    {
      "name": "Model_Log",
      "one_line_profile": "Lightweight ML/DL training metrics visualization",
      "detailed_description": "A lightweight Python tool for logging and visualizing machine learning and deep learning model training metrics such as loss, accuracy, precision, and F1 score.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "visualization",
        "experiment_tracking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NLP-LOVE/Model_Log",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "visualization",
        "metrics",
        "experiment-tracking"
      ],
      "id": 373
    },
    {
      "name": "NVIDIA-NeMo/Evaluator",
      "one_line_profile": "Scalable and reproducible evaluation library for AI models and benchmarks",
      "detailed_description": "A library designed for the evaluation of AI models, particularly in the domains of Large Language Models (LLMs), Automatic Speech Recognition (ASR), and Text-to-Speech (TTS). It provides tools for running benchmarks and computing metrics.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking",
        "metrics_calculation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NVIDIA-NeMo/Evaluator",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "evaluation",
        "benchmarks",
        "llm",
        "asr",
        "tts"
      ],
      "id": 374
    },
    {
      "name": "ggforestplot",
      "one_line_profile": "R package for creating forest plots with confidence intervals",
      "detailed_description": "An R package designed to visualize measures of effects and their confidence intervals using forest plots, commonly used in medical and statistical research.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_visualization",
        "confidence_intervals"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/NightingaleHealth/ggforestplot",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "visualization",
        "forestplot",
        "confidence-intervals",
        "r-package"
      ],
      "id": 375
    },
    {
      "name": "Open Metric Learning",
      "one_line_profile": "Library for metric learning pipelines and models",
      "detailed_description": "A library providing pipelines, models, and a model zoo for metric learning and retrieval tasks. It facilitates the training and evaluation of models designed to learn distance metrics.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "metric_learning",
        "image_retrieval",
        "model_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/OML-Team/open-metric-learning",
      "help_website": [
        "https://oml-team.github.io/open-metric-learning/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "metric-learning",
        "retrieval",
        "pytorch"
      ],
      "id": 376
    },
    {
      "name": "Thermo4PFM",
      "one_line_profile": "Library to evaluate alloy compositions in Phase-Field models",
      "detailed_description": "A C++ library developed by ORNL for evaluating thermodynamic properties of alloy compositions, specifically designed for use within Phase-Field modeling simulations in materials science.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "materials_modeling",
        "thermodynamic_evaluation",
        "phase_field_simulation"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/ORNL/Thermo4PFM",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "materials-science",
        "thermodynamics",
        "phase-field",
        "calphad"
      ],
      "id": 377
    },
    {
      "name": "PyTorch-NLP",
      "one_line_profile": "Basic utilities for PyTorch Natural Language Processing",
      "detailed_description": "A library providing basic utilities, datasets, and metrics for Natural Language Processing (NLP) tasks using PyTorch. It simplifies common NLP operations and data handling.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "nlp_utilities",
        "text_processing",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/PetrochukM/PyTorch-NLP",
      "help_website": [
        "https://pytorchnlp.readthedocs.io/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "nlp",
        "pytorch",
        "utilities"
      ],
      "id": 378
    },
    {
      "name": "basicTrendline",
      "one_line_profile": "R package for adding trendlines and confidence intervals",
      "detailed_description": "An R package that facilitates adding trendlines and confidence intervals for basic linear or nonlinear models to plots, including displaying the equation.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_visualization",
        "trendline_fitting",
        "confidence_intervals"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/PhDMeiwp/basicTrendline",
      "help_website": [],
      "license": null,
      "tags": [
        "r-package",
        "visualization",
        "statistics",
        "trendline"
      ],
      "id": 379
    },
    {
      "name": "ggtrendline",
      "one_line_profile": "R package for adding trendlines and confidence intervals to ggplot",
      "detailed_description": "An R package designed to add trendlines, confidence intervals, and regression equations to 'ggplot2' visualizations.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_visualization",
        "trendline_fitting",
        "confidence_intervals"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/PhDMeiwp/ggtrendline",
      "help_website": [],
      "license": null,
      "tags": [
        "r-package",
        "ggplot2",
        "visualization",
        "statistics"
      ],
      "id": 380
    },
    {
      "name": "Causal Canvas",
      "one_line_profile": "Tool for Causal discovery and Structural Learning",
      "detailed_description": "A tool for Causal discovery using Structural Learning and Probabilistic modelling. It includes features for error analysis of the learnt structure and Pearlian do-why inference.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "causal_discovery",
        "structural_learning",
        "probabilistic_modeling"
      ],
      "application_level": "platform",
      "primary_language": "HTML",
      "repo_url": "https://github.com/PlaytikaOSS/causal-canvas",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "causal-inference",
        "structural-learning",
        "probabilistic-modeling"
      ],
      "id": 381
    },
    {
      "name": "Person ReID Benchmark",
      "one_line_profile": "Systematic evaluation and benchmark for Person Re-Identification",
      "detailed_description": "A comprehensive benchmark and evaluation system for Person Re-Identification (ReID), covering features, metrics, and datasets to standardize performance assessment in the field.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "benchmarking",
        "model_evaluation",
        "person_reid"
      ],
      "application_level": "library",
      "primary_language": "HTML",
      "repo_url": "https://github.com/RSL-NEU/person-reid-benchmark",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "reid",
        "benchmark",
        "evaluation",
        "computer-vision"
      ],
      "id": 382
    },
    {
      "name": "Multi-Camera Object Tracking via Deep Metric Learning",
      "one_line_profile": "Implementation of multi-camera object tracking using deep metric learning",
      "detailed_description": "A solver implementation for multi-camera object tracking that leverages deep metric learning to transfer representations to a top-view perspective.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "object_tracking",
        "metric_learning",
        "computer_vision"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Robootx/Multi-Camera-Object-Tracking-via-Transferring-Representation-to-Top-View",
      "help_website": [],
      "license": null,
      "tags": [
        "tracking",
        "metric-learning",
        "multi-camera"
      ],
      "id": 383
    },
    {
      "name": "wv",
      "one_line_profile": "R package for standard and robust wavelet variance analysis",
      "detailed_description": "An R package providing tools to perform standard and robust wavelet variance analysis for time series signal processing, including inference tools like confidence intervals.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "time_series_analysis",
        "wavelet_variance",
        "statistical_inference"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/SMAC-Group/wv",
      "help_website": [],
      "license": null,
      "tags": [
        "wavelet-analysis",
        "time-series",
        "statistics",
        "r-package"
      ],
      "id": 384
    },
    {
      "name": "Hypothesis Testing for MT",
      "one_line_profile": "Statistical hypothesis testing for Machine Translation metrics",
      "detailed_description": "A tool to evaluate the statistical significance of BLEU scores by comparing Reference and Target files among different machine translation systems.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "hypothesis_testing",
        "mt_evaluation",
        "statistical_significance"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/STC-MT-i2/Hypothesis-Testing-for-MT",
      "help_website": [],
      "license": null,
      "tags": [
        "machine-translation",
        "hypothesis-testing",
        "bleu-score",
        "statistics"
      ],
      "id": 385
    },
    {
      "name": "guardians-mt-eval",
      "one_line_profile": "Metrics for Machine Translation Meta-Evaluation",
      "detailed_description": "Official repository for the ACL 2024 paper 'Guardians of the Machine Translation Meta-Evaluation', providing sentinel metrics for evaluating MT systems.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "mt_evaluation",
        "meta_evaluation",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/SapienzaNLP/guardians-mt-eval",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "machine-translation",
        "evaluation",
        "metrics",
        "nlp"
      ],
      "id": 386
    },
    {
      "name": "TADAM",
      "one_line_profile": "Implementation of Task-Dependent Adaptive Metric for few-shot learning",
      "detailed_description": "Implementation of the TADAM algorithm (Task-Dependent Adaptive Metric) for improved few-shot learning, developed by ServiceNow Research.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "few_shot_learning",
        "metric_learning",
        "image_classification"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/ServiceNow/TADAM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "few-shot-learning",
        "metric-learning",
        "computer-vision"
      ],
      "id": 387
    },
    {
      "name": "QAConv",
      "one_line_profile": "Interpretable and Generalizable Person Re-Identification methods",
      "detailed_description": "Implementation of QAConv (Query-Adaptive Convolution) and GS (Graph Sampling Based Deep Metric Learning) for Person Re-Identification tasks.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "person_reid",
        "metric_learning",
        "graph_sampling"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ShengcaiLiao/QAConv",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "reid",
        "metric-learning",
        "computer-vision"
      ],
      "id": 388
    },
    {
      "name": "onemetric",
      "one_line_profile": "Unified metrics library for machine learning",
      "detailed_description": "A library designed to provide a unified interface for calculating various machine learning metrics, simplifying the evaluation process.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "metrics_calculation",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/SkalskiP/onemetric",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "metrics",
        "machine-learning",
        "evaluation"
      ],
      "id": 389
    },
    {
      "name": "Nyoka",
      "one_line_profile": "Python library for exporting ML models to PMML",
      "detailed_description": "A Python library that facilitates the export of machine learning models into PMML (Predictive Model Markup Language) standard, enabling model interoperability.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_export",
        "interoperability",
        "pmml"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/SoftwareAG/nyoka",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "pmml",
        "model-export",
        "machine-learning"
      ],
      "id": 390
    },
    {
      "name": "stable-audio-metrics",
      "one_line_profile": "Metrics for evaluating music and audio generative models",
      "detailed_description": "A library providing metrics specifically designed for evaluating long-form, full-band, and stereo generations from music and audio generative models.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "audio_evaluation",
        "generative_model_metrics",
        "music_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Stability-AI/stable-audio-metrics",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "audio",
        "metrics",
        "generative-ai",
        "evaluation"
      ],
      "id": 391
    },
    {
      "name": "EasyCNTK",
      "one_line_profile": "C# wrapper library for CNTK Deep Learning API",
      "detailed_description": "A C# library that wraps the CNTK API to provide an easier interface for Deep Learning and Deep Reinforcement Learning, including implementation of layers, optimizers, and evaluation helpers.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "deep_learning",
        "model_training",
        "evaluation"
      ],
      "application_level": "library",
      "primary_language": "C#",
      "repo_url": "https://github.com/StanislavGrigoriev/EasyCNTK",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "cntk",
        "deep-learning",
        "c-sharp",
        "wrapper"
      ],
      "id": 392
    },
    {
      "name": "valor",
      "one_line_profile": "Lightweight library for fast evaluation of machine learning models",
      "detailed_description": "A numpy-based library designed for fast and seamless evaluation of machine learning models, providing tools for computing various performance metrics.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "metrics_calculation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Striveworks/valor",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "evaluation",
        "metrics",
        "machine-learning"
      ],
      "id": 393
    },
    {
      "name": "emh",
      "one_line_profile": "R Package for testing the Efficient Market Hypothesis",
      "detailed_description": "An R package providing statistical tests and tools for evaluating the Efficient Market Hypothesis (EMH) in financial economics.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "hypothesis_testing",
        "financial_analysis",
        "statistical_testing"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/StuartGordonReid/emh",
      "help_website": [],
      "license": null,
      "tags": [
        "economics",
        "finance",
        "hypothesis-testing",
        "r-package"
      ],
      "id": 394
    },
    {
      "name": "ImagenHub",
      "one_line_profile": "Standardized evaluation and inference library for conditional image generation models",
      "detailed_description": "A comprehensive library designed to standardize the inference and evaluation processes for various conditional image generation models, facilitating fair comparison and benchmarking.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking",
        "image_generation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/TIGER-AI-Lab/ImagenHub",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "generative-ai",
        "evaluation",
        "benchmarking",
        "image-generation"
      ],
      "id": 395
    },
    {
      "name": "VideoGenHub",
      "one_line_profile": "Standardized evaluation and inference library for conditional video generation models",
      "detailed_description": "A one-stop library to standardize the inference and evaluation of conditional video generation models, providing metrics and workflows for assessing video quality and consistency.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking",
        "video_generation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/TIGER-AI-Lab/VideoGenHub",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "video-generation",
        "evaluation",
        "benchmarking"
      ],
      "id": 396
    },
    {
      "name": "DiscreteSpeechMetrics",
      "one_line_profile": "Reference-aware automatic speech evaluation toolkit",
      "detailed_description": "A toolkit providing reference-aware automatic evaluation metrics for speech generation tasks, focusing on discrete speech representations.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "speech_evaluation",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Takaaki-Saeki/DiscreteSpeechMetrics",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "speech-processing",
        "evaluation-metrics",
        "audio"
      ],
      "id": 397
    },
    {
      "name": "eli5",
      "one_line_profile": "Library for debugging and explaining machine learning classifiers",
      "detailed_description": "A Python library which allows to visualize and debug various machine learning models using unified API. It has built-in support for several ML frameworks and provides ways to explain black-box models.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_interpretation",
        "debugging",
        "explanation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/TeamHG-Memex/eli5",
      "help_website": [
        "https://eli5.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "interpretability",
        "machine-learning",
        "debugging"
      ],
      "id": 398
    },
    {
      "name": "tonic_validate",
      "one_line_profile": "Metrics framework for evaluating RAG application responses",
      "detailed_description": "A library providing metrics to evaluate the quality of responses from Retrieval Augmented Generation (RAG) applications, helping developers assess answer correctness and relevance.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "rag_evaluation",
        "metrics",
        "llm_validation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/TonicAI/tonic_validate",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "llm",
        "evaluation",
        "metrics"
      ],
      "id": 399
    },
    {
      "name": "AIF360",
      "one_line_profile": "Comprehensive toolkit for fairness metrics and bias mitigation",
      "detailed_description": "The AI Fairness 360 toolkit is an extensible open source library to help detect and mitigate bias in machine learning models throughout the AI application lifecycle.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "fairness_evaluation",
        "bias_mitigation",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Trusted-AI/AIF360",
      "help_website": [
        "https://aif360.mybluemix.net/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "fairness",
        "bias",
        "machine-learning",
        "ethics"
      ],
      "id": 400
    },
    {
      "name": "HtFLlib",
      "one_line_profile": "Heterogeneous Federated Learning library",
      "detailed_description": "A library designed to support model heterogeneity in Federated Learning, allowing consistent GPU memory usage for single or multiple clients and simplifying configuration.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "federated_learning",
        "distributed_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/TsingZ0/HtFLlib",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "federated-learning",
        "heterogeneity",
        "distributed-systems"
      ],
      "id": 401
    },
    {
      "name": "MI-optimize",
      "one_line_profile": "Tool for quantization and evaluation of LLMs",
      "detailed_description": "A versatile tool designed for the quantization and evaluation of large language models (LLMs), integrating various quantization methods and evaluation techniques.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_quantization",
        "model_evaluation",
        "llm"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/TsingmaoAI/MI-optimize",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "quantization",
        "llm",
        "evaluation"
      ],
      "id": 402
    },
    {
      "name": "LLM-judge-reporting",
      "one_line_profile": "Framework for bias correction and confidence intervals in LLM evaluation",
      "detailed_description": "A plug-in framework that corrects bias and computes confidence intervals in reporting LLM-as-a-judge evaluation, including an adaptive algorithm for sample allocation.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_analysis",
        "bias_correction",
        "llm_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/UW-Madison-Lee-Lab/LLM-judge-reporting",
      "help_website": [],
      "license": null,
      "tags": [
        "statistics",
        "llm-as-a-judge",
        "confidence-intervals"
      ],
      "id": 403
    },
    {
      "name": "scope",
      "one_line_profile": "Package for detecting oscillatory signals in time series",
      "detailed_description": "A Python-based package for detecting oscillatory signals in observational or experimental time series using the Empirical Mode Decomposition (EMD) technique and assessing statistical significance.",
      "domains": [
        "AI4",
        "Physics"
      ],
      "subtask_category": [
        "time_series_analysis",
        "signal_detection",
        "statistical_testing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Warwick-Solar/scope",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "time-series",
        "oscillations",
        "emd",
        "physics"
      ],
      "id": 404
    },
    {
      "name": "Adansons Base",
      "one_line_profile": "Data programming tool for error-analysis and dataset organization",
      "detailed_description": "A tool for error-analysis of training results that organizes metadata of unstructured data, creates datasets, and helps find low-quality data to improve AI performance.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "data_quality_control",
        "error_analysis",
        "dataset_management"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/adansons/base",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "data-centric-ai",
        "error-analysis",
        "dataset-creation"
      ],
      "id": 405
    },
    {
      "name": "Flower",
      "one_line_profile": "A Friendly Federated AI Framework",
      "detailed_description": "A unified framework for federated learning, analytics, and evaluation. It allows running federated learning workloads on heterogeneous devices and scales to large numbers of clients.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "federated_learning",
        "distributed_training",
        "privacy_preserving_ml"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/adap/flower",
      "help_website": [
        "https://flower.ai/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "federated-learning",
        "distributed-ml",
        "privacy"
      ],
      "id": 406
    },
    {
      "name": "object-centric-library",
      "one_line_profile": "Library for training and evaluation of object-centric models",
      "detailed_description": "A library dedicated to the training and evaluation of object-centric machine learning models, facilitating research in object-centric representation learning.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_training",
        "model_evaluation",
        "object_centric_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/addtt/object-centric-library",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "object-centric",
        "representation-learning",
        "evaluation"
      ],
      "id": 407
    },
    {
      "name": "ahkab",
      "one_line_profile": "SPICE-like electronic circuit simulator",
      "detailed_description": "A SPICE-like electronic circuit simulator written in Python, capable of performing time-domain analysis, AC analysis, and operating point analysis.",
      "domains": [
        "Physics",
        "Electronics"
      ],
      "subtask_category": [
        "circuit_simulation",
        "scientific_modeling"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ahkab/ahkab",
      "help_website": [
        "https://ahkab.readthedocs.io/"
      ],
      "license": "GPL-2.0",
      "tags": [
        "circuit-simulation",
        "spice",
        "electronics"
      ],
      "id": 408
    },
    {
      "name": "Featuretools",
      "one_line_profile": "Automated feature engineering library for transforming temporal and relational datasets",
      "detailed_description": "Featuretools is an open source library for performing automated feature engineering. It excels at transforming temporal and relational datasets into feature matrices for machine learning, using a method known as Deep Feature Synthesis.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "feature_engineering",
        "data_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/alteryx/featuretools",
      "help_website": [
        "https://featuretools.alteryx.com/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "feature-engineering",
        "automl",
        "data-science"
      ],
      "id": 409
    },
    {
      "name": "BOLD",
      "one_line_profile": "Dataset and metrics for measuring biases in open-ended language generation",
      "detailed_description": "BOLD (Bias in Open-Ended Language Generation Dataset) is a dataset and associated metrics designed to evaluate fairness and bias in open-ended language generation models across multiple domains.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "bias_evaluation",
        "fairness_metrics"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/amazon-science/bold",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "bias",
        "nlp",
        "evaluation-metrics"
      ],
      "id": 410
    },
    {
      "name": "werpy",
      "one_line_profile": "Fast Python package for calculating Word Error Rate (WER) in speech recognition",
      "detailed_description": "A lightweight and fast Python library specifically designed for calculating and analyzing the Word Error Rate (WER), a common metric for evaluating automatic speech recognition (ASR) systems.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "speech_recognition_evaluation",
        "metric_calculation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/analyticsinmotion/werpy",
      "help_website": [
        "https://pypi.org/project/werpy/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "wer",
        "speech-recognition",
        "metrics"
      ],
      "id": 411
    },
    {
      "name": "newsreclib",
      "one_line_profile": "PyTorch-Lightning library for neural news recommendation models",
      "detailed_description": "A library built on PyTorch-Lightning for developing, training, and evaluating neural news recommendation systems, providing standard metrics and model implementations.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "recommendation_system",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/andreeaiana/newsreclib",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "recommender-system",
        "pytorch-lightning",
        "news-recommendation"
      ],
      "id": 412
    },
    {
      "name": "ml_uncertainty",
      "one_line_profile": "Library for prediction intervals and uncertainty estimation in ML models",
      "detailed_description": "A Python library to obtain prediction intervals, confidence intervals, and parameter uncertainties for various machine learning models, aiding in the reliability assessment of predictions.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "uncertainty_estimation",
        "confidence_intervals"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/architdatar/ml_uncertainty",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "uncertainty",
        "confidence-intervals",
        "machine-learning"
      ],
      "id": 413
    },
    {
      "name": "PCAtest",
      "one_line_profile": "R package for statistical significance testing of PCA",
      "detailed_description": "An R package designed to evaluate the statistical significance of Principal Component Analysis (PCA) results, helping to determine the number of significant axes and variable contributions.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_testing",
        "pca_analysis"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/arleyc/PCAtest",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "pca",
        "statistics",
        "significance-testing"
      ],
      "id": 414
    },
    {
      "name": "FactScoreLite",
      "one_line_profile": "Lightweight implementation of FactScore metric for text generation",
      "detailed_description": "A Python package implementing the FactScore metric for assessing the factual accuracy of generated text, serving as a maintained alternative to the original repository.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "text_generation_evaluation",
        "factuality_metric"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/armingh2000/FactScoreLite",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nlp",
        "metrics",
        "factuality"
      ],
      "id": 415
    },
    {
      "name": "T-NER",
      "one_line_profile": "Library for Transformer-based Named Entity Recognition evaluation and fine-tuning",
      "detailed_description": "T-NER is a Python library for fine-tuning transformer-based language models on Named Entity Recognition (NER) tasks, featuring cross-domain evaluation capabilities and an easy-to-use interface.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "ner_evaluation",
        "model_finetuning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/asahi417/tner",
      "help_website": [
        "https://pypi.org/project/tner/"
      ],
      "license": "MIT",
      "tags": [
        "ner",
        "transformers",
        "evaluation"
      ],
      "id": 416
    },
    {
      "name": "hypothetical",
      "one_line_profile": "Hypothesis and statistical testing library for Python",
      "detailed_description": "A Python library dedicated to hypothesis testing and statistical analysis, providing a collection of statistical tests and tools similar to those found in R or other statistical software.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "hypothesis_testing",
        "statistical_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aschleg/hypothetical",
      "help_website": [
        "https://hypothetical.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "statistics",
        "hypothesis-testing",
        "python"
      ],
      "id": 417
    },
    {
      "name": "SimSIMD",
      "one_line_profile": "High-performance SIMD-accelerated similarity metrics and dot products",
      "detailed_description": "SimSIMD provides ultra-fast implementations of distance and similarity metrics (like cosine similarity, dot product) using SIMD instructions (AVX2, AVX-512, NEON, SVE) for multiple languages including Python and C.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "similarity_metric",
        "vector_math"
      ],
      "application_level": "library",
      "primary_language": "C",
      "repo_url": "https://github.com/ashvardanian/SimSIMD",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "simd",
        "metrics",
        "performance"
      ],
      "id": 418
    },
    {
      "name": "hypors",
      "one_line_profile": "Hypothesis testing library for Polars dataframes",
      "detailed_description": "A Rust-based library that brings statistical hypothesis testing capabilities directly to Polars dataframes, enabling efficient statistical analysis on large datasets.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "hypothesis_testing",
        "statistical_analysis"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/astronights/hypors",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "polars",
        "statistics",
        "rust"
      ],
      "id": 419
    },
    {
      "name": "AutoGluon",
      "one_line_profile": "Automated machine learning library for tabular, text, and image data",
      "detailed_description": "AutoGluon automates machine learning tasks, enabling users to achieve strong predictive performance with minimal code. It handles feature engineering, model selection, and ensembling.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "automl",
        "model_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/autogluon/autogluon",
      "help_website": [
        "https://auto.gluon.ai/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "automl",
        "machine-learning",
        "deep-learning"
      ],
      "id": 420
    },
    {
      "name": "auto-sklearn",
      "one_line_profile": "Automated machine learning toolkit based on scikit-learn",
      "detailed_description": "auto-sklearn is an automated machine learning toolkit and a drop-in replacement for a scikit-learn estimator. It leverages Bayesian optimization, meta-learning, and ensemble construction to automate the ML pipeline.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "automl",
        "model_selection"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/automl/auto-sklearn",
      "help_website": [
        "https://automl.github.io/auto-sklearn/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "automl",
        "scikit-learn",
        "optimization"
      ],
      "id": 421
    },
    {
      "name": "Kolibri",
      "one_line_profile": "Framework for search system evaluation and batch processing",
      "detailed_description": "Kolibri is a Scala-based framework designed for concurrent multi-node execution of search system evaluations. It provides out-of-the-box functionality for common IR metrics like NDCG, ERR, Precision, and Recall.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "search_evaluation",
        "ranking_metrics"
      ],
      "application_level": "workflow",
      "primary_language": "Scala",
      "repo_url": "https://github.com/awagen/kolibri",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "evaluation",
        "information-retrieval",
        "metrics"
      ],
      "id": 422
    },
    {
      "name": "ErrorAnalysisToolkit",
      "one_line_profile": "MATLAB toolkit for image registration and tracking error analysis",
      "detailed_description": "A collection of MATLAB tools designed to examine and quantify errors in image registration and tracking tasks, useful for medical imaging or computer vision research.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "image_registration",
        "error_analysis"
      ],
      "application_level": "library",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/awiles/ErrorAnalysisToolkit",
      "help_website": [],
      "license": null,
      "tags": [
        "matlab",
        "image-processing",
        "error-analysis"
      ],
      "id": 423
    },
    {
      "name": "fmeval",
      "one_line_profile": "Library for evaluating Foundation Models",
      "detailed_description": "A library provided by AWS to evaluate Foundation Models (FMs) across various dimensions such as accuracy, toxicity, bias, and robustness, facilitating responsible AI development.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "llm_evaluation",
        "foundation_model_metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aws/fmeval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "evaluation",
        "responsible-ai"
      ],
      "id": 424
    },
    {
      "name": "ggBrackets",
      "one_line_profile": "ggplot2 extension for annotating plots with statistical significance brackets",
      "detailed_description": "An R package that adds a layer to ggplot2 for drawing brackets annotated with p-values and significance testing results, facilitating the visualization of statistical comparisons.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_visualization",
        "significance_annotation"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/azzoam/ggBrackets",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "r",
        "ggplot2",
        "visualization"
      ],
      "id": 425
    },
    {
      "name": "fancylit",
      "one_line_profile": "Streamlit wrapper for data visualization and modeling tasks",
      "detailed_description": "A Python module that provides pre-packaged Streamlit components to facilitate data exploration, visualization, and running simple modeling tasks, acting as a lightweight data science tool.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "data_visualization",
        "data_exploration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/banjtheman/fancylit",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "streamlit",
        "visualization",
        "data-science"
      ],
      "id": 426
    },
    {
      "name": "nlg-metrics",
      "one_line_profile": "Library for Natural Language Generation evaluation metrics",
      "detailed_description": "A Python library implementing various metrics for evaluating Natural Language Generation (NLG) systems, helping researchers assess the quality of generated text.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "nlg_evaluation",
        "text_metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/baojunshan/nlg-metrics",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nlg",
        "metrics",
        "nlp"
      ],
      "id": 427
    },
    {
      "name": "linearmodels",
      "one_line_profile": "Extended linear models for Python including panel data and IV",
      "detailed_description": "A Python library that extends statsmodels with additional linear models, specifically focusing on instrumental variable models, panel data models, and asset pricing tests.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_modeling",
        "econometrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/bashtage/linearmodels",
      "help_website": [
        "https://bashtage.github.io/linearmodels/"
      ],
      "license": "NCSA",
      "tags": [
        "econometrics",
        "statistics",
        "panel-data"
      ],
      "id": 428
    },
    {
      "name": "timbre-dissimilarity-metrics",
      "one_line_profile": "Metrics for evaluating audio timbre dissimilarity",
      "detailed_description": "A collection of metrics implemented using the TorchMetrics API for evaluating timbre dissimilarity in audio signals, useful for music information retrieval and audio synthesis research.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "audio_analysis",
        "timbre_metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ben-hayes/timbre-dissimilarity-metrics",
      "help_website": [],
      "license": null,
      "tags": [
        "audio",
        "metrics",
        "timbre"
      ],
      "id": 429
    },
    {
      "name": "Metrics",
      "one_line_profile": "Collection of machine learning evaluation metrics",
      "detailed_description": "A repository containing implementations of various machine learning evaluation metrics in Python, R, Haskell, and MATLAB, often used in data science competitions.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "metric_calculation",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/benhamner/Metrics",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "metrics",
        "machine-learning",
        "evaluation"
      ],
      "id": 430
    },
    {
      "name": "drtmle",
      "one_line_profile": "Nonparametric estimators of average treatment effect with doubly-robust confidence intervals",
      "detailed_description": "An R package that implements nonparametric estimators of the average treatment effect with doubly-robust confidence intervals and hypothesis tests, suitable for causal inference.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_inference",
        "hypothesis_testing"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/benkeser/drtmle",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "causal-inference",
        "statistics",
        "confidence-intervals"
      ],
      "id": 431
    },
    {
      "name": "Orange3",
      "one_line_profile": "Interactive data analysis and visualization workflow tool",
      "detailed_description": "An open-source machine learning and data visualization software. It features a visual programming front-end for explorative data analysis and interactive data visualization, and can also be used as a Python library.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "data_analysis",
        "visualization",
        "modeling"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/biolab/orange3",
      "help_website": [
        "https://orangedatamining.com/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "data-mining",
        "machine-learning",
        "visualization"
      ],
      "id": 432
    },
    {
      "name": "Deep_Metric",
      "one_line_profile": "Library for Deep Metric Learning algorithms",
      "detailed_description": "A repository providing implementations for various Deep Metric Learning algorithms, facilitating the learning of distance metrics for tasks like image retrieval and clustering.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "metric_learning",
        "representation_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/bnu-wangxun/Deep_Metric",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "metric-learning",
        "deep-learning",
        "computer-vision"
      ],
      "id": 433
    },
    {
      "name": "freqtables",
      "one_line_profile": "R package for creating descriptive statistics tables",
      "detailed_description": "A package designed to quickly make tables of descriptive statistics (counts, percentages, confidence intervals) for categorical variables, compatible with tidyverse pipelines.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "descriptive_statistics",
        "reporting"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/brad-cannell/freqtables",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "statistics",
        "r-package",
        "data-analysis"
      ],
      "id": 434
    },
    {
      "name": "poverlap",
      "one_line_profile": "Significance testing over interval overlaps",
      "detailed_description": "A tool for calculating the significance of overlaps between genomic intervals or other interval data, useful in bioinformatics and statistical analysis of spatial data.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_testing",
        "genomics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/brentp/poverlap",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bioinformatics",
        "statistics",
        "intervals"
      ],
      "id": 435
    },
    {
      "name": "Granger",
      "one_line_profile": "Frequency-domain Granger causality with significance testing",
      "detailed_description": "Matlab code for performing frequency-domain Granger causality analysis, including significance testing to determine causal relationships in time series data.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "causality_analysis",
        "time_series_analysis"
      ],
      "application_level": "library",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/brian-lau/Granger",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "granger-causality",
        "matlab",
        "statistics"
      ],
      "id": 436
    },
    {
      "name": "MatlabAUC",
      "one_line_profile": "AUC calculation with confidence intervals in Matlab",
      "detailed_description": "Matlab code for calculating the area under the receiver operating characteristic curve (AUC) and estimating confidence intervals, essential for evaluating classifier performance.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "performance_evaluation",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/brian-lau/MatlabAUC",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "auc",
        "roc",
        "statistics"
      ],
      "id": 437
    },
    {
      "name": "modeltime.resample",
      "one_line_profile": "Resampling tools for time series forecasting",
      "detailed_description": "An R package providing resampling tools for time series forecasting validation, designed to work with the Modeltime ecosystem.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "time_series_validation",
        "resampling"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/business-science/modeltime.resample",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "time-series",
        "forecasting",
        "validation"
      ],
      "id": 438
    },
    {
      "name": "MediationToolbox",
      "one_line_profile": "Mediation analysis toolbox for neuroimaging and behavioral data",
      "detailed_description": "A toolbox for single-level and multi-level mediation analyses with bootstrap-based significance testing, featuring neuroimaging-oriented functions for parametric mapping.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_analysis",
        "neuroimaging"
      ],
      "application_level": "library",
      "primary_language": "HTML",
      "repo_url": "https://github.com/canlab/MediationToolbox",
      "help_website": [],
      "license": null,
      "tags": [
        "mediation-analysis",
        "neuroscience",
        "statistics"
      ],
      "id": 439
    },
    {
      "name": "scikit-hts",
      "one_line_profile": "Hierarchical Time Series Forecasting library",
      "detailed_description": "A Python library for hierarchical time series forecasting, providing a familiar scikit-learn like API for modeling and analyzing hierarchical data structures.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "time_series_forecasting",
        "modeling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/carlomazzaferro/scikit-hts",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "time-series",
        "forecasting",
        "hierarchical"
      ],
      "id": 440
    },
    {
      "name": "TreeMix-Pipeline",
      "one_line_profile": "Pipeline for TreeMix analysis with bootstrapping and visualization",
      "detailed_description": "Scripts to automate and enhance data analysis using TreeMix, including bootstrapping, migration event estimation, consensus tree creation, and statistical plotting.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "population_genetics",
        "phylogenetics",
        "statistical_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "R",
      "repo_url": "https://github.com/carolindahms/TreeMix",
      "help_website": [],
      "license": null,
      "tags": [
        "genetics",
        "treemix",
        "visualization"
      ],
      "id": 441
    },
    {
      "name": "BinningAnalysis.jl",
      "one_line_profile": "Statistical standard error estimation for correlated data",
      "detailed_description": "A Julia package for statistical standard error estimation tools specifically designed for correlated data, often used in Monte Carlo simulations.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "error_estimation",
        "statistical_analysis"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/carstenbauer/BinningAnalysis.jl",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "statistics",
        "julia",
        "monte-carlo"
      ],
      "id": 442
    },
    {
      "name": "catacomb",
      "one_line_profile": "ML library for launching UIs and running evaluations",
      "detailed_description": "A machine learning library designed to simplify the process of launching user interfaces, running model evaluations, and comparing performance metrics.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "experiment_tracking"
      ],
      "application_level": "library",
      "primary_language": null,
      "repo_url": "https://github.com/catacomb-ai/catacomb",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "machine-learning",
        "evaluation",
        "ui"
      ],
      "id": 443
    },
    {
      "name": "scikits-bootstrap",
      "one_line_profile": "Bootstrap confidence interval estimation for Python",
      "detailed_description": "A Python library providing functions for bootstrap confidence interval estimation, built on top of NumPy.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_estimation",
        "bootstrap"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/cgevans/scikits-bootstrap",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "statistics",
        "bootstrap",
        "confidence-intervals"
      ],
      "id": 444
    },
    {
      "name": "CollMetric",
      "one_line_profile": "Collaborative Metric Learning (CML) implementation",
      "detailed_description": "A TensorFlow implementation of Collaborative Metric Learning (CML), a method for recommendation systems that learns a joint metric space for users and items.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "metric_learning",
        "recommendation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/changun/CollMetric",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "metric-learning",
        "tensorflow",
        "recommender-systems"
      ],
      "id": 445
    },
    {
      "name": "sr-metric",
      "one_line_profile": "No-Reference Quality Metric for Single-Image Super-Resolution",
      "detailed_description": "Implementation of a learning-based no-reference quality metric specifically designed for evaluating single-image super-resolution results.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "image_quality_assessment",
        "super_resolution"
      ],
      "application_level": "solver",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/chaoma99/sr-metric",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "image-quality",
        "super-resolution",
        "metrics"
      ],
      "id": 446
    },
    {
      "name": "PointNet++",
      "one_line_profile": "Deep Hierarchical Feature Learning on Point Sets",
      "detailed_description": "Implementation of PointNet++, a deep neural network architecture for processing point sets in a metric space, widely used for 3D data analysis.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "3d_analysis",
        "feature_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/charlesq34/pointnet2",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "point-cloud",
        "deep-learning",
        "3d-vision"
      ],
      "id": 447
    },
    {
      "name": "bootstrap",
      "one_line_profile": "Library for bootstrapping statistics",
      "detailed_description": "A Python library designed to facilitate bootstrapping statistics, allowing for robust statistical inference.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_inference",
        "bootstrap"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/christopherjenness/bootstrap",
      "help_website": [],
      "license": null,
      "tags": [
        "statistics",
        "bootstrap",
        "python"
      ],
      "id": 448
    },
    {
      "name": "piecewise_linear_fit_py",
      "one_line_profile": "Piecewise linear data fitting tool",
      "detailed_description": "A Python tool to fit piecewise linear data for a specified number of line segments, useful for modeling non-linear relationships with linear approximations.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "curve_fitting",
        "data_modeling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/cjekel/piecewise_linear_fit_py",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "curve-fitting",
        "optimization",
        "modeling"
      ],
      "id": 449
    },
    {
      "name": "Pingouin.jl",
      "one_line_profile": "Statistical package for Julia",
      "detailed_description": "A reimplementation of the Python Pingouin statistical package in Julia, providing simple yet exhaustive statistical functions.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_analysis",
        "hypothesis_testing"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/clementpoiret/Pingouin.jl",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "statistics",
        "julia",
        "pingouin"
      ],
      "id": 450
    },
    {
      "name": "TedEval",
      "one_line_profile": "Evaluation metric for Scene Text Detectors",
      "detailed_description": "Implementation of TedEval, a fair evaluation metric for scene text detectors that accounts for instance-level matching and coverage.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "evaluation_metric",
        "computer_vision"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/clovaai/TedEval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ocr",
        "evaluation",
        "metrics"
      ],
      "id": 451
    },
    {
      "name": "generative-evaluation-prdc",
      "one_line_profile": "Precision, Recall, Density, and Coverage metrics for generative models",
      "detailed_description": "Code base for calculating precision, recall, density, and coverage metrics to evaluate the performance and diversity of generative models.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "generative_model_evaluation",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/clovaai/generative-evaluation-prdc",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "generative-models",
        "evaluation",
        "gan"
      ],
      "id": 452
    },
    {
      "name": "voxceleb_trainer",
      "one_line_profile": "Metric learning framework for speaker recognition",
      "detailed_description": "A training framework for speaker recognition focusing on metric learning approaches, providing state-of-the-art baselines and loss functions.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "speaker_recognition",
        "metric_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/clovaai/voxceleb_trainer",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "speaker-recognition",
        "metric-learning",
        "deep-learning"
      ],
      "id": 453
    },
    {
      "name": "gec-ranking",
      "one_line_profile": "Ground Truth for Grammatical Error Correction Metrics",
      "detailed_description": "Data and code for evaluating Grammatical Error Correction (GEC) metrics, providing a ground truth ranking for metric comparison.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "metric_evaluation",
        "nlp_benchmark"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/cnap/gec-ranking",
      "help_website": [],
      "license": null,
      "tags": [
        "nlp",
        "evaluation",
        "benchmark"
      ],
      "id": 454
    },
    {
      "name": "deepeval",
      "one_line_profile": "LLM Evaluation Framework",
      "detailed_description": "A comprehensive framework for evaluating Large Language Models (LLMs), providing various metrics and test cases to ensure model quality and safety.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "llm_evaluation",
        "metrics"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/confident-ai/deepeval",
      "help_website": [
        "https://docs.confident-ai.com/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "evaluation",
        "testing"
      ],
      "id": 455
    },
    {
      "name": "SpectralGV",
      "one_line_profile": "Spectral Geometric Verification for Metric Localization",
      "detailed_description": "Implementation of Spectral Geometric Verification (SGV) for re-ranking point cloud retrieval, enhancing metric localization accuracy.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "localization",
        "geometric_verification"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/csiro-robotics/SpectralGV",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "robotics",
        "localization",
        "point-cloud"
      ],
      "id": 456
    },
    {
      "name": "sacrerouge",
      "one_line_profile": "Library for text generation evaluation metrics",
      "detailed_description": "A library dedicated to the use and development of evaluation metrics for text generation tasks, with a strong emphasis on summarization.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "text_evaluation",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/danieldeutsch/sacrerouge",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "nlp",
        "summarization",
        "evaluation"
      ],
      "id": 457
    },
    {
      "name": "Dask",
      "one_line_profile": "Flexible parallel computing library for analytic computing",
      "detailed_description": "A flexible library for parallel computing in Python that scales NumPy, Pandas, and Scikit-Learn workflows. It provides advanced parallelism for analytics, enabling performance at scale for scientific computing tasks.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "parallel_computing",
        "data_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/dask/dask",
      "help_website": [
        "https://dask.org/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "parallel-computing",
        "distributed-systems",
        "scaling"
      ],
      "id": 458
    },
    {
      "name": "NER-Evaluation",
      "one_line_profile": "Evaluation metrics for Named Entity Recognition (NER) systems",
      "detailed_description": "An implementation of named-entity evaluation metrics based on SemEval'13 Task 9. It evaluates NER performance by considering full entity spans rather than just tag/token level accuracy.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/davidsbatista/NER-Evaluation",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ner",
        "evaluation-metrics",
        "nlp"
      ],
      "id": 459
    },
    {
      "name": "multiway_bootstrap",
      "one_line_profile": "Implementation of the multiway bootstrap for R",
      "detailed_description": "Provides an implementation of the multiway bootstrap (including Pigeonhole bootstrap and reweighting tensor bootstrap) for bootstrapping data arrays of arbitrary order, useful for statistical inference in complex data structures.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_inference",
        "bootstrap"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/deaneckles/multiway_bootstrap",
      "help_website": [],
      "license": null,
      "tags": [
        "bootstrap",
        "statistics",
        "inference"
      ],
      "id": 460
    },
    {
      "name": "CK-Caffe",
      "one_line_profile": "Collective Knowledge workflow for Caffe optimization",
      "detailed_description": "A Collective Knowledge (CK) workflow to automate the installation, evaluation, and optimization of Caffe-based workloads across diverse hardware and software platforms. It facilitates reproducible research and benchmarking.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "benchmarking",
        "reproducibility"
      ],
      "application_level": "workflow",
      "primary_language": "CMake",
      "repo_url": "https://github.com/dividiti/ck-caffe",
      "help_website": [
        "http://cKnowledge.org/ai"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "caffe",
        "benchmarking",
        "optimization"
      ],
      "id": 461
    },
    {
      "name": "AB Test Advanced Toolkit",
      "one_line_profile": "Advanced toolkit for A/B testing analysis",
      "detailed_description": "A suite of tools for A/B testing that includes advanced techniques like CUPED (Controlled-experiment Using Pre-Experiment Data) and Gradient Boosting for variance reduction and faster statistical significance.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "ab_testing",
        "statistical_significance"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/dmitry-brazhenko/ab-test-advanced-toolkit",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ab-testing",
        "cuped",
        "statistics"
      ],
      "id": 462
    },
    {
      "name": "Delay Discounting Analysis",
      "one_line_profile": "Hierarchical Bayesian estimation for delay discounting",
      "detailed_description": "A MATLAB toolbox for hierarchical Bayesian estimation and hypothesis testing specifically designed for delay discounting tasks. It allows for robust parameter estimation and statistical inference in behavioral economics research.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "bayesian_estimation",
        "hypothesis_testing"
      ],
      "application_level": "library",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/drbenvincent/delay-discounting-analysis",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bayesian",
        "matlab",
        "behavioral-science"
      ],
      "id": 463
    },
    {
      "name": "LANLGeoMag",
      "one_line_profile": "Library for magnetic field models and coordinate transforms",
      "detailed_description": "A C-based library from Los Alamos National Laboratory for computing geophysical quantities, including magnetic field models and high-precision coordinate transformations. Used for geospace research and magnetic field line tracing.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "magnetic_field_modeling",
        "coordinate_conversion"
      ],
      "application_level": "library",
      "primary_language": "C",
      "repo_url": "https://github.com/drsteve/LANLGeoMag",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "geophysics",
        "magnetic-field",
        "lanl"
      ],
      "id": 464
    },
    {
      "name": "performance",
      "one_line_profile": "Assessment of regression models performance",
      "detailed_description": "An R package to compute various performance metrics for regression models, including R2, ICC, LOO, AIC, BIC, and Bayes Factor. It provides a unified interface for assessing model quality.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "performance_metrics"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/easystats/performance",
      "help_website": [
        "https://easystats.github.io/performance/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "r-package",
        "model-metrics",
        "statistics"
      ],
      "id": 465
    },
    {
      "name": "report",
      "one_line_profile": "Automated statistical reporting for R objects",
      "detailed_description": "An R package that automatically generates reports from statistical models and data frames. It interprets the results of statistical tests and models, providing text descriptions suitable for scientific publication.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_reporting",
        "automated_reporting"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/easystats/report",
      "help_website": [
        "https://easystats.github.io/report/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "r-package",
        "reporting",
        "reproducibility"
      ],
      "id": 466
    },
    {
      "name": "Pybotics",
      "one_line_profile": "Python toolbox for robotics kinematics and calibration",
      "detailed_description": "A Python toolbox for robot kinematics and calibration. It provides tools for modeling robot geometry, calculating forward/inverse kinematics, and performing calibration tasks, useful for robotics research.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "robotics_kinematics",
        "calibration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/engnadeau/pybotics",
      "help_website": [
        "https://pybotics.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "robotics",
        "kinematics",
        "calibration"
      ],
      "id": 467
    },
    {
      "name": "Seismometer",
      "one_line_profile": "AI model evaluation framework for healthcare",
      "detailed_description": "A framework for evaluating AI models with a specific focus on healthcare applications. It provides tools for assessing model performance, fairness, and calibration in clinical settings.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "healthcare_ai"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/epic-open-source/seismometer",
      "help_website": [
        "https://epic-open-source.github.io/seismometer/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "healthcare",
        "evaluation",
        "fairness"
      ],
      "id": 468
    },
    {
      "name": "bootstrapped",
      "one_line_profile": "Library for generating bootstrapped confidence intervals for A/B testing",
      "detailed_description": "A Python library designed to generate bootstrapped confidence intervals for A/B testing purposes. It allows researchers to estimate the sampling distribution of a statistic by resampling with replacement from the original data.",
      "domains": [
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_inference",
        "confidence_intervals",
        "ab_testing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookarchive/bootstrapped",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "statistics",
        "bootstrapping",
        "confidence-intervals"
      ],
      "id": 469
    },
    {
      "name": "fronni",
      "one_line_profile": "Fast ML performance metrics and charts with confidence intervals",
      "detailed_description": "A library for calculating machine learning model performance metrics and generating charts with confidence intervals. It is optimized with Numba for high-performance computation.",
      "domains": [
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "performance_metrics",
        "confidence_intervals"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookarchive/fronni",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "metrics",
        "numba",
        "visualization"
      ],
      "id": 470
    },
    {
      "name": "EvalGIM",
      "one_line_profile": "Evaluation library for generative image models",
      "detailed_description": "EvalGIM (EvalGym) is a library designed for the automatic evaluation of text-to-image generative models. It supports reproducible evaluations with user-defined metrics, datasets, and visualizations.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "generative_models",
        "image_generation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/EvalGIM",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "generative-ai",
        "evaluation",
        "text-to-image"
      ],
      "id": 471
    },
    {
      "name": "unibench",
      "one_line_profile": "Evaluation library for VLM robustness across benchmarks",
      "detailed_description": "A Python library specifically designed to evaluate the robustness of Vision-Language Models (VLMs) across a diverse set of benchmarks.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "robustness_testing",
        "vlm"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/facebookresearch/unibench",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "vision-language-models",
        "benchmarking",
        "robustness"
      ],
      "id": 472
    },
    {
      "name": "pooch",
      "one_line_profile": "Data fetching and management library for scientific datasets",
      "detailed_description": "Pooch manages your Python library's sample data files. It automatically downloads and stores data files, checking their integrity via hash comparisons, which is essential for reproducible scientific workflows.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "data_management",
        "data_retrieval",
        "reproducibility"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/fatiando/pooch",
      "help_website": [
        "https://www.fatiando.org/pooch"
      ],
      "license": "NOASSERTION",
      "tags": [
        "data-fetching",
        "reproducibility",
        "scientific-python"
      ],
      "id": 473
    },
    {
      "name": "verde",
      "one_line_profile": "Spatial data processing and gridding library",
      "detailed_description": "Verde is a Python library for processing and gridding spatial data using a machine-learning style API. It provides tools for interpolation, trend removal, and cross-validation of spatial models.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "spatial_analysis",
        "gridding",
        "interpolation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/fatiando/verde",
      "help_website": [
        "https://www.fatiando.org/verde"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "geospatial",
        "interpolation",
        "machine-learning"
      ],
      "id": 474
    },
    {
      "name": "prediction-interval-NN",
      "one_line_profile": "Confidence and prediction intervals for Neural Networks",
      "detailed_description": "A library providing methods to estimate confidence and prediction intervals for feedforward neural networks and RNNs, enabling uncertainty quantification in deep learning models.",
      "domains": [
        "AI4-02"
      ],
      "subtask_category": [
        "uncertainty_quantification",
        "prediction_intervals",
        "neural_networks"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/fishjh2/prediction-interval-NN",
      "help_website": [],
      "license": null,
      "tags": [
        "uncertainty",
        "neural-networks",
        "intervals"
      ],
      "id": 475
    },
    {
      "name": "medmnistc-api",
      "one_line_profile": "Robustness evaluation library for medical image analysis",
      "detailed_description": "A Python library developed for evaluating and enhancing the robustness of machine learning models in medical image analysis, associated with the ADSMI@MICCAI2024 workshop.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "robustness",
        "medical_imaging"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/francescodisalvo05/medmnistc-api",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "medical-imaging",
        "robustness",
        "miccai"
      ],
      "id": 476
    },
    {
      "name": "DeepMatch",
      "one_line_profile": "Deep learning library for metric and embedding learning",
      "detailed_description": "A library for metric and embedding learning using convolutional neural networks, with applications in keypoint matching, stereo matching, and image retrieval.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "metric_learning",
        "image_matching",
        "embedding"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/galad-loth/DeepMatch",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "deep-learning",
        "computer-vision",
        "matching"
      ],
      "id": 477
    },
    {
      "name": "NLPMetrics",
      "one_line_profile": "Collection of NLP evaluation metrics",
      "detailed_description": "A Python repository providing implementations for various Natural Language Processing (NLP) metrics, facilitating the evaluation of language models.",
      "domains": [
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "nlp_metrics"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/gcunhase/NLPMetrics",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nlp",
        "metrics",
        "evaluation"
      ],
      "id": 478
    },
    {
      "name": "long-form-factuality",
      "one_line_profile": "Benchmark for long-form factuality in LLMs",
      "detailed_description": "A library and benchmark suite from Google DeepMind for evaluating the factuality of long-form text generated by Large Language Models (LLMs).",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "factuality_checking",
        "llm_benchmark"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/google-deepmind/long-form-factuality",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "llm",
        "factuality",
        "benchmarking"
      ],
      "id": 479
    },
    {
      "name": "surface-distance",
      "one_line_profile": "Surface distance metrics for segmentation evaluation",
      "detailed_description": "A library to compute surface distance-based performance metrics (e.g., Hausdorff distance) for segmentation tasks, widely used in medical imaging and computer vision.",
      "domains": [
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "segmentation_metrics",
        "surface_distance"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/google-deepmind/surface-distance",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "segmentation",
        "metrics",
        "medical-imaging"
      ],
      "id": 480
    },
    {
      "name": "bleurt",
      "one_line_profile": "Learned metric for Natural Language Generation",
      "detailed_description": "BLEURT is a metric for evaluating Natural Language Generation systems based on transfer learning, providing a more human-correlated evaluation than traditional metrics.",
      "domains": [
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "nlg_metrics",
        "learned_metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/google-research/bleurt",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "nlp",
        "evaluation",
        "transfer-learning"
      ],
      "id": 481
    },
    {
      "name": "rl-reliability-metrics",
      "one_line_profile": "Metrics for measuring reliability of RL algorithms",
      "detailed_description": "A library providing a set of metrics and statistical tools for measuring and comparing the reliability and performance stability of reinforcement learning (RL) algorithms.",
      "domains": [
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "reinforcement_learning",
        "reliability_metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/google-research/rl-reliability-metrics",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "reinforcement-learning",
        "statistics",
        "reliability"
      ],
      "id": 482
    },
    {
      "name": "best",
      "one_line_profile": "Bam Error Stats Tool for aligned reads analysis",
      "detailed_description": "Bam Error Stats Tool (best) is a utility for analyzing error types in aligned sequencing reads (BAM files), useful for quality control in bioinformatics workflows.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "bioinformatics_analysis",
        "quality_control",
        "sequencing_error_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Rust",
      "repo_url": "https://github.com/google/best",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bioinformatics",
        "bam",
        "genomics"
      ],
      "id": 483
    },
    {
      "name": "yggdrasil-decision-forests",
      "one_line_profile": "Library for decision forest models",
      "detailed_description": "A comprehensive library to train, evaluate, interpret, and productionize decision forest models such as Random Forest and Gradient Boosted Decision Trees.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "scientific_modeling",
        "machine_learning",
        "decision_trees"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/google/yggdrasil-decision-forests",
      "help_website": [
        "https://ydf.readthedocs.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "machine-learning",
        "decision-forests",
        "random-forest"
      ],
      "id": 484
    },
    {
      "name": "nlp-metrics",
      "one_line_profile": "Implementations of BLEU and ROUGE metrics",
      "detailed_description": "A Python implementation of standard NLP evaluation metrics, specifically BLEU and ROUGE, used for assessing text generation quality.",
      "domains": [
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "nlp_metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/harpribot/nlp-metrics",
      "help_website": [],
      "license": null,
      "tags": [
        "nlp",
        "bleu",
        "rouge"
      ],
      "id": 485
    },
    {
      "name": "hcrystalball",
      "one_line_profile": "Unified API for time-series forecasting",
      "detailed_description": "A library that unifies the API for various time-series forecasting libraries and modeling techniques in the Python ecosystem, facilitating scientific data analysis and prediction.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "time_series_forecasting",
        "scientific_modeling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/heidelbergcement/hcrystalball",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "time-series",
        "forecasting",
        "machine-learning"
      ],
      "id": 486
    },
    {
      "name": "ros-network-analysis",
      "one_line_profile": "Network analysis tools for ROS",
      "detailed_description": "A ROS package providing tools to analyze wireless network metrics (signal quality, latency, throughput) between ROS nodes, useful for robotics research and engineering.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "network_analysis",
        "robotics_evaluation",
        "performance_metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/herolab-uga/ros-network-analysis",
      "help_website": [],
      "license": null,
      "tags": [
        "ros",
        "robotics",
        "network-analysis"
      ],
      "id": 487
    },
    {
      "name": "hmmlearn",
      "one_line_profile": "Hidden Markov Models in Python",
      "detailed_description": "A library for Hidden Markov Models (HMM) in Python, featuring a scikit-learn like API. It is widely used for modeling sequence data in various scientific domains.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "scientific_modeling",
        "sequence_analysis",
        "statistical_inference"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/hmmlearn/hmmlearn",
      "help_website": [
        "https://hmmlearn.readthedocs.io"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "hmm",
        "machine-learning",
        "sequence-modeling"
      ],
      "id": 488
    },
    {
      "name": "huggingface/evaluate",
      "one_line_profile": "A library for easily evaluating machine learning models and datasets",
      "detailed_description": "A library for easily evaluating machine learning models and datasets. It provides a unified API for a wide range of evaluation metrics, comparison statistics, and measurements, supporting various domains including NLP, Computer Vision, and Audio.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/huggingface/evaluate",
      "help_website": [
        "https://huggingface.co/docs/evaluate"
      ],
      "license": "Apache-2.0",
      "tags": [
        "metrics",
        "evaluation",
        "nlp",
        "machine-learning"
      ],
      "id": 489
    },
    {
      "name": "ing-bank/sparse_dot_topn",
      "one_line_profile": "Fast sparse matrix multiplication and top-n similarity selection",
      "detailed_description": "A Python package to accelerate sparse matrix multiplication and top-n similarity selection. It is widely used for string matching, entity resolution, and other tasks requiring efficient cosine similarity calculations on large datasets.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "similarity_calculation",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/ing-bank/sparse_dot_topn",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "sparse-matrix",
        "similarity",
        "cosine-similarity",
        "optimization"
      ],
      "id": 490
    },
    {
      "name": "insysbio/LikelihoodProfiler.jl",
      "one_line_profile": "Practical identifiability analysis and confidence intervals estimation in Julia",
      "detailed_description": "LikelihoodProfiler is a Julia package designed for practical identifiability analysis and confidence intervals estimation in dynamic modeling, particularly useful in systems biology and pharmacology.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_analysis",
        "confidence_intervals",
        "identifiability_analysis"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/insysbio/LikelihoodProfiler.jl",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "julia",
        "statistics",
        "confidence-intervals",
        "systems-biology"
      ],
      "id": 491
    },
    {
      "name": "interpretml/interpret",
      "one_line_profile": "Fit interpretable models and explain blackbox machine learning",
      "detailed_description": "InterpretML is a library for training interpretable models and explaining blackbox systems. It includes the Explainable Boosting Machine (EBM) and supports various visualization and metric tools for model analysis.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "interpretability",
        "model_analysis"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/interpretml/interpret",
      "help_website": [
        "https://interpret.ml"
      ],
      "license": "MIT",
      "tags": [
        "interpretability",
        "xai",
        "machine-learning",
        "visualization"
      ],
      "id": 492
    },
    {
      "name": "jacobgil/confidenceinterval",
      "one_line_profile": "Library for calculating confidence intervals in Python",
      "detailed_description": "A Python library dedicated to calculating confidence intervals for various statistical distributions and machine learning metrics, filling a gap in standard libraries.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_analysis",
        "confidence_intervals"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/jacobgil/confidenceinterval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "statistics",
        "confidence-intervals",
        "python"
      ],
      "id": 493
    },
    {
      "name": "jhclark/multeval",
      "one_line_profile": "Bootstrap Resampling and Approximate Randomization for MT metrics",
      "detailed_description": "A tool for statistical hypothesis testing in Machine Translation evaluation. It implements bootstrap resampling and approximate randomization to control for optimizer instability and provide reliable significance testing for metrics like BLEU, METEOR, and TER.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_testing",
        "evaluation",
        "nlp_metrics"
      ],
      "application_level": "solver",
      "primary_language": "Groff",
      "repo_url": "https://github.com/jhclark/multeval",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "machine-translation",
        "significance-testing",
        "bootstrap",
        "evaluation"
      ],
      "id": 494
    },
    {
      "name": "jiwei0921/Saliency-Evaluation-Toolbox",
      "one_line_profile": "Evaluation metrics toolbox for salient object detection",
      "detailed_description": "A MATLAB toolbox providing comprehensive evaluation metrics for salient object detection, including E-measure, S-measure, weighted F-measure, MAE, and PR curves.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "evaluation",
        "metrics",
        "computer_vision"
      ],
      "application_level": "library",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/jiwei0921/Saliency-Evaluation-Toolbox",
      "help_website": [],
      "license": null,
      "tags": [
        "saliency-detection",
        "evaluation-metrics",
        "matlab"
      ],
      "id": 495
    },
    {
      "name": "jlsuarezdiaz/pyDML",
      "one_line_profile": "Distance Metric Learning Algorithms for Python",
      "detailed_description": "A Python library implementing various Distance Metric Learning (DML) algorithms. It allows learning distance metrics from data to improve the performance of distance-based machine learning algorithms like k-NN.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "metric_learning",
        "distance_metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/jlsuarezdiaz/pyDML",
      "help_website": [
        "https://pydml.readthedocs.io/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "metric-learning",
        "machine-learning",
        "distance-metric"
      ],
      "id": 496
    },
    {
      "name": "jm4474/SVARIV",
      "one_line_profile": "Inference and confidence intervals for Structural Vector Autoregressions",
      "detailed_description": "A Matlab suite to construct weak-instrument robust confidence intervals for impulse response coefficients in Structural Vector Autoregressions (SVAR) identified with an external instrument.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_inference",
        "confidence_intervals",
        "econometrics"
      ],
      "application_level": "library",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/jm4474/SVARIV",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "econometrics",
        "confidence-intervals",
        "matlab",
        "time-series"
      ],
      "id": 497
    },
    {
      "name": "jongwook/spark-ranking-metrics",
      "one_line_profile": "Offline Recommender System Evaluation for Spark",
      "detailed_description": "A library for calculating ranking metrics (like MAP, NDCG) for recommender system evaluation on Apache Spark, enabling scalable offline evaluation.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "evaluation",
        "metrics",
        "recommender_systems"
      ],
      "application_level": "library",
      "primary_language": "Scala",
      "repo_url": "https://github.com/jongwook/spark-ranking-metrics",
      "help_website": [],
      "license": "Unlicense",
      "tags": [
        "spark",
        "evaluation",
        "ranking-metrics",
        "recommender-system"
      ],
      "id": 498
    },
    {
      "name": "icp",
      "one_line_profile": "Python implementation of Invariant Causal Prediction (ICP) algorithm",
      "detailed_description": "A Python implementation of the Invariant Causal Prediction (ICP) algorithm for causal inference, enabling identification of causal relationships and confidence intervals from data.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "causal_inference",
        "statistical_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/juangamella/icp",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "causal-inference",
        "statistics",
        "python"
      ],
      "id": 499
    },
    {
      "name": "Bootstrap.jl",
      "one_line_profile": "Statistical bootstrapping library for Julia",
      "detailed_description": "A Julia library for statistical bootstrapping, providing methods to estimate sampling distributions and confidence intervals for various statistics.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_analysis",
        "confidence_intervals"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/juliangehring/Bootstrap.jl",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "julia",
        "statistics",
        "bootstrapping"
      ],
      "id": 500
    },
    {
      "name": "mantel",
      "one_line_profile": "Python implementation of the Mantel test for correlation between distance matrices",
      "detailed_description": "A Python library implementing the Mantel test, a statistical test used to evaluate the correlation between two distance matrices, commonly used in ecology and genetics.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_analysis",
        "hypothesis_testing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/jwcarr/mantel",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "statistics",
        "mantel-test",
        "correlation"
      ],
      "id": 501
    },
    {
      "name": "CodeBLEU",
      "one_line_profile": "Evaluation metric for code generation tasks",
      "detailed_description": "A Python implementation of CodeBLEU, a metric designed to evaluate the quality of code generated by AI models by considering syntactic and semantic features.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "metrics",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/k4black/codebleu",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nlp",
        "metrics",
        "code-generation"
      ],
      "id": 502
    },
    {
      "name": "boundedline-pkg",
      "one_line_profile": "Matlab tool for plotting lines with error bounds/confidence intervals",
      "detailed_description": "A Matlab package for visualizing data with associated uncertainty, allowing the plotting of lines with shaded error bounds or confidence intervals.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "visualization",
        "statistical_analysis"
      ],
      "application_level": "library",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/kakearney/boundedline-pkg",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "matlab",
        "visualization",
        "plotting",
        "error-bars"
      ],
      "id": 503
    },
    {
      "name": "matrixTests",
      "one_line_profile": "R package for high-performance matrix-based hypothesis testing",
      "detailed_description": "An R package designed for efficient computation of multiple hypothesis tests (t-tests, F-tests, etc.) on rows or columns of matrices, useful for high-dimensional biological data.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_analysis",
        "hypothesis_testing"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/karoliskoncevicius/matrixTests",
      "help_website": [],
      "license": null,
      "tags": [
        "r",
        "statistics",
        "matrix-operations"
      ],
      "id": 504
    },
    {
      "name": "ROUGE-2.0",
      "one_line_profile": "Toolkit for evaluating automatic summarization using ROUGE metrics",
      "detailed_description": "A Java-based toolkit for computing ROUGE metrics (Recall-Oriented Understudy for Gisting Evaluation) to evaluate automatic text summarization systems.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "metrics",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/kavgan/ROUGE-2.0",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "nlp",
        "summarization",
        "metrics",
        "rouge"
      ],
      "id": 505
    },
    {
      "name": "rouge",
      "one_line_profile": "JavaScript implementation of ROUGE evaluation metric",
      "detailed_description": "A JavaScript library implementing the ROUGE metric for evaluating automatic summarization, enabling evaluation in web-based or Node.js environments.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "metrics",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/kenlimmj/rouge",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "javascript",
        "nlp",
        "metrics"
      ],
      "id": 506
    },
    {
      "name": "hera",
      "one_line_profile": "Dashboard for tracking and visualizing Keras model training metrics",
      "detailed_description": "A tool to stream and visualize metrics from Keras model training to a browser-based dashboard, facilitating experiment monitoring.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "visualization",
        "experiment_tracking"
      ],
      "application_level": "solver",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/keplr-io/hera",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "keras",
        "visualization",
        "dashboard",
        "deep-learning"
      ],
      "id": 507
    },
    {
      "name": "abacus",
      "one_line_profile": "Fast hypothesis testing and experiment design solution",
      "detailed_description": "A Python library for conducting fast hypothesis testing and designing experiments (A/B testing), providing statistical tools for decision making.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_analysis",
        "hypothesis_testing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/kolmogorov-lab/abacus",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "statistics",
        "ab-testing",
        "experiment-design"
      ],
      "id": 508
    },
    {
      "name": "globox",
      "one_line_profile": "Object detection dataset converter and evaluator",
      "detailed_description": "A Python package to read, convert, and evaluate object detection datasets (COCO, YOLO, PascalVOC, etc.) using standard metrics like mAP.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "metrics",
        "data_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/laclouis5/globox",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "object-detection",
        "metrics",
        "dataset-conversion"
      ],
      "id": 509
    },
    {
      "name": "PyIRSTDMetrics",
      "one_line_profile": "Evaluation metrics for Infrared Small Target Detection",
      "detailed_description": "A Python toolbox providing specialized evaluation metrics for the task of Infrared Small Target Detection (IRSTD).",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "metrics",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/lartpang/PyIRSTDMetrics",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "computer-vision",
        "metrics",
        "infrared-detection"
      ],
      "id": 510
    },
    {
      "name": "PySODEvalToolkit",
      "one_line_profile": "Evaluation toolbox for Salient and Camouflaged Object Detection",
      "detailed_description": "A Python-based evaluation toolbox designed for assessing the performance of models in Salient Object Detection (SOD) and Camouflaged Object Detection (COD).",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "metrics",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/lartpang/PySODEvalToolkit",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "computer-vision",
        "saliency-detection",
        "metrics"
      ],
      "id": 511
    },
    {
      "name": "dgm-eval",
      "one_line_profile": "Evaluation metrics for deep generative models",
      "detailed_description": "A codebase for evaluating deep generative models, addressing flaws in existing metrics and providing fairer assessment for diffusion models.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "metrics",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/layer6ai-labs/dgm-eval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "generative-models",
        "metrics",
        "diffusion-models"
      ],
      "id": 512
    },
    {
      "name": "deception",
      "one_line_profile": "Benchmark for evaluating LLM disinformation capabilities",
      "detailed_description": "A benchmark suite designed to evaluate Large Language Models on their ability to create and resist disinformation, including standardized metrics.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "metrics",
        "model_evaluation",
        "benchmark"
      ],
      "application_level": "dataset",
      "primary_language": null,
      "repo_url": "https://github.com/lechmazur/deception",
      "help_website": [],
      "license": null,
      "tags": [
        "llm",
        "benchmark",
        "safety",
        "evaluation"
      ],
      "id": 513
    },
    {
      "name": "cvAUC",
      "one_line_profile": "Confidence intervals for cross-validated AUC estimates in R",
      "detailed_description": "An R package for computationally efficient estimation of confidence intervals for cross-validated Area Under the ROC Curve (AUC) estimates.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_analysis",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/ledell/cvAUC",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "r",
        "statistics",
        "auc",
        "confidence-intervals"
      ],
      "id": 514
    },
    {
      "name": "rouge-metric",
      "one_line_profile": "Python wrapper and re-implementation of ROUGE metrics",
      "detailed_description": "A Python package that wraps the official ROUGE script and provides a native re-implementation for evaluating text summarization.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "metrics",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Perl",
      "repo_url": "https://github.com/li-plus/rouge-metric",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nlp",
        "metrics",
        "rouge"
      ],
      "id": 515
    },
    {
      "name": "librosa",
      "one_line_profile": "Python library for audio and music analysis",
      "detailed_description": "A comprehensive Python library for audio and music analysis, providing tools for feature extraction, signal processing, and visualization.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "signal_processing",
        "feature_extraction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/librosa/librosa",
      "help_website": [
        "https://librosa.org"
      ],
      "license": "ISC",
      "tags": [
        "audio",
        "signal-processing",
        "music-analysis"
      ],
      "id": 516
    },
    {
      "name": "t2v_metrics",
      "one_line_profile": "Evaluation metrics for text-to-visual models using VQAScore",
      "detailed_description": "A toolkit for evaluating text-to-image, text-to-video, and text-to-3D models using VQAScore, a metric based on Visual Question Answering.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "metrics",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/linzhiqiu/t2v_metrics",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "generative-ai",
        "metrics",
        "multimodal"
      ],
      "id": 517
    },
    {
      "name": "lmfit-py",
      "one_line_profile": "Non-Linear Least Squares Minimization and Curve Fitting",
      "detailed_description": "A Python library for non-linear least-squares minimization and curve fitting, building on scipy.optimize to provide a high-level interface for modeling data.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_analysis",
        "curve_fitting"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/lmfit/lmfit-py",
      "help_website": [
        "https://lmfit.github.io/lmfit-py/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "curve-fitting",
        "optimization",
        "statistics"
      ],
      "id": 518
    },
    {
      "name": "KoBERTScore",
      "one_line_profile": "BERTScore implementation for Korean language",
      "detailed_description": "A Python implementation of the BERTScore metric specifically adapted for evaluating Korean text generation.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "metrics",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/lovit/KoBERTScore",
      "help_website": [],
      "license": null,
      "tags": [
        "nlp",
        "metrics",
        "korean"
      ],
      "id": 519
    },
    {
      "name": "ConfidenceIntervals",
      "one_line_profile": "Confidence interval computation for ML evaluation",
      "detailed_description": "A Python package to calculate confidence intervals for machine learning evaluation metrics using bootstrapping methods.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_analysis",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/luferrer/ConfidenceIntervals",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "statistics",
        "machine-learning",
        "confidence-intervals"
      ],
      "id": 520
    },
    {
      "name": "waipy",
      "one_line_profile": "Wavelet analysis library for time series with significance testing",
      "detailed_description": "A Python library for Continuous Wavelet Transform (CWT) and Cross Wavelet Analysis (CWA), including significance tests based on Torrence and Compo (1998) for analyzing time series data.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "time_series_analysis",
        "statistical_test",
        "signal_processing"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/mabelcalim/waipy",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "wavelet-transform",
        "time-series",
        "significance-test"
      ],
      "id": 521
    },
    {
      "name": "Mars",
      "one_line_profile": "Tensor-based unified framework for large-scale data computation",
      "detailed_description": "A tensor-based framework for large-scale data computation that scales libraries like numpy, pandas, and scikit-learn, enabling distributed scientific computing and data analysis.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "distributed_computing",
        "data_processing",
        "scientific_computing"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/mars-project/mars",
      "help_website": [
        "https://mars-project.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "distributed-systems",
        "tensor-computation",
        "numpy-compatible"
      ],
      "id": 522
    },
    {
      "name": "py-img-seg-eval",
      "one_line_profile": "Evaluation metrics for image segmentation",
      "detailed_description": "A Python library implementing standard evaluation metrics for image segmentation tasks, such as pixel accuracy and Intersection over Union (IoU), inspired by FCN literature.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "image_segmentation",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/martinkersner/py-img-seg-eval",
      "help_website": [],
      "license": null,
      "tags": [
        "computer-vision",
        "segmentation",
        "evaluation-metrics"
      ],
      "id": 523
    },
    {
      "name": "ml-stat-util",
      "one_line_profile": "Statistical functions for comparing ML models",
      "detailed_description": "A collection of statistical functions based on bootstrapping for computing confidence intervals and p-values to compare machine learning models against human readers or other models.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_test",
        "model_comparison",
        "confidence_interval"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/mateuszbuda/ml-stat-util",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bootstrapping",
        "p-value",
        "model-evaluation"
      ],
      "id": 524
    },
    {
      "name": "AMeThyst",
      "one_line_profile": "Metrics and hypothesis testing tools for artifact analysis",
      "detailed_description": "A set of tools for calculating metrics and performing hypothesis tests, designed for analyzing specific artifacts or datasets (implied 'Art Metrics').",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "hypothesis_testing",
        "metrics",
        "statistical_analysis"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/mattyamonaca/AMeThyst",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "hypothesis-test",
        "metrics",
        "analysis"
      ],
      "id": 525
    },
    {
      "name": "SAMFailureMetrics",
      "one_line_profile": "Metrics for assessing segmentation object properties",
      "detailed_description": "A library providing metrics for quantifying tree-likeness and textural contrast of objects, used for analyzing failure modes in segmentation models (e.g., SAM).",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "error_analysis",
        "segmentation_metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/mazurowski-lab/SAMFailureMetrics",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "segmentation",
        "failure-analysis",
        "metrics"
      ],
      "id": 526
    },
    {
      "name": "torcheval",
      "one_line_profile": "Performant PyTorch model metrics library",
      "detailed_description": "A library containing a rich collection of performant PyTorch model metrics, tools for creating new metrics, and utilities for distributed training evaluation.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "metrics",
        "distributed_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/meta-pytorch/torcheval",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "pytorch",
        "metrics",
        "evaluation"
      ],
      "id": 527
    },
    {
      "name": "FLAML",
      "one_line_profile": "Fast and lightweight AutoML library",
      "detailed_description": "A fast library for Automated Machine Learning (AutoML) and hyperparameter tuning, designed to find accurate models with low computational cost.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "automl",
        "hyperparameter_tuning",
        "model_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/microsoft/FLAML",
      "help_website": [
        "https://microsoft.github.io/FLAML/"
      ],
      "license": "MIT",
      "tags": [
        "automl",
        "tuning",
        "optimization"
      ],
      "id": 528
    },
    {
      "name": "FeatureBroker",
      "one_line_profile": "Feature collection and inference library for ML evaluation",
      "detailed_description": "A library for collecting features and performing inference for machine learning evaluations, facilitating scenarios where feature publishing is decoupled from model consumption.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "feature_extraction",
        "model_inference",
        "evaluation_infrastructure"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/microsoft/FeatureBroker",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "feature-engineering",
        "inference",
        "evaluation"
      ],
      "id": 529
    },
    {
      "name": "LMChallenge",
      "one_line_profile": "Library and tools for evaluating predictive language models",
      "detailed_description": "A library and set of tools designed to evaluate predictive language models against standard benchmarks and challenges.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "nlp_metrics",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/LMChallenge",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "nlp",
        "language-models",
        "evaluation"
      ],
      "id": 530
    },
    {
      "name": "dstoolkit-e2e-presidio-evaluation",
      "one_line_profile": "End-to-end evaluation toolkit for PII detection",
      "detailed_description": "A toolkit for assessing PII detection frameworks (specifically Presidio) using Hugging Face transformers and Azure services, providing an end-to-end evaluation pipeline.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "pii_detection",
        "security_metrics"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/dstoolkit-e2e-presidio-evaluation",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pii",
        "evaluation",
        "presidio"
      ],
      "id": 531
    },
    {
      "name": "Hummingbird",
      "one_line_profile": "Compiler for translating ML models to tensor computations",
      "detailed_description": "A library that compiles trained traditional machine learning models into tensor computations (e.g., PyTorch, TorchScript, ONNX) for faster inference.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "model_compilation",
        "inference_optimization",
        "scientific_computing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/hummingbird",
      "help_website": [
        "https://microsoft.github.io/hummingbird/"
      ],
      "license": "MIT",
      "tags": [
        "inference",
        "compiler",
        "optimization"
      ],
      "id": 532
    },
    {
      "name": "ONNX Runtime",
      "one_line_profile": "Cross-platform high-performance ML inference accelerator",
      "detailed_description": "A cross-platform, high-performance engine for machine learning inference and training, supporting models from various frameworks via the ONNX format.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "model_inference",
        "model_training",
        "acceleration"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/microsoft/onnxruntime",
      "help_website": [
        "https://onnxruntime.ai/"
      ],
      "license": "MIT",
      "tags": [
        "onnx",
        "inference",
        "acceleration"
      ],
      "id": 533
    },
    {
      "name": "rankerEval",
      "one_line_profile": "Numpy-based ranking metrics implementation",
      "detailed_description": "A fast, numpy-based implementation of ranking metrics (such as NDCG, ERR) for evaluating information retrieval and recommendation systems.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "ranking_evaluation",
        "metrics",
        "information_retrieval"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/rankerEval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ranking",
        "metrics",
        "ir"
      ],
      "id": 534
    },
    {
      "name": "Table Transformer",
      "one_line_profile": "Table extraction model and GriTS evaluation metric",
      "detailed_description": "A deep learning model for extracting tables from unstructured documents, which includes the official implementation of the GriTS evaluation metric for table structure recognition.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "table_extraction",
        "model_evaluation",
        "metrics"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/table-transformer",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "table-extraction",
        "grits-metric",
        "document-analysis"
      ],
      "id": 535
    },
    {
      "name": "sacrebleu",
      "one_line_profile": "Standardized BLEU score implementation for NLP",
      "detailed_description": "A reference implementation of the BLEU metric that automatically downloads test sets and reports version strings to facilitate reproducible cross-lab comparisons in machine translation.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "nlp_evaluation",
        "metrics",
        "translation_quality"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/mjpost/sacrebleu",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "bleu",
        "nlp",
        "reproducibility"
      ],
      "id": 536
    },
    {
      "name": "ML Workspace",
      "one_line_profile": "All-in-one web-based IDE for machine learning",
      "detailed_description": "A web-based Integrated Development Environment (IDE) specialized for machine learning and data science, pre-configured with popular libraries and tools.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "development_environment",
        "scientific_workflow",
        "reproducibility"
      ],
      "application_level": "platform",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/ml-tooling/ml-workspace",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ide",
        "docker",
        "data-science"
      ],
      "id": 537
    },
    {
      "name": "mljar-supervised",
      "one_line_profile": "AutoML for tabular data with explanation generation",
      "detailed_description": "A Python package for Automated Machine Learning (AutoML) on tabular data, featuring automatic feature engineering, hyperparameter tuning, model explanations, and documentation generation.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "automl",
        "tabular_data",
        "explainable_ai"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/mljar/mljar-supervised",
      "help_website": [
        "https://supervised.mljar.com/"
      ],
      "license": "MIT",
      "tags": [
        "automl",
        "tabular",
        "explainability"
      ],
      "id": 538
    },
    {
      "name": "glm-sklearn",
      "one_line_profile": "Scikit-learn wrappers for Statsmodels GLM",
      "detailed_description": "A library providing scikit-learn compatible wrappers for Generalized Linear Models (GLM) from the statsmodels library, facilitating their use in scikit-learn pipelines.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_modeling",
        "regression",
        "glm"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/modusdatascience/glm-sklearn",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "sklearn",
        "statsmodels",
        "glm"
      ],
      "id": 539
    },
    {
      "name": "Pweave",
      "one_line_profile": "Scientific report generator and literate programming tool for Python",
      "detailed_description": "Pweave is a scientific report generator and a literate programming tool for Python, capable of capturing results and plots from data analysis, similar to R Markdown.",
      "domains": [
        "AI4",
        "Scientific Reporting"
      ],
      "subtask_category": [
        "literate_programming",
        "report_generation"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/mpastell/Pweave",
      "help_website": [
        "http://mpastell.com/pweave"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "literate-programming",
        "reporting",
        "reproducible-research"
      ],
      "id": 540
    },
    {
      "name": "MS-Loss",
      "one_line_profile": "Multi-Similarity Loss implementation for Deep Metric Learning",
      "detailed_description": "Implementation of Multi-Similarity Loss for Deep Metric Learning, designed to improve the training of models requiring metric learning objectives.",
      "domains": [
        "AI4",
        "Deep Learning"
      ],
      "subtask_category": [
        "metric_learning",
        "loss_function"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/msight-tech/research-ms-loss",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "metric-learning",
        "loss-function",
        "deep-learning"
      ],
      "id": 541
    },
    {
      "name": "Crab",
      "one_line_profile": "Flexible recommender engine for Python",
      "detailed_description": "Crab is a recommender engine for Python that integrates classic information filtering recommendation algorithms within the scientific Python ecosystem (numpy, scipy).",
      "domains": [
        "AI4",
        "Recommender Systems"
      ],
      "subtask_category": [
        "recommendation",
        "information_filtering"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/muricoca/crab",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "recommender-system",
        "collaborative-filtering",
        "scikit"
      ],
      "id": 542
    },
    {
      "name": "Seaborn",
      "one_line_profile": "Statistical data visualization library based on matplotlib",
      "detailed_description": "Seaborn is a Python data visualization library based on matplotlib that provides a high-level interface for drawing attractive and informative statistical graphics.",
      "domains": [
        "AI4",
        "Visualization"
      ],
      "subtask_category": [
        "data_visualization",
        "statistical_plotting"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/mwaskom/seaborn",
      "help_website": [
        "https://seaborn.pydata.org"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "visualization",
        "statistics",
        "plotting"
      ],
      "id": 543
    },
    {
      "name": "scores",
      "one_line_profile": "Metrics for verification and evaluation of forecasts and models",
      "detailed_description": "A library containing metrics for the verification, evaluation, and optimisation of forecasts, predictions, or models, particularly in scientific contexts.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "forecasting_verification"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/nci/scores",
      "help_website": [
        "https://scores.readthedocs.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "metrics",
        "forecasting",
        "evaluation"
      ],
      "id": 544
    },
    {
      "name": "cute_ranking",
      "one_line_profile": "Python module for calculating ranking metrics",
      "detailed_description": "A lightweight Python module for calculating various information retrieval ranking metrics such as MAP, NDCG, etc.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "ranking_evaluation",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ncoop57/cute_ranking",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ranking",
        "metrics",
        "information-retrieval"
      ],
      "id": 545
    },
    {
      "name": "image-similarity-measures",
      "one_line_profile": "Evaluation metrics for image similarity",
      "detailed_description": "Implementation of eight evaluation metrics to assess the similarity between two images, including RMSE, PSNR, SSIM, ISSM, FSIM, SRE, SAM, and UIQ.",
      "domains": [
        "AI4",
        "AI4-02",
        "Computer Vision"
      ],
      "subtask_category": [
        "image_similarity",
        "evaluation_metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/nekhtiari/image-similarity-measures",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "image-processing",
        "metrics",
        "similarity"
      ],
      "id": 546
    },
    {
      "name": "CodeBERTScore",
      "one_line_profile": "Automatic metric for code generation based on BERTScore",
      "detailed_description": "An automatic evaluation metric for code generation tasks, adapting BERTScore to evaluate the quality of generated code.",
      "domains": [
        "AI4",
        "AI4-02",
        "NLP"
      ],
      "subtask_category": [
        "code_generation_evaluation",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/neulab/code-bert-score",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "evaluation",
        "code-generation",
        "bertscore"
      ],
      "id": 547
    },
    {
      "name": "ROUGE (Perl)",
      "one_line_profile": "Implementation of ROUGE metrics for summarization",
      "detailed_description": "An implementation of the ROUGE family of metrics, widely used for evaluating automatic summarization and machine translation.",
      "domains": [
        "AI4",
        "AI4-02",
        "NLP"
      ],
      "subtask_category": [
        "summarization_evaluation",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "Perl",
      "repo_url": "https://github.com/neural-dialogue-metrics/rouge",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rouge",
        "nlp",
        "evaluation"
      ],
      "id": 548
    },
    {
      "name": "hyppo",
      "one_line_profile": "Multivariate hypothesis testing library",
      "detailed_description": "A comprehensive Python package for multivariate hypothesis testing, including independence testing and k-sample testing.",
      "domains": [
        "AI4",
        "AI4-02",
        "Statistics"
      ],
      "subtask_category": [
        "hypothesis_testing",
        "statistical_inference"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/neurodata/hyppo",
      "help_website": [
        "https://hyppo.neurodata.io"
      ],
      "license": "MIT",
      "tags": [
        "statistics",
        "hypothesis-testing",
        "multivariate"
      ],
      "id": 549
    },
    {
      "name": "fit_neuron",
      "one_line_profile": "Estimation and evaluation of neural models from recordings",
      "detailed_description": "A neuroscience package for the estimation and evaluation of neural models from patch clamp neural recordings, including spike distance metrics.",
      "domains": [
        "AI4",
        "Neuroscience"
      ],
      "subtask_category": [
        "neural_modeling",
        "parameter_estimation",
        "spike_metrics"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/nicodjimenez/fit_neuron",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "neuroscience",
        "modeling",
        "spiking-neurons"
      ],
      "id": 550
    },
    {
      "name": "Igel",
      "one_line_profile": "CLI tool for training and using machine learning models",
      "detailed_description": "A machine learning tool that allows users to train, test, and use models without writing code, using a YAML configuration approach.",
      "domains": [
        "AI4",
        "AutoML"
      ],
      "subtask_category": [
        "model_training",
        "automl"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/nidhaloff/igel",
      "help_website": [
        "https://igel.readthedocs.io"
      ],
      "license": "MIT",
      "tags": [
        "automl",
        "cli",
        "machine-learning"
      ],
      "id": 551
    },
    {
      "name": "NMSLIB",
      "one_line_profile": "Non-Metric Space Library for efficient similarity search",
      "detailed_description": "An efficient similarity search library and a toolkit for evaluation of k-NN methods for generic non-metric spaces.",
      "domains": [
        "AI4",
        "Algorithms"
      ],
      "subtask_category": [
        "similarity_search",
        "knn",
        "nearest_neighbor"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/nmslib/nmslib",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "similarity-search",
        "knn",
        "indexing"
      ],
      "id": 552
    },
    {
      "name": "NTUSD-Fin",
      "one_line_profile": "Financial sentiment dictionary and scoring methods",
      "detailed_description": "A financial sentiment analysis resource providing scoring methods (frequency, CFIDF, etc.) and a dictionary of words, hashtags, and emojis with sentiment scores.",
      "domains": [
        "AI4",
        "Finance NLP"
      ],
      "subtask_category": [
        "sentiment_analysis",
        "lexicon_creation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/ntunlplab/Finance-NTUSD-Fin",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "sentiment-analysis",
        "finance",
        "nlp-dataset"
      ],
      "id": 553
    },
    {
      "name": "nuclia-eval",
      "one_line_profile": "Library for evaluating RAG pipelines",
      "detailed_description": "A library designed for evaluating Retrieval-Augmented Generation (RAG) systems using Nuclia's models and metrics.",
      "domains": [
        "AI4",
        "AI4-02",
        "NLP"
      ],
      "subtask_category": [
        "rag_evaluation",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/nuclia/nuclia-eval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "evaluation",
        "llm"
      ],
      "id": 554
    },
    {
      "name": "nullranges",
      "one_line_profile": "Generation of null hypothesis ranges for genomic analysis",
      "detailed_description": "A modular R package for generating sets of genomic ranges representing the null hypothesis, including bootstrapped ranges and matched control ranges.",
      "domains": [
        "AI4",
        "Genomics"
      ],
      "subtask_category": [
        "statistical_genomics",
        "null_hypothesis_generation"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/nullranges/nullranges",
      "help_website": [
        "https://nullranges.github.io/nullranges/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "genomics",
        "statistics",
        "bioconductor"
      ],
      "id": 555
    },
    {
      "name": "cuPyNumeric",
      "one_line_profile": "NumPy and SciPy drop-in replacement for multi-node multi-GPU systems",
      "detailed_description": "A library that enables NumPy and SciPy code to run on multi-node multi-GPU systems with minimal code changes, leveraging the Legate system.",
      "domains": [
        "AI4",
        "HPC"
      ],
      "subtask_category": [
        "numerical_computing",
        "distributed_computing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/nv-legate/cupynumeric",
      "help_website": [
        "https://cupynumeric.readthedocs.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "hpc",
        "gpu",
        "numpy"
      ],
      "id": 556
    },
    {
      "name": "cosine_metric_learning",
      "one_line_profile": "Deep Cosine Metric Learning for Person Re-identification",
      "detailed_description": "Implementation of Deep Cosine Metric Learning, a method for training deep networks to learn a metric space suitable for person re-identification tasks.",
      "domains": [
        "AI4",
        "Computer Vision"
      ],
      "subtask_category": [
        "metric_learning",
        "person_reidentification"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/nwojke/cosine_metric_learning",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "metric-learning",
        "reid",
        "deep-learning"
      ],
      "id": 557
    },
    {
      "name": "ONNX",
      "one_line_profile": "Open standard format and tools for machine learning model interoperability",
      "detailed_description": "Open Neural Network Exchange (ONNX) is an open standard format for representing machine learning models, enabling interoperability between different frameworks and providing tools for model optimization and validation.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_interoperability",
        "model_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/onnx/onnx",
      "help_website": [
        "https://onnx.ai/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "interoperability",
        "deep-learning",
        "standard"
      ],
      "id": 558
    },
    {
      "name": "MMEval",
      "one_line_profile": "Unified evaluation library for multiple machine learning frameworks",
      "detailed_description": "A unified evaluation library that provides a wide range of metrics for various machine learning tasks, supporting multiple frameworks like PyTorch and TensorFlow, designed to streamline the model evaluation process.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "metrics_calculation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/open-mmlab/mmeval",
      "help_website": [
        "https://mmeval.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "evaluation",
        "metrics",
        "computer-vision"
      ],
      "id": 559
    },
    {
      "name": "OpenLIT",
      "one_line_profile": "OpenTelemetry-native platform for LLM observability and evaluation",
      "detailed_description": "An open-source platform for AI engineering that provides observability, monitoring, and evaluation capabilities for Large Language Models (LLMs) and GPUs, integrating with OpenTelemetry standards.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "llm_evaluation",
        "observability",
        "monitoring"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/openlit/openlit",
      "help_website": [
        "https://docs.openlit.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "observability",
        "opentelemetry",
        "evaluation"
      ],
      "id": 560
    },
    {
      "name": "XProf",
      "one_line_profile": "Profiling and performance analysis tool for machine learning workloads",
      "detailed_description": "A profiling tool designed to analyze the performance of machine learning models and workloads, helping developers identify bottlenecks and optimize execution on various hardware accelerators.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "performance_profiling",
        "optimization"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/openxla/xprof",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "profiling",
        "performance",
        "machine-learning"
      ],
      "id": 561
    },
    {
      "name": "optimagic",
      "one_line_profile": "Unified interface for numerical optimization and statistical inference",
      "detailed_description": "A Python package for numerical optimization that provides a unified interface to various optimizers (SciPy, NlOpt, etc.) and includes tools for diagnostic analysis and parallel numerical derivatives.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "numerical_optimization",
        "parameter_estimation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/optimagic-dev/optimagic",
      "help_website": [
        "https://optimagic.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "optimization",
        "statistics",
        "numerical-methods"
      ],
      "id": 562
    },
    {
      "name": "pandas-ml",
      "one_line_profile": "Integration library for pandas, scikit-learn, and xgboost",
      "detailed_description": "A library that integrates pandas with scikit-learn, xgboost, and seaborn to streamline data analysis, modeling, and visualization workflows in Python.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "data_analysis",
        "machine_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pandas-ml/pandas-ml",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "pandas",
        "scikit-learn",
        "integration"
      ],
      "id": 563
    },
    {
      "name": "dtreeviz",
      "one_line_profile": "Visualization and interpretation library for decision trees",
      "detailed_description": "A Python library for visualizing decision trees and interpreting their structure and prediction paths, aiding in model analysis and explainability.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_visualization",
        "interpretability"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/parrt/dtreeviz",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "visualization",
        "decision-trees",
        "explainable-ai"
      ],
      "id": 564
    },
    {
      "name": "sigclust2",
      "one_line_profile": "Statistical significance testing for clustering results",
      "detailed_description": "An R package for testing the statistical significance of clustering results, helping to determine if identified clusters are genuine or artifacts of random sampling.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_testing",
        "clustering_analysis"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/pkimes/sigclust2",
      "help_website": [],
      "license": null,
      "tags": [
        "statistics",
        "clustering",
        "significance-test"
      ],
      "id": 565
    },
    {
      "name": "patchworklib",
      "one_line_profile": "Subplot manager for matplotlib and seaborn layouts",
      "detailed_description": "A library that provides an intuitive interface for creating complex subplot layouts with matplotlib and seaborn, facilitating scientific data visualization.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "scientific_visualization",
        "plotting"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/ponnhide/patchworklib",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "matplotlib",
        "visualization",
        "plotting"
      ],
      "id": 566
    },
    {
      "name": "SecML-Torch",
      "one_line_profile": "Library for robustness evaluation of deep learning models",
      "detailed_description": "A library designed for evaluating the robustness of deep learning models against adversarial attacks, providing tools for security analysis in AI.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "adversarial_attacks"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pralab/secml-torch",
      "help_website": [
        "https://secml-torch.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "security",
        "robustness",
        "adversarial-ml"
      ],
      "id": 567
    },
    {
      "name": "Skore",
      "one_line_profile": "Library for ML model evaluation and reporting",
      "detailed_description": "An open-source Python library that accelerates machine learning model development by providing automated evaluation reports, methodological guidance, and cross-validation analysis.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "reporting"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/probabl-ai/skore",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "evaluation",
        "data-science",
        "reporting"
      ],
      "id": 568
    },
    {
      "name": "Psi4NumPy",
      "one_line_profile": "Interactive quantum chemistry programming environment using Psi4 and NumPy",
      "detailed_description": "A framework and collection of tutorials that bridges the Psi4 quantum chemistry package with NumPy, allowing for the development and prototyping of new quantum chemical methods and algorithms in Python.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "quantum_chemistry_modeling",
        "electronic_structure_calculation"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/psi4/psi4numpy",
      "help_website": [
        "http://psi4numpy.readthedocs.io/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "quantum-chemistry",
        "psi4",
        "numpy"
      ],
      "id": 569
    },
    {
      "name": "MultiPy",
      "one_line_profile": "Python library for multiple hypothesis testing",
      "detailed_description": "A Python library dedicated to controlling error rates in multiple hypothesis testing, implementing various statistical correction methods like Bonferroni and FDR.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "hypothesis_testing",
        "statistical_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/puolival/multipy",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "statistics",
        "hypothesis-testing",
        "p-value"
      ],
      "id": 570
    },
    {
      "name": "pyannote-metrics",
      "one_line_profile": "Evaluation toolkit for speaker diarization systems",
      "detailed_description": "A toolkit for reproducible evaluation, diagnostic, and error analysis of speaker diarization systems, providing standard metrics for audio processing research.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "speaker_diarization_evaluation",
        "error_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pyannote/pyannote-metrics",
      "help_website": [
        "http://pyannote.github.io/pyannote-metrics"
      ],
      "license": "MIT",
      "tags": [
        "audio",
        "speaker-diarization",
        "metrics"
      ],
      "id": 571
    },
    {
      "name": "PyTorch Ignite",
      "one_line_profile": "High-level library for training and evaluating neural networks in PyTorch",
      "detailed_description": "A high-level library to help with training and evaluating neural networks in PyTorch flexibly and transparently, providing metrics and event handlers.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_training",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pytorch/ignite",
      "help_website": [
        "https://pytorch.org/ignite/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "pytorch",
        "training",
        "evaluation"
      ],
      "id": 572
    },
    {
      "name": "Object-Detection-Metrics",
      "one_line_profile": "Implementation of common metrics for object detection evaluation",
      "detailed_description": "A repository providing implementations of the most popular metrics used to evaluate object detection algorithms, serving as a standard reference for performance measurement.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "object_detection_evaluation",
        "metrics_calculation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/rafaelpadilla/Object-Detection-Metrics",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "object-detection",
        "metrics",
        "computer-vision"
      ],
      "id": 573
    },
    {
      "name": "Pingouin",
      "one_line_profile": "Statistical package in Python based on Pandas",
      "detailed_description": "A statistical package written in Python that provides a wide range of statistical tests and plotting functions, designed to be a simple yet exhaustive alternative to other statistical libraries.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_analysis",
        "hypothesis_testing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/raphaelvallat/pingouin",
      "help_website": [
        "https://pingouin-stats.org/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "statistics",
        "pandas",
        "hypothesis-testing"
      ],
      "id": 574
    },
    {
      "name": "stable-worldmodel",
      "one_line_profile": "Library for evaluating and conducting world model research",
      "detailed_description": "A reliable, minimal, and scalable library designed to facilitate research and evaluation of world models in reinforcement learning and AI.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "reinforcement_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/rbalestr-lab/stable-worldmodel",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "world-models",
        "reinforcement-learning",
        "evaluation"
      ],
      "id": 575
    },
    {
      "name": "esci",
      "one_line_profile": "Estimation Statistics with Confidence Intervals for R",
      "detailed_description": "An R package for estimation statistics, focusing on effect sizes and confidence intervals to support better statistical practices in scientific research.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_analysis",
        "estimation_statistics"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/rcalinjageman/esci",
      "help_website": [],
      "license": null,
      "tags": [
        "statistics",
        "confidence-intervals",
        "effect-size",
        "r-package"
      ],
      "id": 576
    },
    {
      "name": "scikit-plot",
      "one_line_profile": "Visualization library for scikit-learn objects",
      "detailed_description": "An intuitive library to add plotting functionality to scikit-learn objects, facilitating the visualization of machine learning metrics like confusion matrices, ROC curves, and precision-recall curves.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "visualization",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/reiinakano/scikit-plot",
      "help_website": [
        "https://scikit-plot.readthedocs.io"
      ],
      "license": "MIT",
      "tags": [
        "visualization",
        "scikit-learn",
        "machine-learning",
        "plotting"
      ],
      "id": 577
    },
    {
      "name": "supervision",
      "one_line_profile": "Reusable computer vision tools for processing and visualization",
      "detailed_description": "A comprehensive library for computer vision tasks, providing utilities for filtering, processing, and visualizing detections, segmentations, and other vision data.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "visualization",
        "data_processing",
        "computer_vision"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/roboflow/supervision",
      "help_website": [
        "https://supervision.roboflow.com"
      ],
      "license": "MIT",
      "tags": [
        "computer-vision",
        "visualization",
        "object-detection",
        "metrics"
      ],
      "id": 578
    },
    {
      "name": "eulerian-remote-heartrate-detection",
      "one_line_profile": "Remote heart rate detection via Eulerian video magnification",
      "detailed_description": "A tool that implements Eulerian magnification to detect heart rates remotely from face videos, serving as a solver for physiological signal extraction.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "signal_processing",
        "physiological_measurement"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/rohintangirala/eulerian-remote-heartrate-detection",
      "help_website": [],
      "license": null,
      "tags": [
        "computer-vision",
        "signal-processing",
        "heart-rate",
        "eulerian-magnification"
      ],
      "id": 579
    },
    {
      "name": "deep_metric_learning",
      "one_line_profile": "Deep metric learning methods implemented in Chainer",
      "detailed_description": "A library implementing various deep metric learning algorithms and loss functions using the Chainer framework.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "metric_learning",
        "model_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ronekko/deep_metric_learning",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "metric-learning",
        "chainer",
        "deep-learning"
      ],
      "id": 580
    },
    {
      "name": "abcTau",
      "one_line_profile": "Unbiased estimation of timescales and hypothesis testing",
      "detailed_description": "A Python package for unbiased estimation of timescales from autocorrelation functions and performing hypothesis testing, useful in neuroscience and time series analysis.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_analysis",
        "time_series_analysis",
        "hypothesis_testing"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/roxana-zeraati/abcTau",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "neuroscience",
        "timescales",
        "statistics",
        "hypothesis-testing"
      ],
      "id": 581
    },
    {
      "name": "pylift",
      "one_line_profile": "Uplift modeling and evaluation library",
      "detailed_description": "A library designed for uplift modeling, providing tools for model training and evaluation to measure the incremental impact of treatments.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "causal_inference",
        "model_evaluation",
        "uplift_modeling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/rsyi/pylift",
      "help_website": [
        "https://pylift.readthedocs.io"
      ],
      "license": "BSD-2-Clause",
      "tags": [
        "uplift-modeling",
        "causal-inference",
        "machine-learning"
      ],
      "id": 582
    },
    {
      "name": "paired-perm-test",
      "one_line_profile": "Exact paired permutation significance test for accuracy",
      "detailed_description": "A Python implementation of the exact paired permutation test for comparing the accuracy of two classifiers, endorsed by Rycolab.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_testing",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/rycolab/paired-perm-test",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "statistics",
        "significance-test",
        "permutation-test",
        "nlp"
      ],
      "id": 583
    },
    {
      "name": "nbashots",
      "one_line_profile": "NBA shot chart visualization tool",
      "detailed_description": "A tool for creating and visualizing NBA shot charts using matplotlib, seaborn, and bokeh, facilitating sports analytics.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "visualization",
        "sports_analytics"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/savvastj/nbashots",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "visualization",
        "sports-analytics",
        "nba",
        "matplotlib"
      ],
      "id": 584
    },
    {
      "name": "pysepm",
      "one_line_profile": "Speech enhancement performance metrics implementation",
      "detailed_description": "A Python implementation of standard objective quality metrics for speech enhancement (e.g., PESQ, STOI) based on Loizou's book.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "signal_processing",
        "quality_metrics",
        "speech_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/schmiph2/pysepm",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "speech-enhancement",
        "metrics",
        "pesq",
        "stoi"
      ],
      "id": 585
    },
    {
      "name": "scikit-fuzzy",
      "one_line_profile": "Fuzzy logic toolkit for SciPy",
      "detailed_description": "A collection of fuzzy logic algorithms for use in Python, working on top of NumPy and SciPy.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "modeling",
        "fuzzy_logic"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/scikit-fuzzy/scikit-fuzzy",
      "help_website": [
        "https://pythonhosted.org/scikit-fuzzy/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "fuzzy-logic",
        "scipy",
        "control-systems"
      ],
      "id": 586
    },
    {
      "name": "pyhf",
      "one_line_profile": "Pure-Python HistFactory implementation with tensors and autodiff",
      "detailed_description": "A library for statistical modeling in High Energy Physics (HEP), implementing the HistFactory specification with support for automatic differentiation and hardware acceleration.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_modeling",
        "high_energy_physics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/scikit-hep/pyhf",
      "help_website": [
        "https://pyhf.readthedocs.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "physics",
        "statistics",
        "histfactory",
        "fitting"
      ],
      "id": 587
    },
    {
      "name": "forest-confidence-interval",
      "one_line_profile": "Confidence intervals for scikit-learn forest algorithms",
      "detailed_description": "A library that adds the capability to calculate confidence intervals for predictions made by scikit-learn's random forest regressors.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "uncertainty_estimation",
        "statistical_analysis"
      ],
      "application_level": "library",
      "primary_language": "HTML",
      "repo_url": "https://github.com/scikit-learn-contrib/forest-confidence-interval",
      "help_website": [
        "http://scikit-learn-contrib.github.io/forest-confidence-interval/"
      ],
      "license": "MIT",
      "tags": [
        "random-forest",
        "confidence-intervals",
        "uncertainty",
        "scikit-learn"
      ],
      "id": 588
    },
    {
      "name": "metric-learn",
      "one_line_profile": "Metric learning algorithms in Python",
      "detailed_description": "A Python module implementing various supervised and weakly supervised metric learning algorithms, compatible with scikit-learn.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "metric_learning",
        "dimensionality_reduction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/scikit-learn-contrib/metric-learn",
      "help_website": [
        "http://contrib.scikit-learn.org/metric-learn/"
      ],
      "license": "MIT",
      "tags": [
        "metric-learning",
        "scikit-learn",
        "machine-learning"
      ],
      "id": 589
    },
    {
      "name": "scikit-learn",
      "one_line_profile": "Machine learning in Python",
      "detailed_description": "A comprehensive machine learning library for Python, providing simple and efficient tools for data mining, data analysis, and modeling.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "machine_learning",
        "data_analysis",
        "modeling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/scikit-learn/scikit-learn",
      "help_website": [
        "https://scikit-learn.org"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "machine-learning",
        "data-science",
        "statistics"
      ],
      "id": 590
    },
    {
      "name": "scikit-optimize",
      "one_line_profile": "Sequential model-based optimization",
      "detailed_description": "A library for sequential model-based optimization, built on top of NumPy, SciPy, and scikit-learn, useful for hyperparameter tuning and black-box optimization.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "optimization",
        "hyperparameter_tuning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/scikit-optimize/scikit-optimize",
      "help_website": [
        "https://scikit-optimize.github.io/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "optimization",
        "bayesian-optimization",
        "hyperparameter-tuning"
      ],
      "id": 591
    },
    {
      "name": "scipy",
      "one_line_profile": "Scientific computing library for Python",
      "detailed_description": "A fundamental library for scientific computing in Python, providing algorithms for optimization, integration, interpolation, eigenvalue problems, algebraic equations, and more.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "scientific_computing",
        "mathematical_modeling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/scipy/scipy",
      "help_website": [
        "https://scipy.org"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "scientific-computing",
        "mathematics",
        "physics",
        "engineering"
      ],
      "id": 592
    },
    {
      "name": "SDMetrics",
      "one_line_profile": "Metrics library for evaluating the quality and efficacy of synthetic datasets",
      "detailed_description": "A Python library designed to evaluate synthetic data by comparing it with real data using various statistical metrics and machine learning efficacy tests.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "data_evaluation",
        "synthetic_data_metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/sdv-dev/SDMetrics",
      "help_website": [
        "https://docs.sdv.dev/sdmetrics"
      ],
      "license": "MIT",
      "tags": [
        "synthetic-data",
        "evaluation-metrics",
        "statistics"
      ],
      "id": 593
    },
    {
      "name": "PermTest",
      "one_line_profile": "Permutation algorithms for statistical significance testing",
      "detailed_description": "A C++ library implementing efficient permutation-based statistical tests to evaluate the significance of experimental results, particularly useful in information retrieval and machine learning.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_testing",
        "significance_test"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/searchivarius/PermTest",
      "help_website": [],
      "license": null,
      "tags": [
        "permutation-test",
        "statistics",
        "significance"
      ],
      "id": 594
    },
    {
      "name": "matchmaker",
      "one_line_profile": "Library for training and evaluating neural re-ranking and retrieval models",
      "detailed_description": "A PyTorch-based library for the training, evaluation, and analysis of dense retrieval and neural re-ranking models in information retrieval tasks.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "information_retrieval"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/sebastian-hofstaetter/matchmaker",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "neural-ir",
        "re-ranking",
        "evaluation"
      ],
      "id": 595
    },
    {
      "name": "mht",
      "one_line_profile": "Multiple Hypothesis Testing Procedure implementation",
      "detailed_description": "A MATLAB implementation of the Multiple Hypothesis Testing procedures described in List, Shaikh, and Xu (2015) for robust statistical inference.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_testing",
        "hypothesis_testing"
      ],
      "application_level": "library",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/seidelj/mht",
      "help_website": [],
      "license": null,
      "tags": [
        "statistics",
        "multiple-hypothesis",
        "matlab"
      ],
      "id": 596
    },
    {
      "name": "pycomets",
      "one_line_profile": "Significance testing for supervised learning with multimodal data",
      "detailed_description": "A Python package for performing algorithm-agnostic statistical significance testing on the performance of supervised learning models, specifically designed for multimodal data contexts.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_testing",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/shimenghuang/pycomets",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "significance-testing",
        "multimodal-learning",
        "statistics"
      ],
      "id": 597
    },
    {
      "name": "qstest",
      "one_line_profile": "Significance test for individual communities in networks",
      "detailed_description": "A Python implementation of a generalized significance test for evaluating the quality and statistical significance of individual communities detected in network data.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_testing",
        "network_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/skojaku/qstest",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "network-science",
        "community-detection",
        "significance-test"
      ],
      "id": 598
    },
    {
      "name": "skorch",
      "one_line_profile": "Scikit-learn compatible neural network library wrapping PyTorch",
      "detailed_description": "A library that wraps PyTorch to provide a scikit-learn compatible interface, facilitating the training, evaluation, and pipeline integration of neural networks.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_training",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/skorch-dev/skorch",
      "help_website": [
        "https://skorch.readthedocs.io/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "pytorch",
        "scikit-learn",
        "wrapper"
      ],
      "id": 599
    },
    {
      "name": "sktime",
      "one_line_profile": "Unified framework for machine learning with time series",
      "detailed_description": "A comprehensive library for time series analysis in Python, providing unified interfaces for various learning tasks including forecasting, classification, and regression, along with evaluation tools.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "time_series_analysis",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/sktime/sktime",
      "help_website": [
        "https://www.sktime.net/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "time-series",
        "machine-learning",
        "forecasting"
      ],
      "id": 600
    },
    {
      "name": "POPPER",
      "one_line_profile": "Automated Hypothesis Testing with Agentic Sequential Falsifications",
      "detailed_description": "A framework for automated scientific discovery that uses agentic sequential falsifications to test hypotheses, designed to support the scientific inference process.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "hypothesis_testing",
        "scientific_discovery"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/snap-stanford/POPPER",
      "help_website": [],
      "license": null,
      "tags": [
        "hypothesis-testing",
        "automated-discovery",
        "falsification"
      ],
      "id": 601
    },
    {
      "name": "snips-nlu-metrics",
      "one_line_profile": "Metrics for NLU intent parsing pipelines",
      "detailed_description": "A Python package providing metrics to evaluate the performance of Natural Language Understanding (NLU) intent parsing pipelines, including precision, recall, and f1-scores.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "nlp_metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/snipsco/snips-nlu-metrics",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "nlp",
        "metrics",
        "intent-parsing"
      ],
      "id": 602
    },
    {
      "name": "FPTaylor",
      "one_line_profile": "Rigorous estimation of round-off floating-point errors",
      "detailed_description": "A tool for the formal verification and rigorous estimation of round-off errors in floating-point computations, useful for ensuring numerical stability in scientific computing.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "numerical_analysis",
        "error_estimation"
      ],
      "application_level": "solver",
      "primary_language": "OCaml",
      "repo_url": "https://github.com/soarlab/FPTaylor",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "floating-point",
        "error-analysis",
        "formal-methods"
      ],
      "id": 603
    },
    {
      "name": "TidyDensity",
      "one_line_profile": "Tidy probability/density tibbles and plots in R",
      "detailed_description": "An R package that provides functions to generate and visualize probability distributions and density estimates in a tidy format, facilitating statistical analysis.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_analysis",
        "visualization"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/spsanderson/TidyDensity",
      "help_website": [
        "https://www.spsanderson.com/TidyDensity/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "r-package",
        "statistics",
        "probability-distributions"
      ],
      "id": 604
    },
    {
      "name": "pauc",
      "one_line_profile": "ROC AUC calculation with confidence intervals",
      "detailed_description": "A Python package to calculate the Area Under the ROC Curve (AUC) along with confidence intervals using DeLong's method, essential for rigorous model evaluation.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "statistical_metrics"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/srijitseal/pauc",
      "help_website": [],
      "license": null,
      "tags": [
        "roc-auc",
        "confidence-intervals",
        "delong-method"
      ],
      "id": 605
    },
    {
      "name": "recmetrics",
      "one_line_profile": "Metrics library for evaluating recommender systems",
      "detailed_description": "A Python library containing a suite of metrics specifically designed for evaluating the performance and quality of recommender systems.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "recommender_metrics"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/statisticianinstilettos/recmetrics",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "recommender-systems",
        "evaluation",
        "metrics"
      ],
      "id": 606
    },
    {
      "name": "dnn-inference",
      "one_line_profile": "Significance tests of feature relevance for black-box learners",
      "detailed_description": "A Python library for conducting statistical significance tests on feature relevance in deep neural networks and other black-box models, aiding in interpretability.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_testing",
        "feature_importance"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/statmlben/dnn-inference",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "significance-testing",
        "interpretability",
        "deep-learning"
      ],
      "id": 607
    },
    {
      "name": "statsmodels",
      "one_line_profile": "Statistical modeling and econometrics in Python",
      "detailed_description": "A comprehensive Python module that provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests and statistical data exploration.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_analysis",
        "statistical_modeling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/statsmodels/statsmodels",
      "help_website": [
        "https://www.statsmodels.org/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "statistics",
        "econometrics",
        "data-analysis"
      ],
      "id": 608
    },
    {
      "name": "XCurve",
      "one_line_profile": "Library for X-Curve metrics optimizations in machine learning",
      "detailed_description": "An end-to-end PyTorch library focused on optimizing and evaluating X-Curve metrics (like AUROC, AUPRC) for machine learning models, particularly in imbalance learning scenarios.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_optimization",
        "evaluation_metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/statusrank/XCurve",
      "help_website": [],
      "license": null,
      "tags": [
        "optimization",
        "metrics",
        "auroc"
      ],
      "id": 609
    },
    {
      "name": "mean-opinion-score",
      "one_line_profile": "Calculate MOS and confidence intervals for TTS ratings",
      "detailed_description": "A Python library for calculating the Mean Opinion Score (MOS) and its 95% confidence interval from text-to-speech ratings, implementing standard statistical methods.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_analysis",
        "audio_metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/stefantaubert/mean-opinion-score",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mos",
        "statistics",
        "tts"
      ],
      "id": 610
    },
    {
      "name": "nestedcv",
      "one_line_profile": "Nested cross-validation for prediction error confidence intervals",
      "detailed_description": "An R package implementing nested cross-validation procedures to provide accurate confidence intervals for prediction errors in statistical learning models.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_validation",
        "error_estimation"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/stephenbates19/nestedcv",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "cross-validation",
        "confidence-intervals",
        "r-package"
      ],
      "id": 611
    },
    {
      "name": "sjstats",
      "one_line_profile": "Effect size measures and significance tests in R",
      "detailed_description": "An R package providing a collection of convenient functions for common statistical computations, including effect size measures and significance tests.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_analysis",
        "effect_size"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/strengejacke/sjstats",
      "help_website": [],
      "license": null,
      "tags": [
        "r-package",
        "statistics",
        "significance-tests"
      ],
      "id": 612
    },
    {
      "name": "randomForestCI",
      "one_line_profile": "Confidence intervals for random forests (Deprecated)",
      "detailed_description": "An R package for calculating confidence intervals for predictions made by random forests. (Note: Deprecated in favor of 'grf' or 'ranger').",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_analysis",
        "uncertainty_quantification"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/swager/randomForestCI",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "random-forest",
        "confidence-intervals",
        "r-package"
      ],
      "id": 613
    },
    {
      "name": "ESSENCE",
      "one_line_profile": "Statistical significance evaluation for interferometric images",
      "detailed_description": "A Python package for evaluating the statistical significance of image analysis and signal detection under correlated noise, specifically for interferometric data (e.g., ALMA, NOEMA).",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_analysis",
        "image_analysis"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/takafumi291/ESSENCE",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "astronomy",
        "interferometry",
        "statistics"
      ],
      "id": 614
    },
    {
      "name": "GAN_Metrics-Tensorflow",
      "one_line_profile": "Tensorflow implementation of GAN evaluation metrics",
      "detailed_description": "A library providing Tensorflow implementations of standard metrics for evaluating Generative Adversarial Networks (GANs), including Inception Score, FID, and KID.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "generative_models"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/taki0112/GAN_Metrics-Tensorflow",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gan",
        "metrics",
        "tensorflow"
      ],
      "id": 615
    },
    {
      "name": "wer-sigtest",
      "one_line_profile": "Statistical significance test for ASR hypotheses",
      "detailed_description": "A script to perform statistical significance testing (e.g., bootstrap tests) between Automatic Speech Recognition (ASR) hypotheses, evaluating Word Error Rate (WER) differences.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_testing",
        "speech_recognition"
      ],
      "application_level": "solver",
      "primary_language": "Shell",
      "repo_url": "https://github.com/talhanai/wer-sigtest",
      "help_website": [],
      "license": null,
      "tags": [
        "asr",
        "significance-test",
        "wer"
      ],
      "id": 616
    },
    {
      "name": "Skflow",
      "one_line_profile": "Simplified interface for TensorFlow mimicking Scikit Learn",
      "detailed_description": "Scikit Flow (skflow) provides a simplified interface for TensorFlow, allowing users to build and train deep learning models using a syntax similar to Scikit Learn. It serves as a high-level wrapper to facilitate rapid prototyping and experimentation in machine learning research.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "model_training",
        "api_wrapper"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/tensorflow/skflow",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "tensorflow",
        "scikit-learn",
        "deep-learning",
        "wrapper"
      ],
      "id": 617
    },
    {
      "name": "financial-data-science",
      "one_line_profile": "Library for financial data science workflows and econometrics",
      "detailed_description": "A Python library designed to support financial data science workflows. It provides tools for managing large structured and unstructured datasets and applying financial econometrics and machine learning techniques for analysis and modeling.",
      "domains": [
        "AI4",
        "FinTech"
      ],
      "subtask_category": [
        "data_analysis",
        "econometrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/terence-lim/financial-data-science",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "finance",
        "econometrics",
        "data-science"
      ],
      "id": 618
    },
    {
      "name": "COTK",
      "one_line_profile": "Toolkit for fast development and fair evaluation of text generation",
      "detailed_description": "Conversational Toolkit (COTK) is an open-source library designed to facilitate the development and fair evaluation of text generation models. It provides standard metrics and benchmark datasets to ensure reproducible research in natural language processing.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "evaluation",
        "metrics",
        "text_generation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/thu-coai/cotk",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "nlp",
        "evaluation",
        "benchmark",
        "text-generation"
      ],
      "id": 619
    },
    {
      "name": "edaviz",
      "one_line_profile": "Library for Exploratory Data Analysis and Visualization in Jupyter",
      "detailed_description": "Edaviz is a Python library tailored for Exploratory Data Analysis (EDA) and visualization within Jupyter Notebook or Jupyter Lab environments. It aims to streamline the process of inspecting data distributions and relationships for data science tasks.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "visualization",
        "exploratory_data_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/tkrabel/edaviz",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "eda",
        "visualization",
        "jupyter",
        "data-analysis"
      ],
      "id": 620
    },
    {
      "name": "torch-fidelity",
      "one_line_profile": "High-fidelity performance metrics for generative models in PyTorch",
      "detailed_description": "Torch-fidelity is a library for calculating high-fidelity performance metrics for generative models, such as Inception Score (IS) and Fréchet Inception Distance (FID), within the PyTorch framework. It ensures precise and reproducible evaluation of generative AI models.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "evaluation",
        "metrics",
        "generative_models"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/toshas/torch-fidelity",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "pytorch",
        "metrics",
        "fid",
        "inception-score",
        "generative-ai"
      ],
      "id": 621
    },
    {
      "name": "statannotations",
      "one_line_profile": "Statistical significance annotations for seaborn plots",
      "detailed_description": "Statannotations is a Python library that adds statistical significance annotations (such as p-values) to plots generated by seaborn. It automates the process of conducting statistical tests and visualizing the results on boxplots, barplots, and other visualizations.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "visualization",
        "statistical_test"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/trevismd/statannotations",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "statistics",
        "visualization",
        "seaborn",
        "significance-testing"
      ],
      "id": 622
    },
    {
      "name": "sovereign",
      "one_line_profile": "Tools for state-dependent empirical analysis and forecasting",
      "detailed_description": "Sovereign is an R package providing tools for state-dependent empirical analysis, including state-dependent forecasts, impulse response functions, historical decomposition, and forecast error variance decomposition. It is designed for econometric and time-series research.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "forecasting",
        "time_series_analysis",
        "econometrics"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/tylerJPike/sovereign",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "econometrics",
        "forecasting",
        "time-series",
        "r-package"
      ],
      "id": 623
    },
    {
      "name": "deltacomp",
      "one_line_profile": "Analysis of compositional data with confidence intervals",
      "detailed_description": "Deltacomp is an R package containing functions to analyze compositional data. It specifically produces confidence intervals for relative increases and decreases in compositional components, aiding in statistical inference for compositional datasets.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_analysis",
        "confidence_intervals"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/tystan/deltacomp",
      "help_website": [],
      "license": "GPL-2.0",
      "tags": [
        "compositional-data",
        "statistics",
        "confidence-intervals",
        "r-package"
      ],
      "id": 624
    },
    {
      "name": "HyperLearn",
      "one_line_profile": "High-performance machine learning algorithms library",
      "detailed_description": "HyperLearn is a machine learning library designed for speed and efficiency, offering algorithms that are significantly faster and use less memory than traditional implementations. It aims to accelerate data science and scientific computing tasks on modern hardware.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "machine_learning",
        "optimization"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/unslothai/hyperlearn",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "machine-learning",
        "performance",
        "optimization",
        "acceleration"
      ],
      "id": 625
    },
    {
      "name": "KAIROS Scoring",
      "one_line_profile": "Scoring and analysis software for KAIROS evaluation",
      "detailed_description": "Software developed by NIST for the scoring and analysis of the Knowledge Directed Artificial Intelligence Reasoning Over Schemas (KAIROS) program. It provides tools for evaluating AI reasoning capabilities against defined schemas.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "evaluation",
        "scoring"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/usnistgov/KAIROS",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "evaluation",
        "nist",
        "ai-reasoning",
        "scoring"
      ],
      "id": 626
    },
    {
      "name": "Errudite",
      "one_line_profile": "Interactive tool for scalable and reproducible error analysis",
      "detailed_description": "Errudite is an interactive tool designed for scalable and reproducible error analysis in NLP models. It allows researchers to group and analyze error patterns to better understand model failure modes.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "error_analysis",
        "evaluation"
      ],
      "application_level": "tool",
      "primary_language": "Python",
      "repo_url": "https://github.com/uwdata/errudite",
      "help_website": [],
      "license": "GPL-2.0",
      "tags": [
        "nlp",
        "error-analysis",
        "visualization",
        "debugging"
      ],
      "id": 627
    },
    {
      "name": "ulab",
      "one_line_profile": "Numpy-like fast vector module for MicroPython",
      "detailed_description": "ulab is a numpy-like fast vector module written in C for MicroPython and CircuitPython. It enables efficient numerical and scientific computing on embedded systems and microcontrollers.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "numerical_computing",
        "embedded_computing"
      ],
      "application_level": "library",
      "primary_language": "C",
      "repo_url": "https://github.com/v923z/micropython-ulab",
      "help_website": [
        "https://micropython-ulab.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "micropython",
        "numpy",
        "numerical-computing",
        "embedded"
      ],
      "id": 628
    },
    {
      "name": "open-rag-eval",
      "one_line_profile": "RAG evaluation tool without golden answers",
      "detailed_description": "open-rag-eval is a library for evaluating Retrieval-Augmented Generation (RAG) systems without the need for ground truth 'golden answers'. It provides metrics to assess the quality of retrieved context and generated responses.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "evaluation",
        "metrics",
        "rag"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/vectara/open-rag-eval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "evaluation",
        "llm",
        "metrics"
      ],
      "id": 629
    },
    {
      "name": "HyPhy",
      "one_line_profile": "Hypothesis testing using Phylogenies",
      "detailed_description": "HyPhy (Hypothesis Testing using Phylogenies) is a software package for the analysis of genetic sequences using techniques from phylogenetics, molecular evolution, and machine learning. It is widely used for detecting natural selection and evolutionary modeling.",
      "domains": [
        "AI4",
        "Bio"
      ],
      "subtask_category": [
        "phylogenetics",
        "hypothesis_testing",
        "evolutionary_analysis"
      ],
      "application_level": "solver",
      "primary_language": "HyPhy",
      "repo_url": "https://github.com/veg/hyphy",
      "help_website": [
        "http://hyphy.org/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "bioinformatics",
        "phylogenetics",
        "evolution",
        "hypothesis-testing"
      ],
      "id": 630
    },
    {
      "name": "RC-FIAP",
      "one_line_profile": "Platform for seismic vulnerability evaluation of reinforced concrete frames",
      "detailed_description": "RC-FIAP is an open virtual platform for evaluating the seismic vulnerability of reinforced concrete frames. Built on OpenSeesPy, it facilitates performance-based earthquake engineering, risk assessment, and fragility analysis of structural archetypes.",
      "domains": [
        "AI4",
        "CivilEng"
      ],
      "subtask_category": [
        "simulation",
        "risk_assessment",
        "structural_analysis"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/vfceball/RC-FIAP",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "seismic-analysis",
        "civil-engineering",
        "opensees",
        "simulation"
      ],
      "id": 631
    },
    {
      "name": "PyUMLS_Similarity",
      "one_line_profile": "Similarity metrics for UMLS concepts",
      "detailed_description": "PyUMLS_Similarity is a package that computes various similarity metrics between concepts in the Unified Medical Language System (UMLS). It serves as an interface to UMLS and supports biomedical informatics research.",
      "domains": [
        "AI4",
        "Bio"
      ],
      "subtask_category": [
        "similarity_calculation",
        "metrics",
        "biomedical_informatics"
      ],
      "application_level": "library",
      "primary_language": "Perl",
      "repo_url": "https://github.com/victormurcia/PyUMLS_Similarity",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "umls",
        "similarity-metrics",
        "biomedical",
        "perl"
      ],
      "id": 632
    },
    {
      "name": "marginaleffects",
      "one_line_profile": "R package for model predictions, comparisons, and hypothesis tests",
      "detailed_description": "marginaleffects is an R package to compute and plot predictions, slopes, marginal means, and comparisons for over 100 classes of statistical and ML models. It supports linear and non-linear hypothesis tests and uncertainty estimation.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "statistical_analysis",
        "hypothesis_testing",
        "inference"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/vincentarelbundock/marginaleffects",
      "help_website": [
        "https://vincentarelbundock.github.io/marginaleffects/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "statistics",
        "r-package",
        "hypothesis-testing",
        "marginal-effects"
      ],
      "id": 633
    },
    {
      "name": "MagnetLoss-PyTorch",
      "one_line_profile": "PyTorch implementation of Magnet Loss for deep metric learning",
      "detailed_description": "A PyTorch implementation of Magnet Loss, a deep metric learning technique. This repository provides a reusable component for training models with advanced metric learning objectives.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "metric_learning",
        "loss_function"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/vithursant/MagnetLoss-PyTorch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pytorch",
        "metric-learning",
        "loss-function",
        "deep-learning"
      ],
      "id": 634
    },
    {
      "name": "Hemm",
      "one_line_profile": "Holistic evaluation library for multi-modal generative models",
      "detailed_description": "Hemm is a library for the holistic evaluation of multi-modal generative models. It integrates with Weave to provide comprehensive metrics and tracking for generative AI research.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "evaluation",
        "metrics",
        "multimodal_ai"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/wandb/Hemm",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "evaluation",
        "generative-ai",
        "multimodal",
        "metrics"
      ],
      "id": 635
    },
    {
      "name": "pretty-print-confusion-matrix",
      "one_line_profile": "Utility for plotting confusion matrices in Python",
      "detailed_description": "A Python utility to plot aesthetically pleasing confusion matrices using seaborn and matplotlib, similar to Matlab's style. It aids in the visualization of classification model performance metrics.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "visualization",
        "metrics_visualization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/wcipriano/pretty-print-confusion-matrix",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "confusion-matrix",
        "visualization",
        "python",
        "matplotlib"
      ],
      "id": 636
    },
    {
      "name": "statannot",
      "one_line_profile": "Statistical annotations for seaborn boxplots",
      "detailed_description": "Statannot is a Python package that adds statistical significance annotations (p-values) to existing boxplots generated by seaborn. It helps in visualizing the results of statistical tests directly on data plots.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "visualization",
        "statistical_test"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/webermarcolivier/statannot",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "statistics",
        "visualization",
        "seaborn",
        "p-value"
      ],
      "id": 637
    },
    {
      "name": "LangKit",
      "one_line_profile": "Open-source toolkit for monitoring and evaluating Large Language Models (LLMs)",
      "detailed_description": "A comprehensive toolkit designed for monitoring LLMs by extracting signals from prompts and responses. It provides metrics for text quality, relevance, sentiment analysis, and safety, ensuring the security and reliability of LLM applications.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "monitoring",
        "text_quality_metrics",
        "sentiment_analysis"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/whylabs/langkit",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "observability",
        "nlp",
        "metrics"
      ],
      "id": 638
    },
    {
      "name": "neleval",
      "one_line_profile": "Evaluation and error analysis tool for Named Entity Linking (NEL) systems",
      "detailed_description": "A tool designed to evaluate entity disambiguation and named entity linking systems. It facilitates error analysis and provides standard metrics to assess the performance of NEL models.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "entity_linking",
        "evaluation",
        "error_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/wikilinks/neleval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "nel",
        "nlp",
        "evaluation",
        "disambiguation"
      ],
      "id": 639
    },
    {
      "name": "wmt-format-tools",
      "one_line_profile": "Tools for formatting WMT hypothesis and test sets",
      "detailed_description": "A collection of utilities for processing and formatting data for the Workshop on Machine Translation (WMT). It handles XML conversion and standardization of hypothesis and test sets for translation benchmarks.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "data_formatting",
        "benchmark_preparation"
      ],
      "application_level": "workflow",
      "primary_language": "Hare",
      "repo_url": "https://github.com/wmt-conference/wmt-format-tools",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "wmt",
        "machine-translation",
        "xml",
        "formatting"
      ],
      "id": 640
    },
    {
      "name": "bm25s",
      "one_line_profile": "Fast Python implementation of BM25 algorithm for lexical search",
      "detailed_description": "A high-performance implementation of the BM25 ranking function using Numpy, Numba, and Scipy. It is designed for fast lexical search and information retrieval tasks in Python.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "information_retrieval",
        "ranking",
        "search"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/xhluca/bm25s",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bm25",
        "search",
        "ranking",
        "numba"
      ],
      "id": 641
    },
    {
      "name": "AB3DMOT",
      "one_line_profile": "3D Multi-Object Tracking baseline and evaluation metrics",
      "detailed_description": "An official implementation providing a baseline for 3D Multi-Object Tracking (MOT) along with new evaluation metrics. It serves as a standard benchmark tool for autonomous driving perception tasks.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "object_tracking",
        "benchmark",
        "evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/xinshuoweng/AB3DMOT",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "3d-tracking",
        "autonomous-driving",
        "metrics",
        "benchmark"
      ],
      "id": 642
    },
    {
      "name": "MLmetrics",
      "one_line_profile": "Collection of Machine Learning evaluation metrics for R",
      "detailed_description": "An R package that provides a comprehensive set of standard evaluation metrics for machine learning tasks, including classification and regression performance indicators.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_evaluation",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/yanyachen/MLmetrics",
      "help_website": [],
      "license": "GPL-2.0",
      "tags": [
        "r",
        "machine-learning",
        "evaluation",
        "metrics"
      ],
      "id": 643
    },
    {
      "name": "summary-reward-no-reference",
      "one_line_profile": "Reference-free metric for measuring text summary quality",
      "detailed_description": "A Python implementation of a reference-free metric for evaluating the quality of text summaries. The metric is learned from human ratings, allowing for quality assessment without ground truth references.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "text_summarization",
        "evaluation_metric"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/yg211/summary-reward-no-reference",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "nlp",
        "summarization",
        "metrics",
        "reference-free"
      ],
      "id": 644
    },
    {
      "name": "AlignScore",
      "one_line_profile": "Metric for factual consistency evaluation in text generation",
      "detailed_description": "An implementation of AlignScore, a metric designed to evaluate the factual consistency of generated text against source information, addressing hallucination issues in NLP models.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "factual_consistency",
        "evaluation_metric",
        "hallucination_detection"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/yuh-zha/AlignScore",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nlp",
        "consistency",
        "metrics",
        "hallucination"
      ],
      "id": 645
    },
    {
      "name": "ashpy",
      "one_line_profile": "TensorFlow 2.0 library for distributed training and evaluation",
      "detailed_description": "A library built on TensorFlow 2.0 that facilitates distributed training, evaluation, model selection, and fast prototyping of deep learning models.",
      "domains": [
        "AI4",
        "AI4-02"
      ],
      "subtask_category": [
        "model_training",
        "evaluation",
        "model_selection"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/zurutech/ashpy",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "tensorflow",
        "training",
        "evaluation",
        "deep-learning"
      ],
      "id": 646
    },
    {
      "name": "GraphRAG Agent Framework",
      "one_line_profile": "Integrated framework for GraphRAG construction, search, and custom evaluation",
      "detailed_description": "A comprehensive framework integrating GraphRAG, LightRAG, and Neo4j for knowledge graph construction and search, featuring a custom evaluation framework for assessing GraphRAG performance.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "evaluation",
        "rag_framework"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/1517005260/graph-rag-agent",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "graph-rag",
        "evaluation-framework"
      ],
      "id": 647
    },
    {
      "name": "lm-evaluation",
      "one_line_profile": "Evaluation suite for large-scale language models",
      "detailed_description": "A suite of tools and metrics for evaluating the performance of large-scale language models, developed by AI21 Labs.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "nlp_metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/AI21Labs/lm-evaluation",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "evaluation",
        "nlp"
      ],
      "id": 648
    },
    {
      "name": "AutoQuant",
      "one_line_profile": "Automation framework for ML, forecasting, and model evaluation",
      "detailed_description": "An R framework for automating machine learning tasks, time series forecasting, model evaluation, and interpretation.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "automl",
        "forecasting",
        "model_evaluation"
      ],
      "application_level": "framework",
      "primary_language": "R",
      "repo_url": "https://github.com/AdrianAntico/AutoQuant",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "automl",
        "forecasting",
        "evaluation"
      ],
      "id": 649
    },
    {
      "name": "Agenta",
      "one_line_profile": "Open-source LLMOps platform for prompt management and evaluation",
      "detailed_description": "A platform for LLM operations that includes features for prompt playground, management, LLM evaluation, and observability.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "llmops",
        "model_evaluation",
        "prompt_engineering"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/Agenta-AI/agenta",
      "help_website": [
        "https://agenta.ai"
      ],
      "license": "NOASSERTION",
      "tags": [
        "llmops",
        "evaluation",
        "observability"
      ],
      "id": 650
    },
    {
      "name": "Bjontegaard Metric",
      "one_line_profile": "Calculation tool for Bjontegaard metric (BD-PSNR and BD-rate)",
      "detailed_description": "A Python implementation for calculating the Bjontegaard metric, including BD-PSNR and BD-rate, commonly used for evaluating video compression performance.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "metric_calculation",
        "video_compression_eval"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Anserw/Bjontegaard_metric",
      "help_website": [],
      "license": null,
      "tags": [
        "video-coding",
        "metrics",
        "bd-rate"
      ],
      "id": 651
    },
    {
      "name": "cell-eval",
      "one_line_profile": "Evaluation suite for perturbation prediction models",
      "detailed_description": "A comprehensive suite for evaluating models designed to predict cellular perturbations, developed by Arc Institute.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "bioinformatics_eval",
        "perturbation_prediction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ArcInstitute/cell-eval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "biology",
        "perturbation",
        "evaluation"
      ],
      "id": 652
    },
    {
      "name": "BigDataBench MicroBenchmark",
      "one_line_profile": "Micro-benchmark suite for Big Data systems",
      "detailed_description": "A micro-benchmark suite from BigDataBench V5.0 for evaluating the performance of big data systems and components.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "benchmarking",
        "system_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Java",
      "repo_url": "https://github.com/BenchCouncil/BigDataBench_V5.0_BigData_MicroBenchmark",
      "help_website": [
        "http://www.benchcouncil.org/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "big-data",
        "benchmark",
        "microbenchmark"
      ],
      "id": 653
    },
    {
      "name": "Ethereum Economic Model",
      "one_line_profile": "Dynamical-systems model of Ethereum validator economics",
      "detailed_description": "A modular dynamical-systems model for simulating and analyzing the economics of Ethereum validators.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "economic_simulation",
        "system_modeling"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/CADLabs/ethereum-economic-model",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "ethereum",
        "economics",
        "simulation"
      ],
      "id": 654
    },
    {
      "name": "Gadget",
      "one_line_profile": "Benchmark harness for streaming state stores",
      "detailed_description": "A benchmark harness designed for the systematic and robust evaluation of streaming state stores in data systems.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "benchmarking",
        "streaming_systems"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/CASP-Systems-BU/Gadget",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "streaming",
        "benchmark",
        "state-store"
      ],
      "id": 655
    },
    {
      "name": "CS-Eval",
      "one_line_profile": "Evaluation suite for cybersecurity models",
      "detailed_description": "A comprehensive evaluation suite for assessing the cybersecurity capabilities of fundamental models or large language models.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "cybersecurity_eval"
      ],
      "application_level": "framework",
      "primary_language": null,
      "repo_url": "https://github.com/CS-EVAL/CS-Eval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "cybersecurity",
        "llm",
        "evaluation"
      ],
      "id": 656
    },
    {
      "name": "Matcha (VariationAnalysis)",
      "one_line_profile": "Framework for training and evaluating genomic variation models",
      "detailed_description": "The Matcha framework, part of the VariationAnalysis project, used to train and evaluate deep learning models for calling genomic variations.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "genomics_eval",
        "model_training"
      ],
      "application_level": "framework",
      "primary_language": "Java",
      "repo_url": "https://github.com/CampagneLaboratory/variationanalysis",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "genomics",
        "deep-learning",
        "variation-calling"
      ],
      "id": 657
    },
    {
      "name": "SURE",
      "one_line_profile": "Library for assessing synthetic tabular data utility and privacy",
      "detailed_description": "An open-source Python library for evaluating the utility and privacy performance of tabular synthetic datasets.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "data_evaluation",
        "synthetic_data"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Clearbox-AI/SURE",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "synthetic-data",
        "evaluation",
        "privacy"
      ],
      "id": 658
    },
    {
      "name": "AI Agents Reality Check",
      "one_line_profile": "Mathematical benchmark for AI agent performance",
      "detailed_description": "A benchmark suite that exposes the performance gap between real agents and LLM wrappers through rigorous multi-dimensional evaluation and statistical validation.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "agent_evaluation",
        "benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Cre4T3Tiv3/ai-agents-reality-check",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ai-agents",
        "benchmark",
        "evaluation"
      ],
      "id": 659
    },
    {
      "name": "DMind Benchmark",
      "one_line_profile": "Benchmark for LLMs on blockchain and Web3 knowledge",
      "detailed_description": "A comprehensive framework for evaluating large language models on their knowledge of blockchain, cryptocurrency, and Web3 domains.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "domain_evaluation",
        "llm_benchmark"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/DMindAI/DMind-Benchmark",
      "help_website": [],
      "license": null,
      "tags": [
        "blockchain",
        "llm",
        "benchmark"
      ],
      "id": 660
    },
    {
      "name": "Benchmarking Big Streams Systems",
      "one_line_profile": "Benchmark suite for big streaming systems",
      "detailed_description": "An extension of Yahoo!'s benchmarking suite designed for evaluating the performance of big streaming data systems.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "benchmarking",
        "streaming_systems"
      ],
      "application_level": "solver",
      "primary_language": "Java",
      "repo_url": "https://github.com/DataSystemsGroupUT/Benchmarking-Big-Streams-Systems",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "streaming",
        "benchmark",
        "big-data"
      ],
      "id": 661
    },
    {
      "name": "DreamLayer",
      "one_line_profile": "Benchmarking and evaluation automation tool for diffusion models",
      "detailed_description": "A tool designed to benchmark diffusion models faster by automating evaluations, seed management, and metric calculation to ensure reproducible results.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking",
        "reproducibility"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/DreamLayer-AI/DreamLayer",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "diffusion-models",
        "benchmarking",
        "evaluation-harness"
      ],
      "id": 662
    },
    {
      "name": "E3SM",
      "one_line_profile": "Energy Exascale Earth System Model",
      "detailed_description": "The Energy Exascale Earth System Model (E3SM) is a state-of-the-art earth system modeling project designed to simulate the earth's climate system at high resolution.",
      "domains": [
        "AI4",
        "Earth Science"
      ],
      "subtask_category": [
        "climate_modeling",
        "simulation"
      ],
      "application_level": "solver",
      "primary_language": "Fortran",
      "repo_url": "https://github.com/E3SM-Project/E3SM",
      "help_website": [
        "https://e3sm.org/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "earth-system-model",
        "climate-simulation",
        "exascale"
      ],
      "id": 663
    },
    {
      "name": "lm-evaluation-harness",
      "one_line_profile": "Few-shot evaluation framework for language models",
      "detailed_description": "A framework for few-shot evaluation of language models, providing a unified interface to benchmark models on a wide variety of tasks.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/EleutherAI/lm-evaluation-harness",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "evaluation",
        "few-shot",
        "nlp"
      ],
      "id": 664
    },
    {
      "name": "fdsvismap",
      "one_line_profile": "Visibility verification tool for fire safety assessment",
      "detailed_description": "A tool for waypoint-based verification of visibility within the scope of performance-based fire safety assessment, likely interfacing with Fire Dynamics Simulator (FDS) data.",
      "domains": [
        "AI4",
        "Physics"
      ],
      "subtask_category": [
        "safety_assessment",
        "simulation_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/FireDynamics/fdsvismap",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fire-dynamics",
        "safety-engineering",
        "visibility-analysis"
      ],
      "id": 665
    },
    {
      "name": "llm-benchmarker-suite",
      "one_line_profile": "Leaderboard and benchmarking suite for LLM evaluations",
      "detailed_description": "A suite designed for benchmarking Large Language Models, providing tools to run evaluations and generate leaderboard rankings.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/FormulaMonks/llm-benchmarker-suite",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "leaderboard",
        "benchmarking"
      ],
      "id": 666
    },
    {
      "name": "LLMZoo",
      "one_line_profile": "Data, models, and evaluation benchmarks for LLMs",
      "detailed_description": "A project providing a collection of data, models, and evaluation benchmarks specifically for Large Language Models, facilitating comparative analysis.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "dataset_management"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/FreedomIntelligence/LLMZoo",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "benchmark",
        "model-zoo"
      ],
      "id": 667
    },
    {
      "name": "gee_landcover_metrics",
      "one_line_profile": "Land cover metrics calculation for Google Earth Engine",
      "detailed_description": "A tool for calculating landscape metrics on land cover data within the Google Earth Engine platform, supporting spatial analysis in earth sciences.",
      "domains": [
        "AI4",
        "Earth Science"
      ],
      "subtask_category": [
        "spatial_analysis",
        "metrics_calculation"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/Helmholtz-UFZ/gee_landcover_metrics",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "google-earth-engine",
        "land-cover",
        "landscape-metrics"
      ],
      "id": 668
    },
    {
      "name": "UHGEval",
      "one_line_profile": "Evaluation framework for hallucination in LLMs",
      "detailed_description": "A user-friendly evaluation framework and benchmark suite (including HaluEval, HalluQA) designed to assess hallucination generation in Large Language Models.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "hallucination_detection",
        "model_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/IAAR-Shanghai/UHGEval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "hallucination",
        "llm-evaluation",
        "benchmark"
      ],
      "id": 669
    },
    {
      "name": "GraFiTe",
      "one_line_profile": "Platform for tracking domain-specific model issues",
      "detailed_description": "A platform to track and manage domain-specific model issues for continuous evaluation of Large Language Models.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "issue_tracking",
        "continuous_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/IBM/grafite",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-evaluation",
        "model-governance",
        "issue-tracking"
      ],
      "id": 670
    },
    {
      "name": "FakeFinder",
      "one_line_profile": "Framework for evaluating deepfake detection models",
      "detailed_description": "A modular framework for evaluating various deepfake detection models, providing tools to assess model performance against manipulated media.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "deepfake_detection",
        "model_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/IQTLabs/FakeFinder",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "deepfake",
        "evaluation-framework",
        "media-forensics"
      ],
      "id": 671
    },
    {
      "name": "CGM_Performance_Assessment",
      "one_line_profile": "Statistical performance assessment for CGM systems",
      "detailed_description": "A collection of software packages for the statistical performance assessment of Continuous Glucose Monitoring (CGM) systems, used in medical data analysis.",
      "domains": [
        "AI4",
        "Life Science"
      ],
      "subtask_category": [
        "statistical_analysis",
        "performance_assessment"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/IfDTUlm/CGM_Performance_Assessment",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "cgm",
        "diabetes-technology",
        "statistical-analysis"
      ],
      "id": 672
    },
    {
      "name": "Deepmark",
      "one_line_profile": "Testing environment for LLM assessment",
      "detailed_description": "A testing environment enabling the assessment of language models (LLMs) on task-specific metrics and custom data for predictable performance.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "performance_testing"
      ],
      "application_level": "platform",
      "primary_language": "PHP",
      "repo_url": "https://github.com/IngestAI/deepmark",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "llm-testing",
        "metrics",
        "evaluation"
      ],
      "id": 673
    },
    {
      "name": "Factorio Learning Environment",
      "one_line_profile": "Environment for evaluating LLMs in Factorio",
      "detailed_description": "A non-saturating, open-ended environment designed for evaluating Large Language Models and Reinforcement Learning agents within the Factorio game simulation.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "rl_environment",
        "agent_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/JackHopkins/factorio-learning-environment",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "reinforcement-learning",
        "factorio",
        "benchmark-environment"
      ],
      "id": 674
    },
    {
      "name": "ModelMetrics",
      "one_line_profile": "R package for rapid calculation of model metrics",
      "detailed_description": "An R library designed to facilitate the rapid calculation of various statistical model performance metrics.",
      "domains": [
        "AI4",
        "Statistics"
      ],
      "subtask_category": [
        "statistical_analysis",
        "metrics_calculation"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/JackStat/ModelMetrics",
      "help_website": [],
      "license": null,
      "tags": [
        "r-package",
        "statistics",
        "model-evaluation"
      ],
      "id": 675
    },
    {
      "name": "DetectionMetrics",
      "one_line_profile": "Evaluation tool for perception models",
      "detailed_description": "A tool designed to unify and streamline the evaluation of perception (object detection) models across different frameworks and datasets.",
      "domains": [
        "AI4",
        "Computer Vision"
      ],
      "subtask_category": [
        "object_detection",
        "model_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/JdeRobot/DetectionMetrics",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "computer-vision",
        "object-detection",
        "metrics"
      ],
      "id": 676
    },
    {
      "name": "MixEval",
      "one_line_profile": "Evaluation suite for MixEval benchmark",
      "detailed_description": "The official evaluation suite and dynamic data release for the MixEval benchmark, designed for assessing language model performance.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/JinjieNi/MixEval",
      "help_website": [],
      "license": null,
      "tags": [
        "llm",
        "benchmark",
        "evaluation-suite"
      ],
      "id": 677
    },
    {
      "name": "LLM-Writing-Assessment-Psychometric-Framework",
      "one_line_profile": "Psychometric framework for evaluating LLMs as raters",
      "detailed_description": "A repository and framework for evaluating large language models when used as raters in large-scale writing assessments, focusing on reliability and validity from a psychometric perspective.",
      "domains": [
        "AI4",
        "Social Science"
      ],
      "subtask_category": [
        "psychometrics",
        "model_evaluation",
        "reliability_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/John-Wang-0809/LLM-Writing-Assessment-Psychometric-Framework",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "psychometrics",
        "llm-evaluation",
        "automated-scoring"
      ],
      "id": 678
    },
    {
      "name": "multihop-edit-eval",
      "one_line_profile": "Fine-grained evaluation framework for multi-hop knowledge editing in LLMs",
      "detailed_description": "A specialized evaluation framework designed to assess the performance of Large Language Models in multi-hop knowledge editing tasks. It provides metrics and datasets to measure how well models can update their knowledge base across connected facts.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "knowledge_editing_assessment"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/KUNLP/multihop-edit-eval",
      "help_website": [],
      "license": null,
      "tags": [
        "llm-evaluation",
        "knowledge-editing",
        "multi-hop-reasoning"
      ],
      "id": 679
    },
    {
      "name": "q-evaluation-harness",
      "one_line_profile": "Evaluation framework for LLMs on Q/kdb+ code generation tasks",
      "detailed_description": "An open-source framework developed by KX Systems to evaluate the performance of Large Language Models specifically on generating Q/kdb+ code. It serves as a domain-specific benchmark harness for financial time-series database languages.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "code_generation_evaluation",
        "llm_benchmark"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/KxSystems/q-evaluation-harness",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "q",
        "kdb+",
        "llm-evaluation",
        "code-generation"
      ],
      "id": 680
    },
    {
      "name": "aac-metrics",
      "one_line_profile": "Metrics library for evaluating Automated Audio Captioning systems",
      "detailed_description": "A Python library designed for PyTorch that implements various metrics for evaluating Automated Audio Captioning (AAC) systems. It provides a standardized way to measure the quality of generated audio captions against reference annotations.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "audio_captioning_evaluation",
        "metrics_calculation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Labbeti/aac-metrics",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "audio-captioning",
        "evaluation-metrics",
        "pytorch"
      ],
      "id": 681
    },
    {
      "name": "eurybia",
      "one_line_profile": "Model drift monitoring and data validation library",
      "detailed_description": "A Python library designed to monitor machine learning model drift over time and secure model deployment through rigorous data validation. It helps in maintaining the reliability of AI systems in production environments.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_monitoring",
        "drift_detection",
        "data_validation"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/MAIF/eurybia",
      "help_website": [
        "https://eurybia.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "model-drift",
        "data-validation",
        "mlops"
      ],
      "id": 682
    },
    {
      "name": "VideoEval",
      "one_line_profile": "Benchmark suite for evaluation of Video Foundation Models",
      "detailed_description": "A comprehensive benchmark suite designed for the low-cost and efficient evaluation of Video Foundation Models. It provides a set of tasks and metrics to assess the capabilities of video understanding and generation models.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "video_model_evaluation",
        "benchmark_suite"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/MCG-NJU/VideoEval",
      "help_website": [],
      "license": null,
      "tags": [
        "video-understanding",
        "foundation-models",
        "benchmark"
      ],
      "id": 683
    },
    {
      "name": "MHKiT-MATLAB",
      "one_line_profile": "Marine and Hydrokinetic Toolkit (MATLAB)",
      "detailed_description": "A MATLAB toolkit providing standardized data processing, visualization, quality control, and resource assessment tools for the marine renewable energy community. It supports wave, tidal, and river energy research.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "data_processing",
        "resource_assessment",
        "quality_control"
      ],
      "application_level": "library",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/MHKiT-Software/MHKiT-MATLAB",
      "help_website": [
        "https://mhkit-software.github.io/MHKiT/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "marine-energy",
        "hydrokinetic",
        "data-analysis"
      ],
      "id": 684
    },
    {
      "name": "MHKiT-Python",
      "one_line_profile": "Marine and Hydrokinetic Toolkit (Python)",
      "detailed_description": "A Python toolkit providing standardized data processing, visualization, quality control, and resource assessment tools for the marine renewable energy community. It serves as the Python counterpart to MHKiT-MATLAB.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "data_processing",
        "resource_assessment",
        "quality_control"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/MHKiT-Software/MHKiT-Python",
      "help_website": [
        "https://mhkit-software.github.io/MHKiT/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "marine-energy",
        "hydrokinetic",
        "data-analysis"
      ],
      "id": 685
    },
    {
      "name": "AutoRAG",
      "one_line_profile": "AutoML-style framework for RAG evaluation and optimization",
      "detailed_description": "An open-source framework designed to evaluate and optimize Retrieval-Augmented Generation (RAG) pipelines. It applies AutoML principles to automatically find the best RAG configuration for a given dataset and use case.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "rag_evaluation",
        "pipeline_optimization",
        "automl"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/Marker-Inc-Korea/AutoRAG",
      "help_website": [
        "https://docs.autorag.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "evaluation",
        "automl",
        "optimization"
      ],
      "id": 686
    },
    {
      "name": "ContainerInception (Gerber)",
      "one_line_profile": "Generalized Easy Reproducible Bioinformatics Environment wRapper",
      "detailed_description": "A tool developed during NCBI Hackathons to facilitate reproducible bioinformatics research by wrapping environments in containers. It aims to simplify the creation and sharing of reproducible scientific workflows.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "reproducibility",
        "workflow_management",
        "containerization"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/NCBI-Hackathons/ContainerInception",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bioinformatics",
        "reproducibility",
        "docker"
      ],
      "id": 687
    },
    {
      "name": "OpenOA",
      "one_line_profile": "Framework for assessing wind plant performance",
      "detailed_description": "An open-source framework developed by NREL for assessing wind plant performance using operational assessment (OA) methodologies. It provides data structures and analysis methods for processing time-series data from wind plants.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "performance_assessment",
        "data_analysis",
        "wind_energy"
      ],
      "application_level": "framework",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/NREL/OpenOA",
      "help_website": [
        "https://openoa.readthedocs.io/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "wind-energy",
        "operational-assessment",
        "nrel"
      ],
      "id": 688
    },
    {
      "name": "compute-eval",
      "one_line_profile": "Evaluation framework for LLM-based CUDA code generation",
      "detailed_description": "A framework by NVIDIA designed to generate and evaluate CUDA code produced by Large Language Models. It provides tools to assess the correctness and performance of AI-generated GPU kernels.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "code_generation_evaluation",
        "cuda_benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/NVIDIA/compute-eval",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "cuda",
        "llm",
        "code-generation",
        "evaluation"
      ],
      "id": 689
    },
    {
      "name": "atropos",
      "one_line_profile": "LLM Reinforcement Learning Environments framework",
      "detailed_description": "A framework for collecting and evaluating Large Language Model (LLM) trajectories through diverse environments. It serves as a testbed for Reinforcement Learning with LLMs.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "rlhf_evaluation",
        "trajectory_collection",
        "environment_simulation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/NousResearch/atropos",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "reinforcement-learning",
        "evaluation-environment"
      ],
      "id": 690
    },
    {
      "name": "gptables",
      "one_line_profile": "Tool for writing consistently formatted statistical tables",
      "detailed_description": "A Python wrapper around XlsxWriter developed by the Office for National Statistics (ONS) to produce consistently formatted statistical tables in Excel, ensuring data reporting standards.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "data_reporting",
        "statistical_formatting"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ONSdigital/gptables",
      "help_website": [
        "https://gptables.readthedocs.io/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "statistics",
        "reporting",
        "excel",
        "ons"
      ],
      "id": 691
    },
    {
      "name": "OneIG-Benchmark",
      "one_line_profile": "Fine-grained evaluation benchmark for Text-to-Image models",
      "detailed_description": "A comprehensive benchmark framework designed for the fine-grained evaluation of Text-to-Image (T2I) models. It assesses dimensions such as subject-element alignment, text rendering precision, reasoning, stylization, and diversity.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "t2i_evaluation",
        "image_generation_benchmark"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/OneIG-Bench/OneIG-Benchmark",
      "help_website": [],
      "license": null,
      "tags": [
        "text-to-image",
        "benchmark",
        "evaluation"
      ],
      "id": 692
    },
    {
      "name": "llm-colosseum",
      "one_line_profile": "Game-based evaluation harness for LLMs",
      "detailed_description": "A unique benchmarking tool that evaluates Large Language Models by having them compete in the Street Fighter 3 game. It tests the models' ability to make real-time decisions and strategize in a dynamic environment.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "agent_evaluation",
        "game_benchmark"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/OpenGenerativeAI/llm-colosseum",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm-agent",
        "game-benchmark",
        "evaluation"
      ],
      "id": 693
    },
    {
      "name": "Paddle Continuous Evaluation",
      "one_line_profile": "Continuous evaluation platform for PaddlePaddle framework",
      "detailed_description": "A macro continuous evaluation platform designed for the PaddlePaddle deep learning framework to monitor model performance and regression.",
      "domains": [
        "AI Toolchain",
        "Deep Learning"
      ],
      "subtask_category": [
        "continuous_evaluation",
        "model_regression_testing"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/PaddlePaddle/continuous_evaluation",
      "help_website": [],
      "license": null,
      "tags": [
        "paddlepaddle",
        "ci",
        "evaluation",
        "regression"
      ],
      "id": 694
    },
    {
      "name": "RAG-evaluation-harnesses",
      "one_line_profile": "Evaluation suite for Retrieval-Augmented Generation (RAG)",
      "detailed_description": "A specialized evaluation harness designed to assess the performance of Retrieval-Augmented Generation systems.",
      "domains": [
        "AI4",
        "NLP"
      ],
      "subtask_category": [
        "rag_evaluation",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/RulinShao/RAG-evaluation-harnesses",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "evaluation",
        "nlp"
      ],
      "id": 695
    },
    {
      "name": "RuSentEval",
      "one_line_profile": "Probing suite for Russian language models",
      "detailed_description": "A benchmark and probing suite for evaluating the linguistic capabilities of Russian embedding and language models.",
      "domains": [
        "AI4",
        "NLP"
      ],
      "subtask_category": [
        "probing",
        "model_evaluation",
        "linguistics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/RussianNLP/RuSentEval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "russian-nlp",
        "probing-tasks",
        "evaluation"
      ],
      "id": 696
    },
    {
      "name": "MAP (Mapping Accessibility)",
      "one_line_profile": "Urban network modeling and accessibility metric calculation tool",
      "detailed_description": "A software package for creating urban network models and calculating cumulative accessibility metrics and spatial justice indicators for urban planning.",
      "domains": [
        "Urban Planning",
        "Spatial Science"
      ],
      "subtask_category": [
        "network_modeling",
        "accessibility_analysis",
        "spatial_metrics"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/RuthJNelson/MAP-Mapping-Accessibility-for-Ethically-Informed-Urban-Planning",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "urban-planning",
        "accessibility",
        "network-analysis"
      ],
      "id": 697
    },
    {
      "name": "Langtrace",
      "one_line_profile": "Observability and evaluation tool for LLM applications",
      "detailed_description": "An Open Telemetry based tool for tracing, evaluating, and monitoring LLM applications, supporting various LLMs and vector databases.",
      "domains": [
        "AI4",
        "MLOps"
      ],
      "subtask_category": [
        "observability",
        "tracing",
        "model_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/Scale3-Labs/langtrace",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "llm-ops",
        "observability",
        "evaluation",
        "opentelemetry"
      ],
      "id": 698
    },
    {
      "name": "ScienceEval",
      "one_line_profile": "Evaluation suite for scientific foundation models",
      "detailed_description": "An open-source evaluation suite specifically designed for assessing the capabilities of ScienceOne Base models and other scientific LLMs.",
      "domains": [
        "AI4S",
        "Scientific AI"
      ],
      "subtask_category": [
        "model_evaluation",
        "scientific_reasoning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ScienceOne-AI/ScienceEval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "science-llm",
        "evaluation",
        "benchmark"
      ],
      "id": 699
    },
    {
      "name": "Gorilla",
      "one_line_profile": "Benchmark and training framework for LLM function calling",
      "detailed_description": "A framework and benchmark for training and evaluating Large Language Models on their ability to perform function calls (tool use).",
      "domains": [
        "AI4",
        "NLP"
      ],
      "subtask_category": [
        "function_calling",
        "model_evaluation",
        "training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ShishirPatil/gorilla",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "function-calling",
        "benchmark",
        "tool-use"
      ],
      "id": 700
    },
    {
      "name": "TransformerQuant",
      "one_line_profile": "Framework for deep learning models in quantitative trading",
      "detailed_description": "A framework for training and evaluating deep learning models specifically for the quantitative trading domain.",
      "domains": [
        "Quantitative Finance",
        "AI4"
      ],
      "subtask_category": [
        "model_training",
        "model_evaluation",
        "quantitative_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/StateOfTheArt-quant/transformerquant",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "quantitative-trading",
        "deep-learning",
        "evaluation"
      ],
      "id": 701
    },
    {
      "name": "CellSPA",
      "one_line_profile": "Cell Segmentation Performance Assessment tool",
      "detailed_description": "A tool for assessing the performance of cell segmentation algorithms in biological imaging.",
      "domains": [
        "Bioinformatics",
        "Imaging"
      ],
      "subtask_category": [
        "segmentation_evaluation",
        "image_analysis"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/SydneyBioX/CellSPA",
      "help_website": [],
      "license": null,
      "tags": [
        "cell-segmentation",
        "evaluation",
        "bioinformatics"
      ],
      "id": 702
    },
    {
      "name": "HPCPerfStats",
      "one_line_profile": "HPC resource-usage monitoring and analysis package",
      "detailed_description": "An automated resource-usage monitoring and analysis package for High Performance Computing (HPC) clusters, supporting scientific computing infrastructure.",
      "domains": [
        "HPC",
        "Scientific Computing"
      ],
      "subtask_category": [
        "performance_monitoring",
        "resource_analysis"
      ],
      "application_level": "tool",
      "primary_language": "C",
      "repo_url": "https://github.com/TACC/HPCPerfStats",
      "help_website": [],
      "license": "LGPL-2.1",
      "tags": [
        "hpc",
        "monitoring",
        "performance-analysis"
      ],
      "id": 703
    },
    {
      "name": "AgentBench",
      "one_line_profile": "Comprehensive benchmark to evaluate LLMs as Agents",
      "detailed_description": "A benchmark suite designed to evaluate the capabilities of Large Language Models acting as autonomous agents across various environments.",
      "domains": [
        "AI4",
        "NLP"
      ],
      "subtask_category": [
        "agent_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/THUDM/AgentBench",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-agent",
        "benchmark",
        "evaluation"
      ],
      "id": 704
    },
    {
      "name": "AICGSecEval",
      "one_line_profile": "AI-generated code security evaluation benchmark",
      "detailed_description": "A repository-level benchmark for evaluating the security of code generated by Artificial Intelligence models.",
      "domains": [
        "AI4",
        "Software Security"
      ],
      "subtask_category": [
        "security_evaluation",
        "code_generation_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Tencent/AICGSecEval",
      "help_website": [],
      "license": null,
      "tags": [
        "ai-security",
        "code-generation",
        "evaluation"
      ],
      "id": 705
    },
    {
      "name": "Afrobench Eval Suite",
      "one_line_profile": "LLM evaluation leaderboard for African Languages",
      "detailed_description": "An evaluation suite and leaderboard for assessing Large Language Model performance on African languages.",
      "domains": [
        "AI4",
        "NLP"
      ],
      "subtask_category": [
        "multilingual_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/The-African-Research-Collective/afrobench-eval-suite",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "african-languages",
        "nlp",
        "evaluation"
      ],
      "id": 706
    },
    {
      "name": "TimeCopilot",
      "one_line_profile": "GenAI Forecasting Agent for time series analysis",
      "detailed_description": "An agent-based tool leveraging LLMs and Time Series Foundation Models for forecasting, cross-validation, and anomaly detection in domains like finance and energy.",
      "domains": [
        "AI4S",
        "Time Series Analysis"
      ],
      "subtask_category": [
        "forecasting",
        "anomaly_detection",
        "cross_validation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/TimeCopilot/timecopilot",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "time-series",
        "forecasting",
        "llm-agent",
        "anomaly-detection"
      ],
      "id": 707
    },
    {
      "name": "Inspect",
      "one_line_profile": "Framework for large language model evaluations",
      "detailed_description": "An open-source framework for creating and running evaluations for Large Language Models (LLMs).",
      "domains": [
        "AI4",
        "NLP"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/UKGovernmentBEIS/inspect_ai",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "evaluation",
        "framework"
      ],
      "id": 708
    },
    {
      "name": "VMEvalKit",
      "one_line_profile": "Evaluation framework for reasoning capabilities in foundational video models",
      "detailed_description": "A comprehensive framework designed to evaluate the reasoning abilities of video foundational models, providing metrics and protocols for assessing model performance in understanding complex video content.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "reasoning_assessment"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Video-Reason/VMEvalKit",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "video-understanding",
        "evaluation-framework",
        "reasoning"
      ],
      "id": 709
    },
    {
      "name": "owl-eval",
      "one_line_profile": "Evaluation harness for diffusion world models",
      "detailed_description": "A specialized evaluation harness designed to assess the performance and capabilities of diffusion-based world models, facilitating reproducible benchmarking in generative modeling research.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "diffusion_models"
      ],
      "application_level": "solver",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/Wayfarer-Labs/owl-eval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "evaluation-harness",
        "diffusion-models",
        "world-models"
      ],
      "id": 710
    },
    {
      "name": "OmniBenchmark",
      "one_line_profile": "Benchmark for evaluating pre-trained vision models and contrastive learning",
      "detailed_description": "A benchmark suite and framework for evaluating pre-trained computer vision models, featuring a supervised contrastive learning framework to assess model robustness and transferability.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "computer_vision"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ZhangYuanhan-AI/OmniBenchmark",
      "help_website": [],
      "license": null,
      "tags": [
        "benchmark",
        "pre-trained-models",
        "contrastive-learning"
      ],
      "id": 711
    },
    {
      "name": "PertEval",
      "one_line_profile": "Evaluation suite for transcriptomic perturbation effect prediction models",
      "detailed_description": "A specialized evaluation suite for assessing models that predict transcriptomic perturbation effects, including support for single-cell foundation models, aiding in computational biology research.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "computational_biology"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/aaronwtr/PertEval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "transcriptomics",
        "perturbation-prediction",
        "single-cell"
      ],
      "id": 712
    },
    {
      "name": "indic_eval",
      "one_line_profile": "Evaluation suite for assessing Indic LLMs across diverse tasks",
      "detailed_description": "A lightweight evaluation suite tailored for benchmarking Large Language Models on Indic language tasks, facilitating the assessment of multilingual and low-resource language models.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "nlp"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/adithya-s-k/indic_eval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "indic-languages",
        "llm-evaluation",
        "nlp-benchmark"
      ],
      "id": 713
    },
    {
      "name": "OLMo-Eval",
      "one_line_profile": "Evaluation suite for Open Language Models (OLMo)",
      "detailed_description": "A comprehensive evaluation suite developed by AllenAI for benchmarking Large Language Models, specifically supporting the OLMo ecosystem and general LLM performance assessment.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "llm_benchmark"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/allenai/OLMo-Eval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "evaluation",
        "allenai"
      ],
      "id": 714
    },
    {
      "name": "deepfabric",
      "one_line_profile": "Platform for dataset curation, training, and evaluation of AI models",
      "detailed_description": "An integrated framework designed to streamline the lifecycle of AI model development, including high-quality dataset curation, model training, and evaluation, facilitating reproducible AI research.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "workflow_automation",
        "model_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/always-further/deepfabric",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "mlops",
        "dataset-curation",
        "evaluation-pipeline"
      ],
      "id": 715
    },
    {
      "name": "Apache Liminal",
      "one_line_profile": "Workflow orchestration for automating machine learning pipelines",
      "detailed_description": "An Apache incubator project that provides a domain-specific language to build, orchestrate, and operationalize machine learning workflows, bridging the gap between experimentation and production inference.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "workflow_automation",
        "pipeline_orchestration"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/apache/incubator-liminal",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "mlops",
        "workflow",
        "apache-airflow"
      ],
      "id": 716
    },
    {
      "name": "rcaaqs",
      "one_line_profile": "R package for calculating air quality metrics based on Canadian standards",
      "detailed_description": "An R package developed by the British Columbia Government to facilitate the calculation of air quality metrics (PM2.5, Ozone, NO2, SO2) according to the Canadian Ambient Air Quality Standards (CAAQS). It aids in environmental science data processing and regulatory reporting.",
      "domains": [
        "Environmental Science",
        "Atmospheric Science"
      ],
      "subtask_category": [
        "data_processing",
        "metrics_calculation"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/bcgov/rcaaqs",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "air-quality",
        "environmental-metrics",
        "r-package"
      ],
      "id": 717
    },
    {
      "name": "ASAP-AES Metrics",
      "one_line_profile": "Evaluation metrics for Automated Essay Scoring (Quadratic Weighted Kappa)",
      "detailed_description": "Provides the reference implementation of evaluation metrics, specifically the Quadratic Weighted Kappa, used for the Automated Student Assessment Prize (ASAP) Automated Essay Scoring competition. Useful for evaluating NLP models in scoring tasks.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "metrics_calculation",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/benhamner/ASAP-AES",
      "help_website": [],
      "license": "BSD-2-Clause",
      "tags": [
        "nlp",
        "evaluation-metric",
        "kappa"
      ],
      "id": 718
    },
    {
      "name": "survcomp",
      "one_line_profile": "Performance assessment and comparison for survival analysis models",
      "detailed_description": "An R package providing functions to assess and compare the performance of risk prediction models in survival analysis, widely used in biostatistics and medical research (e.g., cancer prognosis).",
      "domains": [
        "Biostatistics",
        "Medical Science"
      ],
      "subtask_category": [
        "model_evaluation",
        "statistical_analysis"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/bhklab/survcomp",
      "help_website": [
        "https://www.bioconductor.org/packages/release/bioc/html/survcomp.html"
      ],
      "license": "NOASSERTION",
      "tags": [
        "survival-analysis",
        "bioconductor",
        "risk-prediction"
      ],
      "id": 719
    },
    {
      "name": "BigCode Evaluation Harness",
      "one_line_profile": "Evaluation framework for code generation language models",
      "detailed_description": "A framework for the evaluation of autoregressive code generation language models, supporting various coding tasks and metrics. Developed by the BigCode project to standardize code LLM assessment.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmark"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/bigcode-project/bigcode-evaluation-harness",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "code-generation",
        "llm-evaluation",
        "benchmark"
      ],
      "id": 720
    },
    {
      "name": "Realistic SSL Evaluation",
      "one_line_profile": "Benchmark suite for realistic evaluation of Semi-Supervised Learning",
      "detailed_description": "A benchmark suite and codebase for evaluating Deep Semi-Supervised Learning (SSL) algorithms under realistic conditions, developed by Google Brain research.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmark"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/brain-research/realistic-ssl-evaluation",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "semi-supervised-learning",
        "benchmark",
        "deep-learning"
      ],
      "id": 721
    },
    {
      "name": "BytevalKit-Emb",
      "one_line_profile": "Modular evaluation framework for embedding models",
      "detailed_description": "A modular framework developed by ByteDance for evaluating embedding models, supporting automated performance assessment across multiple task types and standardized processes.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmark"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/bytedance/BytevalKit-Emb",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "embedding-models",
        "evaluation-framework",
        "nlp"
      ],
      "id": 722
    },
    {
      "name": "TMU",
      "one_line_profile": "Library implementing Tsetlin Machine algorithms for logic-based AI",
      "detailed_description": "A library implementing the Tsetlin Machine and its variants (Coalesced, Convolutional, etc.) for interpretable pattern recognition and logic-based machine learning, with CUDA support.",
      "domains": [
        "AI4",
        "Machine Learning"
      ],
      "subtask_category": [
        "modeling",
        "solver"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/cair/tmu",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "tsetlin-machine",
        "logic-learning",
        "interpretable-ai"
      ],
      "id": 723
    },
    {
      "name": "XLM-T",
      "one_line_profile": "Framework for evaluating multilingual language models on Twitter data",
      "detailed_description": "A framework and repository for training and evaluating multilingual language models specifically on Twitter data, enabling consistent benchmarking in social media NLP tasks.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmark"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/cardiffnlp/xlm-t",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "nlp",
        "multilingual",
        "twitter",
        "evaluation"
      ],
      "id": 724
    },
    {
      "name": "Yet Another Applied LLM Benchmark",
      "one_line_profile": "Applied benchmark for evaluating LLMs on practical coding and reasoning tasks",
      "detailed_description": "A benchmark suite developed by Nicholas Carlini to evaluate Large Language Models on a set of practical, applied questions and coding tasks, serving as a personal but influential baseline for model capability assessment.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmark"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/carlini/yet-another-applied-llm-benchmark",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "llm",
        "benchmark",
        "reasoning"
      ],
      "id": 725
    },
    {
      "name": "Opik",
      "one_line_profile": "Platform for evaluating, debugging, and monitoring LLM applications",
      "detailed_description": "Opik is a comprehensive platform designed for the evaluation and monitoring of Large Language Model (LLM) applications, RAG systems, and agentic workflows. It provides tracing capabilities, automated evaluation metrics, and dashboards to ensure production readiness of AI models.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "monitoring",
        "tracing"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/comet-ml/opik",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-evaluation",
        "observability",
        "rag",
        "debugging"
      ],
      "id": 726
    },
    {
      "name": "Compl-AI",
      "one_line_profile": "Compliance-centered evaluation framework for Generative AI models",
      "detailed_description": "Compl-AI is an open-source framework focused on evaluating Generative AI models against compliance standards. It enables the assessment of models for technical and ethical compliance, providing a structured approach to AI safety and regulation adherence.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "compliance_testing",
        "ai_safety"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/compl-ai/compl-ai",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "generative-ai",
        "compliance",
        "evaluation",
        "safety"
      ],
      "id": 727
    },
    {
      "name": "DeepEval",
      "one_line_profile": "Unit testing and evaluation framework for LLMs",
      "detailed_description": "DeepEval is an evaluation framework designed to unit test Large Language Models (LLMs). It offers a suite of metrics to assess RAG pipelines and agents, facilitating continuous integration and regression testing for AI application development.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "unit_testing",
        "rag_evaluation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/confident-ai/deepeval",
      "help_website": [
        "https://docs.confident-ai.com"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "testing",
        "metrics",
        "rag"
      ],
      "id": 728
    },
    {
      "name": "LogiEval",
      "one_line_profile": "Benchmark suite for testing logical reasoning in prompt-based models",
      "detailed_description": "LogiEval is a benchmark suite designed to evaluate the logical reasoning capabilities of prompt-based AI models. It provides a set of tasks and metrics to assess how well models perform on logic-intensive problems.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking",
        "logical_reasoning"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/csitfun/LogiEval",
      "help_website": [],
      "license": null,
      "tags": [
        "benchmark",
        "logical-reasoning",
        "llm",
        "prompting"
      ],
      "id": 729
    },
    {
      "name": "Bisheng",
      "one_line_profile": "Open LLM DevOps and evaluation platform for enterprise AI",
      "detailed_description": "Bisheng is a comprehensive platform for developing and managing LLM applications. It includes features for evaluation, dataset management, supervised fine-tuning (SFT), and RAG workflows, serving as a unified environment for AI model lifecycle management.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_management",
        "evaluation",
        "workflow_orchestration"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/dataelement/bisheng",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llmops",
        "evaluation",
        "rag",
        "fine-tuning"
      ],
      "id": 730
    },
    {
      "name": "Deepchecks",
      "one_line_profile": "Continuous validation and testing for ML models and data",
      "detailed_description": "Deepchecks is a holistic open-source solution for testing and validating machine learning models and data. It supports the entire ML lifecycle from research to production, offering checks for data integrity, model performance, and distribution drift.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_validation",
        "data_quality_control",
        "drift_detection"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/deepchecks/deepchecks",
      "help_website": [
        "https://docs.deepchecks.com"
      ],
      "license": "NOASSERTION",
      "tags": [
        "ml-testing",
        "validation",
        "data-quality",
        "model-monitoring"
      ],
      "id": 731
    },
    {
      "name": "Ollama Grid Search",
      "one_line_profile": "Desktop application to evaluate and compare LLM models",
      "detailed_description": "Ollama Grid Search is a multi-platform desktop tool that facilitates the evaluation and comparison of various Large Language Models (LLMs) managed via Ollama. It allows users to run grid searches over prompts and model parameters to assess performance.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "parameter_tuning",
        "comparison"
      ],
      "application_level": "solver",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/dezoito/ollama-grid-search",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "grid-search",
        "evaluation",
        "ollama"
      ],
      "id": 732
    },
    {
      "name": "Docling Eval",
      "one_line_profile": "Evaluation framework for document processing models",
      "detailed_description": "Docling Eval is a framework designed to evaluate the performance of document processing models and services. It provides metrics and workflows to assess the accuracy of information extraction and document layout analysis.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "document_processing",
        "ocr_evaluation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/docling-project/docling-eval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "document-ai",
        "evaluation",
        "ocr",
        "layout-analysis"
      ],
      "id": 733
    },
    {
      "name": "Uni-Dock-Benchmarks",
      "one_line_profile": "Benchmark datasets for molecular docking systems",
      "detailed_description": "A curated collection of datasets and benchmarking tests specifically for evaluating the performance and accuracy of the Uni-Dock molecular docking system. It serves as a standard for comparing docking results in drug discovery research.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "benchmarking",
        "molecular_docking",
        "drug_discovery"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/dptech-corp/Uni-Dock-Benchmarks",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "molecular-docking",
        "benchmark",
        "drug-discovery",
        "bioinformatics"
      ],
      "id": 734
    },
    {
      "name": "Encord Active",
      "one_line_profile": "Active learning toolkit for model evaluation and data curation",
      "detailed_description": "Encord Active is an open-source toolkit for testing, validating, and evaluating computer vision models. It focuses on data-centric AI, helping users prioritize data for labeling, detect errors, and analyze model performance through actionable metrics.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "active_learning",
        "data_curation"
      ],
      "application_level": "toolkit",
      "primary_language": "Python",
      "repo_url": "https://github.com/encord-team/encord-active",
      "help_website": [
        "https://docs.encord.com/active/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "computer-vision",
        "active-learning",
        "evaluation",
        "data-centric-ai"
      ],
      "id": 735
    },
    {
      "name": "Text-to-Image Eval",
      "one_line_profile": "Evaluation metrics for text-to-image generation models",
      "detailed_description": "A toolkit for evaluating text-to-image and zero-shot image classification models (e.g., CLIP, SigLIP). It implements metrics such as Zero-shot accuracy, Linear Probe, and Image retrieval to assess model quality and alignment.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "image_generation",
        "zero_shot_classification"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/encord-team/text-to-image-eval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "text-to-image",
        "evaluation",
        "clip",
        "metrics"
      ],
      "id": 736
    },
    {
      "name": "OBR",
      "one_line_profile": "Runner for OpenFOAM benchmarks",
      "detailed_description": "OBR (OpenFOAM Benchmark Runner) is a tool designed to automate the execution and management of benchmarks for OpenFOAM, a popular computational fluid dynamics (CFD) software.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "simulation_runner",
        "benchmarking"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/exasim-project/OBR",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "openfoam",
        "cfd",
        "benchmark",
        "hpc"
      ],
      "id": 737
    },
    {
      "name": "LLM Speedrunner",
      "one_line_profile": "Automated benchmark for LLM agents in language modeling innovation",
      "detailed_description": "The Automated LLM Speedrunning Benchmark measures the capability of LLM agents to reproduce previous innovations and discover new ones in the field of language modeling, serving as a metric for agentic research capabilities.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "agent_evaluation",
        "benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/facebookresearch/llm-speedrunner",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "llm-agents",
        "benchmark",
        "research-automation"
      ],
      "id": 738
    },
    {
      "name": "Video Transformers",
      "one_line_profile": "Fine-tuning framework for HuggingFace video classification models",
      "detailed_description": "A lightweight library designed to simplify the fine-tuning process of video classification models from the HuggingFace ecosystem, providing easy-to-use interfaces for training and evaluation.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_finetuning",
        "video_classification"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/fcakyon/video-transformers",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "video-classification",
        "transformers",
        "fine-tuning"
      ],
      "id": 739
    },
    {
      "name": "divraster",
      "one_line_profile": "R package for calculating diversity metrics on rasterized data",
      "detailed_description": "divraster is an R package that provides functions to calculate various diversity metrics (e.g., alpha, beta diversity) directly on raster data, commonly used in ecology and biogeography.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "data_analysis",
        "diversity_metrics"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/flaviomoc/divraster",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "ecology",
        "raster",
        "diversity-metrics",
        "r-package"
      ],
      "id": 740
    },
    {
      "name": "js-quantities",
      "one_line_profile": "JavaScript library for quantity calculation and unit conversion",
      "detailed_description": "A library to handle physical quantities and unit conversions in JavaScript, useful for scientific data processing and frontend scientific visualizations.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "unit_conversion",
        "data_processing"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/gentooboontoo/js-quantities",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "unit-conversion",
        "physics",
        "quantities"
      ],
      "id": 741
    },
    {
      "name": "Google Agent Development Toolkit (Go)",
      "one_line_profile": "Go toolkit for building and evaluating AI agents",
      "detailed_description": "An open-source toolkit that provides components and infrastructure for building, evaluating, and deploying AI agents, facilitating research and development in agentic AI.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "agent_evaluation",
        "agent_development"
      ],
      "application_level": "library",
      "primary_language": "Go",
      "repo_url": "https://github.com/google/adk-go",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ai-agents",
        "evaluation",
        "toolkit"
      ],
      "id": 742
    },
    {
      "name": "Google Agent Development Toolkit (Java)",
      "one_line_profile": "Java toolkit for building and evaluating AI agents",
      "detailed_description": "An open-source toolkit that provides components and infrastructure for building, evaluating, and deploying AI agents using Java.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "agent_evaluation",
        "agent_development"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/google/adk-java",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ai-agents",
        "evaluation",
        "toolkit"
      ],
      "id": 743
    },
    {
      "name": "Google Agent Development Toolkit (Python)",
      "one_line_profile": "Python toolkit for building and evaluating AI agents",
      "detailed_description": "An open-source toolkit that provides components and infrastructure for building, evaluating, and deploying AI agents using Python, widely used in AI research.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "agent_evaluation",
        "agent_development"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/google/adk-python",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ai-agents",
        "evaluation",
        "toolkit"
      ],
      "id": 744
    },
    {
      "name": "Gromit",
      "one_line_profile": "Decentralized systems benchmarking and experiment runner framework",
      "detailed_description": "Gromit is a framework designed to automate the deployment, execution, and benchmarking of decentralized systems experiments, facilitating reproducible research in distributed computing.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "experiment_runner",
        "system_benchmarking"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/grimadas/gromit",
      "help_website": [],
      "license": null,
      "tags": [
        "distributed-systems",
        "benchmarking",
        "experiment-runner"
      ],
      "id": 745
    },
    {
      "name": "haddock-runner",
      "one_line_profile": "Runner for large scale HADDOCK biomolecular simulations",
      "detailed_description": "A utility to automate and manage large-scale docking simulations using HADDOCK, enabling high-throughput structural biology research.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "simulation_runner",
        "molecular_docking"
      ],
      "application_level": "workflow",
      "primary_language": "Go",
      "repo_url": "https://github.com/haddocking/haddock-runner",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "structural-biology",
        "docking",
        "haddock",
        "simulation"
      ],
      "id": 746
    },
    {
      "name": "C-Eval",
      "one_line_profile": "Comprehensive Chinese evaluation suite for foundation models",
      "detailed_description": "C-Eval is a comprehensive evaluation suite designed to assess the advanced knowledge and reasoning abilities of foundation models in a Chinese context.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmark_suite"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/hkust-nlp/ceval",
      "help_website": [
        "https://cevalbenchmark.com/"
      ],
      "license": "MIT",
      "tags": [
        "llm",
        "benchmark",
        "chinese",
        "evaluation"
      ],
      "id": 747
    },
    {
      "name": "HMOG Dataset",
      "one_line_profile": "Multimodal dataset for evaluating continuous authentication performance",
      "detailed_description": "A multimodal dataset designed for evaluating continuous authentication performance on smartphones, capturing various sensor data to benchmark security models.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "dataset",
        "security_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": null,
      "repo_url": "https://github.com/hmog-dataset/hmog",
      "help_website": [],
      "license": null,
      "tags": [
        "dataset",
        "authentication",
        "biometrics",
        "mobile"
      ],
      "id": 748
    },
    {
      "name": "Lighteval",
      "one_line_profile": "All-in-one toolkit for evaluating LLMs across multiple backends",
      "detailed_description": "A comprehensive library for evaluating Large Language Models (LLMs) on various benchmarks and tasks. It supports multiple backends and provides a unified interface for assessing model performance, making it a critical component in the AI model development and evaluation lifecycle.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/huggingface/lighteval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "evaluation",
        "nlp",
        "benchmarking"
      ],
      "id": 749
    },
    {
      "name": "RAVEN",
      "one_line_profile": "Probabilistic risk analysis, validation, and uncertainty quantification framework",
      "detailed_description": "A flexible framework for probabilistic risk analysis, uncertainty quantification, parameter optimization, and model reduction. It is designed to perform parametric and stochastic analysis of complex system codes, widely used in nuclear engineering and other safety-critical scientific domains.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "uncertainty_quantification",
        "risk_analysis",
        "parameter_optimization"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/idaholab/raven",
      "help_website": [
        "https://raven.inl.gov"
      ],
      "license": "Apache-2.0",
      "tags": [
        "uncertainty-quantification",
        "risk-analysis",
        "simulation",
        "model-validation"
      ],
      "id": 750
    },
    {
      "name": "Probatus",
      "one_line_profile": "SHAP-based validation toolkit for linear and tree-based models",
      "detailed_description": "A Python library for validating binary, multiclass, and regression models using SHAP (SHapley Additive exPlanations) values. It provides tools for feature selection, model analysis, and ensuring model robustness in scientific and financial ML applications.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_validation",
        "feature_selection",
        "interpretability"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ing-bank/probatus",
      "help_website": [
        "https://ing-bank.github.io/probatus/"
      ],
      "license": "MIT",
      "tags": [
        "shap",
        "model-validation",
        "machine-learning",
        "feature-selection"
      ],
      "id": 751
    },
    {
      "name": "ReadabilityMetrics",
      "one_line_profile": "Library and service for computing text readability metrics",
      "detailed_description": "A tool that calculates various readability metrics (e.g., ARI, Coleman-Liau, Flesch-Kincaid) for text data. While implemented as a service, the underlying logic serves as a library for linguistic analysis and text quality assessment in NLP research.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "text_analysis",
        "metric_calculation"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/ipeirotis/ReadabilityMetrics",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "nlp",
        "readability",
        "text-analysis",
        "metrics"
      ],
      "id": 752
    },
    {
      "name": "CML",
      "one_line_profile": "Continuous Machine Learning (CML) for CI/CD of ML experiments",
      "detailed_description": "An open-source library for implementing Continuous Integration/Continuous Delivery (CI/CD) in machine learning projects. It enables researchers to automate model training, evaluation, and report generation (e.g., plots, metrics) directly within pull requests, facilitating reproducible research and regression testing.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "experiment_tracking",
        "ci_regression",
        "reproducibility"
      ],
      "application_level": "workflow",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/iterative/cml",
      "help_website": [
        "https://cml.dev"
      ],
      "license": "Apache-2.0",
      "tags": [
        "mlops",
        "ci-cd",
        "reproducibility",
        "experiment-tracking"
      ],
      "id": 753
    },
    {
      "name": "Matbench Discovery",
      "one_line_profile": "Evaluation framework for ML models in high-throughput materials discovery",
      "detailed_description": "A benchmark and evaluation framework designed to simulate high-throughput materials discovery using machine learning models. It assesses the ability of models to predict stable materials and guide experimental synthesis, serving as a critical tool for materials informatics.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "materials_discovery",
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/janosh/matbench-discovery",
      "help_website": [
        "https://matbench-discovery.materialsproject.org"
      ],
      "license": "MIT",
      "tags": [
        "materials-science",
        "machine-learning",
        "benchmarking",
        "discovery"
      ],
      "id": 754
    },
    {
      "name": "Runcharter",
      "one_line_profile": "Automated run chart analysis for faceted data displays",
      "detailed_description": "An R package for automating the creation and analysis of run charts, which are used for quality control and performance monitoring in healthcare and scientific processes. It supports identifying trends and shifts in data across multiple metrics or locations.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "statistical_analysis",
        "quality_control",
        "visualization"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/johnmackintosh/runcharter",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "r",
        "statistics",
        "quality-improvement",
        "visualization"
      ],
      "id": 755
    },
    {
      "name": "Eva",
      "one_line_profile": "Evaluation framework for oncology foundation models",
      "detailed_description": "A specialized framework for evaluating foundation models in the context of oncology. It provides metrics and workflows to assess the performance of AI models on cancer-related tasks, facilitating the development of reliable medical AI tools.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "medical_ai",
        "oncology"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/kaiko-ai/eva",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "oncology",
        "foundation-models",
        "evaluation",
        "medical-imaging"
      ],
      "id": 756
    },
    {
      "name": "VSDFLOW",
      "one_line_profile": "Automated RTL-to-GDS flow for semiconductor design",
      "detailed_description": "An automated Electronic Design Automation (EDA) toolchain that converts Register Transfer Level (RTL) designs to GDSII layout. It integrates synthesis, placement, routing, and timing analysis tools, enabling hardware engineers and researchers to produce physical chip designs from logic descriptions.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "chip_design",
        "eda",
        "synthesis"
      ],
      "application_level": "workflow",
      "primary_language": "Verilog",
      "repo_url": "https://github.com/kunalg123/vsdflow",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "eda",
        "vlsi",
        "rtl-to-gds",
        "hardware-design"
      ],
      "id": 757
    },
    {
      "name": "Langfuse",
      "one_line_profile": "Open source LLM engineering platform for observability, metrics, and evaluations",
      "detailed_description": "A comprehensive platform for LLM engineering that includes tools for tracing, dataset management, and running evaluations on model outputs. It supports the lifecycle of developing and refining LLM applications through rigorous metrics and observability.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "observability",
        "dataset_management"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/langfuse/langfuse",
      "help_website": [
        "https://langfuse.com/docs"
      ],
      "license": "NOASSERTION",
      "tags": [
        "llm-ops",
        "evaluation",
        "observability",
        "metrics"
      ],
      "id": 758
    },
    {
      "name": "LangWatch",
      "one_line_profile": "LLM Ops platform for analytics, evaluations, and prompt optimization",
      "detailed_description": "A platform designed for monitoring and evaluating Large Language Models (LLMs). It provides capabilities for tracing execution, analyzing performance analytics, managing datasets, and optimizing prompts through systematic evaluation.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "analytics",
        "prompt_optimization"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/langwatch/langwatch",
      "help_website": [
        "https://docs.langwatch.ai"
      ],
      "license": "NOASSERTION",
      "tags": [
        "llm-ops",
        "analytics",
        "evaluation"
      ],
      "id": 759
    },
    {
      "name": "Latitude LLM",
      "one_line_profile": "Open-source prompt engineering and evaluation platform",
      "detailed_description": "A platform focused on the engineering and refinement of prompts for Large Language Models. It allows users to build, evaluate, and iterate on prompts using AI-assisted feedback and performance metrics.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "prompt_engineering",
        "model_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/latitude-dev/latitude-llm",
      "help_website": [
        "https://docs.latitude.so"
      ],
      "license": "LGPL-3.0",
      "tags": [
        "prompt-engineering",
        "evaluation",
        "llm"
      ],
      "id": 760
    },
    {
      "name": "Les Audits Affaires Eval Harness",
      "one_line_profile": "CLI for benchmarking French LLMs in business law tasks",
      "detailed_description": "A lightweight Python command-line interface designed to benchmark and test French Large Language Models specifically on business law tasks, including action determination, deadline analysis, document processing, cost estimation, and risk assessment.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "domain_specific_evaluation",
        "legal_benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/legml-ai/les-audits-affaires-eval-harness",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "legal-ai",
        "benchmark",
        "llm-evaluation",
        "french-nlp"
      ],
      "id": 761
    },
    {
      "name": "BioMonTools",
      "one_line_profile": "R tools for biomonitoring and bioassessment metric calculation",
      "detailed_description": "A suite of tools for calculating metrics related to biomonitoring and bioassessment, specifically for benthic macroinvertebrates, fish, and periphyton. It aids in the analysis of ecological data for environmental health assessment.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "metric_calculation",
        "bioassessment",
        "ecological_analysis"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/leppott/BioMonTools",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "biomonitoring",
        "ecology",
        "r-package",
        "metrics"
      ],
      "id": 762
    },
    {
      "name": "S3Eval",
      "one_line_profile": "Synthetic, Scalable and Systematic Evaluation Suite for LLMs",
      "detailed_description": "An evaluation suite presented at NAACL 2024 designed to assess Large Language Models. It focuses on providing a synthetic, scalable, and systematic approach to benchmarking LLM performance across various tasks.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/lfy79001/S3Eval",
      "help_website": [],
      "license": null,
      "tags": [
        "llm-evaluation",
        "benchmark",
        "nlp"
      ],
      "id": 763
    },
    {
      "name": "LinearityIQA",
      "one_line_profile": "Norm-in-Norm Loss implementation for Image Quality Assessment",
      "detailed_description": "Implementation of the Norm-in-Norm Loss function for Image Quality Assessment (IQA), as presented at ACM MM 2020. It provides a method for evaluating image quality with faster convergence and better performance compared to standard metrics.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "image_quality_assessment",
        "loss_function"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/lidq92/LinearityIQA",
      "help_website": [],
      "license": null,
      "tags": [
        "computer-vision",
        "iqa",
        "image-quality",
        "metric"
      ],
      "id": 764
    },
    {
      "name": "Arena-Hard-Auto",
      "one_line_profile": "Automatic benchmark for Large Language Models",
      "detailed_description": "An automated benchmarking tool for Large Language Models, designed to replicate the difficulty and discrimination power of the Chatbot Arena. It provides a pipeline for evaluating models against hard prompts using judge models.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_benchmarking",
        "automated_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/lmarena/arena-hard-auto",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-benchmark",
        "evaluation",
        "arena-hard"
      ],
      "id": 765
    },
    {
      "name": "LEP-Hybrid-Visual-Odometry",
      "one_line_profile": "Real-time monocular Hybrid Visual Odometry system",
      "detailed_description": "A C++ implementation of a hybrid visual odometry formulation that combines indirect (feature-based) and direct methods. It uses lines, edges, and points (LEP) for robust tracking and mapping, serving as a tool for robotics and computer vision research.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "visual_odometry",
        "slam",
        "robotics_navigation"
      ],
      "application_level": "solver",
      "primary_language": "CMake",
      "repo_url": "https://github.com/maazmb/LEP-Hybrid-Visual-Odometry",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "slam",
        "visual-odometry",
        "computer-vision",
        "robotics"
      ],
      "id": 766
    },
    {
      "name": "FixEval",
      "one_line_profile": "Dataset and test suite for competitive programming bug fixing",
      "detailed_description": "A dataset and evaluation framework for automated program repair in the context of competitive programming. It emphasizes execution-based evaluation over match-based metrics to accurately assess bug-fixing capabilities of models.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "program_repair_evaluation",
        "code_generation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/mahimanzum/FixEval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "automated-program-repair",
        "benchmark",
        "dataset"
      ],
      "id": 767
    },
    {
      "name": "llm_eval_suite",
      "one_line_profile": "Tool to evaluate LLMs using Ollama and Hugging Face datasets",
      "detailed_description": "A Python-based utility to facilitate the evaluation of Large Language Models. It integrates with Ollama for model inference and Hugging Face for dataset retrieval, allowing for streamlined performance testing.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "inference_benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/majesticio/llm_eval_suite",
      "help_website": [],
      "license": null,
      "tags": [
        "llm",
        "evaluation",
        "ollama",
        "huggingface"
      ],
      "id": 768
    },
    {
      "name": "saliency-faithfulness-eval",
      "one_line_profile": "Tests to assess attention faithfulness for explainability",
      "detailed_description": "A suite of tests designed to evaluate the faithfulness of saliency maps and attention mechanisms in explainable AI (XAI). It helps researchers verify if the explanations provided by models accurately reflect their decision-making process.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "xai_evaluation",
        "saliency_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/matteo-rizzo/saliency-faithfulness-eval",
      "help_website": [],
      "license": null,
      "tags": [
        "explainable-ai",
        "saliency-maps",
        "evaluation",
        "faithfulness"
      ],
      "id": 769
    },
    {
      "name": "Evalite",
      "one_line_profile": "TypeScript library for evaluating LLM-powered applications",
      "detailed_description": "A lightweight TypeScript library designed to help developers and researchers evaluate the performance of LLM-powered applications. It provides a structured way to define and run evaluations on model outputs.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "app_testing"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/mattpocock/evalite",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm-evaluation",
        "typescript",
        "testing"
      ],
      "id": 770
    },
    {
      "name": "CVRR-Evaluation-Suite",
      "one_line_profile": "Complex Video Reasoning and Robustness Evaluation Suite for Video-LMMs",
      "detailed_description": "An evaluation suite for assessing Video Large Multimodal Models (Video-LMMs). It focuses on complex reasoning tasks and robustness, providing a benchmark to measure model performance in challenging video understanding scenarios.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "video_lmm_evaluation",
        "multimodal_benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/mbzuai-oryx/CVRR-Evaluation-Suite",
      "help_website": [],
      "license": "CC-BY-4.0",
      "tags": [
        "video-lmm",
        "benchmark",
        "robustness",
        "reasoning"
      ],
      "id": 771
    },
    {
      "name": "HEAL-T",
      "one_line_profile": "PPG-based Heart Rate and IBI estimation method",
      "detailed_description": "A MATLAB and Bash based implementation for estimating Interbeat-interval (IBI) and Heart-rate (HR) from artifactual Blood Volume Pulse (BVP) signals during physical exercise. It serves as a tool for physiological signal processing.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "signal_processing",
        "physiological_metrics"
      ],
      "application_level": "solver",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/meiyor/HEAL-T-AN-EFFICIENT-PPG-BASED-HEART-RATE-AND-IBI-ESTIMATION-METHOD-DURING-PHYSICAL-EXERCISE",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "ppg",
        "heart-rate",
        "signal-processing",
        "biomedical"
      ],
      "id": 772
    },
    {
      "name": "MSMARCO-Conversational-Search",
      "one_line_profile": "Dataset and evaluation paradigm for conversational search",
      "detailed_description": "A dataset and evaluation framework designed to study conversational search behavior. It provides artificial sessions and a methodology to evaluate model performance on real user behavior without compromising privacy.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "dataset_creation",
        "conversational_search_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/MSMARCO-Conversational-Search",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "conversational-search",
        "dataset",
        "evaluation",
        "ir"
      ],
      "id": 773
    },
    {
      "name": "Prompty",
      "one_line_profile": "Tool to create, manage, debug, and evaluate LLM prompts",
      "detailed_description": "An asset class and toolset for managing LLM prompts. It facilitates the creation, debugging, and evaluation of prompts, enhancing observability and portability for AI application development.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "prompt_management",
        "prompt_evaluation"
      ],
      "application_level": "tool",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/prompty",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "prompt-engineering",
        "llm-ops",
        "evaluation"
      ],
      "id": 774
    },
    {
      "name": "minerl-wrappers",
      "one_line_profile": "Standardized wrappers for reproducibility in MineRL environment",
      "detailed_description": "A collection of wrappers for the MineRL environment to standardize code and ensure reproducibility in Reinforcement Learning experiments involving Minecraft.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "rl_environment_wrapper",
        "reproducibility"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/minerl-wrappers/minerl-wrappers",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "reinforcement-learning",
        "minerl",
        "reproducibility"
      ],
      "id": 775
    },
    {
      "name": "ModelBench",
      "one_line_profile": "Safety benchmarks for AI models with detailed reporting",
      "detailed_description": "A tool from MLCommons for running safety benchmarks against AI models. It generates detailed reports on model performance regarding safety metrics, facilitating standardized safety assessments.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "ai_safety_benchmarking",
        "model_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/mlcommons/modelbench",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ai-safety",
        "benchmark",
        "mlcommons"
      ],
      "id": 776
    },
    {
      "name": "FSEB",
      "one_line_profile": "Energy-based Monthly Simulation of Land Surface Temperature",
      "detailed_description": "Code for modeling Land Surface Temperature (LST) using Surface Energy Balance (SEB) principles. It allows for the simulation of environmental factors and evapotranspiration at a monthly temporal scale.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "environmental_modeling",
        "surface_energy_balance"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/mnasserimn/FSEB",
      "help_website": [],
      "license": null,
      "tags": [
        "remote-sensing",
        "land-surface-temperature",
        "modeling"
      ],
      "id": 777
    },
    {
      "name": "Prognostic Algorithm Package",
      "one_line_profile": "Framework for model-based prognostics and remaining useful life computation of engineering systems",
      "detailed_description": "A Python framework for model-based prognostics that provides algorithms for state estimation, prediction, and uncertainty propagation. It allows for the rapid development and comparative study of prognostics solutions for engineering components and systems.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "prognostics",
        "state_estimation",
        "uncertainty_propagation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/nasa/prog_algs",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "nasa",
        "prognostics",
        "engineering-systems"
      ],
      "id": 778
    },
    {
      "name": "Evals",
      "one_line_profile": "Framework for evaluating LLMs and an open-source registry of benchmarks",
      "detailed_description": "A framework developed by OpenAI for evaluating Large Language Models (LLMs) and LLM systems. It includes an open-source registry of benchmarks to test model capabilities and ensure quality.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmark_registry"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/openai/evals",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "llm",
        "openai",
        "benchmark"
      ],
      "id": 779
    },
    {
      "name": "pChem",
      "one_line_profile": "Modification-centric assessment tool for chemoproteomic probe performance",
      "detailed_description": "A tool for the assessment of chemoproteomic probes, focusing on modification-centric performance evaluation. It aids in the analysis of probe efficiency and specificity in proteomic studies.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "chemoproteomics",
        "probe_assessment",
        "data_analysis"
      ],
      "application_level": "tool",
      "primary_language": "Python",
      "repo_url": "https://github.com/pFindStudio/pChem",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "proteomics",
        "chemistry",
        "bioinformatics"
      ],
      "id": 780
    },
    {
      "name": "Japanese LM Financial Evaluation Harness",
      "one_line_profile": "Evaluation harness for Japanese language models in the financial domain",
      "detailed_description": "A specialized evaluation harness designed for assessing the performance of Japanese Language Models within the financial domain. It provides benchmarks and metrics tailored to financial texts and tasks.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "domain_specific_benchmarking"
      ],
      "application_level": "harness",
      "primary_language": "Shell",
      "repo_url": "https://github.com/pfnet-research/japanese-lm-fin-harness",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nlp",
        "finance",
        "japanese"
      ],
      "id": 781
    },
    {
      "name": "InstructEval",
      "one_line_profile": "Evaluation suite for instruction selection methods in NLP",
      "detailed_description": "A systematic evaluation suite designed to assess instruction selection methods for Large Language Models (LLMs), supporting the reproduction of findings from NAACL 2024.",
      "domains": [
        "AI4",
        "AI4-03",
        "Natural Language Processing"
      ],
      "subtask_category": [
        "model_evaluation",
        "instruction_tuning_assessment"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/princeton-nlp/InstructEval",
      "help_website": [],
      "license": null,
      "tags": [
        "nlp",
        "llm-evaluation",
        "instruction-following",
        "benchmark"
      ],
      "id": 782
    },
    {
      "name": "skore",
      "one_line_profile": "Python library for ML model evaluation and reporting",
      "detailed_description": "An open-source library that accelerates machine learning model development by providing automated evaluation reports, methodological guidance, and cross-validation analysis.",
      "domains": [
        "AI4",
        "AI4-03",
        "Machine Learning"
      ],
      "subtask_category": [
        "model_evaluation",
        "reporting",
        "cross_validation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/probabl-ai/skore",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "machine-learning",
        "evaluation-metrics",
        "data-science",
        "reporting"
      ],
      "id": 783
    },
    {
      "name": "etalon",
      "one_line_profile": "Performance evaluation harness for LLM serving systems",
      "detailed_description": "A harness designed to benchmark and evaluate the performance (latency, throughput) of Large Language Model serving systems.",
      "domains": [
        "AI4",
        "AI4-03",
        "Machine Learning Systems"
      ],
      "subtask_category": [
        "performance_benchmarking",
        "llm_serving"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/project-etalon/etalon",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "benchmarking",
        "performance",
        "serving"
      ],
      "id": 784
    },
    {
      "name": "prometheus-eval",
      "one_line_profile": "LLM response evaluation tool using Prometheus and GPT-4",
      "detailed_description": "A library for evaluating Large Language Model responses, leveraging Prometheus models and GPT-4 as judges for automated assessment.",
      "domains": [
        "AI4",
        "AI4-03",
        "Natural Language Processing"
      ],
      "subtask_category": [
        "llm_evaluation",
        "automated_grading"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/prometheus-eval/prometheus-eval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "evaluation",
        "prometheus",
        "gpt-4"
      ],
      "id": 785
    },
    {
      "name": "promptfoo",
      "one_line_profile": "CLI tool for testing and evaluating LLM prompts and agents",
      "detailed_description": "A tool for testing prompts, agents, and RAG systems, supporting red teaming, vulnerability scanning, and performance comparison across multiple LLM providers.",
      "domains": [
        "AI4",
        "AI4-03",
        "Natural Language Processing"
      ],
      "subtask_category": [
        "prompt_engineering",
        "model_evaluation",
        "red_teaming"
      ],
      "application_level": "solver",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/promptfoo/promptfoo",
      "help_website": [
        "https://www.promptfoo.dev"
      ],
      "license": "MIT",
      "tags": [
        "llm",
        "testing",
        "prompt-engineering",
        "red-teaming"
      ],
      "id": 786
    },
    {
      "name": "epm",
      "one_line_profile": "R package for eco-phylogenetic metrics calculation",
      "detailed_description": "An R package designed for calculating taxonomic, phenotypic, and phylogenetic metrics across spatial grid cells for ecological research.",
      "domains": [
        "AI4",
        "AI4-03",
        "Ecology",
        "Phylogenetics"
      ],
      "subtask_category": [
        "metric_calculation",
        "spatial_analysis",
        "biodiversity_metrics"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/ptitle/epm",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "ecology",
        "phylogenetics",
        "spatial-analysis",
        "r-package"
      ],
      "id": 787
    },
    {
      "name": "speciesRaster",
      "one_line_profile": "R package for species indexing and community metric calculation",
      "detailed_description": "A tool for indexing species by grid cell and calculating community ecology metrics, serving as a companion to spatial ecological analysis.",
      "domains": [
        "AI4",
        "AI4-03",
        "Ecology"
      ],
      "subtask_category": [
        "community_ecology",
        "metric_calculation"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/ptitle/speciesRaster",
      "help_website": [],
      "license": null,
      "tags": [
        "ecology",
        "biodiversity",
        "r-package"
      ],
      "id": 788
    },
    {
      "name": "aeromancy",
      "one_line_profile": "Framework for reproducible AI and ML experiments",
      "detailed_description": "A framework designed to facilitate reproducible artificial intelligence and machine learning workflows, likely with a focus on environmental or atmospheric sciences given the organization name (quant-aq).",
      "domains": [
        "AI4",
        "AI4-03",
        "Machine Learning"
      ],
      "subtask_category": [
        "reproducibility",
        "experiment_tracking"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/quant-aq/aeromancy",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "reproducibility",
        "machine-learning",
        "framework"
      ],
      "id": 789
    },
    {
      "name": "RagaAI-Catalyst",
      "one_line_profile": "SDK for Agent AI observability and evaluation",
      "detailed_description": "A Python SDK providing a framework for observability, monitoring, and evaluation of AI agents, including tracing and debugging capabilities for multi-agent systems.",
      "domains": [
        "AI4",
        "AI4-03",
        "Artificial Intelligence"
      ],
      "subtask_category": [
        "agent_evaluation",
        "observability",
        "monitoring"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/raga-ai-hub/RagaAI-Catalyst",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ai-agents",
        "observability",
        "evaluation",
        "sdk"
      ],
      "id": 790
    },
    {
      "name": "crossfit",
      "one_line_profile": "GPU-accelerated metric calculation library",
      "detailed_description": "A library from RAPIDS AI for calculating metrics, optimized for GPU execution, typically used in recommender systems or ranking tasks.",
      "domains": [
        "AI4",
        "AI4-03",
        "Data Science"
      ],
      "subtask_category": [
        "metric_calculation",
        "gpu_acceleration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/rapidsai/crossfit",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rapids",
        "gpu",
        "metrics",
        "ranking"
      ],
      "id": 791
    },
    {
      "name": "tensorflow-qnd",
      "one_line_profile": "Command framework for TensorFlow model training and evaluation",
      "detailed_description": "A framework to simplify the creation of TensorFlow commands for training, evaluating, and inferencing with machine learning models.",
      "domains": [
        "AI4",
        "AI4-03",
        "Machine Learning"
      ],
      "subtask_category": [
        "model_training",
        "model_evaluation"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/raviqqe/tensorflow-qnd",
      "help_website": [],
      "license": "Unlicense",
      "tags": [
        "tensorflow",
        "cli",
        "training",
        "inference"
      ],
      "id": 792
    },
    {
      "name": "continuous-eval",
      "one_line_profile": "Data-driven evaluation framework for LLM applications",
      "detailed_description": "A framework for the continuous, data-driven evaluation of Large Language Model (LLM) powered applications, focusing on pipeline performance.",
      "domains": [
        "AI4",
        "AI4-03",
        "Natural Language Processing"
      ],
      "subtask_category": [
        "llm_evaluation",
        "pipeline_assessment"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/relari-ai/continuous-eval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "evaluation",
        "rag",
        "metrics"
      ],
      "id": 793
    },
    {
      "name": "auto-evaluator",
      "one_line_profile": "Evaluation tool for LLM QA chains",
      "detailed_description": "A tool designed to evaluate Question Answering (QA) chains built with Large Language Models, automating the assessment of response quality.",
      "domains": [
        "AI4",
        "AI4-03",
        "Natural Language Processing"
      ],
      "subtask_category": [
        "qa_evaluation",
        "llm_assessment"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/rlancemartin/auto-evaluator",
      "help_website": [],
      "license": null,
      "tags": [
        "llm",
        "qa",
        "evaluation",
        "langchain"
      ],
      "id": 794
    },
    {
      "name": "nanopore_assembly_and_polishing_assessment",
      "one_line_profile": "Pipeline for assessing Nanopore assembly and polishing",
      "detailed_description": "Automated pipelines for evaluating the performance of genome assembly and polishing tools specifically for Nanopore sequencing data, including visualization.",
      "domains": [
        "AI4",
        "AI4-03",
        "Bioinformatics",
        "Genomics"
      ],
      "subtask_category": [
        "assembly_evaluation",
        "quality_assessment",
        "nanopore_sequencing"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/rlorigro/nanopore_assembly_and_polishing_assessment",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "genomics",
        "nanopore",
        "assembly",
        "evaluation"
      ],
      "id": 795
    },
    {
      "name": "TestSuiteEval",
      "one_line_profile": "Semantic evaluation tool for Text-to-SQL models",
      "detailed_description": "Implements the semantic evaluation method for Text-to-SQL tasks using a distilled test suite, as proposed in EMNLP 2020.",
      "domains": [
        "AI4",
        "AI4-03",
        "Natural Language Processing"
      ],
      "subtask_category": [
        "model_evaluation",
        "text_to_sql",
        "semantic_parsing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ruiqi-zhong/TestSuiteEval",
      "help_website": [],
      "license": null,
      "tags": [
        "nlp",
        "text-to-sql",
        "evaluation",
        "emnlp"
      ],
      "id": 796
    },
    {
      "name": "auto_reports",
      "one_line_profile": "Dashboard for ocean model skill assessment",
      "detailed_description": "An interactive dashboard application for assessing the skill of oceanographic models, specifically for tidal analysis and regional performance evaluation.",
      "domains": [
        "AI4",
        "AI4-03",
        "Oceanography"
      ],
      "subtask_category": [
        "model_assessment",
        "tidal_analysis",
        "visualization"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/seareport/auto_reports",
      "help_website": [],
      "license": "EUPL-1.2",
      "tags": [
        "oceanography",
        "model-evaluation",
        "dashboard",
        "tides"
      ],
      "id": 797
    },
    {
      "name": "bjontegaard2",
      "one_line_profile": "Calculation of Bjontegaard metrics for video coding",
      "detailed_description": "A MATLAB implementation for calculating Bjontegaard metrics (BD-Rate), a standard method for evaluating the coding efficiency of video compression algorithms.",
      "domains": [
        "AI4",
        "AI4-03",
        "Signal Processing",
        "Video Compression"
      ],
      "subtask_category": [
        "metric_calculation",
        "performance_evaluation"
      ],
      "application_level": "library",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/serge-m/bjontegaard2",
      "help_website": [],
      "license": null,
      "tags": [
        "video-coding",
        "metrics",
        "matlab",
        "compression"
      ],
      "id": 798
    },
    {
      "name": "SigOpt EvalSet",
      "one_line_profile": "Benchmark suite of test functions for black-box optimization strategies",
      "detailed_description": "A collection of test functions and benchmarks designed to evaluate and compare the performance of various black-box optimization algorithms, facilitating research in hyperparameter tuning and optimization.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "optimization_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/sigopt/evalset",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "optimization",
        "black-box",
        "benchmarking"
      ],
      "id": 799
    },
    {
      "name": "pg_landmetrics",
      "one_line_profile": "PostgreSQL extension for landscape metrics calculations",
      "detailed_description": "A PostGIS extension that enables the calculation of landscape metrics directly within a database environment, supporting spatial analysis for ecology and geography.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "spatial_analysis",
        "landscape_metrics"
      ],
      "application_level": "library",
      "primary_language": "TSQL",
      "repo_url": "https://github.com/siose-innova/pg_landmetrics",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "gis",
        "landscape-ecology",
        "postgis"
      ],
      "id": 800
    },
    {
      "name": "dismay",
      "one_line_profile": "R package for calculating distance metrics on matrices",
      "detailed_description": "An R library providing efficient implementations for calculating various distance metrics on matrices, commonly used in statistical analysis and bioinformatics.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "statistical_analysis",
        "distance_metrics"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/skinnider/dismay",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "r-package",
        "statistics",
        "matrices"
      ],
      "id": 801
    },
    {
      "name": "Open Supply Chains",
      "one_line_profile": "Tool for modeling and analyzing supply chain sustainability metrics",
      "detailed_description": "An open-source codebase for visualizing and analyzing supply chains, including modules for calculating evaluation metrics such as carbon footprints, supporting research in sustainable operations.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "sustainability_analysis",
        "supply_chain_modeling"
      ],
      "application_level": "platform",
      "primary_language": "PHP",
      "repo_url": "https://github.com/supplychainstudies/OpenSupplyChains",
      "help_website": [],
      "license": null,
      "tags": [
        "supply-chain",
        "sustainability",
        "carbon-footprint"
      ],
      "id": 802
    },
    {
      "name": "BIG-Bench Hard",
      "one_line_profile": "Challenging subset of BIG-Bench tasks for LLM evaluation",
      "detailed_description": "A benchmark suite focusing on tasks from BIG-Bench where language models previously struggled, used to evaluate the reasoning capabilities of Chain-of-Thought prompting.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "llm_evaluation",
        "reasoning_benchmark"
      ],
      "application_level": "dataset",
      "primary_language": null,
      "repo_url": "https://github.com/suzgunmirac/BIG-Bench-Hard",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "benchmark",
        "chain-of-thought"
      ],
      "id": 803
    },
    {
      "name": "Test Suite SQL Eval",
      "one_line_profile": "Semantic evaluation harness for Text-to-SQL models",
      "detailed_description": "A testing suite and evaluation framework for Text-to-SQL systems, using distilled test suites to provide semantic correctness evaluation beyond simple string matching.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "text_to_sql"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/taoyds/test-suite-sql-eval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "nlp",
        "sql",
        "evaluation"
      ],
      "id": 804
    },
    {
      "name": "AlpacaEval",
      "one_line_profile": "Automatic evaluator for instruction-following language models",
      "detailed_description": "An LLM-based automatic evaluation framework designed to simulate human evaluation of instruction-following models, providing a fast and low-cost alternative to human annotation.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "llm_evaluation",
        "instruction_following"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/tatsu-lab/alpaca_eval",
      "help_website": [
        "https://tatsu-lab.github.io/alpaca_eval/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "evaluation",
        "automation"
      ],
      "id": 805
    },
    {
      "name": "TensorZero",
      "one_line_profile": "Unified stack for LLM engineering, optimization, and evaluation",
      "detailed_description": "An industrial-grade platform for building LLM applications that integrates gateway services with observability, optimization, and evaluation workflows to improve model performance.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "llmops",
        "model_evaluation",
        "optimization"
      ],
      "application_level": "platform",
      "primary_language": "Rust",
      "repo_url": "https://github.com/tensorzero/tensorzero",
      "help_website": [
        "https://www.tensorzero.com/docs"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llmops",
        "evaluation",
        "gateway"
      ],
      "id": 806
    },
    {
      "name": "hlb-gpt",
      "one_line_profile": "Minimalistic and fast researcher's toolbench for training and evaluating GPT models",
      "detailed_description": "A hackable and highly optimized toolbench designed for researchers to train and evaluate GPT-style models. It features extremely fast training speeds (e.g., <100 seconds for wikitext-103 on A100) and a minimalistic codebase to facilitate experimentation and scaling.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_training",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/tysam-code/hlb-gpt",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "gpt",
        "llm-training",
        "research-toolbench",
        "optimization"
      ],
      "id": 807
    },
    {
      "name": "aibench-llm-endpoints",
      "one_line_profile": "Metric collection runner for LLM inference endpoints",
      "detailed_description": "A runner tool designed to collect and report performance metrics from various LLM inference endpoints. It serves as a component of the Unify Hub ecosystem for benchmarking and monitoring LLM service quality.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "inference_benchmarking",
        "metric_collection"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/unifyai/aibench-llm-endpoints",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-inference",
        "benchmarking",
        "metrics",
        "unify-hub"
      ],
      "id": 808
    },
    {
      "name": "Gale",
      "one_line_profile": "PyTorch framework for reproducible deep learning experiments",
      "detailed_description": "Gale is a framework built on PyTorch designed to facilitate reproducible deep learning experiments. It provides structure and utilities to standardize experimental workflows, ensuring consistency in model training and evaluation.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "experiment_management",
        "reproducibility"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/v7labs/Gale",
      "help_website": [],
      "license": "LGPL-3.0",
      "tags": [
        "pytorch",
        "reproducibility",
        "deep-learning",
        "experiment-framework"
      ],
      "id": 809
    },
    {
      "name": "whisper-finetune",
      "one_line_profile": "Tool for fine-tuning and evaluating Whisper ASR models",
      "detailed_description": "A comprehensive tool for fine-tuning OpenAI's Whisper models on custom or Hugging Face datasets. It includes functionality for evaluating the performance of fine-tuned models for Automatic Speech Recognition (ASR) tasks.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "model_finetuning",
        "model_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/vasistalodagala/whisper-finetune",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "whisper",
        "asr",
        "fine-tuning",
        "speech-recognition"
      ],
      "id": 810
    },
    {
      "name": "Tiny QA Benchmark++",
      "one_line_profile": "Micro-benchmark suite and CI-ready eval harness for LLM smoke-testing",
      "detailed_description": "A lightweight benchmark suite and evaluation harness designed for fast smoke-testing and regression detection in Large Language Models. It includes a generator CLI and supports on-demand multilingual synthetic packs.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "benchmark",
        "regression_testing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/vincentkoc/tiny_qa_benchmark_pp",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-benchmark",
        "regression-testing",
        "ci-cd",
        "qa"
      ],
      "id": 811
    },
    {
      "name": "caption-eval",
      "one_line_profile": "Automated metric evaluation tool for image captions",
      "detailed_description": "A Python-based tool for evaluating sentence and image captions using standard automated metrics such as BLEU, METEOR, ROUGE, CIDEr, and SPICE. It facilitates the quantitative assessment of captioning models.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "caption_evaluation",
        "metric_calculation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/vsubhashini/caption-eval",
      "help_website": [],
      "license": null,
      "tags": [
        "nlp-evaluation",
        "image-captioning",
        "metrics",
        "bleu"
      ],
      "id": 812
    },
    {
      "name": "BVQA_Benchmark",
      "one_line_profile": "Benchmark for Blind Video Quality Assessment on User Generated Content",
      "detailed_description": "A resource list and performance benchmark for Blind Video Quality Assessment (BVQA) models, specifically targeting User Generated Content (UGC) datasets. It supports the evaluation and comparison of video quality assessment algorithms.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "video_quality_assessment",
        "benchmark"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/vztu/BVQA_Benchmark",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "video-quality",
        "bvqa",
        "benchmark",
        "ugc"
      ],
      "id": 813
    },
    {
      "name": "RMBench-2022",
      "one_line_profile": "Benchmark for robotic manipulation reinforcement learning",
      "detailed_description": "RMBench is a benchmark suite for robotic manipulation tasks involving high-dimensional continuous action and state spaces. It evaluates reinforcement learning algorithms that use pixel-based observations.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "robotics_benchmark",
        "reinforcement_learning"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/xiangyanfei212/RMBench-2022",
      "help_website": [],
      "license": null,
      "tags": [
        "robotics",
        "reinforcement-learning",
        "benchmark",
        "manipulation"
      ],
      "id": 814
    },
    {
      "name": "thoughtful-agents",
      "one_line_profile": "Framework for building and evaluating proactive LLM agents",
      "detailed_description": "A Python framework for constructing proactive LLM agents that simulate human-like cognitive processes. It enables agents to generate and evaluate thoughts in parallel with conversations, facilitating the development of more autonomous AI systems.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "agent_framework",
        "cognitive_simulation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/xybruceliu/thoughtful-agents",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-agents",
        "cognitive-architecture",
        "proactive-agents"
      ],
      "id": 815
    },
    {
      "name": "AC-EVAL",
      "one_line_profile": "Ancient Chinese evaluation suite for Large Language Models",
      "detailed_description": "AC-EVAL is a specialized benchmark suite designed to evaluate the performance of Large Language Models (LLMs) on tasks involving Ancient Chinese. It serves as a resource for assessing model capabilities in historical language understanding.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "benchmark",
        "nlp_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/yuting-wei/AC-EVAL",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ancient-chinese",
        "llm-benchmark",
        "nlp"
      ],
      "id": 816
    },
    {
      "name": "fastmri-reproducible-benchmark",
      "one_line_profile": "Reproducible benchmark methods for MRI reconstruction on fastMRI dataset",
      "detailed_description": "A repository providing reproducible methods and benchmarks for MRI reconstruction using the fastMRI dataset. It includes implementations of models like XPDNet to facilitate comparison and advancement in medical image reconstruction.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "mri_reconstruction",
        "benchmark"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/zaccharieramzi/fastmri-reproducible-benchmark",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mri-reconstruction",
        "fastmri",
        "medical-imaging",
        "benchmark"
      ],
      "id": 817
    },
    {
      "name": "ZeroEntropy Evals",
      "one_line_profile": "Evaluation suite for benchmarking retrievers and rerankers",
      "detailed_description": "An evaluation suite developed by ZeroEntropy to benchmark the performance of various information retrieval components, specifically retrievers and rerankers. It aids in selecting optimal components for search and RAG systems.",
      "domains": [
        "AI4",
        "AI4-03"
      ],
      "subtask_category": [
        "retrieval_benchmarking",
        "reranker_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/zeroentropy-ai/evals",
      "help_website": [],
      "license": null,
      "tags": [
        "retrieval",
        "reranking",
        "benchmark",
        "search"
      ],
      "id": 818
    },
    {
      "name": "Arline Benchmarks",
      "one_line_profile": "Benchmarking platform for quantum circuit mapping and compression algorithms",
      "detailed_description": "A platform designed to benchmark various quantum circuit mapping and compression algorithms against each other on a list of predefined hardware types and target circuit classes, facilitating evaluation in quantum computing research.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "benchmarking",
        "quantum_circuit_optimization",
        "algorithm_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/ArlineQ/arline_benchmarks",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "quantum-computing",
        "benchmarking",
        "circuit-mapping"
      ],
      "id": 819
    },
    {
      "name": "rd-cdm",
      "one_line_profile": "Ontology-based rare disease common data model",
      "detailed_description": "A repository for the ontology-based rare disease common data model (RD-CDM) that harmonizes international registries, FHIR, and Phenopackets to support rare disease research and data interoperability.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "data_harmonization",
        "ontology_mapping",
        "rare_disease_registry"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/BIH-CEI/rd-cdm",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rare-disease",
        "common-data-model",
        "fhir",
        "phenopackets"
      ],
      "id": 820
    },
    {
      "name": "CTF for Science Framework",
      "one_line_profile": "Benchmarking framework for dynamic system modeling methods",
      "detailed_description": "A modular and extensible platform designed for benchmarking modeling methods on dynamic systems. It supports the evaluation and comparison of models for systems like ordinary differential equations (ODEs) and partial differential equations (PDEs) using standardized datasets and metrics.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "benchmarking",
        "ode_modeling",
        "pde_modeling",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/CTF-for-Science/ctf4science",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "benchmarking",
        "dynamic-systems",
        "ode",
        "pde"
      ],
      "id": 821
    },
    {
      "name": "EvalAI",
      "one_line_profile": "Open source platform for evaluating and comparing AI algorithms",
      "detailed_description": "An open-source platform for hosting AI challenges and evaluating state-of-the-art AI algorithms. It provides a standardized environment for benchmarking models across various tasks, including scientific datasets and competitions.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "leaderboard",
        "model_evaluation",
        "benchmarking_platform"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/Cloud-CV/EvalAI",
      "help_website": [
        "https://eval.ai/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "benchmarking",
        "leaderboard",
        "ai-challenges",
        "evaluation"
      ],
      "id": 822
    },
    {
      "name": "DagsHub Client",
      "one_line_profile": "Client libraries for DagsHub scientific collaboration platform",
      "detailed_description": "Python client for DagsHub that facilitates scientific experiment tracking, data versioning, and collaboration. It integrates with MLflow and DVC to provide a unified interface for managing research projects.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_tracking",
        "data_versioning",
        "collaboration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/DagsHub/client",
      "help_website": [
        "https://dagshub.com/docs/"
      ],
      "license": "MIT",
      "tags": [
        "experiment-tracking",
        "mlflow",
        "dvc",
        "data-science"
      ],
      "id": 823
    },
    {
      "name": "Fast Data Science (fds)",
      "one_line_profile": "CLI for version controlling data and code in scientific projects",
      "detailed_description": "A command-line tool that wraps Git and DVC (Data Version Control) to streamline the versioning of code and large datasets in data science and scientific research workflows.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "data_versioning",
        "version_control",
        "workflow_management"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/DagsHub/fds",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "dvc",
        "git",
        "data-versioning",
        "cli"
      ],
      "id": 824
    },
    {
      "name": "OpenVectorFormat",
      "one_line_profile": "Data format for scanner-based laser-processing jobs",
      "detailed_description": "A library and data format description for storing scanner-based laser-processing jobs, including vector-geometry data, processing parameters, and metadata, used in manufacturing science and material processing.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "data_format",
        "manufacturing_process",
        "laser_processing"
      ],
      "application_level": "library",
      "primary_language": "Starlark",
      "repo_url": "https://github.com/Digital-Production-Aachen/OpenVectorFormat",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "laser-processing",
        "vector-data",
        "manufacturing",
        "file-format"
      ],
      "id": 825
    },
    {
      "name": "GeoMet Data Registry",
      "one_line_profile": "Registry system for managing and serving meteorological data via OGC standards",
      "detailed_description": "A system to manage access to Environment and Climate Change Canada's Meteorological Service of Canada (MSC) open data, including raw numerical weather prediction (NWP) model data layers and weather radar mosaics, via Open Geospatial Consortium (OGC) standards.",
      "domains": [
        "Meteorology",
        "Earth Science",
        "Data Management"
      ],
      "subtask_category": [
        "data_registry",
        "data_serving",
        "geospatial_data"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/ECCC-MSC/geomet-data-registry",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "meteorology",
        "ogc",
        "wms",
        "data-registry"
      ],
      "id": 826
    },
    {
      "name": "Otter",
      "one_line_profile": "Multi-modal model based on OpenFlamingo for instruction following",
      "detailed_description": "A multi-modal model based on OpenFlamingo, trained on MIMIC-IT, showcasing improved instruction-following and in-context learning ability for visual-language tasks.",
      "domains": [
        "Computer Vision",
        "Multimodal AI"
      ],
      "subtask_category": [
        "multimodal_modeling",
        "instruction_following"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/EvolvingLMMs-Lab/Otter",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "multimodal",
        "openflamingo",
        "vlm"
      ],
      "id": 827
    },
    {
      "name": "precisionFDA",
      "one_line_profile": "Cloud-based platform for benchmarking NGS analysis pipelines",
      "detailed_description": "A cloud-based platform that provides an environment where the community can test, pilot, and benchmark new approaches to validating their next-generation sequencing (NGS) analysis pipelines.",
      "domains": [
        "Bioinformatics",
        "Genomics",
        "AI4-04"
      ],
      "subtask_category": [
        "benchmarking",
        "pipeline_validation",
        "ngs_analysis"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/FDA/precisionFDA",
      "help_website": [
        "https://precision.fda.gov/"
      ],
      "license": "CC0-1.0",
      "tags": [
        "ngs",
        "benchmarking",
        "genomics",
        "fda"
      ],
      "id": 828
    },
    {
      "name": "SUPIR",
      "one_line_profile": "Model for photo-realistic image restoration",
      "detailed_description": "A deep learning model aiming at developing practical algorithms for photo-realistic image restoration in the wild.",
      "domains": [
        "Computer Vision",
        "Image Processing"
      ],
      "subtask_category": [
        "image_restoration",
        "super_resolution"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Fanghua-Yu/SUPIR",
      "help_website": [
        "https://suppixel.ai/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "image-restoration",
        "deep-learning",
        "restoration"
      ],
      "id": 829
    },
    {
      "name": "FastTrackML",
      "one_line_profile": "High-performance experiment tracking server compatible with MLflow",
      "detailed_description": "An experiment tracking server focused on speed and scalability, designed to be compatible with the MLflow client for tracking machine learning experiments.",
      "domains": [
        "AI4",
        "AI4-04",
        "MLOps"
      ],
      "subtask_category": [
        "experiment_tracking",
        "model_management"
      ],
      "application_level": "platform",
      "primary_language": "Go",
      "repo_url": "https://github.com/G-Research/fasttrackml",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "mlflow",
        "experiment-tracking",
        "mlops"
      ],
      "id": 830
    },
    {
      "name": "kedro-mlflow",
      "one_line_profile": "Kedro plugin for MLflow integration",
      "detailed_description": "A plugin for the Kedro framework that integrates MLflow capabilities, enabling machine learning model versioning, packaging, and experiment tracking within Kedro projects.",
      "domains": [
        "AI4",
        "AI4-04",
        "MLOps"
      ],
      "subtask_category": [
        "experiment_tracking",
        "pipeline_versioning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Galileo-Galilei/kedro-mlflow",
      "help_website": [
        "https://kedro-mlflow.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "kedro",
        "mlflow",
        "experiment-tracking"
      ],
      "id": 831
    },
    {
      "name": "Graph Learning Indexer (GLI)",
      "one_line_profile": "Benchmarking platform for graph learning",
      "detailed_description": "A contributor-friendly and metadata-rich platform for graph learning benchmarks, facilitating dataloading, benchmarking, and tagging of graph datasets and models.",
      "domains": [
        "AI4",
        "AI4-04",
        "Graph Learning"
      ],
      "subtask_category": [
        "benchmarking",
        "dataset_management",
        "graph_learning"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/Graph-Learning-Benchmarks/gli",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "graph-learning",
        "benchmark",
        "gnn"
      ],
      "id": 832
    },
    {
      "name": "DVC (Deep Video Compression)",
      "one_line_profile": "End-to-end deep video compression framework",
      "detailed_description": "An end-to-end deep learning framework for video compression, providing a model implementation for efficient video coding.",
      "domains": [
        "Computer Vision",
        "Video Processing"
      ],
      "subtask_category": [
        "video_compression",
        "neural_compression"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/GuoLusjtu/DVC",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "video-compression",
        "deep-learning",
        "cvpr"
      ],
      "id": 833
    },
    {
      "name": "LH-VLN",
      "one_line_profile": "Benchmark and platform for Long-Horizon Vision-Language Navigation",
      "detailed_description": "A platform and benchmark suite for Long-Horizon Vision-Language Navigation (LH-VLN), designed to evaluate agents in complex navigation tasks.",
      "domains": [
        "AI4",
        "AI4-04",
        "Robotics",
        "Computer Vision"
      ],
      "subtask_category": [
        "benchmarking",
        "vision_language_navigation"
      ],
      "application_level": "platform",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/HCPLab-SYSU/LH-VLN",
      "help_website": [],
      "license": null,
      "tags": [
        "vln",
        "benchmark",
        "navigation"
      ],
      "id": 834
    },
    {
      "name": "Snowman",
      "one_line_profile": "Data matching benchmark platform",
      "detailed_description": "A benchmarking platform designed for evaluating and comparing data matching (entity resolution) algorithms.",
      "domains": [
        "AI4",
        "AI4-04",
        "Data Science"
      ],
      "subtask_category": [
        "benchmarking",
        "data_matching",
        "entity_resolution"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/HPI-Information-Systems/snowman",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "benchmark",
        "data-matching",
        "entity-resolution"
      ],
      "id": 835
    },
    {
      "name": "Fast-AgingGAN",
      "one_line_profile": "Deep learning model for face aging",
      "detailed_description": "A deep learning model designed to simulate aging effects on face images, running efficiently on GPUs.",
      "domains": [
        "Computer Vision",
        "Image Synthesis"
      ],
      "subtask_category": [
        "image_synthesis",
        "face_aging"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/HasnainRaz/Fast-AgingGAN",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gan",
        "face-aging",
        "deep-learning"
      ],
      "id": 836
    },
    {
      "name": "Fast-SRGAN",
      "one_line_profile": "Fast Super Resolution GAN for video upsampling",
      "detailed_description": "A fast deep learning model (SRGAN) to upsample low-resolution videos to high resolution, optimized for speed.",
      "domains": [
        "Computer Vision",
        "Image Processing"
      ],
      "subtask_category": [
        "super_resolution",
        "video_upsampling"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/HasnainRaz/Fast-SRGAN",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "srgan",
        "super-resolution",
        "upsampling"
      ],
      "id": 837
    },
    {
      "name": "CMF (Common Metadata Framework)",
      "one_line_profile": "Library for tracking metadata and lineage in ML pipelines",
      "detailed_description": "A library to collect, store, and query metadata associated with ML pipelines, tracking lineage for artifacts and executions in distributed AI workflows.",
      "domains": [
        "AI4",
        "AI4-04",
        "MLOps"
      ],
      "subtask_category": [
        "metadata_tracking",
        "lineage_tracking",
        "experiment_logging"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/HewlettPackard/cmf",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "metadata",
        "mlops",
        "lineage"
      ],
      "id": 838
    },
    {
      "name": "FakeSV",
      "one_line_profile": "Multimodal benchmark for fake news detection",
      "detailed_description": "A multimodal benchmark dataset and platform with rich social context for fake news detection on short video platforms.",
      "domains": [
        "AI4",
        "AI4-04",
        "Social Computing"
      ],
      "subtask_category": [
        "benchmarking",
        "fake_news_detection"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/ICTMCG/FakeSV",
      "help_website": [],
      "license": null,
      "tags": [
        "benchmark",
        "fake-news",
        "multimodal"
      ],
      "id": 839
    },
    {
      "name": "Sacred",
      "one_line_profile": "Experiment configuration and tracking tool",
      "detailed_description": "A tool to help configure, organize, log, and reproduce computational experiments, widely used in machine learning research.",
      "domains": [
        "AI4",
        "AI4-04",
        "MLOps"
      ],
      "subtask_category": [
        "experiment_tracking",
        "reproducibility",
        "configuration_management"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/IDSIA/sacred",
      "help_website": [
        "https://sacred.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "experiment-tracking",
        "reproducibility",
        "logging"
      ],
      "id": 840
    },
    {
      "name": "INCF EEG Database",
      "one_line_profile": "Portal for managing and sharing EEG/ERP data",
      "detailed_description": "A portal enabling researchers to store, update, download, and search data and metadata from EEG/ERP experiments.",
      "domains": [
        "Neuroscience",
        "Data Management"
      ],
      "subtask_category": [
        "data_sharing",
        "eeg_database",
        "metadata_management"
      ],
      "application_level": "platform",
      "primary_language": "Java",
      "repo_url": "https://github.com/INCF/eeg-database",
      "help_website": [],
      "license": null,
      "tags": [
        "eeg",
        "neuroscience",
        "database"
      ],
      "id": 841
    },
    {
      "name": "Incense",
      "one_line_profile": "Interactive data retrieval for Sacred experiments",
      "detailed_description": "A library to interactively retrieve and analyze data from experiments logged with the Sacred tool, facilitating experiment visualization and comparison.",
      "domains": [
        "AI4",
        "AI4-04",
        "MLOps"
      ],
      "subtask_category": [
        "experiment_analysis",
        "data_retrieval"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/JarnoRFB/incense",
      "help_website": [
        "https://incense.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "sacred",
        "experiment-analysis",
        "visualization"
      ],
      "id": 842
    },
    {
      "name": "MLJModels.jl",
      "one_line_profile": "Model registry for the MLJ framework in Julia",
      "detailed_description": "The model registry and tools for model queries and code loading for the MLJ (Machine Learning in Julia) framework.",
      "domains": [
        "AI4",
        "AI4-04",
        "Machine Learning"
      ],
      "subtask_category": [
        "model_registry",
        "model_loading"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/JuliaAI/MLJModels.jl",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "julia",
        "mlj",
        "model-registry"
      ],
      "id": 843
    },
    {
      "name": "Snowboy",
      "one_line_profile": "DNN-based hotword detection library",
      "detailed_description": "A customizable hotword detection engine based on deep neural networks, widely used in robotics and AI applications for audio processing.",
      "domains": [
        "Audio Processing",
        "AI"
      ],
      "subtask_category": [
        "hotword_detection",
        "audio_analysis"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/Kitt-AI/snowboy",
      "help_website": [
        "https://snowboy.kitt.ai/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "audio",
        "speech-recognition",
        "hotword"
      ],
      "id": 844
    },
    {
      "name": "METR Eval Analysis",
      "one_line_profile": "DVC pipeline for analyzing AI model evaluation data",
      "detailed_description": "A public repository containing METR's data version control (DVC) pipeline designed for the analysis of evaluation data, facilitating reproducible research in model evaluation.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "evaluation_analysis",
        "reproducibility"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/METR/eval-analysis-public",
      "help_website": [],
      "license": null,
      "tags": [
        "dvc",
        "evaluation",
        "pipeline"
      ],
      "id": 845
    },
    {
      "name": "MLReef",
      "one_line_profile": "Collaboration workspace for Machine Learning experiment tracking",
      "detailed_description": "A collaboration platform for machine learning that provides experiment tracking, model management, and reproducibility features to streamline the ML lifecycle.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_tracking",
        "collaboration"
      ],
      "application_level": "platform",
      "primary_language": "Kotlin",
      "repo_url": "https://github.com/MLReef/mlreef",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "mlops",
        "experiment-tracking",
        "collaboration"
      ],
      "id": 846
    },
    {
      "name": "OpenCaptchaWorld",
      "one_line_profile": "Benchmark for evaluating MLLM agents via CAPTCHA puzzles",
      "detailed_description": "A web-based benchmark and platform designed to evaluate the visual reasoning and interaction capabilities of Multimodal LLM agents through diverse CAPTCHA puzzles.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "benchmarking",
        "visual_reasoning"
      ],
      "application_level": "dataset",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/MetaAgentX/OpenCaptchaWorld",
      "help_website": [],
      "license": null,
      "tags": [
        "benchmark",
        "mllm",
        "captcha"
      ],
      "id": 847
    },
    {
      "name": "MetaBox",
      "one_line_profile": "Benchmarking platform for Meta-Black-Box Optimization",
      "detailed_description": "A comprehensive benchmarking platform designed for evaluating Meta-Black-Box Optimization algorithms, facilitating comparison and analysis of optimization strategies.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "benchmarking",
        "optimization"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/MetaEvo/MetaBox",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "optimization",
        "benchmark",
        "black-box"
      ],
      "id": 848
    },
    {
      "name": "PipelineX",
      "one_line_profile": "ML pipeline builder integrating Kedro and MLflow",
      "detailed_description": "A Python package that simplifies building machine learning pipelines for experimentation by integrating frameworks like Kedro and MLflow, focusing on developer efficiency.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "pipeline_orchestration",
        "experiment_tracking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Minyus/pipelinex",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "pipeline",
        "mlflow",
        "kedro"
      ],
      "id": 849
    },
    {
      "name": "MiroFlow",
      "one_line_profile": "Open-source deep research agent for reproducible benchmarks",
      "detailed_description": "A fully open-source deep research agent framework designed to achieve reproducible state-of-the-art performance on various AI benchmarks like GAIA and FutureX.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "research_agent",
        "reproducibility"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/MiroMindAI/MiroFlow",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "agent",
        "benchmark",
        "reproducibility"
      ],
      "id": 850
    },
    {
      "name": "CoolGraph",
      "one_line_profile": "Library for simplifying Graph Neural Networks (GNN) usage",
      "detailed_description": "A machine learning library designed to make Graph Neural Networks (GNNs) easier to implement and use for scientific and industrial graph learning tasks.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "graph_learning",
        "modeling"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/MobileTeleSystems/CoolGraph",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gnn",
        "graph-learning",
        "deep-learning"
      ],
      "id": 851
    },
    {
      "name": "ModelChimp",
      "one_line_profile": "Experiment tracking for deep learning projects",
      "detailed_description": "An experiment tracking tool tailored for machine and deep learning projects, allowing researchers to log, visualize, and compare model experiments.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_tracking",
        "visualization"
      ],
      "application_level": "platform",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/ModelChimp/modelchimp",
      "help_website": [],
      "license": "BSD-2-Clause",
      "tags": [
        "experiment-tracking",
        "visualization",
        "deep-learning"
      ],
      "id": 852
    },
    {
      "name": "NVIDIA DeepLearningExamples",
      "one_line_profile": "State-of-the-art deep learning scripts for reproduction and benchmarking",
      "detailed_description": "A comprehensive collection of state-of-the-art deep learning model implementations optimized for NVIDIA GPUs, serving as a primary resource for performance benchmarking and experimental reproduction.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "model_reproduction",
        "benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/NVIDIA/DeepLearningExamples",
      "help_website": [],
      "license": null,
      "tags": [
        "benchmark",
        "reproducibility",
        "nvidia"
      ],
      "id": 853
    },
    {
      "name": "DANCE",
      "one_line_profile": "Deep learning library and benchmark for single-cell analysis",
      "detailed_description": "DANCE is a deep learning library and benchmark platform specifically designed for single-cell analysis in computational biology, providing tools for clustering, imputation, and integration.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "single_cell_analysis",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/OmicsML/dance",
      "help_website": [],
      "license": "BSD-2-Clause",
      "tags": [
        "single-cell",
        "benchmark",
        "deep-learning"
      ],
      "id": 854
    },
    {
      "name": "What-If Tool",
      "one_line_profile": "Interactive interface for probing and analyzing ML models",
      "detailed_description": "A visual interface designed to help researchers and practitioners analyze machine learning models, understand their behavior, and assess fairness and performance across different data slices.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "model_interpretation",
        "analysis"
      ],
      "application_level": "platform",
      "primary_language": "HTML",
      "repo_url": "https://github.com/PAIR-code/what-if-tool",
      "help_website": [
        "https://pair-code.github.io/what-if-tool/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "visualization",
        "fairness",
        "interpretability"
      ],
      "id": 855
    },
    {
      "name": "Open-Sora-Plan",
      "one_line_profile": "Open source reproduction of Sora text-to-video model",
      "detailed_description": "An open-source project aiming to reproduce the Sora text-to-video model, providing training scripts, model architectures, and inference tools for the research community.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "video_generation",
        "model_reproduction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/PKU-YuanGroup/Open-Sora-Plan",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "text-to-video",
        "reproduction",
        "sora"
      ],
      "id": 856
    },
    {
      "name": "mousetrap-os",
      "one_line_profile": "Mouse-tracking plugin for OpenSesame psychological experiments",
      "detailed_description": "A plugin for the OpenSesame experiment builder that enables mouse-tracking data collection for psychological and behavioral research.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "data_collection",
        "behavioral_tracking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/PascalKieslich/mousetrap-os",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "psychology",
        "mouse-tracking",
        "opensesame",
        "behavioral-science"
      ],
      "id": 857
    },
    {
      "name": "PLIP",
      "one_line_profile": "Pathology Language and Image Pre-Training foundation model",
      "detailed_description": "A vision and language foundation model for pathology AI, fine-tuned from CLIP to extract visual and language features from pathology images and text descriptions.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "feature_extraction",
        "image_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/PathologyFoundation/plip",
      "help_website": [],
      "license": null,
      "tags": [
        "pathology",
        "foundation-model",
        "medical-imaging",
        "clip"
      ],
      "id": 858
    },
    {
      "name": "ProteoBench",
      "one_line_profile": "Community-curated benchmarks for proteomics data analysis",
      "detailed_description": "An open and collaborative platform for continuous, easy, and controlled comparison of proteomics data analysis workflows and pipelines.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "benchmarking",
        "proteomics_analysis"
      ],
      "application_level": "platform",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/Proteobench/ProteoBench",
      "help_website": [
        "https://proteobench.cubimed.rub.de/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "proteomics",
        "benchmark",
        "bioinformatics",
        "data-analysis"
      ],
      "id": 859
    },
    {
      "name": "topsacred",
      "one_line_profile": "Viewer and toolset for Sacred experiment databases",
      "detailed_description": "A collection of functions and tools to interact with and visualize data stored by Sacred, a tool for configuring and organizing computational experiments.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_tracking",
        "visualization"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/Qwlouse/topsacred",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "sacred",
        "experiment-tracking",
        "reproducibility"
      ],
      "id": 860
    },
    {
      "name": "rocHPCG",
      "one_line_profile": "HPCG benchmark for ROCm platform",
      "detailed_description": "Implementation of the High Performance Conjugate Gradients (HPCG) benchmark, designed to measure performance of HPC systems, optimized for the ROCm platform.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_analysis"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/ROCm/rocHPCG",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "hpc",
        "benchmark",
        "rocm",
        "gpu-computing"
      ],
      "id": 861
    },
    {
      "name": "RagView",
      "one_line_profile": "Unified evaluation platform for RAG methods",
      "detailed_description": "A platform to benchmark different Retrieval-Augmented Generation (RAG) methods on specific datasets, facilitating comparative analysis of AI models.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "benchmarking",
        "model_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/RagView/RagView",
      "help_website": [],
      "license": null,
      "tags": [
        "rag",
        "benchmark",
        "nlp",
        "evaluation"
      ],
      "id": 862
    },
    {
      "name": "RapidFire AI",
      "one_line_profile": "Toolchain for rapid AI model customization",
      "detailed_description": "A framework facilitating rapid AI customization ranging from Retrieval-Augmented Generation (RAG) to Fine-Tuning, supporting AI research workflows.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "model_customization",
        "fine_tuning"
      ],
      "application_level": "workflow",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/RapidFireAI/rapidfireai",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "fine-tuning",
        "ai-workflow"
      ],
      "id": 863
    },
    {
      "name": "RoboVerse",
      "one_line_profile": "Benchmark and platform for robot learning",
      "detailed_description": "A unified platform, dataset, and benchmark designed for scalable and generalizable robot learning research, enabling simulation and evaluation of robotic agents.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "benchmarking",
        "simulation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/RoboVerseOrg/RoboVerse",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "robotics",
        "benchmark",
        "simulation",
        "embodied-ai"
      ],
      "id": 864
    },
    {
      "name": "SREGym",
      "one_line_profile": "Benchmark platform for SRE AI agents",
      "detailed_description": "An AI-native platform for benchmarking Site Reliability Engineering (SRE) agents, evaluating their performance in managing software reliability tasks.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "benchmarking",
        "agent_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/SREGym/SREGym",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "sre",
        "ai-agents",
        "benchmark",
        "reliability-engineering"
      ],
      "id": 865
    },
    {
      "name": "QA-Board",
      "one_line_profile": "Experiment tracker and visualization tool",
      "detailed_description": "An experiment tracker to organize, visualize, compare, and share algorithm runs, designed to remove toil from R&D and performance tuning.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_tracking",
        "visualization"
      ],
      "application_level": "platform",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/Samsung/qaboard",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "experiment-tracking",
        "visualization",
        "r-and-d"
      ],
      "id": 866
    },
    {
      "name": "SpeechIO Leaderboard",
      "one_line_profile": "Benchmarking platform for Automatic Speech Recognition",
      "detailed_description": "A robust and comprehensive benchmarking platform for evaluating Automatic Speech Recognition (ASR) systems.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "benchmarking",
        "model_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/SpeechColab/Leaderboard",
      "help_website": [],
      "license": null,
      "tags": [
        "asr",
        "speech-recognition",
        "benchmark",
        "leaderboard"
      ],
      "id": 867
    },
    {
      "name": "tensorboard-aggregator",
      "one_line_profile": "Tool to aggregate TensorBoard runs",
      "detailed_description": "A utility to aggregate multiple TensorBoard runs into new summary or CSV files, facilitating the analysis of multiple experiments.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_tracking",
        "data_aggregation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Spenhouet/tensorboard-aggregator",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "tensorboard",
        "experiment-analysis",
        "visualization"
      ],
      "id": 868
    },
    {
      "name": "BEHAVIOR-1K",
      "one_line_profile": "Benchmark platform for Embodied AI",
      "detailed_description": "A platform for accelerating Embodied AI research, providing a benchmark for evaluating agents on human-centric activities in simulation.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "benchmarking",
        "simulation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/StanfordVL/BEHAVIOR-1K",
      "help_website": [],
      "license": null,
      "tags": [
        "embodied-ai",
        "benchmark",
        "simulation",
        "robotics"
      ],
      "id": 869
    },
    {
      "name": "RaySAR",
      "one_line_profile": "3D Synthetic Aperture Radar (SAR) simulator",
      "detailed_description": "A 3D synthetic aperture radar (SAR) simulator that generates SAR image layers from detailed 3D object models, enabling localization and signal analysis.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "simulation",
        "data_generation"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/StefanJAuer/RaySAR",
      "help_website": [],
      "license": null,
      "tags": [
        "sar",
        "radar",
        "simulation",
        "remote-sensing"
      ],
      "id": 870
    },
    {
      "name": "FHIRFLARE-IG-Toolkit",
      "one_line_profile": "Toolkit for managing FHIR Implementation Guides",
      "detailed_description": "A comprehensive web app for managing FHIR Implementation Guides (IGs), including import, validation, processing, and conversion features for medical informatics.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "data_processing",
        "standardization"
      ],
      "application_level": "platform",
      "primary_language": "HTML",
      "repo_url": "https://github.com/Sudo-JHare/FHIRFLARE-IG-Toolkit",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "fhir",
        "medical-informatics",
        "healthcare-data",
        "interoperability"
      ],
      "id": 871
    },
    {
      "name": "SwanLab",
      "one_line_profile": "AI training tracking and visualization tool",
      "detailed_description": "An open-source AI training tracking and visualization tool that integrates with popular frameworks like PyTorch and Transformers to monitor experiments.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_tracking",
        "visualization"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/SwanHubX/SwanLab",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "experiment-tracking",
        "visualization",
        "mlops",
        "training-monitoring"
      ],
      "id": 872
    },
    {
      "name": "FedScale",
      "one_line_profile": "Federated Learning benchmarking platform",
      "detailed_description": "A scalable and extensible open-source platform for benchmarking and deploying federated learning (FL) algorithms and systems.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "benchmarking",
        "federated_learning"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/SymbioticLab/FedScale",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "federated-learning",
        "benchmark",
        "distributed-systems"
      ],
      "id": 873
    },
    {
      "name": "tensorboard_logger",
      "one_line_profile": "Library to log TensorBoard events without TensorFlow dependency",
      "detailed_description": "A Python library that allows users to log metrics, images, and histograms to TensorBoard event files without needing to install the full TensorFlow framework. It is useful for lightweight experiment tracking in PyTorch or other non-TF environments.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_tracking",
        "visualization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/TeamHG-Memex/tensorboard_logger",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "tensorboard",
        "logging",
        "visualization"
      ],
      "id": 874
    },
    {
      "name": "AI-Infra-Guard",
      "one_line_profile": "Comprehensive AI Red Teaming and safety evaluation platform",
      "detailed_description": "A platform developed by Tencent Zhuque Lab for AI Red Teaming, focusing on evaluating the safety, security, and robustness of AI models through adversarial testing and vulnerability scanning.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "red_teaming",
        "safety_testing"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/Tencent/AI-Infra-Guard",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "red-teaming",
        "ai-safety",
        "security"
      ],
      "id": 875
    },
    {
      "name": "GFPGAN",
      "one_line_profile": "Practical algorithm for real-world face restoration",
      "detailed_description": "A blind face restoration algorithm that leverages a Generative Facial Prior (GFP) to restore low-quality face images. It is widely used for image enhancement and preprocessing in computer vision research.",
      "domains": [
        "Computer Vision",
        "Image Processing"
      ],
      "subtask_category": [
        "image_restoration",
        "face_enhancement"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/TencentARC/GFPGAN",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "gan",
        "face-restoration",
        "super-resolution"
      ],
      "id": 876
    },
    {
      "name": "LLMstudio",
      "one_line_profile": "Framework and UI for LLM fine-tuning and prompt engineering",
      "detailed_description": "A framework designed to manage the lifecycle of Large Language Model (LLM) applications, offering tools for prompt engineering, fine-tuning, and evaluating models, bridging the gap between experimentation and production.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "llm_finetuning",
        "prompt_engineering"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/TensorOpsAI/LLMstudio",
      "help_website": [],
      "license": "MPL-2.0",
      "tags": [
        "llm",
        "finetuning",
        "mlops"
      ],
      "id": 877
    },
    {
      "name": "Local-LLM-Comparison-Colab-UI",
      "one_line_profile": "WebUI for benchmarking and comparing local LLMs",
      "detailed_description": "A Colab-compatible WebUI tool designed to run and compare the performance and outputs of various Large Language Models (LLMs) on consumer hardware, facilitating model evaluation and selection.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "model_benchmarking",
        "model_comparison"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/Troyanovsky/Local-LLM-Comparison-Colab-UI",
      "help_website": [],
      "license": null,
      "tags": [
        "llm",
        "benchmarking",
        "colab"
      ],
      "id": 878
    },
    {
      "name": "CPM-1-Generate",
      "one_line_profile": "Inference code for Chinese Pre-Trained Language Models (CPM-LM)",
      "detailed_description": "Provides the generation code and model interface for CPM-1, a large-scale Chinese pre-trained language model, enabling researchers to perform text generation and downstream NLP tasks.",
      "domains": [
        "NLP",
        "AI Models"
      ],
      "subtask_category": [
        "text_generation",
        "language_modeling"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/TsinghuaAI/CPM-1-Generate",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nlp",
        "chinese-llm",
        "text-generation"
      ],
      "id": 879
    },
    {
      "name": "ModelDB",
      "one_line_profile": "Open source system for ML model versioning and experiment management",
      "detailed_description": "A system to manage machine learning models, including versioning models, tracking metadata, and managing experiments. It helps researchers ensure reproducibility and track the history of model development.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_management",
        "model_versioning"
      ],
      "application_level": "platform",
      "primary_language": "Java",
      "repo_url": "https://github.com/VertaAI/modeldb",
      "help_website": [
        "https://www.verta.ai/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "mlops",
        "experiment-tracking",
        "model-registry"
      ],
      "id": 880
    },
    {
      "name": "VevestaX",
      "one_line_profile": "Lightweight library for tracking ML experiments and EDA",
      "detailed_description": "A Python library designed to track machine learning experiments and exploratory data analysis (EDA) with minimal code changes, automatically logging features, parameters, and performance metrics.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_tracking",
        "eda_logging"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/Vevesta/VevestaX",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "experiment-tracking",
        "eda",
        "mlops"
      ],
      "id": 881
    },
    {
      "name": "NeuroNLP2",
      "one_line_profile": "Deep neural models library for core NLP tasks",
      "detailed_description": "A PyTorch-based library implementing deep neural network models for various core Natural Language Processing tasks, serving as a toolkit for NLP research and experimentation.",
      "domains": [
        "NLP"
      ],
      "subtask_category": [
        "nlp_modeling",
        "sequence_labeling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/XuezheMax/NeuroNLP2",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "nlp",
        "pytorch",
        "neural-networks"
      ],
      "id": 882
    },
    {
      "name": "Xwin-LM",
      "one_line_profile": "Models and methods for LLM alignment and reproducibility",
      "detailed_description": "Provides a suite of Large Language Models and alignment methodologies (like RLHF) focused on stability and reproducibility, serving as a resource for research into LLM alignment techniques.",
      "domains": [
        "AI4",
        "AI Models"
      ],
      "subtask_category": [
        "llm_alignment",
        "rlhf"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Xwin-LM/Xwin-LM",
      "help_website": [],
      "license": null,
      "tags": [
        "llm",
        "alignment",
        "rlhf"
      ],
      "id": 883
    },
    {
      "name": "Geochemistrypi",
      "one_line_profile": "Automated machine learning framework for geochemistry",
      "detailed_description": "An open-source Python framework designed for data-driven discovery in geochemistry. It automates the machine learning pipeline for analyzing geochemical data, supporting tasks like classification, regression, and clustering in geological contexts.",
      "domains": [
        "AI4S",
        "Geochemistry"
      ],
      "subtask_category": [
        "geochemical_analysis",
        "scientific_discovery"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/ZJUEarthData/Geochemistrypi",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "geochemistry",
        "automl",
        "ai4science"
      ],
      "id": 884
    },
    {
      "name": "ACTS",
      "one_line_profile": "Toolkit for charged particle track reconstruction in high energy physics",
      "detailed_description": "An experiment-independent toolkit for track reconstruction in high energy physics experiments. It provides modern, thread-safe implementations of tracking algorithms and geometry handling for particle physics research.",
      "domains": [
        "Physics",
        "High Energy Physics"
      ],
      "subtask_category": [
        "track_reconstruction",
        "particle_physics_simulation"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/acts-project/acts",
      "help_website": [
        "https://acts.readthedocs.io/"
      ],
      "license": "MPL-2.0",
      "tags": [
        "physics",
        "tracking",
        "cern"
      ],
      "id": 885
    },
    {
      "name": "OlliePy",
      "one_line_profile": "Library for ML experiment evaluation and data exploration",
      "detailed_description": "A Python package that assists data scientists in exploring data and evaluating machine learning experiments by generating web-based reports and dashboards for analysis.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_evaluation",
        "eda"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ahmed-mohamed-sn/olliePy",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "data-science",
        "visualization",
        "evaluation"
      ],
      "id": 886
    },
    {
      "name": "Aim",
      "one_line_profile": "Open-source experiment tracker for machine learning metadata",
      "detailed_description": "Aim is an easy-to-use and supercharged open-source experiment tracker that logs training runs, hyperparameters, and metrics, providing a UI for comparison and visualization of ML experiments.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_tracking",
        "metrics_logging",
        "visualization"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/aimhubio/aim",
      "help_website": [
        "https://aimstack.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "experiment-tracking",
        "mlops",
        "visualization",
        "metadata"
      ],
      "id": 887
    },
    {
      "name": "aimlflow",
      "one_line_profile": "Integration tool to sync MLflow runs with Aim",
      "detailed_description": "A synchronization tool that allows users to track and visualize MLflow experiments using the Aim UI, bridging the gap between the two experiment tracking ecosystems.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_tracking",
        "interoperability"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aimhubio/aimlflow",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "mlflow",
        "aim",
        "integration",
        "experiment-tracking"
      ],
      "id": 888
    },
    {
      "name": "Mandala",
      "one_line_profile": "Experiment tracking framework with integrated persistence logic",
      "detailed_description": "Mandala is a framework that integrates experiment tracking directly with Python's persistence logic, enabling automatic memoization and queryable storage of computational graph results for data science projects.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_tracking",
        "data_persistence",
        "reproducibility"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/amakelov/mandala",
      "help_website": [
        "https://amakelov.github.io/mandala/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "experiment-tracking",
        "memoization",
        "data-management"
      ],
      "id": 889
    },
    {
      "name": "MLflow Export Import",
      "one_line_profile": "Tools to export and import MLflow experiments and models",
      "detailed_description": "A utility set for copying MLflow objects (experiments, runs, registered models) from one MLflow tracking server to another, facilitating data migration and backup in research workflows.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_management",
        "data_migration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/amesar/mlflow-export-import",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "mlflow",
        "migration",
        "experiment-management"
      ],
      "id": 890
    },
    {
      "name": "Grand",
      "one_line_profile": "Scalable and interoperable Python graph library",
      "detailed_description": "Grand allows using familiar Python graph APIs (like NetworkX) on scalable backends (SQL, graph databases), enabling analysis of large-scale scientific network data that exceeds memory limits.",
      "domains": [
        "AI4",
        "Data Science"
      ],
      "subtask_category": [
        "graph_analysis",
        "data_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aplbrain/grand",
      "help_website": [
        "https://grand.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "graph-theory",
        "network-analysis",
        "scalability"
      ],
      "id": 891
    },
    {
      "name": "Labwatch",
      "one_line_profile": "Hyperparameter optimization extension for Sacred",
      "detailed_description": "Labwatch connects the Sacred experiment tracking tool with various hyperparameter optimization libraries, enabling automated search and logging of optimal parameters in ML experiments.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "hyperparameter_optimization",
        "experiment_tracking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/automl/labwatch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "hpo",
        "sacred",
        "optimization"
      ],
      "id": 892
    },
    {
      "name": "Ogama",
      "one_line_profile": "OpenGazeAndMouseAnalyzer for eye and mouse tracking data",
      "detailed_description": "Ogama is a software application for recording and analyzing eye- and mouse-tracking data from experimental setups, providing features for fixation detection, heatmaps, and statistical analysis.",
      "domains": [
        "Neuroscience",
        "HCI"
      ],
      "subtask_category": [
        "data_analysis",
        "behavioral_tracking",
        "visualization"
      ],
      "application_level": "solver",
      "primary_language": "C#",
      "repo_url": "https://github.com/avosskuehler/ogama",
      "help_website": [
        "http://www.ogama.net/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "eye-tracking",
        "data-analysis",
        "experiment-control"
      ],
      "id": 893
    },
    {
      "name": "SageMaker Entrypoint Utilities",
      "one_line_profile": "Utilities for SageMaker training entrypoints",
      "detailed_description": "A library providing logging handlers, progress bar management, and hyperparameter parsing utilities to simplify the creation of robust training entrypoint scripts for scientific ML workflows on SageMaker.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "workflow_utility",
        "logging"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aws-samples/amazon-sagemaker-entrypoint-utilities",
      "help_website": [],
      "license": "MIT-0",
      "tags": [
        "sagemaker",
        "utilities",
        "logging"
      ],
      "id": 894
    },
    {
      "name": "Foundation Model Benchmarking Tool",
      "one_line_profile": "Benchmarking tool for foundation models on AWS",
      "detailed_description": "A tool to benchmark the performance (latency, throughput, cost) of foundation models across various instance types and serving stacks, facilitating model selection for research and production.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/aws-samples/foundation-model-benchmarking-tool",
      "help_website": [],
      "license": "MIT-0",
      "tags": [
        "benchmarking",
        "foundation-models",
        "llm"
      ],
      "id": 895
    },
    {
      "name": "ML Lineage Helper",
      "one_line_profile": "Wrapper for SageMaker ML Lineage Tracking",
      "detailed_description": "A helper library that extends SageMaker's lineage tracking capabilities to cover end-to-end ML lifecycles, including feature store groups and artifact queries, ensuring reproducibility and auditability.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "lineage_tracking",
        "provenance"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aws-samples/ml-lineage-helper",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "lineage",
        "sagemaker",
        "reproducibility"
      ],
      "id": 896
    },
    {
      "name": "AWS SDK for pandas",
      "one_line_profile": "Pandas integration for AWS data services",
      "detailed_description": "An open-source Python library (formerly AWS Wrangler) that extends Pandas to easily connect with AWS data services like Athena, Glue, and Redshift, facilitating data processing for scientific workflows.",
      "domains": [
        "AI4",
        "Data Science"
      ],
      "subtask_category": [
        "data_processing",
        "data_integration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aws/aws-sdk-pandas",
      "help_website": [
        "https://aws-sdk-pandas.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "pandas",
        "etl",
        "data-engineering"
      ],
      "id": 897
    },
    {
      "name": "Graph Notebook",
      "one_line_profile": "Jupyter notebook extension for graph databases",
      "detailed_description": "A library extending Jupyter notebooks to integrate with graph databases (Gremlin, SPARQL, openCypher), providing visualization and query capabilities for graph data analysis.",
      "domains": [
        "AI4",
        "Data Science"
      ],
      "subtask_category": [
        "visualization",
        "data_analysis",
        "graph_querying"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/aws/graph-notebook",
      "help_website": [
        "https://graph-notebook.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "graph-visualization",
        "jupyter",
        "sparql"
      ],
      "id": 898
    },
    {
      "name": "SageMaker Experiments",
      "one_line_profile": "Experiment tracking SDK for Amazon SageMaker",
      "detailed_description": "A Python SDK for tracking, organizing, and comparing machine learning experiments, runs, and metrics within the Amazon SageMaker environment.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_tracking",
        "metrics_logging"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aws/sagemaker-experiments",
      "help_website": [
        "https://sagemaker-experiments.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "experiment-tracking",
        "sagemaker",
        "mlops"
      ],
      "id": 899
    },
    {
      "name": "MXBoard",
      "one_line_profile": "Logging MXNet data for visualization in TensorBoard",
      "detailed_description": "A Python package that provides APIs for logging MXNet data for visualization in TensorBoard, enabling monitoring of training metrics and model performance.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "visualization",
        "experiment_tracking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/awslabs/mxboard",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "mxnet",
        "tensorboard",
        "visualization",
        "logging"
      ],
      "id": 900
    },
    {
      "name": "Cloud Detection Model",
      "one_line_profile": "Cloud Detection Model for Sentinel-2 Imagery",
      "detailed_description": "A semantic segmentation model and tool for detecting clouds in Sentinel-2 satellite imagery, supporting earth observation data processing.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "image_segmentation",
        "remote_sensing",
        "data_processing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/azavea/cloud-model",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "sentinel-2",
        "cloud-detection",
        "remote-sensing",
        "satellite-imagery"
      ],
      "id": 901
    },
    {
      "name": "ml_board",
      "one_line_profile": "Machine learning dashboard for hyperparameter and log visualization",
      "detailed_description": "A machine learning dashboard that displays hyperparameter settings alongside visualizations and logs, designed to track the scientist's thoughts and experiment progress.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_tracking",
        "visualization",
        "dashboard"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/bbli/ml_board",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "dashboard",
        "machine-learning",
        "visualization",
        "experiment-tracking"
      ],
      "id": 902
    },
    {
      "name": "calkit",
      "one_line_profile": "Project management and reproducibility tool for research",
      "detailed_description": "A tool for simplified version control, environment management, and single-button reproducible pipelines specifically designed for research projects.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "reproducibility",
        "version_control",
        "pipeline_management"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/calkit/calkit",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "research-ops",
        "reproducibility",
        "version-control",
        "pipeline"
      ],
      "id": 903
    },
    {
      "name": "Neptune",
      "one_line_profile": "Trajectory planning framework for multiple robots",
      "detailed_description": "A trajectory planning framework for multiple robots, supporting scientific modeling and control theory applications in robotics.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "trajectory_planning",
        "robotics",
        "control_theory"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/caomuqing/neptune",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "robotics",
        "trajectory-planning",
        "multi-agent"
      ],
      "id": 904
    },
    {
      "name": "Anserini",
      "one_line_profile": "Lucene toolkit for reproducible information retrieval research",
      "detailed_description": "A Lucene-based toolkit built to support reproducible information retrieval research, providing standard indexing and retrieval capabilities for scientific evaluation.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "information_retrieval",
        "indexing",
        "evaluation"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/castorini/anserini",
      "help_website": [
        "http://anserini.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "information-retrieval",
        "lucene",
        "reproducibility",
        "search"
      ],
      "id": 905
    },
    {
      "name": "Pyserini",
      "one_line_profile": "Python toolkit for reproducible information retrieval research",
      "detailed_description": "A Python toolkit for reproducible information retrieval research that supports sparse and dense representations, serving as a Python interface to Anserini.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "information_retrieval",
        "dense_retrieval",
        "evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/castorini/pyserini",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "information-retrieval",
        "python",
        "search",
        "bm25"
      ],
      "id": 906
    },
    {
      "name": "Catalyst",
      "one_line_profile": "Accelerated deep learning R&D framework",
      "detailed_description": "A PyTorch framework for accelerated deep learning research and development, focusing on reproducibility, rapid experimentation, and codebase reuse.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "model_training",
        "experiment_management",
        "deep_learning"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/catalyst-team/catalyst",
      "help_website": [
        "https://catalyst-team.github.io/catalyst/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "pytorch",
        "deep-learning",
        "framework",
        "reproducibility"
      ],
      "id": 907
    },
    {
      "name": "chaiNNer",
      "one_line_profile": "Node-based image processing GUI for chaining tasks",
      "detailed_description": "A node-based image processing GUI aimed at making chaining image processing tasks easy and customizable, supporting AI upscaling and programmatic image manipulation workflows.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "image_processing",
        "workflow_automation",
        "upscaling"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/chaiNNer-org/chaiNNer",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "image-processing",
        "gui",
        "node-based",
        "workflow"
      ],
      "id": 908
    },
    {
      "name": "jupyterlab_tensorboard",
      "one_line_profile": "Tensorboard extension for JupyterLab",
      "detailed_description": "A JupyterLab extension that facilitates the integration of TensorBoard, allowing for seamless visualization of machine learning experiments within the Jupyter environment.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "visualization",
        "experiment_tracking",
        "ide_extension"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/chesterli29/jupyterlab_tensorboard",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "jupyterlab",
        "tensorboard",
        "visualization",
        "extension"
      ],
      "id": 909
    },
    {
      "name": "Sacredboard",
      "one_line_profile": "Dashboard for Sacred experiment tracking",
      "detailed_description": "A web-based dashboard for Sacred, enabling users to monitor, access, and analyze past machine learning experiments and their configurations.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_tracking",
        "visualization",
        "dashboard"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/chovanecm/sacredboard",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "sacred",
        "dashboard",
        "experiment-tracking",
        "machine-learning"
      ],
      "id": 910
    },
    {
      "name": "Beholder",
      "one_line_profile": "TensorBoard plugin for visualizing arbitrary tensors",
      "detailed_description": "A TensorBoard plugin that allows for the visualization of arbitrary tensors as video overlays during network training, aiding in model debugging and interpretation.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "visualization",
        "model_debugging",
        "tensorboard_plugin"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/chrisranderson/beholder",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "tensorboard",
        "visualization",
        "deep-learning",
        "plugin"
      ],
      "id": 911
    },
    {
      "name": "ClearML",
      "one_line_profile": "MLOps platform for experiment management and orchestration",
      "detailed_description": "An integrated MLOps platform that provides experiment management, data management, pipeline orchestration, and model serving to streamline AI workflows.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_tracking",
        "mlops",
        "orchestration",
        "data_management"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/clearml/clearml",
      "help_website": [
        "https://clear.ml/docs/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "mlops",
        "experiment-tracking",
        "orchestration",
        "reproducibility"
      ],
      "id": 912
    },
    {
      "name": "ClearML Agent",
      "one_line_profile": "Orchestration agent for ClearML",
      "detailed_description": "The agent component for ClearML that handles the execution, scheduling, and orchestration of machine learning experiments and pipelines.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "orchestration",
        "scheduling",
        "mlops"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/clearml/clearml-agent",
      "help_website": [
        "https://clear.ml/docs/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "mlops",
        "agent",
        "orchestration",
        "scheduler"
      ],
      "id": 913
    },
    {
      "name": "ClearML Server",
      "one_line_profile": "Backend infrastructure for the ClearML MLOps and experiment tracking platform",
      "detailed_description": "ClearML Server is the backend service for the ClearML platform, providing experiment tracking, model management, and orchestration capabilities. It serves as the central repository for logging metrics, artifacts, and execution details of machine learning experiments.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_tracking",
        "model_management",
        "mlops"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/clearml/clearml-server",
      "help_website": [
        "https://clear.ml/docs"
      ],
      "license": "NOASSERTION",
      "tags": [
        "experiment-tracking",
        "mlops",
        "model-management"
      ],
      "id": 914
    },
    {
      "name": "benchmark_VAE",
      "one_line_profile": "Unified PyTorch implementation and benchmarking framework for Variational Autoencoders",
      "detailed_description": "A comprehensive library for implementing and benchmarking various Variational Autoencoder (VAE) models. It provides a unified interface to reproduce results and compare performance across different VAE architectures on standard datasets.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "model_benchmarking",
        "reproducibility",
        "generative_models"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/clementchadebec/benchmark_VAE",
      "help_website": [
        "https://github.com/clementchadebec/benchmark_VAE"
      ],
      "license": "Apache-2.0",
      "tags": [
        "vae",
        "benchmarking",
        "pytorch",
        "generative-ai"
      ],
      "id": 915
    },
    {
      "name": "Codabench",
      "one_line_profile": "Flexible and reproducible benchmarking platform for AI competitions and tasks",
      "detailed_description": "Codabench is a framework for creating and hosting benchmarks and competitions. It allows researchers to define tasks, metrics, and datasets to evaluate algorithms in a reproducible manner, supporting the organization of scientific challenges.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "benchmarking_platform",
        "competition_hosting",
        "evaluation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/codalab/codabench",
      "help_website": [
        "https://github.com/codalab/codabench/wiki"
      ],
      "license": "Apache-2.0",
      "tags": [
        "benchmarking",
        "competitions",
        "reproducibility"
      ],
      "id": 916
    },
    {
      "name": "WA-Testing-Tool",
      "one_line_profile": "Evaluation and testing scripts for Watson Assistant models",
      "detailed_description": "A set of tools to perform K-fold validation and blind testing on Watson Assistant workspaces. It generates precision curves and other metrics to evaluate the performance of NLU models.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "validation",
        "metrics_calculation"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/cognitive-catalyst/WA-Testing-Tool",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "nlp",
        "evaluation",
        "watson-assistant",
        "metrics"
      ],
      "id": 917
    },
    {
      "name": "sna-js",
      "one_line_profile": "JavaScript library for egocentric social network analysis metrics",
      "detailed_description": "A library to visualize and calculate relevant metrics for egocentric social network analysis (SNA) using adjacency matrices. It supports the computation of network statistics useful in social science research.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "network_analysis",
        "metrics_calculation"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/craigtutterow/sna-js",
      "help_website": [],
      "license": "GPL-2.0",
      "tags": [
        "social-network-analysis",
        "metrics",
        "visualization"
      ],
      "id": 918
    },
    {
      "name": "CK-Crowdtuning",
      "one_line_profile": "Crowdsourcing extension for Collective Knowledge benchmarking workflows",
      "detailed_description": "An extension for the Collective Knowledge (CK) framework that enables crowdsourcing of experiments, such as performance benchmarking and auto-tuning of machine learning models across diverse hardware platforms.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "benchmarking",
        "auto_tuning",
        "crowdsourcing"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/ctuning/ck-crowdtuning",
      "help_website": [
        "http://cKnowledge.org"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "benchmarking",
        "crowdsourcing",
        "collective-knowledge",
        "optimization"
      ],
      "id": 919
    },
    {
      "name": "The Omega Project",
      "one_line_profile": "Constraint-based tools for compiler dependence analysis and transformation",
      "detailed_description": "A collection of tools and libraries (Omega Library, Omega Calculator, Omega Test) for manipulating sets of affine constraints and performing dependence analysis. These tools are fundamental for research in compiler optimization and polyhedral models.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "dependence_analysis",
        "constraint_solving",
        "compiler_optimization"
      ],
      "application_level": "solver",
      "primary_language": "C",
      "repo_url": "https://github.com/davewathaverford/the-omega-project",
      "help_website": [],
      "license": "BSD-2-Clause",
      "tags": [
        "compiler",
        "dependence-analysis",
        "polyhedral-model",
        "constraints"
      ],
      "id": 920
    },
    {
      "name": "gtsummary",
      "one_line_profile": "R package for creating presentation-ready data summary and analytic tables",
      "detailed_description": "An R package designed to create publication-ready analytical and summary tables. It automates the reporting of regression models, descriptive statistics, and other scientific data analysis results.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "reporting",
        "statistical_summary",
        "visualization"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/ddsjoberg/gtsummary",
      "help_website": [
        "http://www.danieldsjoberg.com/gtsummary/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "r",
        "statistics",
        "reporting",
        "tables"
      ],
      "id": 921
    },
    {
      "name": "Determined",
      "one_line_profile": "Deep learning training platform with integrated experiment tracking",
      "detailed_description": "Determined is an open-source deep learning training platform that handles distributed training, hyperparameter tuning, experiment tracking, and resource management. It enables researchers to track and reproduce experiments efficiently.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_tracking",
        "hyperparameter_tuning",
        "resource_management"
      ],
      "application_level": "platform",
      "primary_language": "Go",
      "repo_url": "https://github.com/determined-ai/determined",
      "help_website": [
        "https://docs.determined.ai/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "deep-learning",
        "experiment-tracking",
        "distributed-training"
      ],
      "id": 922
    },
    {
      "name": "machine-learning-interactive-visualization",
      "one_line_profile": "Interactive visualization tools for ML model evaluation metrics",
      "detailed_description": "A collection of notebooks and tools for interactively visualizing machine learning evaluation metrics (like ROC curves, precision-recall, etc.) to aid in model analysis and understanding.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "visualization",
        "metrics_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/dhaitz/machine-learning-interactive-visualization",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "visualization",
        "metrics",
        "machine-learning"
      ],
      "id": 923
    },
    {
      "name": "NLG-Metricverse",
      "one_line_profile": "End-to-end library for evaluating Natural Language Generation",
      "detailed_description": "A comprehensive library for evaluating Natural Language Generation (NLG) models. It provides a unified interface to access a wide range of evaluation metrics, facilitating the comparison and benchmarking of NLG systems.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "nlg_evaluation",
        "metrics_calculation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/disi-unibo-nlp/nlg-metricverse",
      "help_website": [
        "https://nlg-metricverse.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "nlg",
        "evaluation",
        "metrics",
        "nlp"
      ],
      "id": 924
    },
    {
      "name": "PromptSite",
      "one_line_profile": "Version control and tracking system for LLM prompts",
      "detailed_description": "PromptSite is a lightweight package for managing, versioning, and tracking LLM prompts. It helps researchers experiment with and debug prompts by maintaining a history of changes and their effects.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "prompt_tracking",
        "version_control",
        "experiment_management"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/dkuang1980/promptsite",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "prompt-engineering",
        "version-control",
        "llm"
      ],
      "id": 925
    },
    {
      "name": "TensorBoard",
      "one_line_profile": "Visualization toolkit for machine learning experimentation",
      "detailed_description": "A suite of visualization tools to understand and debug machine learning models, enabling tracking of metrics like loss and accuracy, visualizing the model graph, and projecting embeddings.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_tracking",
        "visualization"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/dmlc/tensorboard",
      "help_website": [
        "https://www.tensorflow.org/tensorboard"
      ],
      "license": "Apache-2.0",
      "tags": [
        "visualization",
        "deep-learning",
        "experiment-tracking"
      ],
      "id": 926
    },
    {
      "name": "Open LLM Leaderboard Report",
      "one_line_profile": "Visualization report generator for Open LLM Leaderboard",
      "detailed_description": "A tool to generate weekly visualization reports of Open LLM model performance based on various metrics, aiding in the tracking and comparison of large language models.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "leaderboard_visualization",
        "performance_tracking"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/dsdanielpark/open-llm-leaderboard-report",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "leaderboard",
        "visualization"
      ],
      "id": 927
    },
    {
      "name": "MET (Model Evaluation Tools)",
      "one_line_profile": "Verification and validation tools for meteorological models",
      "detailed_description": "A suite of tools for evaluating the performance of meteorological models, providing state-of-the-art verification methods and metrics.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "meteorology"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/dtcenter/MET",
      "help_website": [
        "https://dtcenter.org/community-code/model-evaluation-tools-met"
      ],
      "license": "Apache-2.0",
      "tags": [
        "meteorology",
        "verification",
        "model-evaluation"
      ],
      "id": 928
    },
    {
      "name": "mltraq",
      "one_line_profile": "Experiment tracking and collaboration tool for ML",
      "detailed_description": "A library to track and collaborate on Machine Learning and AI experiments, supporting logging of parameters, metrics, and artifacts.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_tracking",
        "collaboration"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/elehcimd/mltraq",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "experiment-tracking",
        "mlops",
        "logging"
      ],
      "id": 929
    },
    {
      "name": "isaaclab_rl",
      "one_line_profile": "RL training library for Isaac Lab robotics simulation",
      "detailed_description": "A library for training robotic agents in Isaac Lab using PPO, featuring built-in hyperparameter optimisation and extensive logging capabilities.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "robotics_simulation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/elle-miller/isaaclab_rl",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "robotics",
        "reinforcement-learning",
        "simulation"
      ],
      "id": 930
    },
    {
      "name": "tf-faster-rcnn",
      "one_line_profile": "TensorFlow implementation of Faster R-CNN",
      "detailed_description": "A widely used implementation of Faster R-CNN for object detection in TensorFlow, serving as a baseline and tool for computer vision research.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "object_detection",
        "computer_vision"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/endernewton/tf-faster-rcnn",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "object-detection",
        "faster-rcnn",
        "tensorflow"
      ],
      "id": 931
    },
    {
      "name": "NdLinear",
      "one_line_profile": "PyTorch module for model compression",
      "detailed_description": "A drop-in PyTorch module that shrinks models with no accuracy loss, supporting export to various frameworks like ONNX and TensorRT.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "model_compression",
        "optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ensemble-core/NdLinear",
      "help_website": [
        "https://app.ensemblecore.ai/signup"
      ],
      "license": "Apache-2.0",
      "tags": [
        "pytorch",
        "model-compression",
        "optimization"
      ],
      "id": 932
    },
    {
      "name": "PyGaze",
      "one_line_profile": "Toolbox for eye tracking experiments",
      "detailed_description": "An open-source, cross-platform toolbox for minimal-effort programming of eye tracking experiments, supporting various eye trackers.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "eye_tracking",
        "experiment_control"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/esdalmaijer/PyGaze",
      "help_website": [
        "http://www.pygaze.org/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "eye-tracking",
        "psychophysics",
        "experiment"
      ],
      "id": 933
    },
    {
      "name": "papermill-mlflow",
      "one_line_profile": "Integration for Papermill and MLflow",
      "detailed_description": "A tool to integrate Jupyter notebooks (via Papermill) with MLflow for data science experimentation and tracking.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_tracking",
        "workflow_automation"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/eugeneyan/papermill-mlflow",
      "help_website": [],
      "license": null,
      "tags": [
        "mlflow",
        "papermill",
        "experiment-tracking"
      ],
      "id": 934
    },
    {
      "name": "ieee118_power_flow_data",
      "one_line_profile": "Data pipeline for IEEE-118 power flow cases",
      "detailed_description": "A data pipeline to build power flow cases for the IEEE-118 power system, facilitating power systems research and simulation.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "data_generation",
        "power_systems"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/evgenytsydenov/ieee118_power_flow_data",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "power-systems",
        "ieee-118",
        "data-pipeline"
      ],
      "id": 935
    },
    {
      "name": "maggot",
      "one_line_profile": "Lightweight experiment tracking library",
      "detailed_description": "A lightweight Python library designed to help keep track of numerical experiments, managing configurations and results.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_tracking",
        "configuration_management"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ex4sperans/maggot",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "experiment-tracking",
        "reproducibility",
        "python"
      ],
      "id": 936
    },
    {
      "name": "hermit",
      "one_line_profile": "Hermetic sandbox for reproducible execution",
      "detailed_description": "A tool that launches Linux programs in a hermetically isolated sandbox to ensure deterministic and repeatable behavior, useful for reproducible research artifacts and debugging.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "reproducibility",
        "execution_control"
      ],
      "application_level": "platform",
      "primary_language": "Rust",
      "repo_url": "https://github.com/facebookexperimental/hermit",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "reproducibility",
        "sandbox",
        "determinism"
      ],
      "id": 937
    },
    {
      "name": "PySlowFast",
      "one_line_profile": "Video understanding codebase",
      "detailed_description": "A codebase for reproducing state-of-the-art video models, providing implementations of SlowFast and other video classification algorithms.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "video_understanding",
        "model_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/SlowFast",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "video-understanding",
        "computer-vision",
        "slowfast"
      ],
      "id": 938
    },
    {
      "name": "Meta Agents Research Environments",
      "one_line_profile": "Platform for evaluating AI agents",
      "detailed_description": "A comprehensive platform designed to evaluate AI agents in dynamic, realistic scenarios, supporting evolving environments for agent adaptation research.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "agent_evaluation",
        "benchmark_environment"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/meta-agents-research-environments",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ai-agents",
        "evaluation",
        "benchmark"
      ],
      "id": 939
    },
    {
      "name": "param",
      "one_line_profile": "Benchmark for recommendation and AI models",
      "detailed_description": "A repository for development of micro-benchmarks and end-to-end networks for evaluation of training and inference platforms, specifically for recommendation systems.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "benchmarking",
        "recommendation_systems"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/param",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "benchmark",
        "recommendation",
        "performance"
      ],
      "id": 940
    },
    {
      "name": "keras-ocr",
      "one_line_profile": "Packaged CRAFT and CRNN for OCR",
      "detailed_description": "A flexible and packaged version of the CRAFT text detector and Keras CRNN recognition model for optical character recognition tasks.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "ocr",
        "text_detection"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/faustomorales/keras-ocr",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ocr",
        "keras",
        "computer-vision"
      ],
      "id": 941
    },
    {
      "name": "yolov5-pip",
      "one_line_profile": "Packaged version of YOLOv5",
      "detailed_description": "A pip-installable package of the Ultralytics YOLOv5 object detection model, facilitating easy integration into Python workflows.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "object_detection",
        "computer_vision"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/fcakyon/yolov5-pip",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "yolov5",
        "object-detection",
        "pip-package"
      ],
      "id": 942
    },
    {
      "name": "FaceRank",
      "one_line_profile": "Face ranking using CNN",
      "detailed_description": "A tool that uses a Convolutional Neural Network (CNN) based on TensorFlow/Keras to rank faces, providing a scoring mechanism.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "face_analysis",
        "ranking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/fendouai/FaceRank",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "face-ranking",
        "cnn",
        "tensorflow"
      ],
      "id": 943
    },
    {
      "name": "FFMetrics",
      "one_line_profile": "Video quality metrics visualization",
      "detailed_description": "A tool to visualize video quality metrics such as PSNR, SSIM, and VMAF calculated by FFmpeg, aiding in video compression analysis.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "video_quality_assessment",
        "metrics_visualization"
      ],
      "application_level": "solver",
      "primary_language": null,
      "repo_url": "https://github.com/fifonik/FFMetrics",
      "help_website": [],
      "license": null,
      "tags": [
        "video-quality",
        "psnr",
        "ssim"
      ],
      "id": 944
    },
    {
      "name": "FlagPerf",
      "one_line_profile": "AI chip benchmarking platform",
      "detailed_description": "An open-source software platform for benchmarking AI chips, providing a standardized way to evaluate hardware performance for AI workloads.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "hardware_benchmarking",
        "performance_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/flagos-ai/FlagPerf",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "benchmark",
        "ai-chip",
        "hardware"
      ],
      "id": 945
    },
    {
      "name": "GTO (Git Tag Ops)",
      "one_line_profile": "Git-based registry for versioning machine learning models and artifacts",
      "detailed_description": "A tool that turns a Git repository into an artifact or model registry using Git tags. It allows data scientists to register, version, and manage the lifecycle of models and datasets directly within their version control system.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "model_registry",
        "artifact_versioning"
      ],
      "application_level": "tool",
      "primary_language": "Python",
      "repo_url": "https://github.com/iterative/gto",
      "help_website": [
        "https://iterative.github.io/gto/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "mlops",
        "git",
        "model-registry",
        "version-control"
      ],
      "id": 946
    },
    {
      "name": "tbparse",
      "one_line_profile": "Parser for loading TensorBoard event logs into Pandas DataFrames",
      "detailed_description": "A library designed to parse TensorBoard event logs (tfevents) and convert them into Pandas DataFrames. This facilitates custom analysis, statistical processing, and publication-quality plotting of experiment metrics outside of the TensorBoard UI.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_analysis",
        "log_parsing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/j3soon/tbparse",
      "help_website": [
        "https://tbparse.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "tensorboard",
        "pandas",
        "visualization",
        "experiment-tracking"
      ],
      "id": 947
    },
    {
      "name": "jeelizPupillometry",
      "one_line_profile": "Browser-based real-time pupillometry library using computer vision",
      "detailed_description": "A JavaScript library that uses WebGL and deep learning to perform real-time pupillometry (measurement of pupil size) via a webcam. It is used in cognitive science and psychology experiments to measure physiological responses.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "biometric_measurement",
        "data_acquisition"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/jeeliz/jeelizPupillometry",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "pupillometry",
        "computer-vision",
        "neuroscience",
        "webgl"
      ],
      "id": 948
    },
    {
      "name": "clip-image-sorter",
      "one_line_profile": "Tool for semantic image sorting and filtering using CLIP",
      "detailed_description": "A browser-based tool that utilizes OpenAI's CLIP model to sort and organize local image datasets based on text prompts. It serves as a data processing utility for curating and filtering image datasets for scientific or ML purposes.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "data_filtering",
        "dataset_curation"
      ],
      "application_level": "tool",
      "primary_language": "HTML",
      "repo_url": "https://github.com/josephrocca/clip-image-sorter",
      "help_website": [
        "https://josephrocca.github.io/clip-image-sorter/"
      ],
      "license": "MIT",
      "tags": [
        "clip",
        "dataset-management",
        "computer-vision",
        "data-cleaning"
      ],
      "id": 949
    },
    {
      "name": "OpenJSCAD",
      "one_line_profile": "Programmatic 3D modeling tool for parametric design",
      "detailed_description": "A set of modular tools for creating parametric 2D and 3D designs using JavaScript code. It provides a reproducible method for generating 3D models, useful for designing scientific apparatus, hardware components, and physical visualizations.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "scientific_modeling",
        "hardware_design"
      ],
      "application_level": "tool",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/jscad/OpenJSCAD.org",
      "help_website": [
        "https://openjscad.xyz/"
      ],
      "license": "MIT",
      "tags": [
        "cad",
        "3d-modeling",
        "parametric-design",
        "visualization"
      ],
      "id": 950
    },
    {
      "name": "Kedro",
      "one_line_profile": "Framework for reproducible data science pipelines",
      "detailed_description": "A development workflow framework that applies software engineering best practices to data science code. It helps structure data pipelines to be reproducible, modular, and maintainable, facilitating experiment tracking and collaboration.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "workflow_management",
        "reproducibility"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/kedro-org/kedro",
      "help_website": [
        "https://kedro.org"
      ],
      "license": "Apache-2.0",
      "tags": [
        "pipeline",
        "data-science",
        "reproducibility",
        "workflow"
      ],
      "id": 951
    },
    {
      "name": "Kedro-Viz",
      "one_line_profile": "Visualizer for Kedro data science pipelines",
      "detailed_description": "A visualization tool for Kedro pipelines that displays the data lineage and structure of data science experiments. It helps researchers understand complex workflows and track the flow of data transformations.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "pipeline_visualization",
        "experiment_tracking"
      ],
      "application_level": "tool",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/kedro-org/kedro-viz",
      "help_website": [
        "https://kedro.org"
      ],
      "license": "Apache-2.0",
      "tags": [
        "visualization",
        "data-lineage",
        "pipeline",
        "mlops"
      ],
      "id": 952
    },
    {
      "name": "Hera",
      "one_line_profile": "Real-time metrics dashboard for Keras models",
      "detailed_description": "A tool designed to train and evaluate Keras models while streaming metrics to a browser-based dashboard. It provides real-time visualization of experiment performance, aiding in the monitoring and debugging of deep learning training processes.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_monitoring",
        "metrics_visualization"
      ],
      "application_level": "tool",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/keplr-io/hera",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "keras",
        "dashboard",
        "visualization",
        "deep-learning"
      ],
      "id": 953
    },
    {
      "name": "KitOps",
      "one_line_profile": "Packaging and versioning tool for AI/ML model artifacts",
      "detailed_description": "A DevOps tool that packages AI/ML models, datasets, code, and configurations into OCI (Open Container Initiative) artifacts (ModelKits). It facilitates the versioning, sharing, and deployment of scientific models in a standardized format.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "model_packaging",
        "artifact_management"
      ],
      "application_level": "tool",
      "primary_language": "Go",
      "repo_url": "https://github.com/kitops-ml/kitops",
      "help_website": [
        "https://kitops.ml/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "mlops",
        "oci",
        "model-packaging",
        "reproducibility"
      ],
      "id": 954
    },
    {
      "name": "Klever Model Registry",
      "one_line_profile": "Cloud-native registry for ML model management",
      "detailed_description": "A tool for managing the lifecycle of machine learning models, providing a registry to index, version, and store model metadata. It supports the tracking of model lineage and facilitates collaboration in ML research and production.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "model_registry",
        "metadata_tracking"
      ],
      "application_level": "service",
      "primary_language": "Go",
      "repo_url": "https://github.com/kleveross/klever-model-registry",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "mlops",
        "model-management",
        "kubernetes",
        "registry"
      ],
      "id": 955
    },
    {
      "name": "Kubeflow Model Registry",
      "one_line_profile": "Centralized model registry for the Kubeflow ecosystem",
      "detailed_description": "A component of the Kubeflow project that provides a central interface for indexing and managing ML models, versions, and artifact metadata. It bridges the gap between experimentation and production by tracking model lineage.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "model_registry",
        "experiment_tracking"
      ],
      "application_level": "platform",
      "primary_language": "Go",
      "repo_url": "https://github.com/kubeflow/model-registry",
      "help_website": [
        "https://www.kubeflow.org/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "kubeflow",
        "mlops",
        "model-management",
        "metadata"
      ],
      "id": 956
    },
    {
      "name": "LabML",
      "one_line_profile": "Mobile monitoring tool for deep learning training",
      "detailed_description": "A library and platform that allows researchers to monitor deep learning model training metrics and hardware usage (GPU/CPU) remotely via mobile devices. It assists in tracking long-running experiments.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_monitoring",
        "remote_tracking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/labmlai/labml",
      "help_website": [
        "https://labml.ai/"
      ],
      "license": "MIT",
      "tags": [
        "monitoring",
        "deep-learning",
        "mobile",
        "experiment-tracking"
      ],
      "id": 957
    },
    {
      "name": "LangEvals",
      "one_line_profile": "Platform for benchmarking and evaluating LLMs",
      "detailed_description": "A tool that aggregates various language model evaluators into a single interface. It provides standard metrics and guardrails to benchmark LLM models and pipelines, facilitating the scientific evaluation of NLP models.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/langwatch/langevals",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "evaluation",
        "benchmark",
        "nlp"
      ],
      "id": 958
    },
    {
      "name": "tensorboardX",
      "one_line_profile": "TensorBoard logging adapter for PyTorch and other frameworks",
      "detailed_description": "A library that allows researchers using PyTorch, Chainer, MXNet, and other frameworks to write events and metrics to TensorBoard files. It enables the visualization of training curves, embeddings, and other scientific experiment data.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_logging",
        "visualization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/lanpa/tensorboardX",
      "help_website": [
        "https://tensorboardx.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "pytorch",
        "tensorboard",
        "visualization",
        "logging"
      ],
      "id": 959
    },
    {
      "name": "Layer SDK",
      "one_line_profile": "Metadata store SDK for production ML pipelines",
      "detailed_description": "The SDK for Layer, a metadata store designed to track machine learning experiments, models, and datasets. It enables reproducibility and collaboration by managing the metadata of scientific workflows.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "metadata_management",
        "experiment_tracking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/layerai-archive/sdk",
      "help_website": [
        "https://layer.ai/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "mlops",
        "metadata",
        "experiment-tracking",
        "reproducibility"
      ],
      "id": 960
    },
    {
      "name": "VALL-E (PyTorch)",
      "one_line_profile": "PyTorch implementation of VALL-E zero-shot text-to-speech model",
      "detailed_description": "An open-source implementation of the VALL-E text-to-speech model, capable of zero-shot speech synthesis and voice cloning, serving as a solver for audio generation research.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "speech_synthesis",
        "generative_modeling"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/lifeiteng/vall-e",
      "help_website": [
        "https://lifeiteng.github.io/valle/index.html"
      ],
      "license": "Apache-2.0",
      "tags": [
        "text-to-speech",
        "pytorch",
        "deep-learning",
        "audio-generation"
      ],
      "id": 961
    },
    {
      "name": "Minetorch",
      "one_line_profile": "Lightweight PyTorch wrapper for deep learning training loops",
      "detailed_description": "A minimalist library designed to simplify the training process of deep learning models in PyTorch, providing abstractions for training loops and experiment management.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "model_training",
        "experiment_management"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/louis-she/minetorch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pytorch",
        "training-loop",
        "deep-learning"
      ],
      "id": 962
    },
    {
      "name": "simple_GRPO",
      "one_line_profile": "Minimal implementation of GRPO algorithm for LLM reasoning",
      "detailed_description": "A simplified implementation of the Group Relative Policy Optimization (GRPO) algorithm, designed for reproducing reasoning capabilities in Large Language Models (LLMs) similar to DeepSeek-R1.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "llm_alignment"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/lsdefine/simple_GRPO",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "grpo",
        "llm",
        "reinforcement-learning",
        "reasoning"
      ],
      "id": 963
    },
    {
      "name": "jupyter_tensorboard",
      "one_line_profile": "Jupyter Notebook extension for starting TensorBoard",
      "detailed_description": "A tool that integrates TensorBoard directly into the Jupyter Notebook interface, facilitating the tracking and visualization of machine learning experiments within the interactive coding environment.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_tracking",
        "visualization"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/lspvic/jupyter_tensorboard",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "tensorboard",
        "jupyter",
        "visualization",
        "tracking"
      ],
      "id": 964
    },
    {
      "name": "gaze_tracker",
      "one_line_profile": "Eye gaze tracking system using OpenCV",
      "detailed_description": "A computer vision-based application that estimates eye gaze direction to control a mouse pointer, serving as a tool for human-computer interaction research and behavioral analysis.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "gaze_estimation",
        "computer_vision"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/luca-ant/gaze_tracker",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "opencv",
        "eye-tracking",
        "gaze-estimation"
      ],
      "id": 965
    },
    {
      "name": "ema-pytorch",
      "one_line_profile": "Exponential Moving Average (EMA) wrapper for PyTorch models",
      "detailed_description": "A utility library for maintaining an Exponential Moving Average of model parameters during training, which is a common technique in deep learning to improve model generalization and stability.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "model_optimization",
        "training_utility"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/lucidrains/ema-pytorch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pytorch",
        "ema",
        "deep-learning"
      ],
      "id": 966
    },
    {
      "name": "µBench",
      "one_line_profile": "Microservice benchmark for cloud/edge computing platforms",
      "detailed_description": "A benchmarking tool designed to evaluate the performance of cloud and edge computing platforms by running customizable dummy microservice applications on Kubernetes.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "benchmarking",
        "system_performance"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/mSvcBench/muBench",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "benchmarking",
        "kubernetes",
        "edge-computing",
        "microservices"
      ],
      "id": 967
    },
    {
      "name": "marimo",
      "one_line_profile": "Reactive Python notebook for reproducible research",
      "detailed_description": "A next-generation reactive notebook for Python that ensures reproducibility by executing as a script, supporting SQL queries, and providing a modern AI-native editing environment for scientific experiments.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "scientific_computing",
        "reproducibility",
        "visualization"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/marimo-team/marimo",
      "help_website": [
        "https://marimo.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "notebook",
        "reproducibility",
        "python",
        "data-science"
      ],
      "id": 968
    },
    {
      "name": "Banks",
      "one_line_profile": "Jinja-based LLM prompt engineering and management tool",
      "detailed_description": "A tool for managing and generating LLM prompts using a Jinja-based template language, supporting metadata management, versioning, and storage, facilitating systematic prompt engineering research.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "prompt_engineering",
        "llm_interaction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/masci/banks",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "prompt-engineering",
        "llm",
        "jinja",
        "templates"
      ],
      "id": 969
    },
    {
      "name": "plf_nanotimer",
      "one_line_profile": "Cross-platform C++ microsecond-precision timer for benchmarking",
      "detailed_description": "A simple, low-overhead C++ timer class designed for microsecond-precision benchmarking across different platforms, useful for performance analysis in scientific computing.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_analysis"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/mattreecebentley/plf_nanotimer",
      "help_website": [
        "http://plflib.org/nanotimer.htm"
      ],
      "license": "Zlib",
      "tags": [
        "benchmarking",
        "timer",
        "cpp",
        "performance"
      ],
      "id": 970
    },
    {
      "name": "rl-bh-environment",
      "one_line_profile": "Reinforcement learning environment for bullet hell games",
      "detailed_description": "A custom reinforcement learning environment simulating a bullet hell game (Sacred Curry Shooter), providing an interface for training and testing RL agents in dynamic environments.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "simulation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/michael-pacheco/rl-bh-environment",
      "help_website": [],
      "license": null,
      "tags": [
        "reinforcement-learning",
        "gym-environment",
        "simulation"
      ],
      "id": 971
    },
    {
      "name": "SacredBrowser",
      "one_line_profile": "GUI for browsing Sacred experiment results",
      "detailed_description": "A graphical user interface for browsing, filtering, and analyzing machine learning experiment results stored in MongoDB by the Sacred experiment management framework.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_tracking",
        "visualization"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/michaelwand/SacredBrowser",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "sacred",
        "experiment-tracking",
        "visualization",
        "mongodb"
      ],
      "id": 972
    },
    {
      "name": "Windows Agent Arena",
      "one_line_profile": "Scalable OS platform for benchmarking multi-modal AI agents",
      "detailed_description": "A scalable operating system platform designed for testing and benchmarking multi-modal AI agents, providing a realistic environment for evaluating agent performance on Windows tasks.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "benchmarking",
        "agent_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/WindowsAgentArena",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ai-agent",
        "benchmarking",
        "multimodal",
        "evaluation"
      ],
      "id": 973
    },
    {
      "name": "Coyote",
      "one_line_profile": "Tool for testing concurrent C# code and reproducing bugs",
      "detailed_description": "A library and tool designed for the systematic testing of concurrent C# code, enabling deterministic reproduction of concurrency bugs, widely used in systems research and verification.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "software_verification",
        "concurrency_testing"
      ],
      "application_level": "solver",
      "primary_language": "C#",
      "repo_url": "https://github.com/microsoft/coyote",
      "help_website": [
        "https://microsoft.github.io/coyote/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "testing",
        "concurrency",
        "verification",
        "dotnet"
      ],
      "id": 974
    },
    {
      "name": "Project Malmo",
      "one_line_profile": "AI experimentation platform built on Minecraft",
      "detailed_description": "A platform for Artificial Intelligence experimentation and research built on top of Minecraft, providing a rich, complex environment for testing reinforcement learning and multi-agent systems.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "simulation",
        "ai_environment"
      ],
      "application_level": "platform",
      "primary_language": "Java",
      "repo_url": "https://github.com/microsoft/malmo",
      "help_website": [
        "https://microsoft.github.io/malmo/"
      ],
      "license": "MIT",
      "tags": [
        "minecraft",
        "reinforcement-learning",
        "ai-platform",
        "simulation"
      ],
      "id": 975
    },
    {
      "name": "Qlib",
      "one_line_profile": "AI-oriented quantitative investment platform",
      "detailed_description": "An AI-oriented quantitative investment platform that covers the entire chain of quantitative research, including data processing, model training (RL/Supervised), and backtesting, empowering AI-driven financial modeling.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "quantitative_finance",
        "market_modeling",
        "backtesting"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/qlib",
      "help_website": [
        "https://qlib.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "quantitative-finance",
        "machine-learning",
        "backtesting",
        "investment"
      ],
      "id": 976
    },
    {
      "name": "Responsible AI Toolbox Tracker",
      "one_line_profile": "JupyterLab extension for tracking Responsible AI mitigations",
      "detailed_description": "A JupyterLab extension designed to track, manage, and compare Responsible AI experiments and mitigations, facilitating the evaluation of model fairness and safety.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "responsible_ai",
        "experiment_tracking",
        "model_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/microsoft/responsible-ai-toolbox-tracker",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "responsible-ai",
        "jupyterlab-extension",
        "tracking",
        "fairness"
      ],
      "id": 977
    },
    {
      "name": "MedPerf",
      "one_line_profile": "Open benchmarking platform for medical AI using Federated Evaluation",
      "detailed_description": "MedPerf is an open benchmarking platform designed specifically for medical artificial intelligence. It enables the evaluation of AI models on diverse, real-world medical datasets without requiring data to leave the data owner's premises (Federated Evaluation), ensuring privacy and compliance while providing robust performance metrics.",
      "domains": [
        "AI4",
        "AI4-04",
        "Medical AI"
      ],
      "subtask_category": [
        "benchmarking",
        "federated_evaluation",
        "model_validation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/mlcommons/medperf",
      "help_website": [
        "https://medperf.org"
      ],
      "license": "Apache-2.0",
      "tags": [
        "medical-ai",
        "benchmarking",
        "federated-learning",
        "healthcare"
      ],
      "id": 978
    },
    {
      "name": "MLflow",
      "one_line_profile": "Open source platform for the machine learning lifecycle",
      "detailed_description": "MLflow is a comprehensive platform for managing the end-to-end machine learning lifecycle. It includes components for experiment tracking (logging parameters, metrics, and artifacts), model packaging, model registry, and deployment, widely used in scientific research for reproducibility and experiment management.",
      "domains": [
        "AI4",
        "AI4-04",
        "MLOps"
      ],
      "subtask_category": [
        "experiment_tracking",
        "model_registry",
        "reproducibility"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/mlflow/mlflow",
      "help_website": [
        "https://mlflow.org/docs/latest/index.html"
      ],
      "license": "Apache-2.0",
      "tags": [
        "experiment-tracking",
        "model-management",
        "mlops",
        "reproducibility"
      ],
      "id": 979
    },
    {
      "name": "Neptune Client",
      "one_line_profile": "Client library for Neptune experiment tracking and model registry",
      "detailed_description": "The Neptune Client is the Python interface for Neptune, a metadata store for MLOps. It allows researchers to log, organize, and compare machine learning experiments, including metrics, hyperparameters, and model artifacts, facilitating reproducibility in scientific computing.",
      "domains": [
        "AI4",
        "AI4-04",
        "MLOps"
      ],
      "subtask_category": [
        "experiment_tracking",
        "metadata_logging",
        "reproducibility"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/neptune-ai/neptune-client",
      "help_website": [
        "https://docs.neptune.ai/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "experiment-tracking",
        "mlops",
        "metadata-store"
      ],
      "id": 980
    },
    {
      "name": "ONNX Registry",
      "one_line_profile": "Intelligent Component Registry web service for ONNX models",
      "detailed_description": "A web service designed to act as a registry for managing and retrieving Spiking Neural Networks (SNN), Deep Neural Networks (DNN), and ML models stored in the ONNX format. It supports the tracking and versioning of model components in neuromorphic and IoT research contexts.",
      "domains": [
        "AI4",
        "AI4-04",
        "Neuromorphic Computing"
      ],
      "subtask_category": [
        "model_registry",
        "component_management",
        "model_tracking"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/neurom-iot/onnx-registry",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "onnx",
        "model-registry",
        "neuromorphic",
        "iot"
      ],
      "id": 981
    },
    {
      "name": "Cosmos-Predict2.5",
      "one_line_profile": "World Foundation Models for video-based future state prediction and simulation",
      "detailed_description": "Cosmos-Predict2.5 is a suite of World Foundation Models (WFMs) designed to simulate and predict future states of the world through video generation. It serves as a scientific simulation tool for physical world dynamics.",
      "domains": [
        "AI4",
        "Computer Vision",
        "Simulation"
      ],
      "subtask_category": [
        "simulation",
        "video_generation",
        "future_prediction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/nvidia-cosmos/cosmos-predict2.5",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "world-model",
        "simulation",
        "video-generation",
        "foundation-model"
      ],
      "id": 982
    },
    {
      "name": "Onepanel",
      "one_line_profile": "End-to-end computer vision platform for annotation, training, and workflow automation",
      "detailed_description": "Onepanel is a unified platform for computer vision workflows, integrating data labeling, model training, hyperparameter tuning, and pipeline automation. It supports reproducible research by managing the entire lifecycle of CV models.",
      "domains": [
        "AI4",
        "Computer Vision",
        "MLOps"
      ],
      "subtask_category": [
        "workflow_automation",
        "annotation",
        "training_management"
      ],
      "application_level": "platform",
      "primary_language": "Go",
      "repo_url": "https://github.com/onepanelio/onepanel",
      "help_website": [
        "https://docs.onepanel.ai"
      ],
      "license": "Apache-2.0",
      "tags": [
        "computer-vision",
        "mlops",
        "workflow",
        "annotation"
      ],
      "id": 983
    },
    {
      "name": "GenAIStudio",
      "one_line_profile": "Low-code platform for constructing and benchmarking GenAI applications",
      "detailed_description": "GenAI Studio provides a visual interface and toolkit for building, evaluating, and benchmarking Generative AI applications. It facilitates the assessment of model performance and the creation of deployable AI packages.",
      "domains": [
        "AI4",
        "AI4-04",
        "Generative AI"
      ],
      "subtask_category": [
        "evaluation",
        "benchmarking",
        "application_building"
      ],
      "application_level": "platform",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/opea-project/GenAIStudio",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "genai",
        "evaluation",
        "benchmark",
        "low-code"
      ],
      "id": 984
    },
    {
      "name": "Amphion",
      "one_line_profile": "Toolkit for audio, music, and speech generation research",
      "detailed_description": "Amphion is a toolkit designed to support reproducible research in audio, music, and speech generation. It provides implementations of various generation models and tools for processing audio data, aiming to lower the barrier for entry in audio AI research.",
      "domains": [
        "AI4",
        "Audio",
        "Speech Synthesis"
      ],
      "subtask_category": [
        "audio_generation",
        "speech_synthesis",
        "music_generation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/open-mmlab/Amphion",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "audio-generation",
        "speech-synthesis",
        "music-generation",
        "reproducibility"
      ],
      "id": 985
    },
    {
      "name": "modelstore",
      "one_line_profile": "Library for versioning and exporting machine learning models",
      "detailed_description": "modelstore is a Python library that facilitates the versioning, export, and storage of machine learning models to various storage providers. It helps in tracking model artifacts and maintaining a registry of trained models.",
      "domains": [
        "AI4",
        "AI4-04",
        "MLOps"
      ],
      "subtask_category": [
        "model_versioning",
        "artifact_management",
        "model_registry"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/operatorai/modelstore",
      "help_website": [
        "https://modelstore.readthedocs.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "model-versioning",
        "mlops",
        "artifact-tracking"
      ],
      "id": 986
    },
    {
      "name": "mlogger",
      "one_line_profile": "Lightweight logger for machine learning experiments",
      "detailed_description": "mlogger is a simple and lightweight logging utility specifically designed for machine learning. It helps researchers track metrics and experiment progress without the overhead of heavy frameworks.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_logging",
        "metrics_tracking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/oval-group/mlogger",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "logging",
        "experiment-tracking",
        "ml-tools"
      ],
      "id": 987
    },
    {
      "name": "Android HCI Extractor",
      "one_line_profile": "Multimodal Human-Computer Interaction extractor for Android experiments",
      "detailed_description": "A tool for extracting, monitoring, and tracking multimodal user interactions on Android devices. It is designed for use in scientific experiments to record user behavior data for HCI research and analysis.",
      "domains": [
        "HCI",
        "Data Collection"
      ],
      "subtask_category": [
        "data_extraction",
        "user_monitoring",
        "interaction_tracking"
      ],
      "application_level": "tool",
      "primary_language": "Java",
      "repo_url": "https://github.com/pedromateo/android-hci-extractor-AHE11",
      "help_website": [],
      "license": null,
      "tags": [
        "hci",
        "android",
        "data-collection",
        "interaction-logging"
      ],
      "id": 988
    },
    {
      "name": "PathBench",
      "one_line_profile": "Benchmarking platform for path planning algorithms",
      "detailed_description": "PathBench is a platform for benchmarking both classic and learning-based path planning algorithms. It provides a standardized environment to evaluate and compare the performance of navigation algorithms in robotics and AI.",
      "domains": [
        "AI4",
        "Robotics",
        "AI4-04"
      ],
      "subtask_category": [
        "benchmarking",
        "algorithm_evaluation",
        "path_planning"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/perfectly-balanced/PathBench",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "path-planning",
        "benchmark",
        "robotics",
        "navigation"
      ],
      "id": 989
    },
    {
      "name": "Clean-Offline-RLHF",
      "one_line_profile": "Benchmark suite for Offline Reinforcement Learning with Human Feedback",
      "detailed_description": "This repository implements the Uni-RLHF benchmark suite, providing a platform for evaluating offline Reinforcement Learning with Human Feedback (RLHF) algorithms. It supports reproducible research in alignment and reinforcement learning.",
      "domains": [
        "AI4",
        "AI4-04",
        "RLHF"
      ],
      "subtask_category": [
        "benchmarking",
        "algorithm_evaluation",
        "rlhf"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/pickxiguapi/Clean-Offline-RLHF",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rlhf",
        "benchmark",
        "reinforcement-learning",
        "alignment"
      ],
      "id": 990
    },
    {
      "name": "Uni-RLHF-Platform",
      "one_line_profile": "Universal platform and benchmark suite for Reinforcement Learning with Human Feedback (RLHF)",
      "detailed_description": "A comprehensive platform and benchmark suite designed for Reinforcement Learning with Diverse Human Feedback (RLHF). It supports the evaluation and development of RLHF algorithms, providing a standardized environment for research and comparison.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "benchmarking",
        "reinforcement_learning",
        "human_feedback"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/pickxiguapi/Uni-RLHF-Platform",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rlhf",
        "benchmark",
        "reinforcement-learning"
      ],
      "id": 991
    },
    {
      "name": "ploomber-engine",
      "one_line_profile": "Toolbox for testing, tracking, and debugging Jupyter notebooks",
      "detailed_description": "A toolkit designed to enhance Jupyter notebooks for scientific workflows. It includes features for experiment tracking, debugging, profiling, and testing notebooks, facilitating reproducible research and development in interactive environments.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_tracking",
        "reproducibility",
        "debugging"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ploomber/ploomber-engine",
      "help_website": [
        "https://ploomber-engine.readthedocs.io/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "jupyter",
        "experiment-tracking",
        "profiling"
      ],
      "id": 992
    },
    {
      "name": "sklearn-evaluation",
      "one_line_profile": "Machine learning model evaluation and experiment tracking library",
      "detailed_description": "A library that simplifies machine learning model evaluation by providing tools for generating plots, tables, and HTML reports. It also includes features for experiment tracking and analysis within Jupyter notebooks, aiding in the comparison and selection of models.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "experiment_tracking",
        "visualization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ploomber/sklearn-evaluation",
      "help_website": [
        "https://sklearn-evaluation.ploomber.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "evaluation",
        "metrics",
        "visualization"
      ],
      "id": 993
    },
    {
      "name": "Polyaxon",
      "one_line_profile": "MLOps platform for managing and orchestrating the machine learning lifecycle",
      "detailed_description": "A platform for reproducible and scalable machine learning and deep learning. It provides tools for experiment tracking, hyperparameter tuning, and workflow orchestration, allowing researchers to manage the entire ML lifecycle.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_tracking",
        "workflow_orchestration",
        "mlops"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/polyaxon/polyaxon",
      "help_website": [
        "https://polyaxon.com/docs/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "mlops",
        "experiment-tracking",
        "orchestration"
      ],
      "id": 994
    },
    {
      "name": "TraceML",
      "one_line_profile": "Engine for ML/Data tracking, visualization, and drift detection",
      "detailed_description": "A library and engine dedicated to tracking machine learning experiments, visualizing data and model performance, explaining models, and detecting drift. It serves as the tracking component of the Polyaxon ecosystem but can be used for logging and analysis.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_tracking",
        "visualization",
        "drift_detection"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/polyaxon/traceml",
      "help_website": [
        "https://polyaxon.com/docs/traceml/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "tracking",
        "visualization",
        "logging"
      ],
      "id": 995
    },
    {
      "name": "Promptify",
      "one_line_profile": "Prompt engineering and versioning toolkit for LLMs",
      "detailed_description": "A toolkit for prompt engineering, versioning, and structuring outputs from Large Language Models (LLMs). It facilitates the management of prompts as scientific artifacts, enabling reproducible interactions with NLP models.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "prompt_engineering",
        "versioning",
        "nlp"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/promptslab/Promptify",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "prompt-engineering",
        "llm",
        "versioning"
      ],
      "id": 996
    },
    {
      "name": "EconML",
      "one_line_profile": "Automated Learning and Intelligence for Causation and Economics",
      "detailed_description": "A Python package that applies machine learning techniques to estimate heterogeneous treatment effects from observational data via causal inference. It implements orthogonal machine learning algorithms to measure causal effects, bridging econometrics and machine learning.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "causal_inference",
        "econometrics",
        "estimation"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/py-why/EconML",
      "help_website": [
        "https://econml.azurewebsites.net/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "causal-inference",
        "machine-learning",
        "economics"
      ],
      "id": 997
    },
    {
      "name": "pseudo-3d-pytorch",
      "one_line_profile": "PyTorch implementation of Pseudo-3D Residual Networks (P-3D)",
      "detailed_description": "A PyTorch implementation of Pseudo-3D Residual Networks (P-3D) for spatiotemporal feature learning in videos. It includes support for pretrained models, enabling reproduction of results and application to video analysis tasks.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "model_implementation",
        "reproduction",
        "video_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/qijiezhao/pseudo-3d-pytorch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pytorch",
        "p3d",
        "video-recognition"
      ],
      "id": 998
    },
    {
      "name": "rl-experiments",
      "one_line_profile": "Configuration and tracking for Reinforcement Learning experiments",
      "detailed_description": "A repository dedicated to tracking and reproducing Reinforcement Learning (RL) experiments using Ray RLlib. It serves as a benchmark reference, containing configurations and scripts to reproduce state-of-the-art results.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "reproduction",
        "benchmarking",
        "reinforcement_learning"
      ],
      "application_level": "dataset",
      "primary_language": null,
      "repo_url": "https://github.com/ray-project/rl-experiments",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "reinforcement-learning",
        "reproducibility",
        "benchmarks"
      ],
      "id": 999
    },
    {
      "name": "FuxiCTR",
      "one_line_profile": "Configurable and reproducible library for CTR prediction",
      "detailed_description": "A configurable, tunable, and reproducible library designed for Click-Through Rate (CTR) prediction tasks. It provides a standardized environment for benchmarking various CTR models and ensuring experimental reproducibility.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "benchmarking",
        "ctr_prediction",
        "reproducibility"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/reczoo/FuxiCTR",
      "help_website": [
        "https://fuxictr.github.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "ctr-prediction",
        "benchmark",
        "recommender-systems"
      ],
      "id": 1000
    },
    {
      "name": "garage",
      "one_line_profile": "Toolkit for reproducible reinforcement learning research",
      "detailed_description": "A toolkit for developing and evaluating reinforcement learning algorithms with a focus on reproducibility. It provides a consistent interface for running experiments and benchmarking RL methods.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "reproducibility",
        "reinforcement_learning",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/rlworkgroup/garage",
      "help_website": [
        "https://garage.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "reinforcement-learning",
        "reproducibility",
        "toolkit"
      ],
      "id": 1001
    },
    {
      "name": "drake",
      "one_line_profile": "Pipeline toolkit for reproducibility in R",
      "detailed_description": "An R-focused pipeline toolkit designed for reproducibility and high-performance computing. It manages data analysis workflows, tracks dependencies, and ensures that results are up-to-date and reproducible.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "workflow_management",
        "reproducibility",
        "pipeline"
      ],
      "application_level": "workflow",
      "primary_language": "R",
      "repo_url": "https://github.com/ropensci/drake",
      "help_website": [
        "https://docs.ropensci.org/drake/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "r",
        "reproducibility",
        "pipeline"
      ],
      "id": 1002
    },
    {
      "name": "greadability",
      "one_line_profile": "Graph layout readability metrics library",
      "detailed_description": "A JavaScript library for calculating readability metrics for graph layouts. It provides quantitative measures to evaluate the quality of graph visualizations, supporting research in information visualization and graph theory.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "metrics",
        "graph_visualization",
        "evaluation"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/rpgove/greadability",
      "help_website": [
        "http://rpgove.github.io/greadability/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "graph-layout",
        "metrics",
        "visualization"
      ],
      "id": 1003
    },
    {
      "name": "vetiver-r",
      "one_line_profile": "MLOps tool for versioning, sharing, deploying, and monitoring models in R",
      "detailed_description": "Vetiver provides fluent tooling to version, share, deploy, and monitor machine learning models. It is designed to handle the lifecycle of models, ensuring they can be reliably tracked and maintained in production or research environments.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "model_management",
        "tracking",
        "deployment"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/rstudio/vetiver-r",
      "help_website": [
        "https://vetiver.rstudio.com/"
      ],
      "license": "MIT",
      "tags": [
        "mlops",
        "model-monitoring",
        "r-stats",
        "model-deployment"
      ],
      "id": 1004
    },
    {
      "name": "llm-data-annotation",
      "one_line_profile": "Framework using LLMs for data annotation and iterative active learning",
      "detailed_description": "A framework that leverages Large Language Models (like GPT-3.5) to perform data annotation and model enhancement. It combines human expertise with LLMs and employs Iterative Active Learning and CleanLab (Confident Learning) to improve dataset quality.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "data_annotation",
        "active_learning",
        "dataset_curation"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/saran9991/llm-data-annotation",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "data-annotation",
        "active-learning",
        "cleanlab"
      ],
      "id": 1005
    },
    {
      "name": "DVCP-TE",
      "one_line_profile": "Simulation model of the Tennessee Eastman chemical process for security research",
      "detailed_description": "The Damn Vulnerable Chemical Process (DVCP) - Tennessee Eastman is a simulation environment designed for research into cyber-physical systems security. It provides a realistic model of a chemical process to generate data for attack detection and control system analysis.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "simulation",
        "data_generation",
        "security_benchmark"
      ],
      "application_level": "dataset",
      "primary_language": "HTML",
      "repo_url": "https://github.com/satejnik/DVCP-TE",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "simulation",
        "chemical-process",
        "ics-security",
        "benchmark"
      ],
      "id": 1006
    },
    {
      "name": "Semantic-Shapes",
      "one_line_profile": "Semantic segmentation pipeline for custom image annotation",
      "detailed_description": "A pipeline designed to facilitate custom image annotation for semantic segmentation tasks. It aids in the processing and preparation of image datasets for computer vision research.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "image_annotation",
        "segmentation",
        "data_processing"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/seth814/Semantic-Shapes",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "semantic-segmentation",
        "annotation",
        "computer-vision"
      ],
      "id": 1007
    },
    {
      "name": "EyeLoop",
      "one_line_profile": "Python-based eye-tracking system for dynamic closed-loop experiments",
      "detailed_description": "EyeLoop is a specialized eye-tracking tool tailored for dynamic, closed-loop experiments in neuroscience and psychology. It runs on consumer-grade hardware and provides a platform for behavioral data acquisition.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "data_acquisition",
        "behavioral_tracking",
        "neuroscience"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/simonarvin/eyeloop",
      "help_website": [
        "https://github.com/simonarvin/eyeloop"
      ],
      "license": "GPL-3.0",
      "tags": [
        "eye-tracking",
        "neuroscience",
        "psychophysics",
        "experiment-control"
      ],
      "id": 1008
    },
    {
      "name": "Snakemake",
      "one_line_profile": "Workflow management system for reproducible data analysis",
      "detailed_description": "Snakemake is a workflow management system that aims to reduce the complexity of creating facsimiles of data analysis steps. It is widely used in bioinformatics and data science to ensure reproducibility and scalability of scientific pipelines.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "workflow_management",
        "reproducibility",
        "pipeline_orchestration"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/snakemake/snakemake",
      "help_website": [
        "https://snakemake.readthedocs.io"
      ],
      "license": "MIT",
      "tags": [
        "workflow",
        "bioinformatics",
        "reproducibility",
        "pipeline"
      ],
      "id": 1009
    },
    {
      "name": "Whitebox",
      "one_line_profile": "E2E ML monitoring platform with edge capabilities",
      "detailed_description": "Whitebox is an open-source machine learning monitoring platform designed to track model performance and data drift, specifically supporting edge capabilities and Kubernetes integration.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "model_monitoring",
        "tracking",
        "mlops"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/squaredev-io/whitebox",
      "help_website": [
        "https://whitebox.squaredev.io"
      ],
      "license": "MIT",
      "tags": [
        "ml-monitoring",
        "observability",
        "kubernetes",
        "edge-ai"
      ],
      "id": 1010
    },
    {
      "name": "Julia-LLM-Leaderboard",
      "one_line_profile": "Benchmarking platform for LLM code generation in Julia",
      "detailed_description": "A platform and toolkit for evaluating and comparing the ability of Large Language Models to generate syntactically correct Julia code. It includes structured tests and automated evaluation pipelines.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "benchmarking",
        "leaderboard",
        "code_generation_eval"
      ],
      "application_level": "platform",
      "primary_language": "HTML",
      "repo_url": "https://github.com/svilupp/Julia-LLM-Leaderboard",
      "help_website": [
        "https://svilupp.github.io/Julia-LLM-Leaderboard/"
      ],
      "license": "MIT",
      "tags": [
        "julia",
        "llm-leaderboard",
        "benchmarking",
        "code-generation"
      ],
      "id": 1011
    },
    {
      "name": "tensorboardcolab",
      "one_line_profile": "Utility to run TensorBoard within Google Colab",
      "detailed_description": "A library designed to facilitate the use of TensorBoard, a standard visualization tool for machine learning experiments, directly within the Google Colab environment.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "visualization",
        "experiment_tracking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/taomanwai/tensorboardcolab",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "tensorboard",
        "colab",
        "visualization",
        "ml-tools"
      ],
      "id": 1012
    },
    {
      "name": "envd",
      "one_line_profile": "Reproducible development environment manager for AI/Data Science",
      "detailed_description": "envd is a command-line tool that creates reproducible development environments for AI and data science. It abstracts away Dockerfile complexity to ensure consistent environments for research and development.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "reproducibility",
        "environment_management",
        "infrastructure"
      ],
      "application_level": "solver",
      "primary_language": "Go",
      "repo_url": "https://github.com/tensorchord/envd",
      "help_website": [
        "https://envd.tensorchord.ai"
      ],
      "license": "Apache-2.0",
      "tags": [
        "reproducibility",
        "dev-environment",
        "data-science",
        "docker-wrapper"
      ],
      "id": 1013
    },
    {
      "name": "Iterate",
      "one_line_profile": "Benchmarking and hyperparameter optimization tool for TerraTorch",
      "detailed_description": "A tool designed for benchmarking and hyper-parameter optimization within the TerraTorch ecosystem. It integrates MLFlow for experiment logging, Optuna for optimization, and Ray for parallel execution.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "benchmarking",
        "hyperparameter_optimization"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/terrastackai/iterate",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "benchmarking",
        "hpo",
        "mlflow"
      ],
      "id": 1014
    },
    {
      "name": "Testground",
      "one_line_profile": "Platform for benchmarking and simulating distributed systems",
      "detailed_description": "A platform for testing, benchmarking, and simulating distributed and p2p systems at scale. It allows researchers and developers to verify protocols and systems under various network conditions.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "benchmarking",
        "simulation"
      ],
      "application_level": "platform",
      "primary_language": "Go",
      "repo_url": "https://github.com/testground/testground",
      "help_website": [
        "https://docs.testground.ai"
      ],
      "license": "NOASSERTION",
      "tags": [
        "benchmarking",
        "distributed-systems",
        "simulation"
      ],
      "id": 1015
    },
    {
      "name": "Crayon",
      "one_line_profile": "Language-agnostic interface for TensorBoard visualization",
      "detailed_description": "A tool that provides a language-agnostic interface to TensorBoard, allowing users to log and visualize experiment data from any programming language.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_visualization",
        "logging"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/torrvision/crayon",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "tensorboard",
        "visualization",
        "interface"
      ],
      "id": 1016
    },
    {
      "name": "Torch-Fidelity",
      "one_line_profile": "High-fidelity performance metrics for generative models",
      "detailed_description": "A PyTorch library for calculating high-fidelity performance metrics for generative models, such as Inception Score (IS) and Fréchet Inception Distance (FID), ensuring reproducible evaluation.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "model_evaluation",
        "metrics_calculation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/toshas/torch-fidelity",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "generative-models",
        "metrics",
        "fid"
      ],
      "id": 1017
    },
    {
      "name": "DVC",
      "one_line_profile": "Data Version Control for machine learning projects",
      "detailed_description": "An open-source tool for data management and machine learning experiment tracking. It handles large files, data sets, and machine learning models, making projects reproducible and shareable.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "data_versioning",
        "experiment_tracking"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/treeverse/dvc",
      "help_website": [
        "https://dvc.org"
      ],
      "license": "Apache-2.0",
      "tags": [
        "data-versioning",
        "mlops",
        "reproducibility"
      ],
      "id": 1018
    },
    {
      "name": "DVCLive",
      "one_line_profile": "Library for logging ML metrics and tracking experiments",
      "detailed_description": "A Python library for logging machine learning metrics, parameters, and model artifacts. It integrates with DVC to provide experiment tracking capabilities.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_tracking",
        "metrics_logging"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/treeverse/dvclive",
      "help_website": [
        "https://dvc.org/doc/dvclive"
      ],
      "license": "Apache-2.0",
      "tags": [
        "logging",
        "tracking",
        "mlops"
      ],
      "id": 1019
    },
    {
      "name": "ATOM",
      "one_line_profile": "Automated Tool for Optimized Modelling",
      "detailed_description": "A Python package for fast exploration and experimentation of machine learning pipelines. It automates data cleaning, feature engineering, and model selection.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "automl",
        "pipeline_optimization"
      ],
      "application_level": "library",
      "primary_language": "HTML",
      "repo_url": "https://github.com/tvdboom/ATOM",
      "help_website": [
        "https://tvdboom.github.io/ATOM/"
      ],
      "license": "MIT",
      "tags": [
        "automl",
        "data-science",
        "optimization"
      ],
      "id": 1020
    },
    {
      "name": "Uncertainty Toolbox",
      "one_line_profile": "Toolbox for predictive uncertainty quantification and metrics",
      "detailed_description": "A Python toolbox for predictive uncertainty quantification, calibration, metrics, and visualization. It helps researchers evaluate the reliability of machine learning model predictions.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "uncertainty_quantification",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/uncertainty-toolbox/uncertainty-toolbox",
      "help_website": [
        "https://uncertainty-toolbox.github.io/"
      ],
      "license": "MIT",
      "tags": [
        "uncertainty",
        "calibration",
        "metrics"
      ],
      "id": 1021
    },
    {
      "name": "TVault",
      "one_line_profile": "Lightweight local registry for comparing PyTorch models",
      "detailed_description": "A tool to quickly compare PyTorch models in a local, lightweight registry. It facilitates tracking model versions and performance metrics during development.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "model_registry",
        "model_comparison"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/vessl-ai/tvault",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pytorch",
        "model-management",
        "tracking"
      ],
      "id": 1022
    },
    {
      "name": "vSwarm-u",
      "one_line_profile": "Serverless benchmark suite integrated with gem5",
      "detailed_description": "A framework that integrates the serverless benchmark suite vSwarm with gem5, enabling research into system and microarchitecture for serverless computing.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "benchmarking",
        "system_simulation"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/vhive-serverless/vSwarm-u",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "benchmarking",
        "serverless",
        "gem5"
      ],
      "id": 1023
    },
    {
      "name": "OpenCorr",
      "one_line_profile": "Digital Image and Volume Correlation Library",
      "detailed_description": "A C++ library for Digital Image Correlation (DIC) and Digital Volume Correlation (DVC), used for measuring deformation and strain in materials science and engineering.",
      "domains": [
        "AI4",
        "AI4-11"
      ],
      "subtask_category": [
        "image_analysis",
        "strain_measurement"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/vincentjzy/OpenCorr",
      "help_website": [],
      "license": "MPL-2.0",
      "tags": [
        "dic",
        "dvc",
        "mechanics"
      ],
      "id": 1024
    },
    {
      "name": "Omniboard",
      "one_line_profile": "Web-based dashboard for Sacred experiment tracking",
      "detailed_description": "A web dashboard for visualizing experiments tracked with Sacred. It connects to the MongoDB database used by Sacred to display metrics, configuration, and artifacts.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_visualization",
        "dashboard"
      ],
      "application_level": "platform",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/vivekratnavel/omniboard",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "sacred",
        "visualization",
        "dashboard"
      ],
      "id": 1025
    },
    {
      "name": "SD-Extension-System-Info",
      "one_line_profile": "Benchmarking and system info for Stable Diffusion",
      "detailed_description": "An extension for Stable Diffusion web UIs that provides system information and standardized benchmarking capabilities to evaluate generation performance.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_profiling"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/vladmandic/sd-extension-system-info",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "stable-diffusion",
        "benchmarking",
        "system-info"
      ],
      "id": 1026
    },
    {
      "name": "Deepkit ML",
      "one_line_profile": "Real-time machine learning development and tracking suite",
      "detailed_description": "A collaborative open-source machine learning development tool and training suite. It offers experiment execution, real-time tracking, debugging, and project management features.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_tracking",
        "mlops"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/voided-org/deepkit-ml",
      "help_website": [
        "https://deepkit.io"
      ],
      "license": "MIT",
      "tags": [
        "mlops",
        "tracking",
        "debugging"
      ],
      "id": 1027
    },
    {
      "name": "PytorchAutoDrive",
      "one_line_profile": "Segmentation and lane detection models with benchmarking",
      "detailed_description": "A repository providing implementations of various segmentation and lane detection models for autonomous driving. It includes tools for fast training, visualization, benchmarking, and deployment.",
      "domains": [
        "AI4",
        "AI4-01"
      ],
      "subtask_category": [
        "benchmarking",
        "model_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/voldemortX/pytorch-auto-drive",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "autonomous-driving",
        "segmentation",
        "benchmarking"
      ],
      "id": 1028
    },
    {
      "name": "pref_voting",
      "one_line_profile": "Python library for simulating and analyzing preferential voting methods",
      "detailed_description": "A Python package designed to study and run elections using various preferential voting methods, including graded and cardinal voting systems, suitable for social choice theory research.",
      "domains": [
        "Social Science",
        "Game Theory"
      ],
      "subtask_category": [
        "simulation",
        "social_choice_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/voting-tools/pref_voting",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "voting-systems",
        "social-choice",
        "simulation"
      ],
      "id": 1029
    },
    {
      "name": "HiddenLayer",
      "one_line_profile": "Neural network graph visualization and training metrics library",
      "detailed_description": "A lightweight library for visualizing neural network graphs and tracking training metrics for PyTorch, TensorFlow, and Keras models.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "visualization",
        "training_monitoring"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/waleedka/hiddenlayer",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "visualization",
        "neural-networks",
        "training-metrics"
      ],
      "id": 1030
    },
    {
      "name": "Weights & Biases",
      "one_line_profile": "Developer platform for experiment tracking and model management",
      "detailed_description": "A comprehensive MLOps platform for tracking experiments, visualizing results, managing model versions, and collaborating on machine learning projects.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_tracking",
        "model_management"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/wandb/wandb",
      "help_website": [
        "https://wandb.ai"
      ],
      "license": "MIT",
      "tags": [
        "experiment-tracking",
        "mlops",
        "visualization"
      ],
      "id": 1031
    },
    {
      "name": "MuZero General",
      "one_line_profile": "General implementation of the MuZero reinforcement learning algorithm",
      "detailed_description": "A readable and modular implementation of the MuZero algorithm, serving as a research baseline and tool for reinforcement learning experiments.",
      "domains": [
        "AI4",
        "Reinforcement Learning"
      ],
      "subtask_category": [
        "modeling",
        "reinforcement_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/werner-duvaud/muzero-general",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "muzero",
        "reinforcement-learning",
        "ai-model"
      ],
      "id": 1032
    },
    {
      "name": "Test Tube",
      "one_line_profile": "Library for experiment logging and hyperparameter search",
      "detailed_description": "A Python library designed to easily log deep learning experiments and parallelize hyperparameter search across computing clusters.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "experiment_logging",
        "hyperparameter_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/williamFalcon/test-tube",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "experiment-logging",
        "hpo",
        "deep-learning"
      ],
      "id": 1033
    },
    {
      "name": "pytorch-fcn",
      "one_line_profile": "PyTorch implementation of Fully Convolutional Networks",
      "detailed_description": "A reproducible implementation of Fully Convolutional Networks (FCN) for semantic segmentation, serving as a baseline tool for computer vision research.",
      "domains": [
        "Computer Vision"
      ],
      "subtask_category": [
        "semantic_segmentation",
        "modeling"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/wkentaro/pytorch-fcn",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fcn",
        "semantic-segmentation",
        "pytorch"
      ],
      "id": 1034
    },
    {
      "name": "tensorflow-plot",
      "one_line_profile": "Utility for using Matplotlib within TensorFlow computation graphs",
      "detailed_description": "A library that allows wrapping Matplotlib plots as TensorFlow operations, enabling visualization of training progress directly within TensorBoard.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "visualization",
        "monitoring"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/wookayin/tensorflow-plot",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "tensorflow",
        "matplotlib",
        "visualization"
      ],
      "id": 1035
    },
    {
      "name": "MultiKernelBench",
      "one_line_profile": "Multi-platform benchmark for kernel generation",
      "detailed_description": "A benchmark suite designed to evaluate the performance of kernel generation across multiple hardware platforms, aiding in system performance analysis.",
      "domains": [
        "AI Systems",
        "High Performance Computing"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/wzzll123/MultiKernelBench",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "benchmarking",
        "kernel-generation",
        "hpc"
      ],
      "id": 1036
    },
    {
      "name": "Real-ESRGAN",
      "one_line_profile": "Practical algorithms for general image and video restoration",
      "detailed_description": "A tool implementing Real-ESRGAN for enhancing and restoring images and videos, widely used for data preprocessing and quality improvement in vision tasks.",
      "domains": [
        "Computer Vision"
      ],
      "subtask_category": [
        "image_restoration",
        "super_resolution"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/xinntao/Real-ESRGAN",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "super-resolution",
        "image-restoration",
        "gan"
      ],
      "id": 1037
    },
    {
      "name": "MME",
      "one_line_profile": "Platform for logging, replaying, and benchmarking LLM calls",
      "detailed_description": "A plug-and-play platform designed to log, replay, and benchmark Large Language Model (LLM) API calls, facilitating evaluation and optimization of LLM applications.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "benchmarking",
        "logging",
        "llm_evaluation"
      ],
      "application_level": "library",
      "primary_language": "CSS",
      "repo_url": "https://github.com/xuzeyu91/MME",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "benchmarking",
        "logging"
      ],
      "id": 1038
    },
    {
      "name": "Arrakis",
      "one_line_profile": "Library for mechanistic interpretability experiments",
      "detailed_description": "A library designed to conduct, track, and visualize mechanistic interpretability experiments for neural networks, aiding in understanding model behavior.",
      "domains": [
        "AI4",
        "AI Interpretability"
      ],
      "subtask_category": [
        "interpretability",
        "analysis"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/yash-srivastava19/arrakis",
      "help_website": [],
      "license": null,
      "tags": [
        "interpretability",
        "mechanistic-interpretability",
        "visualization"
      ],
      "id": 1039
    },
    {
      "name": "nerf-pytorch",
      "one_line_profile": "PyTorch implementation of Neural Radiance Fields (NeRF)",
      "detailed_description": "A widely used PyTorch implementation of NeRF for synthesizing novel views of complex scenes, serving as a standard tool for 3D vision research.",
      "domains": [
        "Computer Vision",
        "Graphics"
      ],
      "subtask_category": [
        "novel_view_synthesis",
        "modeling"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/yenchenlin/nerf-pytorch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nerf",
        "3d-vision",
        "rendering"
      ],
      "id": 1040
    },
    {
      "name": "friendly-stable-audio-tools",
      "one_line_profile": "Tools for training and using stable audio generative models",
      "detailed_description": "A refactored and updated library for working with audio/music generative models, facilitating research and application of stable audio generation.",
      "domains": [
        "Audio Processing",
        "AI4"
      ],
      "subtask_category": [
        "audio_generation",
        "modeling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/yukara-ikemiya/friendly-stable-audio-tools",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "generative-audio",
        "stable-audio",
        "deep-learning"
      ],
      "id": 1041
    },
    {
      "name": "TF2DeepFloorplan",
      "one_line_profile": "Deep FloorPlan Recognition tool with multi-task network",
      "detailed_description": "A TensorFlow 2 implementation for floorplan recognition using a multi-task network, including deployment and visualization tools like TensorBoard and TFLite support.",
      "domains": [
        "Computer Vision"
      ],
      "subtask_category": [
        "floorplan_recognition",
        "modeling"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/zcemycl/TF2DeepFloorplan",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "floorplan-recognition",
        "computer-vision",
        "tensorflow"
      ],
      "id": 1042
    },
    {
      "name": "ZnTrack",
      "one_line_profile": "Interface to create, run, and benchmark DVC pipelines in Python",
      "detailed_description": "A Python interface for DVC (Data Version Control) that allows creating, visualizing, and benchmarking data science pipelines directly from Python or Jupyter notebooks.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "pipeline_management",
        "experiment_tracking",
        "benchmarking"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/zincware/ZnTrack",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "dvc",
        "pipeline",
        "reproducibility"
      ],
      "id": 1043
    },
    {
      "name": "pytorch-generative-model-collections",
      "one_line_profile": "Collection of generative model implementations in PyTorch",
      "detailed_description": "A comprehensive library containing implementations of various generative models (GANs, VAEs, etc.) in PyTorch, serving as a reference and toolbox for generative modeling research.",
      "domains": [
        "AI4",
        "Computer Vision"
      ],
      "subtask_category": [
        "generative_modeling",
        "modeling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/znxlwm/pytorch-generative-model-collections",
      "help_website": [],
      "license": null,
      "tags": [
        "generative-models",
        "gan",
        "pytorch"
      ],
      "id": 1044
    },
    {
      "name": "SkySense-O",
      "one_line_profile": "Open-world remote sensing interpretation model",
      "detailed_description": "A vision-centric visual-language model aggregated with CLIP and SAM, designed for open-world remote sensing interpretation tasks.",
      "domains": [
        "Remote Sensing",
        "AI4"
      ],
      "subtask_category": [
        "remote_sensing_interpretation",
        "modeling"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/zqcrafts/SkySense-O",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "remote-sensing",
        "vlm",
        "clip"
      ],
      "id": 1045
    },
    {
      "name": "DiaHalu",
      "one_line_profile": "Dialogue-level hallucination evaluation benchmark for LLMs",
      "detailed_description": "A benchmark dataset and evaluation framework designed to assess hallucination in Large Language Models at the dialogue level, focusing on multi-turn interactions.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_evaluation",
        "safety_benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/141forever/DiaHalu",
      "help_website": [],
      "license": null,
      "tags": [
        "hallucination",
        "llm-evaluation",
        "benchmark"
      ],
      "id": 1046
    },
    {
      "name": "AgentPoison",
      "one_line_profile": "Red-teaming tool for LLM Agents via backdoor poisoning",
      "detailed_description": "A red-teaming framework that evaluates the robustness of LLM agents by injecting backdoors into their memory or knowledge base to trigger targeted misbehaviors.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "red_teaming",
        "adversarial_attack",
        "agent_security"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/AI-secure/AgentPoison",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "red-teaming",
        "llm-agent",
        "backdoor-attack"
      ],
      "id": 1047
    },
    {
      "name": "UDora",
      "one_line_profile": "Unified red teaming framework against LLM Agents",
      "detailed_description": "A comprehensive framework for conducting red teaming operations against Large Language Model agents to identify security vulnerabilities and safety risks.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "red_teaming",
        "safety_evaluation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/AI-secure/UDora",
      "help_website": [],
      "license": null,
      "tags": [
        "red-teaming",
        "llm-agent",
        "safety-framework"
      ],
      "id": 1048
    },
    {
      "name": "aixploit",
      "one_line_profile": "Exploitation toolkit for LLM vulnerabilities",
      "detailed_description": "A toolkit designed for red teams and penetration testers to identify and exploit vulnerabilities in Large Language Model solutions.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "penetration_testing",
        "vulnerability_scanning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/AINTRUST-AI/aixploit",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "exploit",
        "llm-security",
        "red-teaming"
      ],
      "id": 1049
    },
    {
      "name": "hallucinogen",
      "one_line_profile": "Benchmark for evaluating hallucinations in LVLMs",
      "detailed_description": "A benchmark suite specifically designed to evaluate and quantify hallucination phenomena in Large Visual Language Models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_evaluation",
        "multimodal_benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/AikyamLab/hallucinogen",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "lvlm",
        "hallucination",
        "benchmark"
      ],
      "id": 1050
    },
    {
      "name": "Octopus-Family",
      "one_line_profile": "Multi-dimensional safety assessment suite for AI models",
      "detailed_description": "A comprehensive safety testing suite developed by Alibaba-AAIG that provides multi-faceted probing to evaluate the safety and robustness of AI models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_assessment",
        "robustness_testing"
      ],
      "application_level": "framework",
      "primary_language": null,
      "repo_url": "https://github.com/Alibaba-AAIG/Octopus-Family",
      "help_website": [],
      "license": null,
      "tags": [
        "safety-evaluation",
        "robustness",
        "testing-suite"
      ],
      "id": 1051
    },
    {
      "name": "CognitiveLens",
      "one_line_profile": "Analytics tool for human-AI alignment visualization",
      "detailed_description": "A Streamlit-based analytics tool that visualizes fairness, calibration, and interpretability metrics to explore alignment between human and AI decisions.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "alignment_visualization",
        "fairness_auditing"
      ],
      "application_level": "application",
      "primary_language": "Python",
      "repo_url": "https://github.com/AmirhosseinHonardoust/Cognitivelens-AI-Human-Comparison",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "visualization",
        "alignment",
        "fairness"
      ],
      "id": 1052
    },
    {
      "name": "POPE",
      "one_line_profile": "Evaluation method for object hallucination in LVLMs",
      "detailed_description": "A benchmark and evaluation method (Polling-based Object Probing Evaluation) for assessing object hallucination issues in Large Vision-Language Models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_evaluation",
        "object_detection_check"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/AoiDragon/POPE",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "lvlm",
        "hallucination",
        "evaluation-metric"
      ],
      "id": 1053
    },
    {
      "name": "PREPER",
      "one_line_profile": "Dataset for safety evaluation of AI perception systems",
      "detailed_description": "A dataset specifically constructed to evaluate the safety and robustness of AI perception systems under various conditions.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_evaluation",
        "perception_benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/AsymptoticAI/PREPER",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "dataset",
        "safety",
        "perception"
      ],
      "id": 1054
    },
    {
      "name": "Counterfit",
      "one_line_profile": "CLI for assessing security of ML models",
      "detailed_description": "A command-line automation tool by Azure for assessing the security of machine learning models, enabling red teaming and vulnerability scanning.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "security_assessment",
        "vulnerability_scanning"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/Azure/counterfit",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "security",
        "ml-assessment",
        "cli"
      ],
      "id": 1055
    },
    {
      "name": "ALERT",
      "one_line_profile": "Benchmark for assessing LLM safety via red teaming",
      "detailed_description": "A comprehensive benchmark designed to assess the safety of Large Language Models through simulated red teaming attacks and scenarios.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_benchmarking",
        "red_teaming"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/Babelscape/ALERT",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "benchmark",
        "llm-safety",
        "red-teaming"
      ],
      "id": 1056
    },
    {
      "name": "SafeWatch",
      "one_line_profile": "Safety-policy following video guardrail model",
      "detailed_description": "An efficient video guardrail model designed to enforce safety policies with transparent explanations, serving as a tool for content moderation and safety alignment.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "content_moderation",
        "video_safety"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/BillChan226/SafeWatch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "guardrail",
        "video-safety",
        "explainability"
      ],
      "id": 1057
    },
    {
      "name": "LitterBox",
      "one_line_profile": "Secure sandbox for malware analysis with LLM integration",
      "detailed_description": "A secure sandbox environment designed for red teamers to test payloads, featuring integration with LLM agents for enhanced analysis capabilities.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "malware_analysis",
        "red_teaming_environment"
      ],
      "application_level": "platform",
      "primary_language": "YARA",
      "repo_url": "https://github.com/BlackSnufkin/LitterBox",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "sandbox",
        "malware-analysis",
        "llm-agent"
      ],
      "id": 1058
    },
    {
      "name": "advertorch",
      "one_line_profile": "Toolbox for adversarial robustness research",
      "detailed_description": "A Python toolbox for adversarial robustness research, providing implementations of attacks, defenses, and robust training methods for PyTorch.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_robustness",
        "attack_simulation"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/BorealisAI/advertorch",
      "help_website": [],
      "license": "LGPL-3.0",
      "tags": [
        "adversarial-attacks",
        "robustness",
        "pytorch"
      ],
      "id": 1059
    },
    {
      "name": "Bud Runtime",
      "one_line_profile": "Inference stack for AI deployment and optimization",
      "detailed_description": "A comprehensive inference stack and runtime environment for deploying, optimizing, and scaling compound AI systems.",
      "domains": [
        "AI4"
      ],
      "subtask_category": [
        "inference_optimization",
        "model_deployment"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/BudEcosystem/bud-runtime",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "inference",
        "deployment",
        "optimization"
      ],
      "id": 1060
    },
    {
      "name": "agent-attack",
      "one_line_profile": "Adversarial robustness benchmark for multimodal agents",
      "detailed_description": "A framework and benchmark for dissecting and evaluating the adversarial robustness of multimodal language model agents.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "robustness_benchmarking",
        "adversarial_attack"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ChenWu98/agent-attack",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "multimodal-agent",
        "robustness",
        "benchmark"
      ],
      "id": 1061
    },
    {
      "name": "VidHalluc",
      "one_line_profile": "Benchmark for temporal hallucinations in video LLMs",
      "detailed_description": "A benchmark designed to evaluate temporal hallucinations in Multimodal Large Language Models specifically for video understanding tasks.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_evaluation",
        "video_understanding"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/CyL97/VidHalluc",
      "help_website": [],
      "license": null,
      "tags": [
        "video-llm",
        "hallucination",
        "benchmark"
      ],
      "id": 1062
    },
    {
      "name": "CMM",
      "one_line_profile": "Benchmark for evaluating hallucinations across modalities",
      "detailed_description": "The Curse of Multi-Modalities (CMM) is a benchmark for evaluating hallucinations in Large Multimodal Models across language, visual, and audio modalities.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_evaluation",
        "multimodal_benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/DAMO-NLP-SG/CMM",
      "help_website": [],
      "license": null,
      "tags": [
        "multimodal",
        "hallucination",
        "benchmark"
      ],
      "id": 1063
    },
    {
      "name": "ToxiCN",
      "one_line_profile": "Benchmark for Chinese toxic language detection",
      "detailed_description": "A fine-grained benchmark and resource for detecting toxic language in Chinese, including a hierarchical taxonomy and dataset.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "toxicity_detection",
        "safety_benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/DUT-lujunyu/ToxiCN",
      "help_website": [],
      "license": null,
      "tags": [
        "toxicity",
        "chinese-nlp",
        "benchmark"
      ],
      "id": 1064
    },
    {
      "name": "AISecurity",
      "one_line_profile": "AI Firewall for protecting LLMs",
      "detailed_description": "A security tool acting as an AI firewall with multiple detection engines to protect Large Language Models from jailbreaks, injections, and adversarial attacks.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "security_defense",
        "attack_detection"
      ],
      "application_level": "service",
      "primary_language": "HTML",
      "repo_url": "https://github.com/DmitrL-dev/AISecurity",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "firewall",
        "llm-security",
        "jailbreak-detection"
      ],
      "id": 1065
    },
    {
      "name": "DiaHalu (ECNU)",
      "one_line_profile": "Dialogue-level hallucination evaluation benchmark",
      "detailed_description": "A benchmark for evaluating dialogue-level hallucinations in LLMs (Mirror/Fork of the main DiaHalu project).",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": null,
      "repo_url": "https://github.com/ECNU-ICALK/DiaHalu",
      "help_website": [],
      "license": null,
      "tags": [
        "hallucination",
        "benchmark"
      ],
      "id": 1066
    },
    {
      "name": "Robust3DOD",
      "one_line_profile": "Benchmark and robustness evaluation toolkit for LiDAR-based 3D object detectors",
      "detailed_description": "A comprehensive study and toolkit for evaluating the robustness of LiDAR-based 3D object detectors against adversarial attacks, providing benchmark datasets and attack implementations.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "adversarial_attack",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Eaphan/Robust3DOD",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "3d-object-detection",
        "adversarial-robustness",
        "lidar",
        "autonomous-driving"
      ],
      "id": 1067
    },
    {
      "name": "FAIR Metrics",
      "one_line_profile": "Reference implementation of FAIR (Findable, Accessible, Interoperable, Reusable) metrics",
      "detailed_description": "A Ruby-based library implementing the metrics defined by the FAIR Metrics Group to evaluate the FAIRness of digital resources, supporting scientific data management and quality control.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "data_quality_control",
        "fair_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Ruby",
      "repo_url": "https://github.com/FAIRMetrics/Metrics",
      "help_website": [
        "http://fairmetrics.org"
      ],
      "license": "MIT",
      "tags": [
        "fair-principles",
        "data-quality",
        "metrics"
      ],
      "id": 1068
    },
    {
      "name": "LRV-Instruction",
      "one_line_profile": "Dataset and instruction tuning method for mitigating hallucination in LMMs",
      "detailed_description": "A framework and dataset designed to evaluate and mitigate hallucinations in Large Multi-Modal Models (LMMs) via robust instruction tuning.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_mitigation",
        "instruction_tuning",
        "dataset_generation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/FuxiaoLiu/LRV-Instruction",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "hallucination",
        "multimodal-models",
        "instruction-tuning"
      ],
      "id": 1069
    },
    {
      "name": "Giskard Client",
      "one_line_profile": "Python client for the Giskard AI testing platform",
      "detailed_description": "The official API client for interacting with the Giskard platform, enabling programmatic testing, monitoring, and evaluation of AI models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "model_testing",
        "api_client"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Giskard-AI/giskard-client",
      "help_website": [
        "https://docs.giskard.ai"
      ],
      "license": "Apache-2.0",
      "tags": [
        "ai-testing",
        "mlops",
        "client"
      ],
      "id": 1070
    },
    {
      "name": "Giskard Hub SDK",
      "one_line_profile": "SDK for enterprise LLM agent testing and red teaming",
      "detailed_description": "A software development kit for the Giskard Hub, facilitating collaborative testing, continuous red teaming, and evaluation of LLM agents in enterprise environments.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "red_teaming",
        "agent_testing"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/Giskard-AI/giskard-hub",
      "help_website": [
        "https://docs.giskard.ai"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm-testing",
        "red-teaming",
        "sdk"
      ],
      "id": 1071
    },
    {
      "name": "Giskard Vision",
      "one_line_profile": "Evaluation and testing library for Computer Vision models",
      "detailed_description": "A specialized extension of the Giskard framework dedicated to testing and evaluating computer vision AI systems for robustness and correctness.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "vision_evaluation",
        "robustness_testing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Giskard-AI/giskard-vision",
      "help_website": [
        "https://docs.giskard.ai"
      ],
      "license": "Apache-2.0",
      "tags": [
        "computer-vision",
        "testing",
        "evaluation"
      ],
      "id": 1072
    },
    {
      "name": "Phare",
      "one_line_profile": "Benchmark for evaluating LLM security and safety dimensions",
      "detailed_description": "A benchmark suite designed to evaluate Large Language Models across key security and safety dimensions, providing standardized metrics for model comparison.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_benchmarking",
        "security_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/Giskard-AI/phare",
      "help_website": [],
      "license": null,
      "tags": [
        "llm-benchmark",
        "safety",
        "security"
      ],
      "id": 1073
    },
    {
      "name": "Giskard Prompt Injections",
      "one_line_profile": "Dataset of prompt injections for LLM security scanning",
      "detailed_description": "A curated collection of prompt injection attacks used by the Giskard Scan tool to test the vulnerability of Large Language Models to adversarial inputs.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_attack",
        "security_scanning"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/Giskard-AI/prompt-injections",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "prompt-injection",
        "llm-security",
        "dataset"
      ],
      "id": 1074
    },
    {
      "name": "ChatGPT Evaluation",
      "one_line_profile": "Multitask benchmark for evaluating ChatGPT on reasoning and hallucination",
      "detailed_description": "A repository containing test samples and scripts for a multitask, multilingual, and multimodal evaluation of ChatGPT, focusing on reasoning capabilities, hallucination, and interactivity.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "model_benchmarking",
        "hallucination_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/HLTCHKUST/chatgpt-evaluation",
      "help_website": [],
      "license": null,
      "tags": [
        "chatgpt",
        "evaluation",
        "hallucination",
        "multilingual"
      ],
      "id": 1075
    },
    {
      "name": "Smoothing Adversarial",
      "one_line_profile": "Reference implementation for Randomized Smoothing robustness certification",
      "detailed_description": "The official implementation of 'Provably Robust Deep Learning via Adversarially Trained Smoothed Classifiers', serving as a standard solver for certifying the robustness of deep learning models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "robustness_certification",
        "adversarial_training"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Hadisalman/smoothing-adversarial",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "randomized-smoothing",
        "certified-robustness",
        "adversarial-defense"
      ],
      "id": 1076
    },
    {
      "name": "TrustLLM",
      "one_line_profile": "Comprehensive benchmark for trustworthiness in Large Language Models",
      "detailed_description": "A benchmark toolkit evaluating LLM trustworthiness across multiple dimensions including truthfulness, safety, fairness, robustness, privacy, and machine ethics.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "trustworthiness_evaluation",
        "safety_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/HowieHwong/TrustLLM",
      "help_website": [
        "https://trustllmbenchmark.github.io/TrustLLM-Website/"
      ],
      "license": "MIT",
      "tags": [
        "llm",
        "trustworthiness",
        "benchmark",
        "safety"
      ],
      "id": 1077
    },
    {
      "name": "UHGEval-dataset",
      "one_line_profile": "Pipeline for creating hallucination evaluation datasets",
      "detailed_description": "The data generation pipeline used to create the UHGEval hallucination dataset, supporting the creation of custom evaluation benchmarks for LLMs.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "dataset_generation",
        "hallucination_research"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/IAAR-Shanghai/UHGEval-dataset",
      "help_website": [],
      "license": null,
      "tags": [
        "dataset-creation",
        "hallucination",
        "llm"
      ],
      "id": 1078
    },
    {
      "name": "HEART",
      "one_line_profile": "Hardened Extension of the Adversarial Robustness Toolbox",
      "detailed_description": "A library extending the Adversarial Robustness Toolbox (ART) to support the assessment of adversarial AI vulnerabilities specifically in Test & Evaluation workflows.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "vulnerability_assessment",
        "adversarial_robustness"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/IBM/heart-library",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "adversarial-robustness",
        "security",
        "testing"
      ],
      "id": 1079
    },
    {
      "name": "Infosys Responsible AI Toolkit",
      "one_line_profile": "Toolkit for AI safety, security, fairness, and explainability",
      "detailed_description": "A comprehensive toolkit incorporating features for safety, security, explainability, fairness, bias detection, and hallucination detection to ensure trustworthy AI solutions.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "responsible_ai",
        "bias_detection",
        "hallucination_detection"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/Infosys/Infosys-Responsible-AI-Toolkit",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "responsible-ai",
        "fairness",
        "explainability",
        "security"
      ],
      "id": 1080
    },
    {
      "name": "LAION-SAFETY",
      "one_line_profile": "Toolbox for NSFW and toxicity detection in datasets",
      "detailed_description": "An open toolbox providing models and scripts for detecting NSFW content and toxicity, primarily used for filtering and cleaning large-scale datasets like LAION.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "data_filtering",
        "toxicity_detection",
        "nsfw_detection"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/LAION-AI/LAION-SAFETY",
      "help_website": [],
      "license": null,
      "tags": [
        "safety",
        "content-moderation",
        "dataset-cleaning"
      ],
      "id": 1081
    },
    {
      "name": "LLMs-Finetuning-Safety",
      "one_line_profile": "Resources for demonstrating safety guardrail breaches via fine-tuning",
      "detailed_description": "A research toolkit and dataset demonstrating how fine-tuning on a small number of adversarial examples can compromise the safety guardrails of LLMs like GPT-3.5 Turbo.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_evaluation",
        "adversarial_attack",
        "jailbreaking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/LLM-Tuning-Safety/LLMs-Finetuning-Safety",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "jailbreak",
        "fine-tuning",
        "llm-safety"
      ],
      "id": 1082
    },
    {
      "name": "fair-test",
      "one_line_profile": "Library to build and deploy FAIR metrics tests APIs",
      "detailed_description": "A Python library facilitating the creation and deployment of FAIR (Findable, Accessible, Interoperable, Reusable) metrics tests, compatible with FAIR evaluation services.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "fair_evaluation",
        "data_management"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/MaastrichtU-IDS/fair-test",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fair-principles",
        "metrics",
        "api"
      ],
      "id": 1083
    },
    {
      "name": "CIFAR10 Challenge",
      "one_line_profile": "Benchmark challenge for adversarial robustness on CIFAR10",
      "detailed_description": "A benchmark and challenge framework designed to evaluate and compare the adversarial robustness of neural networks on the CIFAR10 dataset.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "robustness_benchmarking",
        "adversarial_defense"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/MadryLab/cifar10_challenge",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "cifar10",
        "adversarial-robustness",
        "benchmark"
      ],
      "id": 1084
    },
    {
      "name": "MNIST Challenge",
      "one_line_profile": "Benchmark challenge for adversarial robustness on MNIST",
      "detailed_description": "A benchmark and challenge framework designed to evaluate and compare the adversarial robustness of neural networks on the MNIST dataset.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "robustness_benchmarking",
        "adversarial_defense"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/MadryLab/mnist_challenge",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mnist",
        "adversarial-robustness",
        "benchmark"
      ],
      "id": 1085
    },
    {
      "name": "Robustness",
      "one_line_profile": "Library for training and evaluating robust neural networks",
      "detailed_description": "A comprehensive library for experimenting with, training, and evaluating neural networks with a specific focus on adversarial robustness, providing standard implementations of adversarial training.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_training",
        "robustness_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/MadryLab/robustness",
      "help_website": [
        "https://robustness.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "adversarial-training",
        "deep-learning",
        "robustness"
      ],
      "id": 1086
    },
    {
      "name": "HaluMem",
      "one_line_profile": "Hallucination evaluation benchmark for agent memory systems",
      "detailed_description": "HaluMem is an operation-level hallucination evaluation benchmark specifically designed for agent memory systems. It provides a framework to assess and detect hallucinations in the memory retrieval and utilization processes of AI agents.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_detection",
        "benchmark"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/MemTensor/HaluMem",
      "help_website": [],
      "license": null,
      "tags": [
        "hallucination",
        "agent-memory",
        "benchmark"
      ],
      "id": 1087
    },
    {
      "name": "Dingo",
      "one_line_profile": "Comprehensive AI quality evaluation tool for data, models, and applications",
      "detailed_description": "Dingo is a comprehensive evaluation tool designed to assess the quality of AI data, models, and applications. It supports various evaluation metrics and scenarios to ensure the reliability and performance of AI systems.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "quality_evaluation",
        "model_assessment"
      ],
      "application_level": "solver",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/MigoXLab/dingo",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "evaluation",
        "quality-assurance",
        "ai-testing"
      ],
      "id": 1088
    },
    {
      "name": "Gandalf LLM Pentester",
      "one_line_profile": "Automated red-teaming toolkit for stress-testing LLM defenses",
      "detailed_description": "Gandalf LLM Pentester is an automated toolkit designed for red-teaming Large Language Models. It focuses on stress-testing LLM defenses through vector attacks and provides insights into potential vulnerabilities and alignment failures.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "red_teaming",
        "adversarial_attack"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/MrMoshkovitz/gandalf-llm-pentester",
      "help_website": [],
      "license": null,
      "tags": [
        "red-teaming",
        "llm-security",
        "penetration-testing"
      ],
      "id": 1089
    },
    {
      "name": "Summarization Eval",
      "one_line_profile": "Reference-free automatic summarization evaluation with hallucination detection",
      "detailed_description": "This tool provides a reference-free method for evaluating automatic summarization. It includes features for detecting potential hallucinations in generated summaries, aiming to improve the reliability of summarization metrics without requiring ground truth references.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_detection",
        "summarization_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Muhtasham/summarization-eval",
      "help_website": [],
      "license": null,
      "tags": [
        "summarization",
        "evaluation",
        "hallucination"
      ],
      "id": 1090
    },
    {
      "name": "NRP",
      "one_line_profile": "Self-supervised approach for adversarial robustness in vision models",
      "detailed_description": "NRP (Neural Representation Purification) is an implementation of a self-supervised approach for enhancing adversarial robustness. It focuses on purifying adversarial perturbations from input images to protect vision models against attacks.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_defense",
        "robustness"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Muzammal-Naseer/NRP",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "adversarial-robustness",
        "computer-vision",
        "self-supervised-learning"
      ],
      "id": 1091
    },
    {
      "name": "Hallu-PI",
      "one_line_profile": "Benchmark for evaluating hallucination in Multi-modal LLMs with perturbed inputs",
      "detailed_description": "Hallu-PI is a benchmark and dataset designed to evaluate hallucinations in Multi-modal Large Language Models (MLLMs). It specifically focuses on scenarios involving perturbed inputs to assess the robustness and faithfulness of model generations.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_evaluation",
        "multimodal_benchmark"
      ],
      "application_level": "dataset",
      "primary_language": null,
      "repo_url": "https://github.com/NJUNLP/Hallu-PI",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "hallucination",
        "mllm",
        "benchmark"
      ],
      "id": 1092
    },
    {
      "name": "NeMo Guardrails",
      "one_line_profile": "Toolkit for adding programmable guardrails to LLM-based systems",
      "detailed_description": "NeMo Guardrails is an open-source toolkit that allows developers to add programmable guardrails to Large Language Model (LLM) based conversational systems. It helps ensure that the models behave within defined safety and topical boundaries.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_guardrails",
        "alignment"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NVIDIA-NeMo/Guardrails",
      "help_website": [
        "https://github.com/NVIDIA-NeMo/Guardrails"
      ],
      "license": null,
      "tags": [
        "guardrails",
        "llm-safety",
        "dialogue-systems"
      ],
      "id": 1093
    },
    {
      "name": "garak",
      "one_line_profile": "Vulnerability scanner for Large Language Models",
      "detailed_description": "garak is a vulnerability scanner specifically designed for Large Language Models (LLMs). It probes LLMs for a wide range of weaknesses, including hallucination, data leakage, prompt injection, and toxicity, acting as an automated red-teaming tool.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "vulnerability_scanning",
        "red_teaming"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/NVIDIA/garak",
      "help_website": [
        "https://garak.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm-security",
        "vulnerability-scanner",
        "red-teaming"
      ],
      "id": 1094
    },
    {
      "name": "ToXCL",
      "one_line_profile": "Unified framework for toxic speech detection and explanation",
      "detailed_description": "ToXCL is a framework designed for the detection and explanation of toxic speech. It integrates methods to identify toxic content and provide explanations for the detection, facilitating research into interpretable safety measures for NLP models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "toxicity_detection",
        "explainable_ai"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NhatHoang2002/ToXCL",
      "help_website": [],
      "license": null,
      "tags": [
        "toxicity-detection",
        "nlp",
        "explainability"
      ],
      "id": 1095
    },
    {
      "name": "HalluQA",
      "one_line_profile": "Benchmark for evaluating hallucinations in Chinese Large Language Models",
      "detailed_description": "HalluQA is a dataset and evaluation framework specifically tailored for assessing hallucinations in Chinese Large Language Models. It provides a set of questions and evaluation scripts to measure the factual correctness and hallucination rates of models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_evaluation",
        "benchmark"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenMOSS/HalluQA",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "hallucination",
        "chinese-llm",
        "benchmark"
      ],
      "id": 1096
    },
    {
      "name": "SafeVLA",
      "one_line_profile": "Safety alignment framework for Vision-Language-Action models",
      "detailed_description": "SafeVLA is a framework for the safety alignment of Vision-Language-Action (VLA) models via constrained learning. It addresses safety concerns in embodied AI agents by enforcing constraints during the learning process.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_alignment",
        "embodied_ai"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/PKU-Alignment/SafeVLA",
      "help_website": [],
      "license": null,
      "tags": [
        "safety-alignment",
        "vla",
        "constrained-learning"
      ],
      "id": 1097
    },
    {
      "name": "BeaverTails",
      "one_line_profile": "Dataset collection for safety alignment in Large Language Models",
      "detailed_description": "BeaverTails is a comprehensive collection of datasets designed to support research on safety alignment in Large Language Models (LLMs). It includes data for training and evaluating models on helpfulness and harmlessness.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_alignment",
        "dataset"
      ],
      "application_level": "dataset",
      "primary_language": "Makefile",
      "repo_url": "https://github.com/PKU-Alignment/beavertails",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "safety-alignment",
        "rlhf",
        "dataset"
      ],
      "id": 1098
    },
    {
      "name": "Safe-RLHF",
      "one_line_profile": "Library for constrained value alignment via Safe Reinforcement Learning",
      "detailed_description": "Safe-RLHF is a library that implements Safe Reinforcement Learning from Human Feedback. It enables the alignment of LLMs with human values while enforcing safety constraints, decoupling helpfulness and harmlessness objectives.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_alignment",
        "rlhf"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/PKU-Alignment/safe-rlhf",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rlhf",
        "safety",
        "alignment"
      ],
      "id": 1099
    },
    {
      "name": "SafeSora",
      "one_line_profile": "Human preference dataset for safety alignment in text-to-video generation",
      "detailed_description": "SafeSora is a dataset of human preferences designed for safety alignment research in text-to-video generation models. It aims to enhance the helpfulness and harmlessness of Large Vision Models (LVMs) by providing safety-oriented preference data.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_alignment",
        "text-to-video"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/PKU-Alignment/safe-sora",
      "help_website": [],
      "license": null,
      "tags": [
        "text-to-video",
        "safety",
        "preference-dataset"
      ],
      "id": 1100
    },
    {
      "name": "FCMI",
      "one_line_profile": "Deep Fair Clustering via Maximizing and Minimizing Mutual Information",
      "detailed_description": "FCMI is a PyTorch implementation of a Deep Fair Clustering algorithm. It utilizes mutual information maximization and minimization to achieve clustering results that are fair with respect to sensitive attributes.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "fairness",
        "clustering"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/PengxinZeng/2023-CVPR-FCMI",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "fair-clustering",
        "mutual-information",
        "fairness"
      ],
      "id": 1101
    },
    {
      "name": "SafeWorld",
      "one_line_profile": "Geo-Diverse Safety Alignment framework",
      "detailed_description": "SafeWorld is a framework and dataset for Geo-Diverse Safety Alignment. It addresses the cultural and geographical variations in safety standards for LLMs, enabling the evaluation and improvement of model alignment across different global contexts.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_alignment",
        "cultural_bias"
      ],
      "application_level": "dataset",
      "primary_language": null,
      "repo_url": "https://github.com/PlusLabNLP/SafeWorld",
      "help_website": [],
      "license": null,
      "tags": [
        "safety",
        "geo-diversity",
        "alignment"
      ],
      "id": 1102
    },
    {
      "name": "ASTRA",
      "one_line_profile": "Adversarial attack framework for AI safety competitions",
      "detailed_description": "ASTRA is an adversarial attack framework developed for AI safety competitions. It includes methods for generating effective adversarial prompts and attacks to evaluate the robustness of AI models against red-teaming efforts.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_attack",
        "red_teaming"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/PurCL/ASTRA",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "adversarial-attack",
        "ai-safety",
        "red-teaming"
      ],
      "id": 1103
    },
    {
      "name": "Decepticon",
      "one_line_profile": "Autonomous Multi-Agent Based Red Team Testing Service",
      "detailed_description": "Decepticon is an autonomous red-teaming service that utilizes a multi-agent system to test AI models. It simulates various attack vectors and interactions to uncover vulnerabilities in LLM deployments.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "red_teaming",
        "multi_agent_simulation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/PurpleAILAB/Decepticon",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "red-teaming",
        "multi-agent",
        "security-testing"
      ],
      "id": 1104
    },
    {
      "name": "Reevaluating NLP Adversarial Examples",
      "one_line_profile": "Code for reevaluating adversarial examples in Natural Language Processing",
      "detailed_description": "This repository contains the code and resources for reevaluating the effectiveness and validity of adversarial examples in NLP. It provides tools to analyze and benchmark different adversarial attack methods.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_evaluation",
        "nlp_robustness"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/QData/Reevaluating-NLP-Adversarial-Examples",
      "help_website": [],
      "license": null,
      "tags": [
        "adversarial-examples",
        "nlp",
        "evaluation"
      ],
      "id": 1105
    },
    {
      "name": "TextAttack",
      "one_line_profile": "Framework for adversarial attacks, data augmentation, and model training in NLP",
      "detailed_description": "TextAttack is a Python framework for adversarial attacks, data augmentation, and adversarial training in NLP. It allows researchers to easily construct attacks, benchmark model robustness, and improve model performance through augmentation.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_attack",
        "data_augmentation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/QData/TextAttack",
      "help_website": [
        "https://textattack.readthedocs.io/en/master/"
      ],
      "license": "MIT",
      "tags": [
        "adversarial-attacks",
        "nlp",
        "robustness"
      ],
      "id": 1106
    },
    {
      "name": "TextAttack-A2T",
      "one_line_profile": "Tools for improving adversarial training of NLP models",
      "detailed_description": "TextAttack-A2T provides implementations for improving adversarial training in NLP models. It builds upon TextAttack to offer specific methods for enhancing model robustness against textual adversarial examples.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_training",
        "robustness"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/QData/TextAttack-A2T",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "adversarial-training",
        "nlp",
        "robustness"
      ],
      "id": 1107
    },
    {
      "name": "TextAttack Search Benchmark",
      "one_line_profile": "Benchmark for search algorithms in generating NLP adversarial examples",
      "detailed_description": "This repository benchmarks various search algorithms used for generating adversarial examples in NLP. It provides a comparative analysis of search strategies within the context of adversarial attacks.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_benchmark",
        "search_algorithms"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/QData/TextAttack-Search-Benchmark",
      "help_website": [],
      "license": null,
      "tags": [
        "benchmark",
        "adversarial-search",
        "nlp"
      ],
      "id": 1108
    },
    {
      "name": "Qwen3Guard",
      "one_line_profile": "Multilingual guardrail model series for AI safety",
      "detailed_description": "Qwen3Guard is a series of multilingual guardrail models designed to ensure the safety of AI interactions. It detects and mitigates unsafe content across multiple languages, serving as a safety layer for LLM deployments.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_guardrails",
        "content_moderation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/QwenLM/Qwen3Guard",
      "help_website": [],
      "license": null,
      "tags": [
        "guardrails",
        "multilingual",
        "safety-model"
      ],
      "id": 1109
    },
    {
      "name": "HaluEval",
      "one_line_profile": "Large-scale hallucination evaluation benchmark for LLMs",
      "detailed_description": "HaluEval is a large-scale benchmark designed to evaluate hallucinations in Large Language Models. It includes a diverse set of generated and human-annotated samples to assess the factual consistency and hallucination tendencies of LLMs.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_evaluation",
        "benchmark"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/RUCAIBox/HaluEval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "hallucination",
        "benchmark",
        "llm-evaluation"
      ],
      "id": 1110
    },
    {
      "name": "RedTeamingforLLMs",
      "one_line_profile": "Framework for executing positive red-teaming experiments on LLMs",
      "detailed_description": "This framework is designed for conducting positive red-teaming experiments on Large Language Models. It provides a structure for testing model behaviors and identifying failure modes in a controlled environment.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "red_teaming",
        "safety_testing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/RedTeamingforLLMs/RedTeamingforLLMs",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "red-teaming",
        "llm",
        "experiment-framework"
      ],
      "id": 1111
    },
    {
      "name": "HalluDetect",
      "one_line_profile": "Token probability approach for detecting hallucinations in LLM generation",
      "detailed_description": "HalluDetect implements a method for detecting hallucinations in LLM generations using token probability features. It uses logistic regression and MLP classifiers trained on features extracted from the text to identify hallucinated content.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_detection",
        "uncertainty_estimation"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/Rivas-AI/HalluDetect",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "hallucination-detection",
        "token-probability",
        "llm"
      ],
      "id": 1112
    },
    {
      "name": "CipherChat",
      "one_line_profile": "Framework to evaluate generalization capability of safety alignment for LLMs",
      "detailed_description": "CipherChat is a framework designed to evaluate the generalization of safety alignment in LLMs, particularly in the context of cipher-based or encrypted conversations. It tests whether safety mechanisms hold up under non-standard input formats.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_evaluation",
        "generalization_testing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/RobustNLP/CipherChat",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "safety-alignment",
        "llm",
        "evaluation"
      ],
      "id": 1113
    },
    {
      "name": "LangBiTe",
      "one_line_profile": "Bias Tester framework for Large Language Models",
      "detailed_description": "LangBiTe is a framework for testing bias in Large Language Models. It provides a structured approach to generate test cases and evaluate models for various types of social biases.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "bias_detection",
        "fairness_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/SOM-Research/LangBiTe",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bias-testing",
        "llm",
        "fairness"
      ],
      "id": 1114
    },
    {
      "name": "PRISM",
      "one_line_profile": "Robust VLM Alignment with Principled Reasoning",
      "detailed_description": "PRISM is a framework for the robust alignment of Vision-Language Models (VLMs). It incorporates principled reasoning to improve the safety and reliability of multimodal systems.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_alignment",
        "vlm_robustness"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/SaFo-Lab/PRISM",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "vlm",
        "alignment",
        "robustness"
      ],
      "id": 1115
    },
    {
      "name": "LLM Testlab",
      "one_line_profile": "Comprehensive testing tool for Large Language Models",
      "detailed_description": "LLM Testlab is a tool designed for the comprehensive testing of Large Language Models. It likely includes features for evaluating performance, safety, and other quality metrics of LLMs.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "model_testing",
        "quality_assurance"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Saivineeth147/llm-testlab",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm-testing",
        "evaluation",
        "qa"
      ],
      "id": 1116
    },
    {
      "name": "RedesignAutonomy",
      "one_line_profile": "AI safety evaluation framework for LLM-assisted software engineering",
      "detailed_description": "RedesignAutonomy is a safety evaluation framework specifically for LLM-assisted software engineering. It assesses risks such as security flaws, overtrust, and misinterpretation in code generated by AI.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_evaluation",
        "code_generation_security"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Satyamkumarnavneet/RedesignAutonomy",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ai-safety",
        "software-engineering",
        "risk-assessment"
      ],
      "id": 1117
    },
    {
      "name": "giskardpy",
      "one_line_profile": "Core library for constraint- and optimization-based robot motion control",
      "detailed_description": "giskardpy is the core Python library of the Giskard framework, designed for robot motion control using constraint-based and optimization-based methods. It allows for the specification and execution of complex robot behaviors.",
      "domains": [
        "AI4",
        "Robotics"
      ],
      "subtask_category": [
        "motion_control",
        "trajectory_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/SemRoCo/giskardpy",
      "help_website": [],
      "license": "LGPL-3.0",
      "tags": [
        "robotics",
        "motion-control",
        "optimization"
      ],
      "id": 1118
    },
    {
      "name": "Face-Robustness-Benchmark",
      "one_line_profile": "Adversarial robustness evaluation library for face recognition",
      "detailed_description": "A comprehensive library and benchmark designed to evaluate the adversarial robustness of face recognition models against various attack methods.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "adversarial_attack"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ShawnXYang/Face-Robustness-Benchmark",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "face-recognition",
        "adversarial-robustness",
        "benchmark"
      ],
      "id": 1119
    },
    {
      "name": "Graph Robustness Benchmark (GRB)",
      "one_line_profile": "Scalable benchmark for evaluating graph machine learning robustness",
      "detailed_description": "A unified, modular, and reproducible benchmark framework for evaluating the adversarial robustness of Graph Machine Learning (GML) models against various attacks.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "graph_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/THUDM/grb",
      "help_website": [
        "https://grb.readthedocs.io"
      ],
      "license": "MIT",
      "tags": [
        "graph-neural-networks",
        "adversarial-robustness",
        "benchmark"
      ],
      "id": 1120
    },
    {
      "name": "Trust & Safety Evals",
      "one_line_profile": "Reference stack for AI model trust and safety evaluation",
      "detailed_description": "A project by The AI Alliance defining a reference stack for AI model and system evaluation, providing benchmarks and tools for assessing trust and safety.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_evaluation",
        "benchmarking"
      ],
      "application_level": "workflow",
      "primary_language": "Makefile",
      "repo_url": "https://github.com/The-AI-Alliance/trust-safety-evals",
      "help_website": [],
      "license": null,
      "tags": [
        "ai-safety",
        "evaluation",
        "benchmarks"
      ],
      "id": 1121
    },
    {
      "name": "AI Fairness 360 (AIF360)",
      "one_line_profile": "Fairness metrics and bias mitigation library",
      "detailed_description": "A comprehensive open-source toolkit containing metrics to check for unwanted bias in datasets and machine learning models, and algorithms to mitigate such bias.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "bias_mitigation",
        "fairness_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Trusted-AI/AIF360",
      "help_website": [
        "https://aif360.mybluemix.net/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "fairness",
        "bias-mitigation",
        "machine-learning"
      ],
      "id": 1122
    },
    {
      "name": "Adversarial Robustness Toolbox (ART)",
      "one_line_profile": "Python library for machine learning security and robustness",
      "detailed_description": "A Python library for machine learning security, providing tools for evasion, poisoning, extraction, and inference attacks, as well as defenses and robustness certification.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "adversarial_defense"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox",
      "help_website": [
        "https://adversarial-robustness-toolbox.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "adversarial-ml",
        "security",
        "robustness"
      ],
      "id": 1123
    },
    {
      "name": "AdvBox",
      "one_line_profile": "Adversarial example generation and robustness benchmarking toolbox",
      "detailed_description": "A toolbox to generate adversarial examples that fool neural networks across multiple frameworks (PaddlePaddle, PyTorch, etc.) and benchmark the robustness of machine learning models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_attack",
        "robustness_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/advboxes/AdvBox",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "adversarial-examples",
        "robustness",
        "paddlepaddle"
      ],
      "id": 1124
    },
    {
      "name": "PromptInject",
      "one_line_profile": "Framework for evaluating LLM robustness to adversarial prompt attacks",
      "detailed_description": "A modular framework that assembles prompts to provide a quantitative analysis of the robustness of Large Language Models (LLMs) to adversarial prompt injection attacks.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "prompt_injection",
        "robustness_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/agencyenterprise/PromptInject",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "prompt-injection",
        "security"
      ],
      "id": 1125
    },
    {
      "name": "Moonshot",
      "one_line_profile": "Modular framework for evaluating and red-teaming LLM applications",
      "detailed_description": "Moonshot is a tool designed to evaluate and red-team Large Language Model (LLM) applications. It provides a modular architecture to test for safety, security, and performance issues.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "red_teaming",
        "safety_evaluation",
        "llm_testing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aiverify-foundation/moonshot",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "red-teaming",
        "llm-evaluation",
        "ai-safety"
      ],
      "id": 1126
    },
    {
      "name": "TurboFuzzLLM",
      "one_line_profile": "Mutation-based fuzzing tool for jailbreaking Large Language Models",
      "detailed_description": "TurboFuzzLLM is a tool that enhances mutation-based fuzzing techniques to effectively jailbreak Large Language Models (LLMs), aiding in safety testing and red teaming.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "fuzzing",
        "jailbreaking",
        "red_teaming"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/amazon-science/TurboFuzzLLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "fuzzing",
        "llm-safety",
        "jailbreak"
      ],
      "id": 1127
    },
    {
      "name": "last_layer",
      "one_line_profile": "High-performance library for LLM prompt injection and jailbreak detection",
      "detailed_description": "last_layer is an ultra-fast, low-latency Python library designed to detect prompt injections and jailbreak attempts in Large Language Models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "prompt_injection_detection",
        "safety_monitoring"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/arekusandr/last_layer",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "prompt-injection",
        "jailbreak-detection",
        "security"
      ],
      "id": 1128
    },
    {
      "name": "Arthur Bench",
      "one_line_profile": "Evaluation tool for comparing and testing LLMs",
      "detailed_description": "Arthur Bench is an open-source tool for evaluating Large Language Models (LLMs) to compare performance across different models, prompts, and hyperparameters.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "llm_evaluation",
        "model_comparison"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/arthur-ai/bench",
      "help_website": [
        "https://docs.arthur.ai/bench"
      ],
      "license": "MIT",
      "tags": [
        "llm-eval",
        "benchmarking",
        "observability"
      ],
      "id": 1129
    },
    {
      "name": "Fairness.jl",
      "one_line_profile": "Julia toolkit for fairness metrics and bias mitigation",
      "detailed_description": "Fairness.jl is a Julia library providing a collection of fairness metrics and bias mitigation algorithms for machine learning models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "bias_mitigation",
        "fairness_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/ashryaagr/Fairness.jl",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fairness",
        "bias",
        "julia"
      ],
      "id": 1130
    },
    {
      "name": "MLIP Arena",
      "one_line_profile": "Benchmark platform for machine learning interatomic potentials",
      "detailed_description": "MLIP Arena is a fair and transparent benchmark framework for evaluating machine learning interatomic potentials (MLIPs), going beyond basic error metrics to assess physical properties.",
      "domains": [
        "AI4",
        "AI4-04"
      ],
      "subtask_category": [
        "model_benchmarking",
        "interatomic_potentials"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/atomind-ai/mlip-arena",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "mlip",
        "materials-science",
        "benchmarking"
      ],
      "id": 1131
    },
    {
      "name": "FaithScore",
      "one_line_profile": "Evaluation metric for hallucinations in Large Vision-Language Models",
      "detailed_description": "FaithScore is a tool for fine-grained evaluation of hallucinations in Large Vision-Language Models (LVLMs), assessing the faithfulness of generated text to the visual input.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_detection",
        "vlm_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/bcdnlp/FAITHSCORE",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "hallucination",
        "vlm",
        "evaluation-metric"
      ],
      "id": 1132
    },
    {
      "name": "nn_robust_attacks",
      "one_line_profile": "Library of robust evasion attacks against neural networks",
      "detailed_description": "A foundational library implementing robust evasion attacks (including the Carlini & Wagner attack) to evaluate the adversarial robustness of neural networks.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_attack",
        "robustness_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/carlini/nn_robust_attacks",
      "help_website": [],
      "license": "BSD-2-Clause",
      "tags": [
        "adversarial-attacks",
        "robustness",
        "neural-networks"
      ],
      "id": 1133
    },
    {
      "name": "RedEval",
      "one_line_profile": "Library for red-teaming LLM applications",
      "detailed_description": "RedEval is a Python library designed for red-teaming LLM applications using other LLMs to generate adversarial inputs and evaluate safety.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "red_teaming",
        "safety_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/chziakas/redeval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "red-teaming",
        "llm",
        "security"
      ],
      "id": 1134
    },
    {
      "name": "DeepTeam",
      "one_line_profile": "Framework for red teaming LLM systems",
      "detailed_description": "DeepTeam is a framework designed to red team LLMs and LLM systems, automating the process of finding vulnerabilities and safety issues.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "red_teaming",
        "vulnerability_scanning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/confident-ai/deepteam",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "red-teaming",
        "llm-security",
        "automation"
      ],
      "id": 1135
    },
    {
      "name": "LangFair",
      "one_line_profile": "Library for LLM bias and fairness assessment",
      "detailed_description": "LangFair is a Python library for conducting use-case level assessments of bias and fairness in Large Language Models (LLMs), providing metrics and evaluation tools.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "bias_assessment",
        "fairness_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/cvs-health/langfair",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "fairness",
        "bias",
        "llm"
      ],
      "id": 1136
    },
    {
      "name": "VerifyML",
      "one_line_profile": "Toolkit for responsible AI workflows and model verification",
      "detailed_description": "VerifyML is an open-source toolkit designed to help implement responsible AI workflows, enabling model verification, documentation, and fairness checks.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "responsible_ai",
        "model_verification",
        "documentation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/cylynx/verifyml",
      "help_website": [
        "https://verifyml.com"
      ],
      "license": "Apache-2.0",
      "tags": [
        "responsible-ai",
        "governance",
        "fairness"
      ],
      "id": 1137
    },
    {
      "name": "PHUDGE",
      "one_line_profile": "Phi-3 based scalable judge for LLM evaluation and hallucination detection",
      "detailed_description": "A framework using Phi-3 as a scalable judge to evaluate Large Language Models (LLMs). It includes tools and methods for detecting hallucinations, grading responses, and performing evaluations with or without custom rubrics and reference answers.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_detection",
        "llm_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/deshwalmahesh/PHUDGE",
      "help_website": [],
      "license": null,
      "tags": [
        "llm-judge",
        "hallucination",
        "evaluation"
      ],
      "id": 1138
    },
    {
      "name": "AIRTBench",
      "one_line_profile": "Benchmark for measuring autonomous AI red teaming capabilities",
      "detailed_description": "A code repository for AIRTBench, designed to evaluate and measure the capabilities of autonomous AI red teaming agents in language models, focusing on safety and robustness testing.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "red_teaming",
        "safety_benchmark"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/dreadnode/AIRTBench-Code",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "red-teaming",
        "benchmark",
        "llm-safety"
      ],
      "id": 1139
    },
    {
      "name": "Aequitas",
      "one_line_profile": "Open-source bias auditing and fair machine learning toolkit",
      "detailed_description": "A toolkit for auditing bias and fairness in machine learning models. It enables developers and researchers to evaluate models for various bias metrics and visualize the results to ensure equitable outcomes.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "bias_auditing",
        "fairness_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/dssg/aequitas",
      "help_website": [
        "http://aequitas.dssg.io"
      ],
      "license": "MIT",
      "tags": [
        "fairness",
        "bias-audit",
        "machine-learning"
      ],
      "id": 1140
    },
    {
      "name": "Robust-Semantic-Segmentation",
      "one_line_profile": "Dynamic divide-and-conquer adversarial training for robust segmentation",
      "detailed_description": "Implementation of the Dynamic Divide-and-Conquer Adversarial Training (DDCAT) method to improve the robustness of semantic segmentation models against adversarial attacks.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_training",
        "robust_segmentation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/dvlab-research/Robust-Semantic-Segmentation",
      "help_website": [],
      "license": null,
      "tags": [
        "adversarial-defense",
        "semantic-segmentation",
        "robustness"
      ],
      "id": 1141
    },
    {
      "name": "ChatProtect",
      "one_line_profile": "Evaluation, detection, and mitigation of self-contradictory hallucinations in LLMs",
      "detailed_description": "Code implementation for detecting and mitigating self-contradictory hallucinations in Large Language Models. It provides a framework for evaluating consistency and improving model reliability.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_detection",
        "consistency_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/eth-sri/ChatProtect",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "hallucination",
        "llm",
        "reliability"
      ],
      "id": 1142
    },
    {
      "name": "DiffAI",
      "one_line_profile": "Certifiable defense against adversarial examples via provable robustness training",
      "detailed_description": "A system for training neural networks to be provably robust against adversarial examples. It implements differentiable abstract interpretation to certify the robustness of deep learning models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "robustness_verification",
        "adversarial_defense"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/eth-sri/diffai",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "certified-robustness",
        "adversarial-defense",
        "abstract-interpretation"
      ],
      "id": 1143
    },
    {
      "name": "RigorLLM",
      "one_line_profile": "Resilient guardrails for LLMs against undesired content",
      "detailed_description": "Implementation of RigorLLM, a framework for creating resilient guardrails to prevent Large Language Models from generating undesired or harmful content, enhancing safety in deployment.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_guardrail",
        "content_moderation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/eurekayuan/RigorLLM",
      "help_website": [],
      "license": null,
      "tags": [
        "guardrails",
        "llm-safety",
        "moderation"
      ],
      "id": 1144
    },
    {
      "name": "ImageNet-Adversarial-Training",
      "one_line_profile": "State-of-the-art adversarial training for ImageNet classifiers",
      "detailed_description": "A repository providing code and pre-trained models for adversarially robust ImageNet classifiers. It serves as a benchmark and toolkit for researching adversarial robustness in large-scale computer vision.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_training",
        "robust_model"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/ImageNet-Adversarial-Training",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "adversarial-robustness",
        "imagenet",
        "computer-vision"
      ],
      "id": 1145
    },
    {
      "name": "Fairlearn",
      "one_line_profile": "Toolkit to assess and improve fairness of machine learning models",
      "detailed_description": "A Python package that empowers developers of artificial intelligence systems to assess their systems' fairness and mitigate any observed unfairness issues. It offers metrics for evaluation and algorithms for bias mitigation.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "fairness_assessment",
        "bias_mitigation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/fairlearn/fairlearn",
      "help_website": [
        "https://fairlearn.org"
      ],
      "license": "MIT",
      "tags": [
        "fairness",
        "machine-learning",
        "bias-mitigation"
      ],
      "id": 1146
    },
    {
      "name": "Jurity",
      "one_line_profile": "Fairness and evaluation library for AI systems",
      "detailed_description": "A Python library for evaluating the fairness of AI models. It provides a set of metrics and tools to detect bias and ensure compliance with fairness standards in machine learning applications.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "fairness_evaluation",
        "bias_detection"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/fidelity/jurity",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "fairness",
        "evaluation",
        "metrics"
      ],
      "id": 1147
    },
    {
      "name": "AutoAttack",
      "one_line_profile": "Ensemble of diverse parameter-free attacks for robust evaluation",
      "detailed_description": "A standard benchmark tool for reliably evaluating the adversarial robustness of machine learning models. It utilizes an ensemble of parameter-free attacks to provide a rigorous assessment of model security.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_attack",
        "robustness_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/fra31/auto-attack",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "adversarial-robustness",
        "benchmark",
        "attack-ensemble"
      ],
      "id": 1148
    },
    {
      "name": "Booster",
      "one_line_profile": "Defense against harmful fine-tuning attacks on LLMs",
      "detailed_description": "Implementation of the Booster method, designed to tackle harmful fine-tuning attacks on Large Language Models by attenuating harmful perturbations, thereby enhancing model safety.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "alignment_defense",
        "fine_tuning_safety"
      ],
      "application_level": "solver",
      "primary_language": "Shell",
      "repo_url": "https://github.com/git-disl/Booster",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-safety",
        "fine-tuning",
        "defense"
      ],
      "id": 1149
    },
    {
      "name": "Lisa",
      "one_line_profile": "Lazy safety alignment for LLMs against harmful fine-tuning",
      "detailed_description": "Code for the 'Lazy Safety Alignment' (Lisa) method, which protects Large Language Models against harmful fine-tuning attacks by employing strategic alignment techniques.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "alignment_defense",
        "fine_tuning_safety"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/git-disl/Lisa",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "safety-alignment",
        "llm",
        "defense"
      ],
      "id": 1150
    },
    {
      "name": "Safety-Tax",
      "one_line_profile": "Evaluation of trade-offs between safety alignment and reasoning in LLMs",
      "detailed_description": "A tool/codebase for analyzing the 'Safety Tax', quantifying how safety alignment processes may impact the reasoning capabilities of Large Language Models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_alignment_evaluation",
        "model_performance_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/git-disl/Safety-Tax",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "safety-alignment",
        "reasoning",
        "evaluation"
      ],
      "id": 1151
    },
    {
      "name": "Virus",
      "one_line_profile": "Harmful fine-tuning attack framework for bypassing LLM guardrails",
      "detailed_description": "Implementation of the 'Virus' attack method, used for red teaming and evaluating the vulnerability of Large Language Models to harmful fine-tuning that bypasses guardrail moderation.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_attack",
        "red_teaming"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/git-disl/Virus",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "red-teaming",
        "jailbreak",
        "fine-tuning"
      ],
      "id": 1152
    },
    {
      "name": "RobNets",
      "one_line_profile": "Neural Architecture Search for robust architectures against adversarial attacks",
      "detailed_description": "A framework combining Neural Architecture Search (NAS) with adversarial robustness to discover network architectures that are inherently more resistant to adversarial attacks.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "robust_architecture_search",
        "adversarial_defense"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/gmh14/RobNets",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nas",
        "robustness",
        "architecture-search"
      ],
      "id": 1153
    },
    {
      "name": "RETVec",
      "one_line_profile": "Efficient, multilingual, and adversarially-robust text vectorizer",
      "detailed_description": "RETVec (Resilient and Efficient Text Vectorizer) is a text processing layer designed to be robust against adversarial attacks (like typos and character perturbations) while remaining efficient and multilingual.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "robust_text_vectorization",
        "adversarial_defense"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/google-research/retvec",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "text-vectorization",
        "robustness",
        "nlp"
      ],
      "id": 1154
    },
    {
      "name": "Guardrails",
      "one_line_profile": "Framework for adding data validation and safety guardrails to LLMs",
      "detailed_description": "A Python package that lets users add structure, type checking, and quality assurance to the outputs of large language models. It enforces safety policies and validates LLM responses against defined specifications.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_guardrail",
        "output_validation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/guardrails-ai/guardrails",
      "help_website": [
        "https://www.guardrailsai.com/docs/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "guardrails",
        "validation",
        "llm-safety"
      ],
      "id": 1155
    },
    {
      "name": "Gyroscopic Diagnostics",
      "one_line_profile": "AI safety diagnostics and alignment evaluation lab",
      "detailed_description": "A toolkit for diagnosing AI safety issues and evaluating alignment. It provides methods to assess how well AI models adhere to intended safety guidelines and alignment principles.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "alignment_evaluation",
        "safety_diagnostics"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/gyrogovernance/diagnostics",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ai-safety",
        "alignment",
        "diagnostics"
      ],
      "id": 1156
    },
    {
      "name": "HolisticAI",
      "one_line_profile": "Open-source tool to assess and improve AI trustworthiness",
      "detailed_description": "A library designed to assess the trustworthiness of AI systems across multiple dimensions including bias, efficacy, robustness, and explainability. It helps in auditing and mitigating risks in AI deployments.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "trustworthiness_assessment",
        "bias_mitigation"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/holistic-ai/holisticai",
      "help_website": [
        "https://holisticai.readthedocs.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "trustworthiness",
        "audit",
        "risk-management"
      ],
      "id": 1157
    },
    {
      "name": "SaLoRA",
      "one_line_profile": "Safety-Alignment Preserved Low-Rank Adaptation for LLMs",
      "detailed_description": "Implementation of SaLoRA, a method for fine-tuning Large Language Models using Low-Rank Adaptation (LoRA) while preserving safety alignment, preventing the degradation of safety features during adaptation.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_alignment",
        "peft_safety"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/homles11/SaLoRA",
      "help_website": [],
      "license": null,
      "tags": [
        "lora",
        "safety-alignment",
        "fine-tuning"
      ],
      "id": 1158
    },
    {
      "name": "Circular Bias Detection",
      "one_line_profile": "Statistical framework for detecting circular reasoning bias in AI evaluation",
      "detailed_description": "A comprehensive statistical framework designed to detect circular reasoning bias in the evaluation of AI algorithms, ensuring that evaluation metrics do not unfairly favor certain models due to data leakage or self-reinforcing patterns.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "bias_detection",
        "evaluation_framework"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/hongping-zh/circular-bias-detection",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bias-detection",
        "circular-reasoning",
        "evaluation"
      ],
      "id": 1159
    },
    {
      "name": "CROWN-IBP",
      "one_line_profile": "Certified defense against adversarial examples using CROWN and IBP",
      "detailed_description": "A certified defense framework for neural networks that uses CROWN (bound propagation) and IBP (Interval Bound Propagation) to verify robustness against adversarial examples and train provably robust models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "robustness_verification",
        "certified_defense"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/huanzhang12/CROWN-IBP",
      "help_website": [],
      "license": "BSD-2-Clause",
      "tags": [
        "certified-robustness",
        "verification",
        "neural-networks"
      ],
      "id": 1160
    },
    {
      "name": "AISploit",
      "one_line_profile": "Package for red teaming and exploiting LLM AI solutions",
      "detailed_description": "A Python package designed to assist red teams and penetration testers in identifying vulnerabilities in Large Language Model (LLM) solutions. It provides tools for exploiting and testing the security of AI deployments.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "red_teaming",
        "adversarial_exploitation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/hupe1980/aisploit",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "red-teaming",
        "exploit",
        "llm-security"
      ],
      "id": 1161
    },
    {
      "name": "Adversarial Robustness PyTorch",
      "one_line_profile": "PyTorch implementation of adversarial training and robustness methods",
      "detailed_description": "An unofficial but widely used implementation of key DeepMind papers on adversarial training and data augmentation for improving adversarial robustness in PyTorch.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_training",
        "robustness_defense"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/imrahulr/adversarial_robustness_pytorch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "adversarial-training",
        "pytorch",
        "robustness"
      ],
      "id": 1162
    },
    {
      "name": "PatchGuard",
      "one_line_profile": "Provably robust defense against adversarial patches",
      "detailed_description": "Code for PatchGuard, a defense mechanism that provides provable robustness against adversarial patch attacks by utilizing small receptive fields and masking techniques in vision models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_defense",
        "robust_vision"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/inspire-group/PatchGuard",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "adversarial-patch",
        "defense",
        "computer-vision"
      ],
      "id": 1163
    },
    {
      "name": "HYDRA",
      "one_line_profile": "Pruning technique for creating adversarially robust neural networks",
      "detailed_description": "A PyTorch implementation of HYDRA (Pruning Adversarially Robust Neural Networks), a technique to compress neural networks while maintaining their robustness against adversarial attacks. It provides code for training, pruning, and evaluating robust models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_robustness",
        "model_compression"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/inspire-group/hydra",
      "help_website": [
        "https://arxiv.org/abs/2002.10509"
      ],
      "license": null,
      "tags": [
        "adversarial-robustness",
        "pruning",
        "neural-network-compression"
      ],
      "id": 1164
    },
    {
      "name": "IMMUNE",
      "one_line_profile": "Inference-time alignment defense against jailbreaks in Multi-modal LLMs",
      "detailed_description": "Official implementation of the CVPR 2025 paper 'IMMUNE'. It provides a defense mechanism to improve the safety of Multi-modal Large Language Models (MLLMs) against jailbreak attacks during inference time.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "jailbreak_defense",
        "safety_alignment"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/itsvaibhav01/Immune",
      "help_website": [],
      "license": null,
      "tags": [
        "jailbreak-defense",
        "mllm",
        "safety-alignment"
      ],
      "id": 1165
    },
    {
      "name": "PhD (Prompted Hallucination Dataset)",
      "one_line_profile": "Large-scale visual hallucination evaluation dataset for LVLMs",
      "detailed_description": "A ChatGPT-Prompted Visual hallucination Evaluation Dataset (PhD) for Large Vision-Language Models. It features over 100,000 data samples with extensive contextual descriptions and counterintuitive images to evaluate hallucination.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_evaluation",
        "dataset"
      ],
      "application_level": "dataset",
      "primary_language": null,
      "repo_url": "https://github.com/jiazhen-code/PhD",
      "help_website": [],
      "license": null,
      "tags": [
        "hallucination",
        "lvlm",
        "benchmark"
      ],
      "id": 1166
    },
    {
      "name": "ToLD-Br",
      "one_line_profile": "Toxic language detection dataset for Brazilian Portuguese",
      "detailed_description": "A dataset and analysis code for toxic language detection in Brazilian Portuguese social media posts. It supports multilingual analysis and benchmarking of toxicity detection models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "toxicity_detection",
        "dataset"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/joaoaleite/ToLD-Br",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "toxicity-detection",
        "nlp",
        "portuguese"
      ],
      "id": 1167
    },
    {
      "name": "AMBER",
      "one_line_profile": "Multi-dimensional benchmark for multi-modal hallucination evaluation",
      "detailed_description": "An LLM-free, multi-dimensional benchmark designed to evaluate hallucinations in Multi-modal Large Language Models (MLLMs). It covers various types of hallucinations and provides a standardized evaluation framework.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_evaluation",
        "benchmark"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/junyangwang0410/AMBER",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "hallucination",
        "mllm",
        "benchmark"
      ],
      "id": 1168
    },
    {
      "name": "ToxVidLM",
      "one_line_profile": "Dataset for toxicity detection in code-mixed Hinglish video content",
      "detailed_description": "Code and datasets from the ACL 2024 paper focusing on toxicity detection in code-mixed Hinglish (Hindi-English) video content, addressing multimodal toxicity challenges.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "toxicity_detection",
        "dataset"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/justaguyalways/ToxVidLM_ACL_2024",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "toxicity",
        "multimodal",
        "hinglish"
      ],
      "id": 1169
    },
    {
      "name": "MobileSafetyBench",
      "one_line_profile": "Benchmark for evaluating safety of autonomous agents in mobile device control",
      "detailed_description": "A benchmark for evaluating the safety of autonomous agents when controlling mobile devices, presented at AAAI 2026 AI Alignment Track. It assesses risks associated with agentic actions on mobile platforms.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_evaluation",
        "agent_safety"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/jylee425/mobilesafetybench",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "agent-safety",
        "benchmark",
        "mobile-agents"
      ],
      "id": 1170
    },
    {
      "name": "AVHBench",
      "one_line_profile": "Cross-modal hallucination evaluation for Audio-Visual LLMs",
      "detailed_description": "Official repository for the ICLR 2025 paper 'AVHBench'. It is a benchmark designed to evaluate hallucinations in Audio-Visual Large Language Models, focusing on cross-modal inconsistencies.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_evaluation",
        "benchmark"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/kaist-ami/AVHBench",
      "help_website": [],
      "license": null,
      "tags": [
        "audio-visual",
        "hallucination",
        "benchmark"
      ],
      "id": 1171
    },
    {
      "name": "BEAF",
      "one_line_profile": "Evaluation method for hallucination in Vision-Language Models",
      "detailed_description": "Official repository for the ECCV 2024 paper 'BEAF'. It proposes a method to evaluate hallucinations in Vision-Language Models by observing Before-After changes in visual inputs.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_evaluation",
        "methodology"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/kaist-ami/BEAF",
      "help_website": [],
      "license": null,
      "tags": [
        "hallucination",
        "vlm",
        "evaluation"
      ],
      "id": 1172
    },
    {
      "name": "Kereva Scanner",
      "one_line_profile": "Code scanner for detecting issues in prompts and LLM calls",
      "detailed_description": "A static analysis tool and scanner designed to check for security issues, PII leakage, and safety concerns in LLM prompts and API calls within codebases.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_scanning",
        "prompt_security"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/kereva-dev/kereva-scanner",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "security-scanner",
        "llm-safety",
        "pii-detection"
      ],
      "id": 1173
    },
    {
      "name": "fairness",
      "one_line_profile": "R package for computing and visualizing fair ML metrics",
      "detailed_description": "An R package that provides functions to compute and visualize various fairness metrics for machine learning models, helping researchers evaluate algorithmic bias.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "fairness_evaluation",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/kozodoi/fairness",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "fairness",
        "r-package",
        "bias-evaluation"
      ],
      "id": 1174
    },
    {
      "name": "convex_adversarial",
      "one_line_profile": "Method for training provably robust neural networks",
      "detailed_description": "A Python library implementing methods for training neural networks that are provably robust to adversarial attacks, using convex relaxation techniques.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_robustness",
        "robust_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/locuslab/convex_adversarial",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "adversarial-robustness",
        "verification",
        "neural-networks"
      ],
      "id": 1175
    },
    {
      "name": "smoothing",
      "one_line_profile": "Randomized smoothing for provable adversarial robustness",
      "detailed_description": "Code for 'Provable adversarial robustness at ImageNet scale', implementing randomized smoothing techniques to certify the robustness of deep learning models against adversarial perturbations.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_robustness",
        "certified_robustness"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/locuslab/smoothing",
      "help_website": [],
      "license": null,
      "tags": [
        "randomized-smoothing",
        "robustness",
        "certification"
      ],
      "id": 1176
    },
    {
      "name": "Square Attack",
      "one_line_profile": "Query-efficient black-box adversarial attack",
      "detailed_description": "Implementation of Square Attack, a score-based black-box adversarial attack that does not require gradient information, suitable for evaluating model robustness.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_attack",
        "robustness_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/max-andr/square-attack",
      "help_website": [
        "https://arxiv.org/abs/1912.00049"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "adversarial-attack",
        "black-box",
        "robustness"
      ],
      "id": 1177
    },
    {
      "name": "FairBench",
      "one_line_profile": "Comprehensive benchmark framework for exploring and evaluating AI fairness",
      "detailed_description": "FairBench is a comprehensive framework designed for the exploration and evaluation of fairness in AI models. It provides a suite of tools and metrics to assess bias and ensure equitable outcomes across different demographic groups in machine learning applications.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "fairness_evaluation",
        "bias_detection"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/mever-team/FairBench",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "fairness",
        "bias",
        "benchmark",
        "ai-ethics"
      ],
      "id": 1178
    },
    {
      "name": "AI Agent Evals",
      "one_line_profile": "Evaluation framework for AI agents using model-as-judge and safety metrics",
      "detailed_description": "A tool designed to evaluate AI agent applications, focusing on performance, content safety, and mathematical metrics. It utilizes a 'model as the judge' approach to assess the quality and safety of agent outputs, suitable for integration into development workflows.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "agent_evaluation",
        "safety_metrics",
        "model_as_judge"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/ai-agent-evals",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ai-agents",
        "evaluation",
        "safety",
        "metrics"
      ],
      "id": 1179
    },
    {
      "name": "R-Bench",
      "one_line_profile": "Benchmark for evaluating relationship hallucinations in Large Vision-Language Models",
      "detailed_description": "R-Bench is a benchmark specifically designed to evaluate and analyze relationship hallucinations in Large Vision-Language Models (LVLMs). It provides a dataset and evaluation scripts to assess how well models perceive and describe relationships between objects in images.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_detection",
        "vision_language_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/mrwu-mac/R-Bench",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "hallucination",
        "lvlm",
        "benchmark",
        "computer-vision"
      ],
      "id": 1180
    },
    {
      "name": "Agentic Security",
      "one_line_profile": "Vulnerability scanner and red teaming toolkit for Agentic LLMs",
      "detailed_description": "Agentic Security is a comprehensive vulnerability scanner and red teaming toolkit designed for Agentic Large Language Models. It helps identify security flaws, safety issues, and potential exploits in AI agents through automated testing and evaluation.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "red_teaming",
        "vulnerability_scanning",
        "safety_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/msoedov/agentic_security",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "red-teaming",
        "llm-security",
        "vulnerability-scanner",
        "ai-safety"
      ],
      "id": 1181
    },
    {
      "name": "OpenGuardrails",
      "one_line_profile": "Open-source customizable AI guardrails for inference pipeline security",
      "detailed_description": "OpenGuardrails is an open-source framework for implementing customizable guardrails in AI applications. It protects the AI inference pipeline by scanning prompts, models, agents, and outputs, allowing users to define custom scanners and security policies.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_guardrails",
        "inference_security",
        "input_output_scanning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/openguardrails/openguardrails",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "guardrails",
        "ai-security",
        "inference-protection"
      ],
      "id": 1182
    },
    {
      "name": "HAI Guardrails",
      "one_line_profile": "TypeScript library providing safety guards for LLM applications",
      "detailed_description": "HAI Guardrails is a TypeScript library that provides a set of guardrails for Large Language Model (LLM) applications. It helps developers implement safety checks and content filtering mechanisms to ensure responsible AI usage.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_guardrails",
        "content_filtering"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/presidio-oss/hai-guardrails",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "guardrails",
        "llm",
        "typescript",
        "safety"
      ],
      "id": 1183
    },
    {
      "name": "NoMIRACL",
      "one_line_profile": "Multilingual hallucination evaluation dataset for RAG robustness",
      "detailed_description": "NoMIRACL is a dataset designed to evaluate the robustness of Large Language Models (LLMs) in Retrieval-Augmented Generation (RAG) settings. It focuses on hallucination detection across 18 languages, specifically targeting scenarios with first-stage retrieval errors.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_evaluation",
        "rag_robustness",
        "multilingual_benchmark"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/project-miracl/nomiracl",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "hallucination",
        "rag",
        "multilingual",
        "dataset"
      ],
      "id": 1184
    },
    {
      "name": "Promptfoo",
      "one_line_profile": "CLI tool for testing, red teaming, and evaluating LLM prompts and agents",
      "detailed_description": "Promptfoo is a command-line tool and library for evaluating Large Language Models (LLMs). It supports testing prompts, agents, and RAG pipelines, offering features for AI red teaming, pentesting, and vulnerability scanning with declarative configuration.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "red_teaming",
        "prompt_evaluation",
        "vulnerability_scanning"
      ],
      "application_level": "solver",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/promptfoo/promptfoo",
      "help_website": [
        "https://www.promptfoo.dev/"
      ],
      "license": "MIT",
      "tags": [
        "red-teaming",
        "llm-testing",
        "prompt-engineering",
        "evaluation"
      ],
      "id": 1185
    },
    {
      "name": "Hallucination Index",
      "one_line_profile": "Ranking and evaluation initiative for LLM hallucination propensity",
      "detailed_description": "The Hallucination Index is an initiative to evaluate and rank popular Large Language Models (LLMs) based on their propensity to hallucinate across various task types. It provides benchmarks and data to help users choose reliable models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_ranking",
        "model_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/rungalileo/hallucination-index",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "hallucination",
        "leaderboard",
        "llm-ranking"
      ],
      "id": 1186
    },
    {
      "name": "OpenAgentSafety",
      "one_line_profile": "Framework for evaluating AI agent safety in realistic environments",
      "detailed_description": "OpenAgentSafety is a framework designed to evaluate the safety of AI agents within realistic environments. It provides methodologies and tools to assess how agents behave in complex scenarios, ensuring they operate safely and reliably.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "agent_safety",
        "environment_simulation",
        "behavioral_evaluation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/sani903/OpenAgentSafety",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ai-agents",
        "safety",
        "evaluation",
        "simulation"
      ],
      "id": 1187
    },
    {
      "name": "frAI",
      "one_line_profile": "Open-source toolkit for responsible AI with scanning and reporting capabilities",
      "detailed_description": "frAI is an open-source toolkit for responsible AI that includes a CLI and SDK. It allows users to scan code, collect evidence, and generate model cards, risk files, and evaluations, facilitating compliance and transparency in AI development.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "responsible_ai",
        "model_card_generation",
        "risk_assessment"
      ],
      "application_level": "toolkit",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/sebuzdugan/frai",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "responsible-ai",
        "compliance",
        "model-cards",
        "risk-management"
      ],
      "id": 1188
    },
    {
      "name": "STATE-ToxiCN",
      "one_line_profile": "Benchmark for span-level target-aware toxicity extraction in Chinese hate speech",
      "detailed_description": "A benchmark dataset and evaluation framework designed for span-level target-aware toxicity extraction in Chinese hate speech detection, supporting research in safety and bias evaluation.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "toxicity_detection",
        "benchmark_dataset"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/shenmeyemeifashengguo/STATE-ToxiCN",
      "help_website": [],
      "license": null,
      "tags": [
        "toxicity-detection",
        "chinese-nlp",
        "benchmark"
      ],
      "id": 1189
    },
    {
      "name": "LLM-Detector-Robustness",
      "one_line_profile": "Red teaming framework for language model detectors",
      "detailed_description": "Code implementation for red teaming language model detectors using other language models, focusing on evaluating the robustness of AI-generated text detectors.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "red_teaming",
        "robustness_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/shizhouxing/LLM-Detector-Robustness",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "red-teaming",
        "llm-detection",
        "robustness"
      ],
      "id": 1190
    },
    {
      "name": "Red-Teaming-Language-Models",
      "one_line_profile": "Implementation of red teaming language models with language models",
      "detailed_description": "A re-implementation of the 'Red Teaming Language Models with Language Models' paper, providing tools to generate adversarial test cases for evaluating LLM safety.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "red_teaming",
        "safety_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/shreyansh26/Red-Teaming-Language-Models-with-Language-Models",
      "help_website": [],
      "license": null,
      "tags": [
        "red-teaming",
        "llm",
        "adversarial-generation"
      ],
      "id": 1191
    },
    {
      "name": "SIUO",
      "one_line_profile": "Cross-modality safety alignment framework",
      "detailed_description": "A framework for cross-modality safety alignment in multimodal models, providing methods to evaluate and improve safety across different data modalities.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_alignment",
        "multimodal_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "HTML",
      "repo_url": "https://github.com/sinwang20/SIUO",
      "help_website": [],
      "license": null,
      "tags": [
        "safety-alignment",
        "multimodal",
        "llm"
      ],
      "id": 1192
    },
    {
      "name": "Kov.jl",
      "one_line_profile": "Black-box red teaming of LLMs using MDPs",
      "detailed_description": "A Julia package for black-box red teaming and jailbreaking of large language models using Markov Decision Processes (MDPs) to discover failure modes.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "red_teaming",
        "jailbreaking"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/sisl/Kov.jl",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "red-teaming",
        "julia",
        "mdp",
        "jailbreak"
      ],
      "id": 1193
    },
    {
      "name": "toxic-comments-detection-in-russian",
      "one_line_profile": "Toxic comments detection model for Russian language",
      "detailed_description": "A tool and model for detecting toxic comments specifically in the Russian language, useful for content moderation and safety evaluation.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "toxicity_detection"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/sismetanin/toxic-comments-detection-in-russian",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "toxicity-detection",
        "russian-nlp",
        "bert"
      ],
      "id": 1194
    },
    {
      "name": "FairClassifier",
      "one_line_profile": "Neural Network fairness evaluation package",
      "detailed_description": "An open source package that evaluates the fairness of a Neural Network using the p% rule fairness metric.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "fairness_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/suraz09/FairClassifier",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "fairness",
        "neural-networks",
        "metrics"
      ],
      "id": 1195
    },
    {
      "name": "Re-Align",
      "one_line_profile": "Alignment framework to mitigate hallucinations in VLMs",
      "detailed_description": "A novel alignment framework that leverages image retrieval to mitigate hallucinations in Vision Language Models (VLMs).",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_mitigation",
        "alignment"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/taco-group/Re-Align",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "vlm",
        "hallucination",
        "alignment"
      ],
      "id": 1196
    },
    {
      "name": "rGAN",
      "one_line_profile": "Label-Noise Robust Generative Adversarial Networks",
      "detailed_description": "Implementation of rGAN, a Generative Adversarial Network designed to be robust against label noise.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "robust_modeling",
        "generative_models"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/takuhirok/rGAN",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gan",
        "robustness",
        "label-noise"
      ],
      "id": 1197
    },
    {
      "name": "ToxiBenchCN",
      "one_line_profile": "Benchmark for multimodal toxic Chinese detection",
      "detailed_description": "A benchmark and taxonomy for exploring multimodal challenges in toxic Chinese detection, providing data and evaluation standards.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "toxicity_detection",
        "multimodal_benchmark"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/thomasyyyoung/ToxiBenchCN",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "toxicity",
        "multimodal",
        "benchmark",
        "chinese"
      ],
      "id": 1198
    },
    {
      "name": "AISafetyLab",
      "one_line_profile": "Comprehensive framework for AI safety attack, defense, and evaluation",
      "detailed_description": "A comprehensive framework covering safety attack, defense, and evaluation for AI systems, facilitating research in AI safety.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_evaluation",
        "adversarial_attack",
        "defense"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/thu-coai/AISafetyLab",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ai-safety",
        "evaluation",
        "attack-defense"
      ],
      "id": 1199
    },
    {
      "name": "cotk",
      "one_line_profile": "Toolkit for fast development and fair evaluation of text generation",
      "detailed_description": "Conversational Toolkit (cotk) is an open-source toolkit designed for fast development and fair evaluation of text generation models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "text_generation_evaluation",
        "fairness"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/thu-coai/cotk",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "text-generation",
        "evaluation",
        "nlp"
      ],
      "id": 1200
    },
    {
      "name": "MMTrustEval",
      "one_line_profile": "Toolbox for benchmarking trustworthiness of multimodal LLMs",
      "detailed_description": "A toolbox for benchmarking the trustworthiness of multimodal large language models, covering various safety and reliability dimensions.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "trustworthiness_evaluation",
        "multimodal_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/thu-ml/MMTrustEval",
      "help_website": [],
      "license": "CC-BY-SA-4.0",
      "tags": [
        "multimodal",
        "trustworthiness",
        "benchmark"
      ],
      "id": 1201
    },
    {
      "name": "STAIR",
      "one_line_profile": "Safety alignment with introspective reasoning",
      "detailed_description": "Codebase for improving safety alignment in language models using introspective reasoning techniques.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_alignment",
        "reasoning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/thu-ml/STAIR",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "safety-alignment",
        "llm",
        "reasoning"
      ],
      "id": 1202
    },
    {
      "name": "ares",
      "one_line_profile": "Library for benchmarking adversarial robustness",
      "detailed_description": "A Python library for adversarial machine learning focusing on benchmarking adversarial robustness of models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_robustness",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/thu-ml/ares",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "adversarial-ml",
        "robustness",
        "benchmark"
      ],
      "id": 1203
    },
    {
      "name": "understanding-fast-adv-training",
      "one_line_profile": "Implementation of Fast Adversarial Training",
      "detailed_description": "Code for understanding and improving fast adversarial training methods to enhance model robustness.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_training",
        "robustness"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/tml-epfl/understanding-fast-adv-training",
      "help_website": [],
      "license": null,
      "tags": [
        "adversarial-training",
        "robustness"
      ],
      "id": 1204
    },
    {
      "name": "redteam-ai-benchmark",
      "one_line_profile": "Benchmark for evaluating uncensored LLMs for offensive security",
      "detailed_description": "A benchmark suite designed to evaluate uncensored Large Language Models in the context of offensive security and red teaming.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "red_teaming",
        "security_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/toxy4ny/redteam-ai-benchmark",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "red-teaming",
        "llm",
        "security"
      ],
      "id": 1205
    },
    {
      "name": "trulens",
      "one_line_profile": "Evaluation and tracking for LLM experiments",
      "detailed_description": "A library for evaluating and tracking Large Language Model (LLM) experiments, providing feedback functions to assess relevance, groundedness, and other metrics.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "llm_evaluation",
        "experiment_tracking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/truera/trulens",
      "help_website": [
        "https://www.trulens.org/"
      ],
      "license": "MIT",
      "tags": [
        "llm-evaluation",
        "observability",
        "metrics"
      ],
      "id": 1206
    },
    {
      "name": "image-crop-analysis",
      "one_line_profile": "Analysis tools for image cropping fairness",
      "detailed_description": "Code and tools for analyzing fairness metrics in image cropping algorithms, specifically addressing representation and bias.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "fairness_analysis",
        "bias_detection"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/twitter-research/image-crop-analysis",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "fairness",
        "image-processing",
        "bias"
      ],
      "id": 1207
    },
    {
      "name": "armory",
      "one_line_profile": "Adversarial Robustness Evaluation Test Bed",
      "detailed_description": "A test bed for evaluating the adversarial robustness of machine learning models, providing a standardized environment for testing defenses.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "adversarial_defense"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/twosixlabs/armory",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "adversarial-robustness",
        "evaluation",
        "testbed"
      ],
      "id": 1208
    },
    {
      "name": "Metric-Fairness",
      "one_line_profile": "Evaluation of social bias in text generation metrics",
      "detailed_description": "Tools and code for analyzing social bias in language model-based metrics (like BERTScore) for text generation.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "metric_evaluation",
        "bias_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/txsun1997/Metric-Fairness",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fairness",
        "metrics",
        "bias"
      ],
      "id": 1209
    },
    {
      "name": "Quantus",
      "one_line_profile": "Explainable AI toolkit for evaluating neural network explanations",
      "detailed_description": "Quantus is an eXplainable AI (XAI) toolkit designed for the responsible evaluation of neural network explanations using various metrics.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "xai_evaluation",
        "interpretability"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/understandable-machine-intelligence-lab/Quantus",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "xai",
        "evaluation",
        "interpretability"
      ],
      "id": 1210
    },
    {
      "name": "Oversight",
      "one_line_profile": "Modular LLM Red Teaming and Vulnerability Research Framework",
      "detailed_description": "A modular framework for reverse engineering, red teaming, and vulnerability research on Large Language Models.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "red_teaming",
        "vulnerability_research"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/user1342/Oversight",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "red-teaming",
        "llm",
        "security"
      ],
      "id": 1211
    },
    {
      "name": "configurable-safety-tuning",
      "one_line_profile": "Safety tuning of LMs with synthetic preference data",
      "detailed_description": "Code and data for configurable safety tuning of language models using synthetic preference data to align models with safety guidelines.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_tuning",
        "alignment"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/vicgalle/configurable-safety-tuning",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "safety-tuning",
        "alignment",
        "synthetic-data"
      ],
      "id": 1212
    },
    {
      "name": "Trojan-Activation-Attack",
      "one_line_profile": "Trojan attack on LLMs using activation steering",
      "detailed_description": "Implementation of Trojan Activation Attack, a method to attack Large Language Models using activation steering to bypass safety alignment.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_attack",
        "safety_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/wang2226/Trojan-Activation-Attack",
      "help_website": [],
      "license": null,
      "tags": [
        "trojan-attack",
        "llm",
        "activation-steering"
      ],
      "id": 1213
    },
    {
      "name": "CHALE",
      "one_line_profile": "Controlled Hallucination-Evaluation Dataset",
      "detailed_description": "A dataset designed for the controlled evaluation of hallucinations in Question-Answering systems.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_evaluation",
        "dataset"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/weijiaheng/CHALE",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "hallucination",
        "dataset",
        "qa"
      ],
      "id": 1214
    },
    {
      "name": "circle-guard-bench",
      "one_line_profile": "Benchmark for evaluating LLM guard systems",
      "detailed_description": "A benchmark for evaluating the protection capabilities of large language model (LLM) guard systems, including guardrails and safeguards.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "safety_evaluation",
        "guardrails_benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/whitecircle-ai/circle-guard-bench",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "guardrails",
        "benchmark",
        "llm-security"
      ],
      "id": 1215
    },
    {
      "name": "drug_det_ro",
      "one_line_profile": "Toxic and narcotic medication detection with rotated object detector",
      "detailed_description": "Source code for detecting toxic and narcotic medications using rotated object detectors, applicable in medical safety and monitoring.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "object_detection",
        "safety_monitoring"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/woodywff/drug_det_ro",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "object-detection",
        "medical-safety",
        "rotated-bbox"
      ],
      "id": 1216
    },
    {
      "name": "Tri-HE",
      "one_line_profile": "Unified triplet-level hallucination evaluation for LVLMs",
      "detailed_description": "Code and data for evaluating hallucinations in Large Vision-Language Models (LVLMs) at the triplet level.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "hallucination_evaluation",
        "vlm"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/wujunjie1998/Tri-HE",
      "help_website": [],
      "license": null,
      "tags": [
        "hallucination",
        "vlm",
        "evaluation"
      ],
      "id": 1217
    },
    {
      "name": "Adversarial_Long-Tail",
      "one_line_profile": "Adversarial robustness under long-tailed distribution",
      "detailed_description": "PyTorch implementation for improving adversarial robustness in scenarios with long-tailed data distributions.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "adversarial_robustness",
        "long_tail_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/wutong16/Adversarial_Long-Tail",
      "help_website": [],
      "license": null,
      "tags": [
        "robustness",
        "long-tail",
        "adversarial-training"
      ],
      "id": 1218
    },
    {
      "name": "veiled-toxicity-detection",
      "one_line_profile": "Detection of veiled toxicity in speech",
      "detailed_description": "Tools and methods for fortifying toxic speech detectors against veiled or subtle toxicity.",
      "domains": [
        "AI4",
        "AI4-05"
      ],
      "subtask_category": [
        "toxicity_detection",
        "robustness"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/xhan77/veiled-toxicity-detection",
      "help_website": [],
      "license": null,
      "tags": [
        "toxicity",
        "nlp",
        "safety"
      ],
      "id": 1219
    }
  ]
}
{
  "leaf_cluster_name": "科研数据格式/解析/转换生态",
  "domain": "Data/Workflow",
  "typical_objects": "domain formats",
  "task_chain": "解析→转换→校验→ETL→版本",
  "tool_form": "解析器 + 验证器 + ETL",
  "total_tools": 944,
  "tools": [
    {
      "name": "telemetry-parser",
      "one_line_profile": "Parser for real-time sensor telemetry metadata from video files and flight logs",
      "detailed_description": "A tool to extract and parse real-time physical measurements (gyroscope, accelerometer, GPS) embedded in video files (GoPro GPMF, Sony, Insta360) and flight logs (Betaflight). This data is essential for drone dynamics analysis, SLAM research, and biomechanics.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_extraction",
        "sensor_processing"
      ],
      "application_level": "solver",
      "primary_language": "Rust",
      "repo_url": "https://github.com/AdrianEddy/telemetry-parser",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "telemetry",
        "gpmf",
        "sensor-data",
        "drone",
        "blackbox"
      ],
      "id": 1
    },
    {
      "name": "xsv",
      "one_line_profile": "A fast CSV command line toolkit for data slicing, indexing, and statistical analysis",
      "detailed_description": "A high-performance command-line toolkit for processing CSV data. It provides commands for indexing, slicing, partitioning, and computing summary statistics (mean, median, frequency) on large tabular datasets without loading them entirely into memory. Widely used in bioinformatics and data science pipelines for preprocessing.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_processing",
        "statistics",
        "filtering"
      ],
      "application_level": "solver",
      "primary_language": "Rust",
      "repo_url": "https://github.com/BurntSushi/xsv",
      "help_website": [],
      "license": "Unlicense",
      "tags": [
        "csv",
        "cli",
        "data-processing",
        "statistics"
      ],
      "id": 2
    },
    {
      "name": "PySysML2",
      "one_line_profile": "Parser for SysML 2.0 textual modeling language for data analysis",
      "detailed_description": "A Python-based parser for the SysML 2.0 textual modeling language. It parses SysML 2.0 models into Python objects to enable data science and analysis on system engineering models, facilitating Model-Based Systems Engineering (MBSE) workflows.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "modeling",
        "systems_engineering"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/DAF-Digital-Transformation-Office/PySysML2",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "sysml",
        "mbse",
        "parsing",
        "systems-engineering"
      ],
      "id": 3
    },
    {
      "name": "configr",
      "one_line_profile": "Configuration file parser (JSON/INI/YAML/TOML) for R language workflows",
      "detailed_description": "An R package that implements parsers for multiple configuration formats (JSON, INI, YAML, TOML) to facilitate the setting and writing of configuration files in R-based scientific workflows and bioinformatics pipelines.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_parsing",
        "workflow_configuration"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/Miachol/configr",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "r-package",
        "configuration",
        "parser",
        "json",
        "yaml"
      ],
      "id": 4
    },
    {
      "name": "dataframely",
      "one_line_profile": "Declarative data frame validation library for Polars and Pandas",
      "detailed_description": "A Python library for validating data frames (Polars and Pandas) using a declarative schema approach. It is used in data science and scientific data processing pipelines to ensure data quality and structural integrity.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_validation",
        "quality_control"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Quantco/dataframely",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "validation",
        "dataframe",
        "polars",
        "pandas",
        "data-quality"
      ],
      "id": 5
    },
    {
      "name": "missingno",
      "one_line_profile": "Missing data visualization module for Python",
      "detailed_description": "A flexible and easy-to-use Python library for visualizing missing data in pandas dataframes. It provides a small toolset of flexible and easy-to-use missing data visualizations and utilities that allow data scientists to get a quick visual summary of the completeness (or lack thereof) of their dataset.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_visualization",
        "quality_control"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ResidentMario/missingno",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "visualization",
        "missing-data",
        "pandas",
        "data-cleaning"
      ],
      "id": 6
    },
    {
      "name": "pyreadstat",
      "one_line_profile": "Reader and writer for SAS, SPSS, and Stata files in Python",
      "detailed_description": "A Python package to read and write SAS (sas7bdat, xport), SPSS (sav, zsav, por), and Stata (dta) data files into/from pandas and polars data frames. It serves as a critical bridge for processing statistical data formats common in social sciences and clinical research within the Python scientific ecosystem.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "format_conversion",
        "data_parsing"
      ],
      "application_level": "library",
      "primary_language": "C",
      "repo_url": "https://github.com/Roche/pyreadstat",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "spss",
        "sas",
        "stata",
        "pandas",
        "polars",
        "converter"
      ],
      "id": 7
    },
    {
      "name": "SPARQL Anything",
      "one_line_profile": "System for querying any data format (JSON, CSV, XML, etc.) with SPARQL",
      "detailed_description": "A system for Semantic Web re-engineering that allows users to query heterogeneous file formats (JSON, CSV, XML, HTML, Markdown, etc.) using SPARQL. It is widely used in scientific data integration and knowledge graph construction to bridge non-RDF data with semantic workflows.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_integration",
        "format_conversion",
        "semantic_query"
      ],
      "application_level": "solver",
      "primary_language": "Java",
      "repo_url": "https://github.com/SPARQL-Anything/sparql.anything",
      "help_website": [
        "http://sparql-anything.cc"
      ],
      "license": "Apache-2.0",
      "tags": [
        "sparql",
        "semantic-web",
        "knowledge-graph",
        "data-integration",
        "rdf"
      ],
      "id": 8
    },
    {
      "name": "StackExchange XML Converter",
      "one_line_profile": "Converter for StackExchange data dumps from XML to CSV format",
      "detailed_description": "A utility tool designed to parse and convert StackExchange data dumps (provided in XML format) into CSV format, facilitating data analysis in social computing and network science research.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_conversion",
        "parsing"
      ],
      "application_level": "solver",
      "primary_language": "Go",
      "repo_url": "https://github.com/SkobelevIgor/stackexchange-xml-converter",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "xml",
        "csv",
        "converter",
        "stackexchange",
        "data-processing"
      ],
      "id": 9
    },
    {
      "name": "jsonschema (Rust)",
      "one_line_profile": "High-performance JSON Schema validator for Rust",
      "detailed_description": "A fast and compliant JSON Schema validator written in Rust. It is essential for validating metadata and data structures in scientific workflows that rely on JSON-based standards.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "validation",
        "parsing"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/Stranger6667/jsonschema",
      "help_website": [
        "https://docs.rs/jsonschema"
      ],
      "license": "MIT",
      "tags": [
        "json-schema",
        "validation",
        "rust",
        "data-integrity"
      ],
      "id": 10
    },
    {
      "name": "Preswald",
      "one_line_profile": "WASM-based packager for interactive scientific data applications",
      "detailed_description": "A tool to bundle Python-based data analysis and visualization workflows (using Pandas, DuckDB, Plotly) into single-file, browser-runnable applications via Pyodide, facilitating the sharing of scientific reports and dashboards.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "visualization",
        "workflow_automation",
        "reporting"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/StructuredLabs/preswald",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "wasm",
        "visualization",
        "dashboard",
        "python",
        "data-science"
      ],
      "id": 11
    },
    {
      "name": "Tablecruncher",
      "one_line_profile": "Lightweight CSV editor and processor",
      "detailed_description": "A desktop tool for opening, editing, and processing large CSV files, supporting JavaScript macros for data cleaning and manipulation tasks common in data science.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_cleaning",
        "data_editing"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/Tablecruncher/tablecruncher",
      "help_website": [
        "https://tablecruncher.com"
      ],
      "license": "GPL-3.0",
      "tags": [
        "csv",
        "editor",
        "data-cleaning",
        "macros"
      ],
      "id": 12
    },
    {
      "name": "RapidJSON",
      "one_line_profile": "Fast JSON parser and generator for C++",
      "detailed_description": "A high-performance C++ library for parsing and generating JSON. It is widely used in scientific computing applications for handling configuration files and data serialization due to its speed and DOM/SAX API support.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "serialization"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/Tencent/rapidjson",
      "help_website": [
        "http://rapidjson.org/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "json",
        "parser",
        "cpp",
        "serialization",
        "high-performance"
      ],
      "id": 13
    },
    {
      "name": "NightConfig",
      "one_line_profile": "Configuration library for TOML, YAML, JSON, and HOCON",
      "detailed_description": "A Java library for reading and writing various configuration formats (TOML, YAML, JSON). It supports scientific software development by providing robust parsing for experiment configurations.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "configuration_management"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/TheElectronWill/night-config",
      "help_website": [],
      "license": "LGPL-3.0",
      "tags": [
        "toml",
        "yaml",
        "json",
        "configuration",
        "java"
      ],
      "id": 14
    },
    {
      "name": "TinyCsvParser",
      "one_line_profile": "High-performance CSV parsing library for .NET",
      "detailed_description": "A library designed for easy and fast parsing of CSV data in .NET applications, suitable for ingesting large scientific datasets stored in CSV format.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "data_ingestion"
      ],
      "application_level": "library",
      "primary_language": "C#",
      "repo_url": "https://github.com/TinyCsvParser/TinyCsvParser",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "csv",
        "parser",
        "dotnet",
        "csharp",
        "data-ingestion"
      ],
      "id": 15
    },
    {
      "name": "dasel",
      "one_line_profile": "Command-line tool for querying and converting data formats (JSON, YAML, TOML, XML, CSV)",
      "detailed_description": "A versatile command-line tool that allows selecting, updating, and deleting data from various structured formats (JSON, TOML, YAML, XML, CSV). It is useful in scientific workflows for data extraction and format conversion.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_conversion",
        "querying",
        "parsing"
      ],
      "application_level": "solver",
      "primary_language": "Go",
      "repo_url": "https://github.com/TomWright/dasel",
      "help_website": [
        "https://daseldocs.tomwright.me/"
      ],
      "license": "MIT",
      "tags": [
        "json",
        "yaml",
        "toml",
        "xml",
        "csv",
        "cli"
      ],
      "id": 16
    },
    {
      "name": "fastexcel",
      "one_line_profile": "Fast Excel reader for Rust and Python",
      "detailed_description": "A high-performance library for reading Excel files (XLSX), enabling efficient ingestion of spreadsheet data into Rust or Python-based scientific analysis pipelines.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "data_ingestion"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/ToucanToco/fastexcel",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "excel",
        "xlsx",
        "parser",
        "rust",
        "python"
      ],
      "id": 17
    },
    {
      "name": "ccorp_yaml_include",
      "one_line_profile": "YAML parser plugin for file inclusion",
      "detailed_description": "A plugin for the Ruamel.YAML parser that adds support for the `!include` tag, allowing modular composition of YAML files, which is useful for managing complex scientific configurations.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "configuration_management"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Tristan-Sweeney-CambridgeConsultants/ccorp_yaml_include",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "yaml",
        "parser-extension",
        "python",
        "configuration"
      ],
      "id": 18
    },
    {
      "name": "VBA-JSON",
      "one_line_profile": "JSON parsing and conversion library for VBA",
      "detailed_description": "A library for parsing and generating JSON within Visual Basic for Applications (VBA), enabling Excel-based scientific workflows to interact with JSON data sources and APIs.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "serialization"
      ],
      "application_level": "library",
      "primary_language": "Visual Basic",
      "repo_url": "https://github.com/VBA-tools/VBA-JSON",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "json",
        "vba",
        "excel",
        "parsing"
      ],
      "id": 19
    },
    {
      "name": "jtoml",
      "one_line_profile": "TOML parser library for Java",
      "detailed_description": "A fully compliant TOML parser for Java, facilitating the use of TOML configuration files in Java-based scientific applications.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "configuration_management"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/WasabiThumb/jtoml",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "toml",
        "parser",
        "java",
        "configuration"
      ],
      "id": 20
    },
    {
      "name": "flatted",
      "one_line_profile": "Circular JSON parser",
      "detailed_description": "A fast and minimal JavaScript parser for JSON structures with circular references, useful for serializing complex data graphs in scientific web applications.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "serialization"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/WebReflection/flatted",
      "help_website": [],
      "license": "ISC",
      "tags": [
        "json",
        "circular-reference",
        "serialization",
        "javascript"
      ],
      "id": 21
    },
    {
      "name": "pxi",
      "one_line_profile": "Command-line data processor for JSON and other formats",
      "detailed_description": "A command-line tool for processing and transforming data, similar to jq and awk, supporting efficient data manipulation in scientific pipelines.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_processing",
        "transformation"
      ],
      "application_level": "solver",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/Yord/pxi",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "cli",
        "data-processing",
        "json",
        "transformation"
      ],
      "id": 22
    },
    {
      "name": "PolarCodeDecodersInMatlab",
      "one_line_profile": "Matlab implementation of Polar Code decoders",
      "detailed_description": "A Matlab library implementing various Polar Code decoding algorithms (CA-SCL, BP), serving as a simulation tool for information theory and telecommunications research.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "simulation",
        "decoding",
        "algorithm_implementation"
      ],
      "application_level": "library",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/YuYongRun/PolarCodeDecodersInMatlab",
      "help_website": [],
      "license": null,
      "tags": [
        "polar-codes",
        "decoding",
        "matlab",
        "information-theory"
      ],
      "id": 23
    },
    {
      "name": "construct (Java)",
      "one_line_profile": "Binary and textual data structure parsing library for Java",
      "detailed_description": "A Java port of the Python 'construct' library, used for declarative parsing and building of binary data structures, essential for handling custom binary formats in scientific data.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "binary_processing"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/ZiglioUK/construct",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "binary-parsing",
        "java",
        "data-structures"
      ],
      "id": 24
    },
    {
      "name": "YamlDotNet",
      "one_line_profile": "YAML library for .NET",
      "detailed_description": "A comprehensive .NET library for parsing and serializing YAML. It is a foundational tool for handling configuration and data serialization in scientific software developed in the .NET ecosystem.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "serialization"
      ],
      "application_level": "library",
      "primary_language": "C#",
      "repo_url": "https://github.com/aaubry/YamlDotNet",
      "help_website": [
        "https://github.com/aaubry/YamlDotNet/wiki"
      ],
      "license": "MIT",
      "tags": [
        "yaml",
        "dotnet",
        "serialization",
        "parsing"
      ],
      "id": 25
    },
    {
      "name": "saneyaml",
      "one_line_profile": "Safer and simpler YAML parsing for Python",
      "detailed_description": "A Python library built on top of PyYAML that provides a safer and cleaner interface for parsing and serializing YAML, reducing risks and complexity in scientific configuration management.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "serialization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aboutcode-org/saneyaml",
      "help_website": [],
      "license": null,
      "tags": [
        "yaml",
        "python",
        "safety",
        "parsing"
      ],
      "id": 26
    },
    {
      "name": "polars_ds_extension",
      "one_line_profile": "Polars extension for data science utilities",
      "detailed_description": "An extension for the Polars dataframe library that adds general data science functionalities, enhancing data processing workflows in Rust and Python.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "data_analysis",
        "statistics"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/abstractqqq/polars_ds_extension",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "polars",
        "data-science",
        "rust",
        "python",
        "extension"
      ],
      "id": 27
    },
    {
      "name": "node-csv",
      "one_line_profile": "Full-featured CSV parser and generator for Node.js",
      "detailed_description": "A comprehensive CSV parsing and generation library for Node.js, widely used for processing tabular data in JavaScript-based scientific applications and data pipelines.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "serialization"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/adaltas/node-csv",
      "help_website": [
        "https://csv.js.org/"
      ],
      "license": "MIT",
      "tags": [
        "csv",
        "parser",
        "nodejs",
        "data-processing"
      ],
      "id": 28
    },
    {
      "name": "node-csv-parse",
      "one_line_profile": "Stream-based CSV parser for Node.js",
      "detailed_description": "A CSV parsing module implementing the Node.js stream API, allowing efficient processing of large scientific datasets row-by-row.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "stream_processing"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/adaltas/node-csv-parse",
      "help_website": [
        "https://csv.js.org/parse/"
      ],
      "license": null,
      "tags": [
        "csv",
        "parser",
        "stream",
        "nodejs"
      ],
      "id": 29
    },
    {
      "name": "node-csv-stringify",
      "one_line_profile": "Stream-based CSV stringifier for Node.js",
      "detailed_description": "A CSV generation module implementing the Node.js stream API, used for exporting scientific data to CSV format efficiently.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "serialization",
        "export"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/adaltas/node-csv-stringify",
      "help_website": [
        "https://csv.js.org/stringify/"
      ],
      "license": null,
      "tags": [
        "csv",
        "generator",
        "stream",
        "nodejs"
      ],
      "id": 30
    },
    {
      "name": "PandasGUI",
      "one_line_profile": "GUI for analyzing Pandas DataFrames",
      "detailed_description": "A graphical user interface tool for viewing, plotting, and analyzing Pandas DataFrames, facilitating interactive data exploration for scientists using Python.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "visualization",
        "data_exploration"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/adamerose/PandasGUI",
      "help_website": [],
      "license": "MIT-0",
      "tags": [
        "pandas",
        "gui",
        "visualization",
        "data-analysis"
      ],
      "id": 31
    },
    {
      "name": "zaml",
      "one_line_profile": "Fast YAML 1.2 parsing library",
      "detailed_description": "A high-performance YAML 1.2 parsing library, providing fast configuration loading for performance-critical scientific applications.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing"
      ],
      "application_level": "library",
      "primary_language": "Zig",
      "repo_url": "https://github.com/adamserafini/zaml",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "yaml",
        "parser",
        "zig",
        "performance"
      ],
      "id": 32
    },
    {
      "name": "Trafilatura",
      "one_line_profile": "Web scraping and text extraction tool",
      "detailed_description": "A tool and library for gathering text and metadata from the web, outputting to formats like CSV, JSON, and XML. It is valuable for creating datasets for NLP and social science research.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "data_collection",
        "text_extraction",
        "scraping"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/adbar/trafilatura",
      "help_website": [
        "https://trafilatura.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "scraping",
        "text-extraction",
        "nlp",
        "dataset-creation"
      ],
      "id": 33
    },
    {
      "name": "frontmatter",
      "one_line_profile": "Go library for parsing content front matter",
      "detailed_description": "A Go library for detecting and decoding front matter (YAML/JSON/TOML metadata) from content files, useful for managing metadata in scientific data repositories and static site generators.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "metadata_extraction"
      ],
      "application_level": "library",
      "primary_language": "Go",
      "repo_url": "https://github.com/adrg/frontmatter",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "frontmatter",
        "metadata",
        "parsing",
        "go"
      ],
      "id": 34
    },
    {
      "name": "Plotlars",
      "one_line_profile": "Integration library for Polars dataframes and Plotly visualization",
      "detailed_description": "A Rust library that facilitates the integration between the Polars data analysis library and the Plotly plotting library, enabling efficient visualization of scientific dataframes.",
      "domains": [
        "D1",
        "D4"
      ],
      "subtask_category": [
        "visualization",
        "data_analysis"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/alceal/plotlars",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "polars",
        "plotly",
        "visualization",
        "rust"
      ],
      "id": 35
    },
    {
      "name": "Ruby Polars",
      "one_line_profile": "High-performance DataFrame library for Ruby based on Polars",
      "detailed_description": "A Ruby binding for the Polars DataFrame library, providing blazingly fast data processing and analysis capabilities suitable for scientific datasets.",
      "domains": [
        "D1",
        "D1-0X"
      ],
      "subtask_category": [
        "data_processing",
        "data_analysis"
      ],
      "application_level": "library",
      "primary_language": "Ruby",
      "repo_url": "https://github.com/ankane/ruby-polars",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "dataframe",
        "polars",
        "ruby",
        "data-science"
      ],
      "id": 36
    },
    {
      "name": "nvParse",
      "one_line_profile": "GPU-accelerated CSV parser",
      "detailed_description": "A fast, GPU-based CSV parser designed to leverage CUDA for high-performance data loading and parsing, suitable for large-scale scientific datasets.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_loading",
        "parsing"
      ],
      "application_level": "library",
      "primary_language": "Cuda",
      "repo_url": "https://github.com/antonmks/nvParse",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "csv",
        "gpu",
        "cuda",
        "hpc"
      ],
      "id": 37
    },
    {
      "name": "Apache Avro",
      "one_line_profile": "Data serialization system for compact binary data exchange",
      "detailed_description": "A data serialization system widely used in big data and scientific computing for efficient data storage and exchange, supporting rich data structures and schema evolution.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "serialization",
        "data_storage"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/apache/avro",
      "help_website": [
        "https://avro.apache.org/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "serialization",
        "big-data",
        "format",
        "schema"
      ],
      "id": 38
    },
    {
      "name": "Apache Fesod",
      "one_line_profile": "Efficient spreadsheet processing library",
      "detailed_description": "A library designed for processing large spreadsheet files (Excel) efficiently without memory overflow (OOM), facilitating the ingestion of scientific data stored in spreadsheets.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_loading",
        "spreadsheet_processing"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/apache/fesod",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "excel",
        "spreadsheet",
        "data-processing",
        "java"
      ],
      "id": 39
    },
    {
      "name": "Apache XTable",
      "one_line_profile": "Cross-table converter for lakehouse table formats",
      "detailed_description": "A cross-table converter for lakehouse table formats (Hudi, Delta, Iceberg) that facilitates interoperability across data processing systems used in large-scale scientific data platforms.",
      "domains": [
        "D1",
        "D1-0X"
      ],
      "subtask_category": [
        "data_conversion",
        "interoperability"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/apache/incubator-xtable",
      "help_website": [
        "https://xtable.apache.org/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "lakehouse",
        "hudi",
        "delta-lake",
        "iceberg",
        "interoperability"
      ],
      "id": 40
    },
    {
      "name": "MyDuckServer",
      "one_line_profile": "DuckDB-powered SQL server for analytics",
      "detailed_description": "A unified server powered by DuckDB that provides MySQL, Postgres, and FlightSQL interfaces, enabling efficient OLAP and data analysis on scientific datasets.",
      "domains": [
        "D1",
        "D1-0X"
      ],
      "subtask_category": [
        "data_analysis",
        "database_service"
      ],
      "application_level": "service",
      "primary_language": "Go",
      "repo_url": "https://github.com/apecloud/myduckserver",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "duckdb",
        "olap",
        "sql",
        "analytics"
      ],
      "id": 41
    },
    {
      "name": "jsonv.sh",
      "one_line_profile": "Bash CLI tool for JSON to CSV conversion",
      "detailed_description": "A Bash command line tool for converting JSON data to CSV format, useful for lightweight data wrangling and pipeline integration in scientific workflows.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_conversion",
        "format_conversion"
      ],
      "application_level": "solver",
      "primary_language": "Awk",
      "repo_url": "https://github.com/archan937/jsonv.sh",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "json",
        "csv",
        "bash",
        "cli"
      ],
      "id": 42
    },
    {
      "name": "Airflow AI SDK",
      "one_line_profile": "SDK for AI/LLM integration in Airflow workflows",
      "detailed_description": "An SDK for integrating Large Language Models (LLMs) and AI Agents into Apache Airflow pipelines, enabling AI-driven scientific workflows and automation.",
      "domains": [
        "D1",
        "D1-0X"
      ],
      "subtask_category": [
        "workflow_orchestration",
        "ai_integration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/astronomer/airflow-ai-sdk",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "airflow",
        "llm",
        "workflow",
        "ai-agent"
      ],
      "id": 43
    },
    {
      "name": "csvdiff",
      "one_line_profile": "Fast diff tool for comparing CSV files",
      "detailed_description": "A fast command-line tool for comparing CSV files, useful for quality control, regression testing, and verifying data processing results in scientific pipelines.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "quality_control",
        "data_comparison"
      ],
      "application_level": "solver",
      "primary_language": "Go",
      "repo_url": "https://github.com/aswinkarthik/csvdiff",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "csv",
        "diff",
        "cli",
        "data-qc"
      ],
      "id": 44
    },
    {
      "name": "AWS SDK for pandas",
      "one_line_profile": "Pandas integration for AWS data services (Athena, Glue, S3)",
      "detailed_description": "An open-source Python library that extends Pandas to easily connect with AWS data services. It simplifies the reading and writing of scientific datasets (Parquet, CSV, JSON) stored in AWS S3, Athena, and Redshift, acting as a critical data engineering tool for cloud-based scientific workflows.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_integration",
        "cloud_io",
        "etl"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aws/aws-sdk-pandas",
      "help_website": [
        "https://aws-sdk-pandas.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "pandas",
        "aws",
        "etl",
        "parquet",
        "data-engineering"
      ],
      "id": 45
    },
    {
      "name": "polars_ols",
      "one_line_profile": "Least squares linear regression extension for Polars",
      "detailed_description": "A plugin for the Polars DataFrame library that enables efficient ordinary least squares (OLS) linear regression directly within Polars expressions. It facilitates fast statistical modeling and inference on large datasets without leaving the Polars ecosystem.",
      "domains": [
        "D1",
        "D4"
      ],
      "subtask_category": [
        "statistical_analysis",
        "linear_modeling",
        "regression"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/azmyrajab/polars_ols",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "polars",
        "statistics",
        "linear-regression",
        "least-squares"
      ],
      "id": 46
    },
    {
      "name": "Orange3",
      "one_line_profile": "Interactive data mining and machine learning toolkit",
      "detailed_description": "An open-source data visualization, machine learning and data mining toolkit. It features a visual programming front-end for explorative data analysis and interactive data visualization, widely used in bioinformatics and social sciences.",
      "domains": [
        "D1",
        "D4"
      ],
      "subtask_category": [
        "data_mining",
        "visualization",
        "machine_learning"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/biolab/orange3",
      "help_website": [
        "https://orangedatamining.com/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "data-mining",
        "visualization",
        "bioinformatics",
        "machine-learning"
      ],
      "id": 47
    },
    {
      "name": "Blaze",
      "one_line_profile": "Interface for querying big data using NumPy/Pandas syntax",
      "detailed_description": "Blaze provides a Python interface to query data on various storage systems (SQL, NoSQL, Spark) using a subset of the NumPy and Pandas API. It abstracts computation and storage, allowing scientific users to process large datasets that exceed memory limits.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "data_processing",
        "big_data_interface",
        "computation_abstraction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/blaze/blaze",
      "help_website": [
        "http://blaze.pydata.org/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "numpy",
        "pandas",
        "big-data",
        "interface"
      ],
      "id": 48
    },
    {
      "name": "pytimetk",
      "one_line_profile": "Time series analysis and forecasting toolkit",
      "detailed_description": "A Python library designed to simplify time series analysis, feature engineering, and forecasting. It integrates with the Pandas and Polars ecosystems to provide fast, functional tools for processing temporal data in scientific and analytical contexts.",
      "domains": [
        "D1",
        "D4"
      ],
      "subtask_category": [
        "time_series_analysis",
        "forecasting",
        "feature_engineering"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/business-science/pytimetk",
      "help_website": [
        "https://business-science.github.io/pytimetk/"
      ],
      "license": "MIT",
      "tags": [
        "time-series",
        "forecasting",
        "pandas",
        "polars"
      ],
      "id": 49
    },
    {
      "name": "datacompy",
      "one_line_profile": "DataFrame comparison tool for Pandas, Polars, and Spark",
      "detailed_description": "A library for comparing two DataFrames (Pandas, Polars, Spark, or Snowpark) to identify differences. It is widely used in data quality control (QC) and validation steps within scientific data processing pipelines to ensure data integrity.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "quality_control",
        "data_comparison",
        "validation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/capitalone/datacompy",
      "help_website": [
        "https://capitalone.github.io/datacompy/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "data-quality",
        "comparison",
        "pandas",
        "polars",
        "spark"
      ],
      "id": 50
    },
    {
      "name": "FastTableViewer",
      "one_line_profile": "High-performance command-line viewer for CSV/TSV and delimited data files",
      "detailed_description": "A fast, feature-rich command-line tool designed for viewing and inspecting large CSV, TSV, and other delimited data files. It supports key navigation, searching, and handling of large datasets, making it useful for quick inspection of tabular scientific data without loading into heavy GUI applications.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_inspection",
        "data_visualization"
      ],
      "application_level": "solver",
      "primary_language": "Go",
      "repo_url": "https://github.com/codechenx/FastTableViewer",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "csv",
        "tsv",
        "cli",
        "data-viewer",
        "tabular-data"
      ],
      "id": 51
    },
    {
      "name": "csv-to-json",
      "one_line_profile": "Command-line utility for converting CSV files to JSON format",
      "detailed_description": "A lightweight Node.js command-line tool for converting CSV data into JSON format. It supports standard CSV parsing and JSON output, facilitating data format conversion in scientific workflows where JSON is required for downstream analysis or web visualization.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "format_conversion",
        "data_processing"
      ],
      "application_level": "workflow",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/cparker15/csv-to-json",
      "help_website": [],
      "license": "LGPL-3.0",
      "tags": [
        "csv",
        "json",
        "converter",
        "cli"
      ],
      "id": 52
    },
    {
      "name": "OctoSQL",
      "one_line_profile": "SQL query tool for analyzing data from multiple file formats and databases",
      "detailed_description": "OctoSQL is a query tool that allows users to join, analyze, and transform data from multiple sources, including CSV, JSON, Parquet, and various databases, using standard SQL. It is highly applicable for scientific data analysis where data resides in heterogeneous file formats.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_analysis",
        "data_query",
        "format_conversion"
      ],
      "application_level": "solver",
      "primary_language": "Go",
      "repo_url": "https://github.com/cube2222/octosql",
      "help_website": [],
      "license": "MPL-2.0",
      "tags": [
        "sql",
        "csv",
        "json",
        "parquet",
        "data-analysis",
        "cli"
      ],
      "id": 53
    },
    {
      "name": "csv2json",
      "one_line_profile": "Ruby command-line tool for converting CSV to JSON",
      "detailed_description": "A Ruby gem providing a command-line interface to convert CSV files into JSON. It serves as a simple utility for data format transformation in data processing pipelines.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "format_conversion",
        "data_processing"
      ],
      "application_level": "workflow",
      "primary_language": "Ruby",
      "repo_url": "https://github.com/darwin/csv2json",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "csv",
        "json",
        "converter",
        "cli",
        "ruby"
      ],
      "id": 54
    },
    {
      "name": "Dask",
      "one_line_profile": "Flexible parallel computing library for analytic computing",
      "detailed_description": "Dask provides advanced parallelism for analytics, enabling performance at scale for the tools of the PyData ecosystem (NumPy, Pandas, and Scikit-Learn). It is fundamental for processing large scientific datasets (e.g., in geoscience, physics) that exceed memory limits.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_processing",
        "parallel_computing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/dask/dask",
      "help_website": [
        "https://dask.org/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "parallel-computing",
        "distributed-systems",
        "numpy",
        "pandas",
        "scaling"
      ],
      "id": 55
    },
    {
      "name": "Yacman",
      "one_line_profile": "YAML configuration manager for scientific workflows",
      "detailed_description": "Developed by the Databio lab, Yacman is a configuration management tool designed to standardize how bioinformatics and scientific tools handle YAML configurations, often used in conjunction with tools like Looper and Peppy.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "configuration_management",
        "workflow_utility"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/databio/yacman",
      "help_website": [
        "http://yacman.databio.org/"
      ],
      "license": "BSD-2-Clause",
      "tags": [
        "yaml",
        "configuration",
        "bioinformatics",
        "workflow"
      ],
      "id": 56
    },
    {
      "name": "Koalas",
      "one_line_profile": "Pandas API on Apache Spark for scalable data science",
      "detailed_description": "Koalas (now integrated into PySpark) allows data scientists to use the familiar pandas API while leveraging the distributed processing power of Apache Spark, enabling the processing of massive scientific datasets.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_processing",
        "big_data_analytics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/databricks/koalas",
      "help_website": [
        "https://koalas.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "pandas",
        "spark",
        "big-data",
        "data-science"
      ],
      "id": 57
    },
    {
      "name": "cad2data",
      "one_line_profile": "Automated conversion pipeline for CAD/BIM data formats",
      "detailed_description": "A workflow tool for converting engineering and architectural CAD files (Revit .rvt, IFC, DWG) into data-accessible formats, facilitating the analysis of construction and structural data.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "format_conversion",
        "data_extraction"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/datadrivenconstruction/cad2data-Revit-IFC-DWG-DGN-pipeline-with-conversion-validation-qto",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "cad",
        "bim",
        "revit",
        "ifc",
        "data-conversion"
      ],
      "id": 58
    },
    {
      "name": "qsv",
      "one_line_profile": "High-performance CSV data wrangling toolkit",
      "detailed_description": "A command-line tool for indexing, slicing, analyzing, and manipulating CSV files. It is widely used in data science pipelines to handle large tabular datasets efficiently without loading them entirely into memory.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_processing",
        "data_wrangling"
      ],
      "application_level": "solver",
      "primary_language": "Rust",
      "repo_url": "https://github.com/dathere/qsv",
      "help_website": [
        "https://qsv.dathere.com"
      ],
      "license": "Unlicense",
      "tags": [
        "csv",
        "cli",
        "rust",
        "data-engineering",
        "etl"
      ],
      "id": 59
    },
    {
      "name": "xlsx2csv",
      "one_line_profile": "Fast converter from XLSX to CSV for data pipelines",
      "detailed_description": "A lightweight tool to convert Microsoft Excel (XLSX) files to CSV format. It is optimized for speed and handling large files, making it useful for ingesting scientific data stored in spreadsheets into analysis pipelines.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "format_conversion",
        "data_ingestion"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/dilshod/xlsx2csv",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "xlsx",
        "csv",
        "excel",
        "conversion"
      ],
      "id": 60
    },
    {
      "name": "DocArray",
      "one_line_profile": "Data structure for multimodal scientific data",
      "detailed_description": "A library for representing, sending, storing, and searching multimodal data (text, image, audio, 3D meshes, tensors). It is used in AI pipelines to handle unstructured data structures common in deep learning applications.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_representation",
        "multimodal_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/docarray/docarray",
      "help_website": [
        "https://docs.docarray.org/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "multimodal",
        "tensor",
        "data-structure",
        "ai",
        "deep-learning"
      ],
      "id": 61
    },
    {
      "name": "fit2gpx",
      "one_line_profile": "Converter for FIT sensor data to GPX format",
      "detailed_description": "A library and tool to convert FIT files (commonly used by GPS devices and sensors) to GPX format. Useful for geospatial analysis and processing of physiological/tracking data in sports science or geography.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "format_conversion",
        "sensor_data_processing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/dodo-saba/fit2gpx",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "fit",
        "gpx",
        "geospatial",
        "sensor-data",
        "strava"
      ],
      "id": 62
    },
    {
      "name": "arrow-tools",
      "one_line_profile": "CLI tools for Apache Arrow and Parquet conversion",
      "detailed_description": "A collection of command-line tools to convert CSV and JSON data into Apache Arrow and Parquet formats. These formats are standard for high-performance scientific data analytics and interchange.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "format_conversion",
        "data_interchange"
      ],
      "application_level": "solver",
      "primary_language": "Rust",
      "repo_url": "https://github.com/domoritz/arrow-tools",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "apache-arrow",
        "parquet",
        "csv",
        "json",
        "cli"
      ],
      "id": 63
    },
    {
      "name": "dsgrid-legacy-efs-api",
      "one_line_profile": "HDF5 data marshalling for energy grid models",
      "detailed_description": "A Python package developed by NREL for marshalling dsgrid (Demand-Side Grid) data into a common HDF5 format, facilitating the analysis and exchange of energy system modeling data.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_marshalling",
        "format_conversion"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/dsgrid/dsgrid-legacy-efs-api",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "hdf5",
        "energy-grid",
        "nrel",
        "data-marshalling"
      ],
      "id": 64
    },
    {
      "name": "ScienceBeam Parser",
      "one_line_profile": "Tool to convert PDF scientific articles into structured XML data",
      "detailed_description": "A set of tools designed to convert scientific publications (PDFs) into structured XML documents, facilitating literature mining and scientific knowledge extraction.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "literature_mining",
        "document_parsing"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/elifesciences/sciencebeam-parser",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pdf-parsing",
        "xml",
        "scientific-literature",
        "nlp"
      ],
      "id": 65
    },
    {
      "name": "MCAP",
      "one_line_profile": "Modular container file format and libraries for robotics data recording",
      "detailed_description": "A modular, performant, and serialization-agnostic container file format designed for robotics applications, serving as a modern alternative to ROS bags for sensor data logging and analysis.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_serialization",
        "sensor_logging"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/foxglove/mcap",
      "help_website": [
        "https://mcap.dev"
      ],
      "license": "MIT",
      "tags": [
        "robotics",
        "serialization",
        "ros",
        "data-logging"
      ],
      "id": 66
    },
    {
      "name": "fst",
      "one_line_profile": "High-performance data frame serialization library for R",
      "detailed_description": "A library for extremely fast serialization and deserialization of data frames in R. It supports random access, compression, and multi-threading, making it essential for handling large scientific datasets in R workflows.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_serialization",
        "data_io"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/fstpackage/fst",
      "help_website": [
        "https://www.fstpackage.org/"
      ],
      "license": "AGPL-3.0",
      "tags": [
        "serialization",
        "data-frame",
        "high-performance",
        "R"
      ],
      "id": 67
    },
    {
      "name": "Fugue",
      "one_line_profile": "Unified interface for distributed computing in scientific workflows",
      "detailed_description": "A unified interface that allows users to execute Python, Pandas, and SQL code on distributed computing frameworks like Spark, Dask, and Ray without rewriting code. It facilitates scaling scientific data analysis workflows.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "distributed_computing",
        "workflow_orchestration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/fugue-project/fugue",
      "help_website": [
        "https://fugue-project.github.io/tutorials/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "distributed-computing",
        "pandas",
        "spark",
        "dask",
        "workflow"
      ],
      "id": 68
    },
    {
      "name": "functime",
      "one_line_profile": "Scalable time-series machine learning library",
      "detailed_description": "A library for time-series machine learning at scale, built on Polars. It provides parallel feature extraction and forecasting capabilities, suitable for analyzing large-scale scientific panel data (e.g., sensor data, environmental metrics).",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "time_series_analysis",
        "forecasting",
        "feature_extraction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/functime-org/functime",
      "help_website": [
        "https://docs.functime.ai/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "time-series",
        "machine-learning",
        "polars",
        "forecasting"
      ],
      "id": 69
    },
    {
      "name": "german-nouns",
      "one_line_profile": "Structured dataset and parser for German linguistic analysis",
      "detailed_description": "A dataset containing ~100,000 German nouns with grammatical properties and a Python module for parsing compound words. It serves as a tool for computational linguistics and NLP research.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "linguistic_analysis",
        "dataset",
        "nlp"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/gambolputty/german-nouns",
      "help_website": [],
      "license": "CC-BY-SA-4.0",
      "tags": [
        "linguistics",
        "nlp",
        "german",
        "dataset"
      ],
      "id": 70
    },
    {
      "name": "simdcsv",
      "one_line_profile": "High-performance SIMD-accelerated CSV parser",
      "detailed_description": "A fast CSV parser for C++ that leverages SIMD instructions. It is designed for high-throughput data loading, which is critical for processing large scientific datasets stored in CSV format.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "data_loading"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/geofflangdale/simdcsv",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "csv",
        "simd",
        "high-performance",
        "parsing"
      ],
      "id": 71
    },
    {
      "name": "GeoPandas",
      "one_line_profile": "Python tools for geographic data analysis",
      "detailed_description": "An open source project to make working with geospatial data in python easier. It extends the datatypes used by pandas to allow spatial operations on geometric types, essential for earth sciences and geography.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "geospatial_analysis",
        "data_manipulation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/geopandas/geopandas",
      "help_website": [
        "https://geopandas.org/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "geospatial",
        "gis",
        "pandas",
        "python"
      ],
      "id": 72
    },
    {
      "name": "reflect-cpp",
      "one_line_profile": "Reflection-based serialization library for C++20",
      "detailed_description": "A C++20 library for fast serialization and deserialization using reflection. It supports multiple formats relevant to scientific computing, including JSON, CSV, Parquet, Avro, and YAML, enabling efficient data exchange.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "serialization",
        "data_io"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/getml/reflect-cpp",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "serialization",
        "reflection",
        "parquet",
        "avro",
        "cpp"
      ],
      "id": 73
    },
    {
      "name": "LangExtract",
      "one_line_profile": "LLM-based structured information extraction library",
      "detailed_description": "A library for extracting structured information from unstructured text using LLMs with source grounding. It is applicable for scientific literature mining and knowledge extraction tasks.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "information_extraction",
        "text_mining",
        "nlp"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/google/langextract",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "information-extraction",
        "nlp",
        "structured-data"
      ],
      "id": 74
    },
    {
      "name": "struct2tensor",
      "one_line_profile": "Structured data manipulation for TensorFlow",
      "detailed_description": "A library for parsing and manipulating structured data (like Protocol Buffers) inside TensorFlow. It is essential for preprocessing complex structured data in AI for Science pipelines.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "data_preprocessing",
        "tensor_manipulation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/google/struct2tensor",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "tensorflow",
        "structured-data",
        "preprocessing",
        "protobuf"
      ],
      "id": 75
    },
    {
      "name": "Hugging Face Datasets",
      "one_line_profile": "Library for easily accessing, sharing, and processing datasets for Audio, Computer Vision, and NLP",
      "detailed_description": "A lightweight and extensible library to easily share and access datasets and evaluation metrics. It features memory-mapped data loading for efficiency, interoperability with NumPy/Pandas/PyTorch/TensorFlow, and supports various data formats (JSON, CSV, Parquet, Arrow) essential for scientific data pipelines.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_loading",
        "data_processing",
        "data_conversion"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/huggingface/datasets",
      "help_website": [
        "https://huggingface.co/docs/datasets"
      ],
      "license": "Apache-2.0",
      "tags": [
        "datasets",
        "etl",
        "data-processing",
        "machine-learning"
      ],
      "id": 76
    },
    {
      "name": "Ibis",
      "one_line_profile": "Portable Python dataframe library for data analysis across various backends",
      "detailed_description": "Ibis provides a standard API for data analysis and manipulation, decoupling the analytics from the backend execution. It supports multiple backends including SQL databases, Pandas, and BigQuery, making it a powerful tool for scientific data workflows and large-scale data processing.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_analysis",
        "data_manipulation",
        "query_generation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ibis-project/ibis",
      "help_website": [
        "https://ibis-project.org"
      ],
      "license": "Apache-2.0",
      "tags": [
        "dataframe",
        "analytics",
        "sql",
        "data-science"
      ],
      "id": 77
    },
    {
      "name": "OpenStreetMap H3 Loader",
      "one_line_profile": "High-performance tool to transform OpenStreetMap data into H3 partitioned formats",
      "detailed_description": "A specialized data processing tool that converts OpenStreetMap (OSM) planet dumps into H3 (Hexagonal Hierarchical Spatial Index) partitioned PostGIS or Arrow/Parquet formats. This facilitates large-scale geospatial scientific analysis and modeling.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_conversion",
        "geospatial_analysis",
        "data_partitioning"
      ],
      "application_level": "tool",
      "primary_language": "Java",
      "repo_url": "https://github.com/igor-suhorukov/openstreetmap_h3",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "openstreetmap",
        "h3",
        "geospatial",
        "etl",
        "parquet"
      ],
      "id": 78
    },
    {
      "name": "Danfo.js",
      "one_line_profile": "Pandas-like data analysis library for JavaScript",
      "detailed_description": "Danfo.js is a JavaScript library that provides high-performance, intuitive data structures (DataFrame and Series) for manipulating and processing structured data, bringing data analysis capabilities similar to Pandas to the JavaScript ecosystem.",
      "domains": [
        "D1",
        "D3"
      ],
      "subtask_category": [
        "data_analysis",
        "data_manipulation"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/javascriptdata/danfojs",
      "help_website": [
        "https://danfo.jsdata.org/"
      ],
      "license": "MIT",
      "tags": [
        "dataframe",
        "data-analysis",
        "javascript",
        "pandas"
      ],
      "id": 79
    },
    {
      "name": "json2csv",
      "one_line_profile": "Command-line tool for converting JSON to CSV",
      "detailed_description": "A command-line utility and library for converting JSON data structures into CSV format, facilitating data exchange, integration, and preprocessing in scientific data workflows.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "format_conversion",
        "data_preprocessing"
      ],
      "application_level": "solver",
      "primary_language": "Go",
      "repo_url": "https://github.com/jehiah/json2csv",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "json",
        "csv",
        "conversion",
        "cli"
      ],
      "id": 80
    },
    {
      "name": "jq",
      "one_line_profile": "Command-line JSON processor",
      "detailed_description": "A lightweight and flexible command-line JSON processor that is widely used in scientific data pipelines for filtering, mapping, transforming, and normalizing JSON-formatted data (e.g., metadata, API responses).",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_processing",
        "filtering",
        "transformation"
      ],
      "application_level": "solver",
      "primary_language": "C",
      "repo_url": "https://github.com/jqlang/jq",
      "help_website": [
        "https://jqlang.github.io/jq/"
      ],
      "license": "MIT",
      "tags": [
        "json",
        "cli",
        "data-processing",
        "filter"
      ],
      "id": 81
    },
    {
      "name": "Flatterer",
      "one_line_profile": "Opinionated JSON to CSV/Parquet/SQL converter for data analysis",
      "detailed_description": "A high-performance CLI tool designed to convert nested JSON data into flat formats like CSV, Parquet, and SQLite. It is particularly useful in scientific data engineering pipelines for preparing large-scale JSON datasets (e.g., from APIs or instruments) for analysis in data science tools.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_conversion",
        "data_preparation"
      ],
      "application_level": "solver",
      "primary_language": "Rust",
      "repo_url": "https://github.com/kindly/flatterer",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "json",
        "parquet",
        "csv",
        "data-engineering",
        "conversion"
      ],
      "id": 82
    },
    {
      "name": "Patito",
      "one_line_profile": "Data modelling and validation layer for Polars dataframes",
      "detailed_description": "A library built on top of Polars and Pydantic that provides a data modeling layer for dataframe validation. It allows scientists and data engineers to define schemas for their dataframes, ensuring data quality and consistency in scientific data processing pipelines.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "quality_control",
        "data_validation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/kolonialno/patito",
      "help_website": [
        "https://patito.readthedocs.io"
      ],
      "license": "MIT",
      "tags": [
        "polars",
        "pydantic",
        "validation",
        "dataframe",
        "data-science"
      ],
      "id": 83
    },
    {
      "name": "Duckling",
      "one_line_profile": "Fast viewer for CSV, Parquet, and database files",
      "detailed_description": "A high-performance data viewer built with Tauri that supports opening and inspecting large CSV and Parquet files, as well as connecting to databases like DuckDB and PostgreSQL. Useful for quick inspection of scientific datasets.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_visualization",
        "data_inspection"
      ],
      "application_level": "solver",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/l1xnan/duckling",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "csv",
        "parquet",
        "data-viewer",
        "duckdb"
      ],
      "id": 84
    },
    {
      "name": "Lance",
      "one_line_profile": "Modern columnar data format for AI and multimodal data",
      "detailed_description": "An open source data format designed for high-performance random access and vector search, optimized for ML workflows and multimodal data (images, text, vectors). It serves as a faster alternative to Parquet for AI training and retrieval tasks.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "data_storage",
        "data_format",
        "vector_search"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/lance-format/lance",
      "help_website": [
        "https://lancedb.github.io/lance/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "data-format",
        "vector-search",
        "machine-learning",
        "parquet-alternative"
      ],
      "id": 85
    },
    {
      "name": "LangChain",
      "one_line_profile": "Framework for developing applications powered by language models",
      "detailed_description": "A comprehensive framework for building agents and workflows using Large Language Models (LLMs). In scientific research (AI4S), it is widely used to orchestrate reasoning agents, automate literature reviews, and build autonomous laboratory controllers.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "workflow_orchestration",
        "inference",
        "agent_framework"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/langchain-ai/langchain",
      "help_website": [
        "https://python.langchain.com/"
      ],
      "license": "MIT",
      "tags": [
        "llm",
        "agents",
        "workflow",
        "ai4s"
      ],
      "id": 86
    },
    {
      "name": "Lark",
      "one_line_profile": "Parsing toolkit for Python",
      "detailed_description": "A modern parsing library for Python that can parse any context-free grammar. It is frequently used in scientific software to build parsers for domain-specific file formats (e.g., chemical formulas, biological sequence notations) and custom configuration languages.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "grammar_definition"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/lark-parser/lark",
      "help_website": [
        "https://lark-parser.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "parser",
        "grammar",
        "dsl",
        "python"
      ],
      "id": 87
    },
    {
      "name": "TinyXML2",
      "one_line_profile": "Simple, small, efficient C++ XML parser",
      "detailed_description": "A lightweight C++ XML parser widely integrated into scientific simulation software and high-performance computing applications for handling configuration files and data exchange formats.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "io"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/leethomason/tinyxml2",
      "help_website": [
        "https://leethomason.github.io/tinyxml2/"
      ],
      "license": "Zlib",
      "tags": [
        "xml",
        "parser",
        "cpp",
        "embedded"
      ],
      "id": 88
    },
    {
      "name": "docker-csv",
      "one_line_profile": "Docker container with CSV processing utilities",
      "detailed_description": "A containerized environment pre-packaged with command-line tools like csvkit, csvcut, and csvsql. It provides a reproducible workflow environment for batch processing and cleaning of tabular scientific data.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_processing",
        "data_cleaning"
      ],
      "application_level": "workflow",
      "primary_language": "Dockerfile",
      "repo_url": "https://github.com/leplusorg/docker-csv",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "csv",
        "docker",
        "etl",
        "cli"
      ],
      "id": 89
    },
    {
      "name": "libexpat",
      "one_line_profile": "Fast streaming XML parser library",
      "detailed_description": "A stream-oriented XML parser library written in C. It is a foundational dependency for many scientific computing packages (e.g., in Python, Perl, and system libraries) to handle XML-based data formats efficiently.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "io"
      ],
      "application_level": "library",
      "primary_language": "C",
      "repo_url": "https://github.com/libexpat/libexpat",
      "help_website": [
        "https://libexpat.github.io/"
      ],
      "license": "MIT",
      "tags": [
        "xml",
        "parser",
        "c",
        "streaming"
      ],
      "id": 90
    },
    {
      "name": "dataclasses-json",
      "one_line_profile": "Serialization for Python Data Classes",
      "detailed_description": "A library that provides simple APIs to convert Python Data Classes to and from JSON. It is extensively used in Python-based scientific data pipelines to structure, validate, and serialize experimental parameters and results.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "serialization",
        "data_binding"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/lidatong/dataclasses-json",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "json",
        "serialization",
        "python",
        "dataclasses"
      ],
      "id": 91
    },
    {
      "name": "zsv",
      "one_line_profile": "High-performance CSV parser and CLI toolkit",
      "detailed_description": "A fast, SIMD-accelerated CSV parser library and command-line utility. It is designed for processing large tabular datasets common in scientific research, offering significant speed advantages over standard parsers.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "data_processing"
      ],
      "application_level": "solver",
      "primary_language": "C",
      "repo_url": "https://github.com/liquidaty/zsv",
      "help_website": [
        "https://zsv-lib.github.io/"
      ],
      "license": "MIT",
      "tags": [
        "csv",
        "simd",
        "high-performance",
        "cli"
      ],
      "id": 92
    },
    {
      "name": "YAJL",
      "one_line_profile": "Fast streaming JSON parsing library in C",
      "detailed_description": "A small, event-driven (SAX-style) JSON parser written in C. It is used in high-performance computing environments to parse large JSON data streams with minimal memory overhead.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "parsing",
        "io"
      ],
      "application_level": "library",
      "primary_language": "C",
      "repo_url": "https://github.com/lloyd/yajl",
      "help_website": [
        "http://lloyd.github.io/yajl/"
      ],
      "license": "ISC",
      "tags": [
        "json",
        "parser",
        "c",
        "streaming"
      ],
      "id": 93
    },
    {
      "name": "marshmallow_dataclass",
      "one_line_profile": "Automatic marshmallow schemas from dataclasses",
      "detailed_description": "A library that automates the creation of Marshmallow schemas from Python dataclasses. It simplifies data validation and serialization/deserialization pipelines in scientific Python applications.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "serialization",
        "validation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/lovasoa/marshmallow_dataclass",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "python",
        "serialization",
        "schema",
        "validation"
      ],
      "id": 94
    },
    {
      "name": "streamlit-pydantic",
      "one_line_profile": "Auto-generate Streamlit UI from Pydantic Models",
      "detailed_description": "A utility that automatically generates Streamlit user interfaces based on Pydantic data models. It is useful for rapidly building interactive dashboards and parameter configuration forms for scientific models and data analysis scripts.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "visualization",
        "ui_generation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/lukasmasuch/streamlit-pydantic",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "streamlit",
        "pydantic",
        "dashboard",
        "ui"
      ],
      "id": 95
    },
    {
      "name": "Lux",
      "one_line_profile": "Intelligent visual discovery for Pandas dataframes",
      "detailed_description": "A Python library that enhances Pandas dataframes by automatically suggesting and generating visualizations. It helps researchers quickly explore and understand trends and patterns in their data without writing complex plotting code.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "data_visualization",
        "eda"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/lux-org/lux",
      "help_website": [
        "https://lux-api.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "visualization",
        "pandas",
        "eda",
        "data-analysis"
      ],
      "id": 96
    },
    {
      "name": "Toasted Marshmallow",
      "one_line_profile": "JIT compiler for Marshmallow serialization",
      "detailed_description": "A performance optimization library for Marshmallow that generates JIT-compiled code for serialization. It significantly speeds up data processing pipelines that rely on Marshmallow for handling large volumes of scientific data.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "serialization",
        "optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/lyft/toasted-marshmallow",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "python",
        "performance",
        "serialization",
        "marshmallow"
      ],
      "id": 97
    },
    {
      "name": "dblp-parser",
      "one_line_profile": "Parser for DBLP bibliography data",
      "detailed_description": "A Python tool to parse the DBLP computer science bibliography XML dataset into structured formats. It is used in scientometrics and network analysis research to study citation graphs and publication trends.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "data_parsing",
        "scientometrics"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/macks22/dblp",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "dblp",
        "bibliography",
        "xml-parser",
        "scientometrics"
      ],
      "id": 98
    },
    {
      "name": "Arctic",
      "one_line_profile": "High performance datastore for time series and tick data",
      "detailed_description": "A high-performance time-series and tick data store built on top of MongoDB. While developed for finance, it is applicable to any scientific domain requiring efficient storage and retrieval of large-scale numerical time-series data.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "data_storage",
        "time_series"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/man-group/arctic",
      "help_website": [
        "https://arctic.readthedocs.io/"
      ],
      "license": "LGPL-2.1",
      "tags": [
        "time-series",
        "database",
        "mongodb",
        "python"
      ],
      "id": 99
    },
    {
      "name": "D-Tale",
      "one_line_profile": "Visualizer for pandas data structures",
      "detailed_description": "A tool that brings a Flask-based backend and a React frontend to visualize and analyze Pandas dataframes. It provides a GUI for data exploration, cleaning, and analysis, making it easier to interact with scientific datasets.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "data_visualization",
        "eda",
        "data_cleaning"
      ],
      "application_level": "solver",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/man-group/dtale",
      "help_website": [
        "https://github.com/man-group/dtale"
      ],
      "license": "LGPL-2.1",
      "tags": [
        "pandas",
        "visualization",
        "gui",
        "eda"
      ],
      "id": 100
    },
    {
      "name": "json_repair",
      "one_line_profile": "Repair invalid JSON from LLMs",
      "detailed_description": "A Python module designed to fix malformed JSON strings, specifically those generated by Large Language Models (LLMs). This is a critical utility in AI4S workflows where LLMs are used to extract structured data from scientific literature or experiments.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_cleaning",
        "parsing",
        "llm_utility"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/mangiucugna/json_repair",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "json",
        "llm",
        "parsing",
        "repair"
      ],
      "id": 101
    },
    {
      "name": "dataclasses-avroschema",
      "one_line_profile": "Generate Avro schemas from Python dataclasses",
      "detailed_description": "A library to generate Avro schemas from Python dataclasses and Pydantic models, and to serialize/deserialize data. Avro is a common format in large-scale scientific data processing (e.g., in bioinformatics and physics pipelines).",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "serialization",
        "schema_generation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/marcosschroh/dataclasses-avroschema",
      "help_website": [
        "https://marcosschroh.github.io/dataclasses-avroschema/"
      ],
      "license": "MIT",
      "tags": [
        "avro",
        "schema",
        "serialization",
        "python"
      ],
      "id": 102
    },
    {
      "name": "sqlparser",
      "one_line_profile": "SQL parser for querying CSV files",
      "detailed_description": "A Go library that implements a SQL parser specifically for querying CSV files. It enables researchers to use standard SQL syntax to filter and aggregate data stored in flat CSV files without loading them into a full database.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_querying",
        "parsing"
      ],
      "application_level": "library",
      "primary_language": "Go",
      "repo_url": "https://github.com/marianogappa/sqlparser",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "sql",
        "csv",
        "query",
        "parser"
      ],
      "id": 103
    },
    {
      "name": "tidypolars",
      "one_line_profile": "Tidy interface to Polars",
      "detailed_description": "A Python library that provides a syntax similar to R's Tidyverse for the Polars dataframe library. It facilitates high-performance data analysis for researchers familiar with R/dplyr conventions.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "data_analysis",
        "data_manipulation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/markfairbanks/tidypolars",
      "help_website": [
        "https://tidypolars.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "polars",
        "tidyverse",
        "data-analysis",
        "python"
      ],
      "id": 104
    },
    {
      "name": "OCT-Converter",
      "one_line_profile": "Tool for extracting raw data from proprietary Optical Coherence Tomography (OCT) file formats",
      "detailed_description": "A Python library designed to extract raw optical coherence tomography (OCT) and fundus data from proprietary file formats (e.g., .fda, .e2e, .img) used in medical imaging devices, facilitating ophthalmic research and data analysis.",
      "domains": [
        "D1",
        "Medical Imaging"
      ],
      "subtask_category": [
        "data_extraction",
        "format_conversion"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/marksgraham/OCT-Converter",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "oct",
        "medical-imaging",
        "ophthalmology",
        "file-conversion"
      ],
      "id": 105
    },
    {
      "name": "nuclei",
      "one_line_profile": "Parser, viewer, and editor for Evaluated Nuclear Structure Data (ENSDF)",
      "detailed_description": "A C++ tool designed to parse, view, and edit files in the Evaluated Nuclear Structure Data File (ENSDF) format, which is the standard format for nuclear structure and decay data in nuclear physics research.",
      "domains": [
        "D1",
        "Nuclear Physics"
      ],
      "subtask_category": [
        "data_parsing",
        "data_visualization"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/martukas/nuclei",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "ensdf",
        "nuclear-physics",
        "nuclear-structure",
        "parser"
      ],
      "id": 106
    },
    {
      "name": "FHIR-Converter",
      "one_line_profile": "Conversion utility to translate legacy healthcare data formats into FHIR",
      "detailed_description": "An open-source project that provides a conversion utility to translate legacy data formats (such as HL7v2 and C-CDA) into the Fast Healthcare Interoperability Resources (FHIR) standard, supporting medical informatics research and data interoperability.",
      "domains": [
        "D1",
        "Medical Informatics"
      ],
      "subtask_category": [
        "format_conversion",
        "data_standardization"
      ],
      "application_level": "library",
      "primary_language": "Liquid",
      "repo_url": "https://github.com/microsoft/FHIR-Converter",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fhir",
        "hl7",
        "medical-informatics",
        "healthcare-data"
      ],
      "id": 107
    },
    {
      "name": "caltech-pedestrian-dataset-converter",
      "one_line_profile": "Converter for Caltech Pedestrian Dataset to standard image formats",
      "detailed_description": "A Python utility to extract and convert the Caltech Pedestrian Dataset from its proprietary .seq video format into standard image files and labels, facilitating computer vision research and benchmarking.",
      "domains": [
        "D1",
        "Computer Vision"
      ],
      "subtask_category": [
        "data_extraction",
        "format_conversion"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/mitmul/caltech-pedestrian-dataset-converter",
      "help_website": [],
      "license": null,
      "tags": [
        "computer-vision",
        "dataset-tools",
        "pedestrian-detection"
      ],
      "id": 108
    },
    {
      "name": "Modin",
      "one_line_profile": "Scalable Pandas implementation for distributed computing",
      "detailed_description": "Modin is a library that scales Pandas workflows by changing a single line of code, utilizing Ray or Dask to distribute computation across cores or clusters for large-scale scientific data analysis.",
      "domains": [
        "D1",
        "Data/Workflow"
      ],
      "subtask_category": [
        "data_processing",
        "parallel_computing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/modin-project/modin",
      "help_website": [
        "https://modin.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "pandas",
        "distributed-computing",
        "data-science"
      ],
      "id": 109
    },
    {
      "name": "Seaborn",
      "one_line_profile": "Statistical data visualization library",
      "detailed_description": "Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics, essential for scientific data exploration.",
      "domains": [
        "Data/Workflow"
      ],
      "subtask_category": [
        "visualization",
        "statistical_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/mwaskom/seaborn",
      "help_website": [
        "https://seaborn.pydata.org/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "visualization",
        "statistics",
        "plotting"
      ],
      "id": 110
    },
    {
      "name": "itables",
      "one_line_profile": "Interactive DataTables for Python DataFrames",
      "detailed_description": "Itables renders Python DataFrames (Pandas, Polars) as interactive HTML DataTables in Jupyter notebooks, facilitating data exploration and inspection in scientific workflows.",
      "domains": [
        "Data/Workflow"
      ],
      "subtask_category": [
        "visualization",
        "data_exploration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/mwouts/itables",
      "help_website": [
        "https://mwouts.github.io/itables/"
      ],
      "license": "MIT",
      "tags": [
        "jupyter",
        "pandas",
        "visualization"
      ],
      "id": 111
    },
    {
      "name": "Pandarallel",
      "one_line_profile": "Parallel processing tool for Pandas",
      "detailed_description": "Pandarallel provides a simple interface to parallelize Pandas operations on all available CPUs, significantly speeding up data preprocessing and analysis tasks in scientific pipelines.",
      "domains": [
        "Data/Workflow"
      ],
      "subtask_category": [
        "data_processing",
        "parallel_computing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/nalepae/pandarallel",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "pandas",
        "parallelization",
        "performance"
      ],
      "id": 112
    },
    {
      "name": "Narwhals",
      "one_line_profile": "Compatibility layer for dataframe libraries",
      "detailed_description": "Narwhals is a lightweight compatibility layer that allows library maintainers to write dataframe-agnostic code, supporting Pandas, Polars, and others, facilitating the development of interoperable scientific tools.",
      "domains": [
        "Data/Workflow"
      ],
      "subtask_category": [
        "data_interoperability",
        "workflow_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/narwhals-dev/narwhals",
      "help_website": [
        "https://narwhals-dev.github.io/narwhals/"
      ],
      "license": "MIT",
      "tags": [
        "dataframe",
        "interoperability",
        "polars"
      ],
      "id": 113
    },
    {
      "name": "trdsql",
      "one_line_profile": "CLI tool to execute SQL queries on CSV/JSON/YAML",
      "detailed_description": "trdsql is a command-line tool that allows executing SQL queries directly on CSV, LTSV, JSON, and YAML files, enabling efficient data filtering, aggregation, and transformation in scientific data pipelines.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_querying",
        "data_conversion"
      ],
      "application_level": "solver",
      "primary_language": "Go",
      "repo_url": "https://github.com/noborus/trdsql",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "sql",
        "csv",
        "data-processing"
      ],
      "id": 114
    },
    {
      "name": "MinerU",
      "one_line_profile": "High-quality data extraction tool for scientific documents",
      "detailed_description": "A data processing tool designed to transform complex scientific documents (PDFs) into machine-readable formats (Markdown/JSON). It handles formulas, tables, and layout analysis, specifically enabling LLM-based scientific literature mining and agentic workflows.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "document_parsing",
        "data_extraction",
        "layout_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/opendatalab/MinerU",
      "help_website": [
        "https://github.com/opendatalab/MinerU"
      ],
      "license": "AGPL-3.0",
      "tags": [
        "pdf-parsing",
        "scientific-literature",
        "llm-data-prep"
      ],
      "id": 115
    },
    {
      "name": "Granola",
      "one_line_profile": "Serialization library for Apple HealthKit clinical data",
      "detailed_description": "A library designed to serialize Apple HealthKit data into Open mHealth compliant JSON formats. It facilitates the standardization and interoperability of clinical and personal health data for medical informatics research.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_serialization",
        "health_informatics",
        "data_standardization"
      ],
      "application_level": "library",
      "primary_language": "Objective-C",
      "repo_url": "https://github.com/openmhealth/Granola",
      "help_website": [
        "https://www.openmhealth.org/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "healthkit",
        "openmhealth",
        "clinical-data"
      ],
      "id": 116
    },
    {
      "name": "Buckaroo",
      "one_line_profile": "Interactive data exploration UI for scientific dataframes",
      "detailed_description": "A GUI tool integrated into Jupyter Notebooks for exploring, cleaning, and visualizing Pandas and Polars dataframes. It accelerates the exploratory data analysis (EDA) phase of scientific workflows by providing instant summary statistics, histograms, and filtering capabilities.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "exploratory_data_analysis",
        "data_visualization",
        "data_cleaning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/paddymul/buckaroo",
      "help_website": [
        "https://buckaroo-data.readthedocs.io/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "pandas",
        "jupyter",
        "eda",
        "visualization"
      ],
      "id": 117
    },
    {
      "name": "pandas",
      "one_line_profile": "Fundamental library for scientific data analysis and manipulation",
      "detailed_description": "The core library for data manipulation and analysis in Python, providing high-performance, easy-to-use data structures (DataFrames) and data analysis tools. It is the foundation for processing structured scientific data across all disciplines.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_manipulation",
        "statistical_analysis",
        "data_cleaning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pandas-dev/pandas",
      "help_website": [
        "https://pandas.pydata.org/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "dataframe",
        "data-analysis",
        "statistics"
      ],
      "id": 118
    },
    {
      "name": "hAMRonization",
      "one_line_profile": "Parser and harmonizer for antimicrobial resistance analysis reports",
      "detailed_description": "A bioinformatics tool designed to parse outputs from various Antimicrobial Resistance (AMR) prediction tools and harmonize them into a unified data structure. It facilitates the comparison and aggregation of AMR analysis results in microbiology research.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "bioinformatics_parsing",
        "data_harmonization",
        "antimicrobial_resistance"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/pha4ge/hAMRonization",
      "help_website": [
        "https://github.com/pha4ge/hAMRonization"
      ],
      "license": "LGPL-3.0",
      "tags": [
        "bioinformatics",
        "amr",
        "data-standardization"
      ],
      "id": 119
    },
    {
      "name": "Placemark",
      "one_line_profile": "Web-based editor and converter for geospatial data",
      "detailed_description": "A versatile tool for creating, editing, converting, and visualizing geospatial data. It supports a wide range of formats (GeoJSON, KML, CSV, etc.) and is used in Earth Science and GIS workflows for data preparation and visualization.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "geospatial_visualization",
        "data_conversion",
        "map_editing"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/placemark/placemark",
      "help_website": [
        "https://www.placemark.io/"
      ],
      "license": "MIT",
      "tags": [
        "gis",
        "geospatial",
        "geojson",
        "visualization"
      ],
      "id": 120
    },
    {
      "name": "GeoPolars",
      "one_line_profile": "Geospatial extensions for the Polars DataFrame library",
      "detailed_description": "GeoPolars extends the Polars DataFrame library with geospatial data types and operations, enabling fast processing of geographic data.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "scientific_data_processing",
        "geospatial_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pola-rs/geopolars",
      "help_website": [
        "https://github.com/pola-rs/geopolars"
      ],
      "license": "MIT",
      "tags": [
        "geospatial",
        "polars",
        "gis",
        "dataframe"
      ],
      "id": 121
    },
    {
      "name": "Polars",
      "one_line_profile": "High-performance DataFrame library for data manipulation and analysis",
      "detailed_description": "Polars is a blazingly fast DataFrames library implemented in Rust using Apache Arrow Columnar Format as the memory model. It is a key tool for scientific data processing and analysis, serving as a modern alternative to Pandas.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "scientific_data_processing",
        "data_analysis"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/pola-rs/polars",
      "help_website": [
        "https://pola.rs/"
      ],
      "license": "MIT",
      "tags": [
        "dataframe",
        "data-analysis",
        "arrow",
        "rust"
      ],
      "id": 122
    },
    {
      "name": "Polars CLI",
      "one_line_profile": "Command-line interface for running SQL queries on data files using Polars",
      "detailed_description": "A command-line tool that allows users to run SQL queries directly on CSV, Parquet, and JSON files using the Polars engine, facilitating quick scientific data inspection and processing.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "scientific_data_processing",
        "data_query"
      ],
      "application_level": "solver",
      "primary_language": "Rust",
      "repo_url": "https://github.com/pola-rs/polars-cli",
      "help_website": [
        "https://github.com/pola-rs/polars-cli"
      ],
      "license": "MIT",
      "tags": [
        "cli",
        "sql",
        "data-processing",
        "polars"
      ],
      "id": 123
    },
    {
      "name": "polars-xdt",
      "one_line_profile": "DateTime extension library for Polars",
      "detailed_description": "A plugin for Polars that provides additional datetime functionality, such as business day calculations and holiday handling, useful for time-series analysis in scientific and economic research.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "scientific_data_processing",
        "time_series_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pola-rs/polars-xdt",
      "help_website": [
        "https://github.com/pola-rs/polars-xdt"
      ],
      "license": "MIT",
      "tags": [
        "datetime",
        "polars-plugin",
        "time-series"
      ],
      "id": 124
    },
    {
      "name": "r-polars",
      "one_line_profile": "R bindings for the Polars DataFrame library",
      "detailed_description": "Provides R language bindings for Polars, enabling high-performance data manipulation and analysis within the R scientific computing ecosystem.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "scientific_data_processing",
        "data_analysis"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/pola-rs/r-polars",
      "help_website": [
        "https://rpolars.github.io/"
      ],
      "license": "MIT",
      "tags": [
        "r",
        "dataframe",
        "data-analysis",
        "polars"
      ],
      "id": 125
    },
    {
      "name": "esri2open",
      "one_line_profile": "Tool to export ESRI Feature Classes to open data formats",
      "detailed_description": "A Python toolbox that converts proprietary ESRI geospatial data formats into open standards like CSV, JSON, and GeoJSON, facilitating open science and geospatial data interoperability.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "scientific_data_processing",
        "format_conversion"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/project-open-data/esri2open",
      "help_website": [
        "https://github.com/project-open-data/esri2open"
      ],
      "license": "MIT",
      "tags": [
        "gis",
        "geospatial",
        "data-conversion",
        "esri"
      ],
      "id": 126
    },
    {
      "name": "PyNLPl",
      "one_line_profile": "Python library for Natural Language Processing and linguistic data parsing",
      "detailed_description": "PyNLPl (Pineapple) is a library for Natural Language Processing that includes parsers for specific linguistic research formats like FoLiA, Giza, and Moses, supporting computational linguistics research.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "scientific_data_processing",
        "linguistic_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/proycon/pynlpl",
      "help_website": [
        "https://pynlpl.readthedocs.io/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "nlp",
        "computational-linguistics",
        "folia",
        "parser"
      ],
      "id": 127
    },
    {
      "name": "xarray",
      "one_line_profile": "N-D labeled arrays and datasets for physical sciences",
      "detailed_description": "Xarray introduces labels in the form of dimensions, coordinates and attributes on top of raw NumPy-like arrays, making it a fundamental tool for processing multi-dimensional scientific data (e.g., climate, physics, oceanography).",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "scientific_data_processing",
        "scientific_modeling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pydata/xarray",
      "help_website": [
        "https://docs.xarray.dev/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "netcdf",
        "climate-science",
        "multi-dimensional-arrays",
        "physics"
      ],
      "id": 128
    },
    {
      "name": "windrose",
      "one_line_profile": "Python library to manage wind data and draw windroses",
      "detailed_description": "A specialized library for meteorology to analyze wind data, draw polar rose plots (windroses), and fit Weibull probability density functions.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "scientific_visualization",
        "scientific_data_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/python-windrose/windrose",
      "help_website": [
        "https://windrose.readthedocs.io/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "meteorology",
        "wind-data",
        "visualization",
        "weibull"
      ],
      "id": 129
    },
    {
      "name": "cuDF",
      "one_line_profile": "GPU-accelerated DataFrame library",
      "detailed_description": "cuDF is a GPU DataFrame library for loading, joining, aggregating, filtering, and manipulating data. It is a core component of the RAPIDS ecosystem, enabling high-performance scientific data analysis on GPUs.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "scientific_data_processing",
        "data_analysis"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/rapidsai/cudf",
      "help_website": [
        "https://docs.rapids.ai/api/cudf/stable/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "gpu",
        "dataframe",
        "cuda",
        "rapids"
      ],
      "id": 130
    },
    {
      "name": "Cufflinks",
      "one_line_profile": "Productivity tool that binds Plotly to Pandas dataframes for easy scientific visualization",
      "detailed_description": "Cufflinks connects the Pandas data analysis library with Plotly, enabling users to create interactive visualizations directly from Pandas DataFrames. It is widely used in scientific data analysis workflows to quickly generate charts for data exploration and reporting.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "visualization",
        "data_exploration"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/santosjorge/cufflinks",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "visualization",
        "pandas",
        "plotly",
        "data-analysis"
      ],
      "id": 131
    },
    {
      "name": "VisiData",
      "one_line_profile": "Terminal interface for exploring and arranging tabular data",
      "detailed_description": "VisiData is an interactive multitool for exploring, analyzing, and manipulating tabular data (CSV, Excel, JSON, HDF5, etc.) directly in the terminal. It supports filtering, summarization, and basic statistical analysis, making it a powerful tool for scientific data quality control and quick inspection.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_exploration",
        "quality_control",
        "data_processing"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/saulpw/visidata",
      "help_website": [
        "https://www.visidata.org/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "cli",
        "data-exploration",
        "csv",
        "tabular-data",
        "statistics"
      ],
      "id": 132
    },
    {
      "name": "PandasAI",
      "one_line_profile": "Generative AI capability wrapper for pandas to enable conversational data analysis",
      "detailed_description": "A Python library that integrates Large Language Models (LLMs) with pandas, allowing users to perform data analysis, manipulation, and visualization on scientific datasets (CSV, Parquet, SQL) using natural language queries.",
      "domains": [
        "D1",
        "D4"
      ],
      "subtask_category": [
        "data_analysis",
        "data_visualization",
        "natural_language_query"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/sinaptik-ai/pandas-ai",
      "help_website": [
        "https://docs.pandas-ai.com/"
      ],
      "license": "MIT",
      "tags": [
        "pandas",
        "llm",
        "data-analysis",
        "conversational-ai"
      ],
      "id": 133
    },
    {
      "name": "PyTorch Forecasting",
      "one_line_profile": "Deep learning library for time series forecasting built on PyTorch",
      "detailed_description": "A high-level library for time series forecasting with neural networks. It provides state-of-the-art models (like Temporal Fusion Transformers) and facilitates data handling for scientific time-series tasks.",
      "domains": [
        "D4",
        "M1"
      ],
      "subtask_category": [
        "time_series_forecasting",
        "modeling",
        "deep_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/sktime/pytorch-forecasting",
      "help_website": [
        "https://pytorch-forecasting.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "pytorch",
        "time-series",
        "forecasting",
        "deep-learning"
      ],
      "id": 134
    },
    {
      "name": "pdbx",
      "one_line_profile": "Python parser for Protein Data Bank (PDB) mmCIF format files",
      "detailed_description": "A specialized parser module developed by the Soeding Lab for handling macromolecular structure data in the PDBx/mmCIF format, essential for structural biology and bioinformatics workflows.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_parsing",
        "structure_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/soedinglab/pdbx",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "pdb",
        "mmcif",
        "protein-structure",
        "bioinformatics"
      ],
      "id": 135
    },
    {
      "name": "GarminDB",
      "one_line_profile": "Parser and database manager for Garmin/FitBit health and physiological data",
      "detailed_description": "A tool that parses binary FIT files, TCX, and other proprietary formats from health wearables (Garmin, Fitbit) into a SQLite database for physiological data analysis and visualization.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_parsing",
        "physiological_data_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/tcgoetz/GarminDB",
      "help_website": [],
      "license": "GPL-2.0",
      "tags": [
        "health-informatics",
        "wearables",
        "fit-format",
        "sqlite",
        "quantified-self"
      ],
      "id": 136
    },
    {
      "name": "vroom",
      "one_line_profile": "High-performance delimited file parser for R",
      "detailed_description": "A fast data reading library for R that indexes delimited files (CSV, TSV) for rapid access, widely used in bioinformatics and data science workflows for handling large datasets.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_loading",
        "parsing"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/tidyverse/vroom",
      "help_website": [
        "https://vroom.r-lib.org"
      ],
      "license": "NOASSERTION",
      "tags": [
        "csv-parser",
        "r-package",
        "high-performance",
        "data-science"
      ],
      "id": 137
    },
    {
      "name": "pubmed_parser",
      "one_line_profile": "Parser for PubMed Open-Access XML and MEDLINE XML datasets",
      "detailed_description": "A Python library specifically designed to parse PubMed and MEDLINE XML datasets, enabling bibliometric analysis, text mining, and meta-science research on biomedical literature.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "literature_mining",
        "metadata_extraction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/titipata/pubmed_parser",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pubmed",
        "medline",
        "xml-parser",
        "nlp",
        "bibliometrics"
      ],
      "id": 138
    },
    {
      "name": "gpxpy",
      "one_line_profile": "GPX file parser and manipulator",
      "detailed_description": "A Python library for parsing and manipulating GPX (GPS Exchange Format) files, commonly used in geospatial analysis, ecology (tracking), and sports science.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "geospatial_parsing",
        "trajectory_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/tkrajina/gpxpy",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "gpx",
        "gps",
        "geospatial",
        "xml-parser"
      ],
      "id": 139
    },
    {
      "name": "OSM2World",
      "one_line_profile": "Converter creating 3D models from OpenStreetMap data",
      "detailed_description": "A tool that parses OpenStreetMap (OSM) data and converts it into three-dimensional models, used for urban simulation, geospatial visualization, and cartography.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "3d_reconstruction",
        "geospatial_visualization"
      ],
      "application_level": "solver",
      "primary_language": "Java",
      "repo_url": "https://github.com/tordanik/OSM2World",
      "help_website": [
        "http://osm2world.org/"
      ],
      "license": "LGPL-3.0",
      "tags": [
        "openstreetmap",
        "3d-modeling",
        "geospatial",
        "visualization"
      ],
      "id": 140
    },
    {
      "name": "Pandera",
      "one_line_profile": "Statistical data validation and testing library for pandas dataframes",
      "detailed_description": "Pandera provides a flexible and expressive API for performing statistical validation on dataframe-like objects. It is widely used in scientific data pipelines to ensure data quality, verify schema consistency, and perform hypothesis testing on tabular data structures.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "quality_control",
        "data_validation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/unionai-oss/pandera",
      "help_website": [
        "https://pandera.readthedocs.io"
      ],
      "license": "MIT",
      "tags": [
        "data-validation",
        "pandas",
        "quality-control",
        "schema-validation"
      ],
      "id": 141
    },
    {
      "name": "BioBear",
      "one_line_profile": "Bioinformatics file processing using Arrow and Polars",
      "detailed_description": "BioBear is a library that enables reading and processing of standard bioinformatics file formats (FASTA, FASTQ, VCF, BAM, GFF) directly into Polars DataFrames or Arrow tables. It facilitates high-performance data analysis workflows for genomics and biological data.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_parsing",
        "format_conversion"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/wheretrue/biobear",
      "help_website": [
        "https://www.wheretrue.com/biobear"
      ],
      "license": "MIT",
      "tags": [
        "bioinformatics",
        "genomics",
        "polars",
        "arrow",
        "fastq",
        "vcf"
      ],
      "id": 142
    },
    {
      "name": "csvkit",
      "one_line_profile": "Command-line suite for converting and processing CSV data",
      "detailed_description": "csvkit is a suite of command-line tools for converting to and working with CSV, the common tabular file format in scientific research. It allows researchers to inspect, filter, slice, join, and analyze tabular data directly from the terminal, facilitating reproducible data cleaning workflows.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_cleaning",
        "format_conversion",
        "data_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/wireservice/csvkit",
      "help_website": [
        "https://csvkit.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "csv",
        "data-cleaning",
        "cli",
        "tabular-data"
      ],
      "id": 143
    },
    {
      "name": "ydata-profiling",
      "one_line_profile": "Automated exploratory data analysis and quality profiling tool for Pandas and Spark DataFrames",
      "detailed_description": "A primary tool for exploratory data analysis (EDA) in scientific data workflows. It generates comprehensive profile reports from dataframes, providing statistical insights, correlation analysis, missing value assessment, and distribution visualization, which are essential for scientific data quality control.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "quality_control",
        "exploratory_data_analysis",
        "statistical_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ydataai/ydata-profiling",
      "help_website": [
        "https://docs.profiling.ydata.ai/latest/"
      ],
      "license": "MIT",
      "tags": [
        "eda",
        "data-profiling",
        "quality-control",
        "pandas",
        "statistics"
      ],
      "id": 144
    },
    {
      "name": "Dataset_to_VOC_converter",
      "one_line_profile": "Scripts to convert computer vision datasets (Caltech, COCO, HDA) to PASCAL VOC format",
      "detailed_description": "A set of utility scripts designed to convert various standard computer vision datasets (Caltech pedestrian, MS COCO, HDA) into the PASCAL VOC XML format. This facilitates the normalization of data inputs for object detection model training in scientific research.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_conversion",
        "dataset_preparation",
        "normalization"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/zongfan2/Dataset_to_VOC_converter",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "computer-vision",
        "dataset-conversion",
        "pascal-voc",
        "coco",
        "object-detection"
      ],
      "id": 145
    },
    {
      "name": "Jupyter Dock",
      "one_line_profile": "Interactive molecular docking protocols and visualization workflow",
      "detailed_description": "A set of Jupyter Notebooks serving as a workflow tool for performing molecular docking protocols, including file format conversion, visualization, and result analysis.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "molecular_docking",
        "visualization",
        "format_conversion"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/AngelRuizMoreno/Jupyter_Dock",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "molecular-docking",
        "bioinformatics",
        "visualization"
      ],
      "id": 146
    },
    {
      "name": "ngio",
      "one_line_profile": "Streamlined OME-Zarr image analysis workflow library",
      "detailed_description": "A Python library designed to streamline OME-Zarr image analysis workflows, facilitating the handling of next-generation bioimaging formats.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "image_analysis",
        "bioimaging"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/BioVisionCenter/ngio",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "ome-zarr",
        "bioimaging",
        "microscopy"
      ],
      "id": 147
    },
    {
      "name": "Rhtslib",
      "one_line_profile": "HTSlib high-throughput sequencing library for R",
      "detailed_description": "An R package providing the HTSlib C library for high-throughput sequencing data processing, enabling access to SAM/BAM/CRAM/VCF files within R.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "sequencing_io",
        "bioinformatics"
      ],
      "application_level": "library",
      "primary_language": "C",
      "repo_url": "https://github.com/Bioconductor/Rhtslib",
      "help_website": [
        "https://bioconductor.org/packages/Rhtslib"
      ],
      "license": "LGPL-2.0",
      "tags": [
        "htslib",
        "bioconductor",
        "genomics"
      ],
      "id": 148
    },
    {
      "name": "b2h5py",
      "one_line_profile": "Optimized Blosc2 reading for h5py",
      "detailed_description": "A library enabling transparent and optimized reading of n-dimensional Blosc2 slices within h5py, enhancing performance for HDF5 data access.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_compression",
        "io_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Blosc/b2h5py",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "hdf5",
        "blosc2",
        "compression"
      ],
      "id": 149
    },
    {
      "name": "grib22json",
      "one_line_profile": "GRIB2 binary data decoder for JavaScript",
      "detailed_description": "A Javascript decoder for parsing GRIB2 binary meteorological data, enabling web-based visualization and processing of weather data.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "meteorology_io",
        "data_decoding"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/BlueNetCat/grib22json",
      "help_website": [],
      "license": null,
      "tags": [
        "grib2",
        "meteorology",
        "weather-data"
      ],
      "id": 150
    },
    {
      "name": "BrkRaw",
      "one_line_profile": "Tool to access raw Bruker Biospin MRI data",
      "detailed_description": "A comprehensive tool designed to access and process raw MRI data from Bruker Biospin systems, facilitating medical imaging research.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "mri_processing",
        "medical_imaging"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/BrkRaw/brkraw",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "mri",
        "bruker",
        "medical-imaging"
      ],
      "id": 151
    },
    {
      "name": "EleFits",
      "one_line_profile": "Modern C++ API for FITS files",
      "detailed_description": "A modern C++ API built on top of CFitsIO, developed by CNES, to facilitate reading and writing of FITS files in astronomical software.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "astronomy_io",
        "fits_handling"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/CNES/EleFits",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "fits",
        "astronomy",
        "cnes"
      ],
      "id": 152
    },
    {
      "name": "zodipy",
      "one_line_profile": "Zodiacal light simulation package",
      "detailed_description": "An Astropy-affiliated Python package for simulating zodiacal light emission, used in cosmological and astronomical data analysis.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "simulation",
        "astronomy"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Cosmoglobe/zodipy",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "astronomy",
        "simulation",
        "zodiacal-light"
      ],
      "id": 153
    },
    {
      "name": "dkist",
      "one_line_profile": "DKIST solar telescope data tools",
      "detailed_description": "A Python library for obtaining, processing, and interacting with calibrated data from the Daniel K. Inouye Solar Telescope (DKIST).",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "solar_physics",
        "data_access"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/DKISTDC/dkist",
      "help_website": [
        "https://docs.dkist.nso.edu/projects/python-tools/en/stable/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "solar-physics",
        "astronomy",
        "dkist"
      ],
      "id": 154
    },
    {
      "name": "geobipy",
      "one_line_profile": "Geophysical Bayesian Inference in Python",
      "detailed_description": "A Python package for performing Bayesian inference on geophysical data, developed by the USGS.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "geophysics",
        "bayesian_inference"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/DOI-USGS/geobipy",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "geophysics",
        "inference",
        "usgs"
      ],
      "id": 155
    },
    {
      "name": "h5pickle",
      "one_line_profile": "Pickle wrapper for h5py",
      "detailed_description": "A wrapper for h5py that adds pickling capabilities, facilitating the serialization of HDF5 file handles in parallel processing workflows.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_serialization",
        "parallel_computing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/DaanVanVugt/h5pickle",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "hdf5",
        "pickle",
        "python"
      ],
      "id": 156
    },
    {
      "name": "netcdf_to_gltf_converter",
      "one_line_profile": "NetCDF to glTF converter for D-HYDRO",
      "detailed_description": "A tool developed by Deltares to convert D-HYDRO output NetCDF data into the glTF format for 3D visualization.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "format_conversion",
        "visualization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Deltares-research/netcdf_to_gltf_converter",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "netcdf",
        "gltf",
        "hydrology"
      ],
      "id": 157
    },
    {
      "name": "BIDScoin",
      "one_line_profile": "Neuroimaging to BIDS converter",
      "detailed_description": "A tool that converts source-level neuroimaging data to the Brain Imaging Data Structure (BIDS) standard, facilitating data sharing and analysis.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "format_conversion",
        "neuroimaging"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/Donders-Institute/bidscoin",
      "help_website": [
        "https://bidscoin.readthedocs.io"
      ],
      "license": "GPL-3.0",
      "tags": [
        "bids",
        "neuroimaging",
        "mri"
      ],
      "id": 158
    },
    {
      "name": "ebvcube",
      "one_line_profile": "EBV NetCDF dataset access tool",
      "detailed_description": "An R package for accessing, visualizing, and creating Essential Biodiversity Variables (EBV) NetCDF datasets from the EBV Data Portal.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "biodiversity_informatics",
        "data_access"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/EBVcube/ebvcube",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "biodiversity",
        "netcdf",
        "ebv"
      ],
      "id": 159
    },
    {
      "name": "Bio-DB-HTS",
      "one_line_profile": "Perl interface to HTSlib",
      "detailed_description": "A Perl module providing bindings to the HTSlib library, enabling high-performance processing of high-throughput sequencing data formats.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "sequencing_io",
        "bioinformatics"
      ],
      "application_level": "library",
      "primary_language": "Perl",
      "repo_url": "https://github.com/Ensembl/Bio-DB-HTS",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "htslib",
        "perl",
        "genomics"
      ],
      "id": 160
    },
    {
      "name": "pypx",
      "one_line_profile": "Python PACS interaction wrapper",
      "detailed_description": "A Python wrapper based on DCMTK and PyDicom to interact with Picture Archiving and Communication Systems (PACS) for medical imaging data management.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "medical_imaging",
        "pacs_interface"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/FNNDSC/pypx",
      "help_website": [],
      "license": null,
      "tags": [
        "dicom",
        "pacs",
        "medical-imaging"
      ],
      "id": 161
    },
    {
      "name": "sat-extractor",
      "one_line_profile": "Satellite imagery extraction tool",
      "detailed_description": "A tool to extract satellite imagery from public constellations at scale, facilitating the creation of datasets for earth observation research.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "remote_sensing",
        "data_extraction"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/FrontierDevelopmentLab/sat-extractor",
      "help_website": [],
      "license": "BSD-2-Clause",
      "tags": [
        "satellite-imagery",
        "remote-sensing",
        "earth-observation"
      ],
      "id": 162
    },
    {
      "name": "vcf-js",
      "one_line_profile": "JavaScript VCF parser",
      "detailed_description": "A high-performance Variant Call Format (VCF) parser written in pure JavaScript, enabling client-side genomic data processing.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "genomics_io",
        "variant_analysis"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/GMOD/vcf-js",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "vcf",
        "genomics",
        "javascript"
      ],
      "id": 163
    },
    {
      "name": "go-dicom-parser",
      "one_line_profile": "Efficient DICOM parser in Go",
      "detailed_description": "A lightweight and efficient library for parsing and processing DICOM medical imaging files, written in the Go programming language.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "medical_imaging",
        "dicom_parsing"
      ],
      "application_level": "library",
      "primary_language": "Go",
      "repo_url": "https://github.com/GoogleCloudPlatform/go-dicom-parser",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "dicom",
        "go",
        "medical-imaging"
      ],
      "id": 164
    },
    {
      "name": "h5pyd",
      "one_line_profile": "Python client for HDF REST API",
      "detailed_description": "A Python client library for the HDF REST API (HDF Server), enabling distributed access to HDF5 data in the cloud.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "cloud_storage",
        "data_access"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/HDFGroup/h5pyd",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "hdf5",
        "cloud",
        "rest-api"
      ],
      "id": 165
    },
    {
      "name": "CFITSIO",
      "one_line_profile": "Standard library for FITS file manipulation",
      "detailed_description": "The standard C and Fortran library for reading and writing FITS (Flexible Image Transport System) data files, widely used in astronomy.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "astronomy_io",
        "fits_handling"
      ],
      "application_level": "library",
      "primary_language": "C",
      "repo_url": "https://github.com/HEASARC/cfitsio",
      "help_website": [
        "https://heasarc.gsfc.nasa.gov/fitsio/"
      ],
      "license": null,
      "tags": [
        "fits",
        "astronomy",
        "nasa"
      ],
      "id": 166
    },
    {
      "name": "digital-elevation-model",
      "one_line_profile": "DEM transformation and visualization tool",
      "detailed_description": "A Python tool to transform, project, visualize, and read Digital Elevation Models (DEM) such as ASTER GDEM and EU-DEM.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "geospatial_analysis",
        "dem_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/HeZhang1994/digital-elevation-model",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "dem",
        "geospatial",
        "visualization"
      ],
      "id": 167
    },
    {
      "name": "Rarr",
      "one_line_profile": "Native R reader for Zarr arrays",
      "detailed_description": "A simple native R package for reading Zarr arrays, enabling efficient access to chunked, compressed N-dimensional arrays in R.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_io",
        "bioinformatics"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/Huber-group-EMBL/Rarr",
      "help_website": [
        "https://bioconductor.org/packages/Rarr"
      ],
      "license": "NOASSERTION",
      "tags": [
        "zarr",
        "r",
        "bioconductor"
      ],
      "id": 168
    },
    {
      "name": "paragraph",
      "one_line_profile": "Graph realignment for structural variants",
      "detailed_description": "A graph realignment tool designed for accurate genotyping of structural variants in genomic data, developed by Illumina.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "variant_calling",
        "genomics"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/Illumina/paragraph",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "genomics",
        "structural-variants",
        "graph-alignment"
      ],
      "id": 169
    },
    {
      "name": "dicomweb-client",
      "one_line_profile": "Python client for DICOMweb services",
      "detailed_description": "A Python client library for interacting with DICOMweb RESTful services, facilitating the exchange of medical imaging data over the web.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "medical_imaging",
        "web_services"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ImagingDataCommons/dicomweb-client",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "dicomweb",
        "medical-imaging",
        "rest-api"
      ],
      "id": 170
    },
    {
      "name": "highdicom",
      "one_line_profile": "High-level DICOM abstractions for Python",
      "detailed_description": "A Python library providing high-level abstractions for creating and handling complex DICOM objects, such as Structured Reports and Segmentation.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "medical_imaging",
        "dicom_creation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ImagingDataCommons/highdicom",
      "help_website": [
        "https://highdicom.readthedocs.io"
      ],
      "license": "MIT",
      "tags": [
        "dicom",
        "medical-imaging",
        "python"
      ],
      "id": 171
    },
    {
      "name": "OiTools",
      "one_line_profile": "Java library for OIFITS files",
      "detailed_description": "A Java library dedicated to reading and writing OIFITS (Optical Interferometry FITS) files, the standard format for optical interferometry data.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "astronomy_io",
        "interferometry"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/JMMC-OpenDev/oitools",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "oifits",
        "interferometry",
        "astronomy"
      ],
      "id": 172
    },
    {
      "name": "AstroImages.jl",
      "one_line_profile": "Astronomical image visualization in Julia",
      "detailed_description": "A Julia package for the visualization of astronomical images, providing tools to handle FITS files and display them with appropriate scaling.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "astronomy_visualization",
        "image_processing"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/JuliaAstro/AstroImages.jl",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "astronomy",
        "julia",
        "visualization"
      ],
      "id": 173
    },
    {
      "name": "CFITSIO.jl",
      "one_line_profile": "C-style interface to the libcfitsio library for Julia",
      "detailed_description": "A Julia wrapper for the CFITSIO library, providing low-level access to FITS (Flexible Image Transport System) files, widely used in astronomy.",
      "domains": [
        "D1",
        "D1-02",
        "Astronomy"
      ],
      "subtask_category": [
        "io",
        "data_access"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/JuliaAstro/CFITSIO.jl",
      "help_website": [
        "https://juliaastro.github.io/CFITSIO.jl/stable/"
      ],
      "license": "MIT",
      "tags": [
        "fits",
        "astronomy",
        "io",
        "julia-wrapper"
      ],
      "id": 174
    },
    {
      "name": "FITSIO.jl",
      "one_line_profile": "Flexible Image Transport System (FITS) file support for Julia",
      "detailed_description": "A high-level Julia library for reading and writing FITS files, providing a more Julian interface compared to the low-level CFITSIO wrapper.",
      "domains": [
        "D1",
        "D1-02",
        "Astronomy"
      ],
      "subtask_category": [
        "io",
        "data_manipulation"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/JuliaAstro/FITSIO.jl",
      "help_website": [
        "https://juliaastro.github.io/FITSIO.jl/stable/"
      ],
      "license": "MIT",
      "tags": [
        "fits",
        "astronomy",
        "io",
        "images"
      ],
      "id": 175
    },
    {
      "name": "GeoIO.jl",
      "one_line_profile": "Geospatial data IO compatible with GeoStats.jl",
      "detailed_description": "A Julia library for loading and saving geospatial data, designed to integrate seamlessly with the GeoStats.jl framework for geostatistical analysis.",
      "domains": [
        "D1",
        "D1-02",
        "Earth Science"
      ],
      "subtask_category": [
        "io",
        "data_loading"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/JuliaEarth/GeoIO.jl",
      "help_website": [
        "https://juliaearth.github.io/GeoIO.jl/stable/"
      ],
      "license": "MIT",
      "tags": [
        "geospatial",
        "io",
        "geostats",
        "gis"
      ],
      "id": 176
    },
    {
      "name": "GDAL.jl",
      "one_line_profile": "Julia wrapper for the Geospatial Data Abstraction Library (GDAL)",
      "detailed_description": "A thin Julia wrapper around the GDAL library, enabling reading and writing of a vast number of raster and vector geospatial data formats.",
      "domains": [
        "D1",
        "D1-02",
        "Earth Science"
      ],
      "subtask_category": [
        "io",
        "format_conversion",
        "data_transformation"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/JuliaGeo/GDAL.jl",
      "help_website": [
        "https://juliageo.org/GDAL.jl/stable/"
      ],
      "license": "MIT",
      "tags": [
        "gdal",
        "geospatial",
        "raster",
        "vector",
        "gis"
      ],
      "id": 177
    },
    {
      "name": "GeoFormatTypes.jl",
      "one_line_profile": "Wrapper types for spatial data formats in Julia",
      "detailed_description": "Provides Julia type definitions for various spatial data formats (WKT, KML, Proj4, etc.) to facilitate interoperability between geospatial libraries.",
      "domains": [
        "D1",
        "D1-02",
        "Earth Science"
      ],
      "subtask_category": [
        "data_modeling",
        "interoperability"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/JuliaGeo/GeoFormatTypes.jl",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "geospatial",
        "types",
        "wkt",
        "kml"
      ],
      "id": 178
    },
    {
      "name": "GeoJSON.jl",
      "one_line_profile": "Utilities for working with GeoJSON data in Julia",
      "detailed_description": "A Julia library for parsing and generating GeoJSON, a format for encoding a variety of geographic data structures.",
      "domains": [
        "D1",
        "D1-02",
        "Earth Science"
      ],
      "subtask_category": [
        "io",
        "parsing"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/JuliaGeo/GeoJSON.jl",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "geojson",
        "geospatial",
        "json",
        "io"
      ],
      "id": 179
    },
    {
      "name": "GeoParquet.jl",
      "one_line_profile": "Julia support for Geospatial Parquet files",
      "detailed_description": "A library for reading and writing GeoParquet files in Julia, enabling efficient storage and retrieval of geospatial data in the Parquet format.",
      "domains": [
        "D1",
        "D1-02",
        "Earth Science"
      ],
      "subtask_category": [
        "io",
        "storage"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/JuliaGeo/GeoParquet.jl",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "geoparquet",
        "parquet",
        "geospatial",
        "io"
      ],
      "id": 180
    },
    {
      "name": "iCGIS",
      "one_line_profile": "Simple GIS program based on Qt and GDAL",
      "detailed_description": "A lightweight Geographic Information System (GIS) desktop application for visualizing and processing geospatial data, built with C++, Qt, and GDAL.",
      "domains": [
        "D1",
        "Earth Science"
      ],
      "subtask_category": [
        "visualization",
        "data_processing"
      ],
      "application_level": "application",
      "primary_language": "C++",
      "repo_url": "https://github.com/Leopard-C/iCGIS",
      "help_website": [],
      "license": null,
      "tags": [
        "gis",
        "qt",
        "gdal",
        "visualization"
      ],
      "id": 181
    },
    {
      "name": "mwalib",
      "one_line_profile": "Library to read Murchison Widefield Array (MWA) data",
      "detailed_description": "A Rust library to read raw visibilities, voltages, and metadata from the Murchison Widefield Array (MWA) radio telescope into a common structure.",
      "domains": [
        "D1",
        "D1-02",
        "Astronomy"
      ],
      "subtask_category": [
        "io",
        "data_access"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/MWATelescope/mwalib",
      "help_website": [
        "https://docs.rs/mwalib/"
      ],
      "license": "MPL-2.0",
      "tags": [
        "radio-astronomy",
        "mwa",
        "io",
        "rust"
      ],
      "id": 182
    },
    {
      "name": "MapServer",
      "one_line_profile": "Open source platform for publishing spatial data to the web",
      "detailed_description": "A major open-source platform for rendering geospatial data and creating map applications for the web, supporting numerous standard data formats.",
      "domains": [
        "D1",
        "Earth Science"
      ],
      "subtask_category": [
        "visualization",
        "data_serving"
      ],
      "application_level": "platform",
      "primary_language": "C",
      "repo_url": "https://github.com/MapServer/MapServer",
      "help_website": [
        "https://mapserver.org/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "gis",
        "mapping",
        "web-map-server",
        "geospatial"
      ],
      "id": 183
    },
    {
      "name": "NEFFy",
      "one_line_profile": "NEFF Calculator and MSA File Converter",
      "detailed_description": "A tool to calculate the Number of Effective Sequences (NEFF) and convert between Multiple Sequence Alignment (MSA) file formats.",
      "domains": [
        "D1",
        "D1-02",
        "Bioinformatics"
      ],
      "subtask_category": [
        "format_conversion",
        "data_analysis"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/Maryam-Haghani/NEFFy",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "msa",
        "bioinformatics",
        "converter",
        "neff"
      ],
      "id": 184
    },
    {
      "name": "gdal.netcore",
      "one_line_profile": "GDAL bindings for .NET applications",
      "detailed_description": "Provides C# and F# bindings for the GDAL library, enabling .NET applications to read and write geospatial raster and vector data formats.",
      "domains": [
        "D1",
        "D1-02",
        "Earth Science"
      ],
      "subtask_category": [
        "io",
        "data_access"
      ],
      "application_level": "library",
      "primary_language": "PowerShell",
      "repo_url": "https://github.com/MaxRev-Dev/gdal.netcore",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gdal",
        "dotnet",
        "csharp",
        "geospatial"
      ],
      "id": 185
    },
    {
      "name": "WOSS",
      "one_line_profile": "World Ocean Simulation System for underwater acoustic channels",
      "detailed_description": "A framework integrating underwater channel simulators (like Bellhop) with network simulators (ns-3) to provide realistic underwater acoustic channel realizations based on environmental data.",
      "domains": [
        "Physics",
        "Oceanography"
      ],
      "subtask_category": [
        "simulation",
        "modeling"
      ],
      "application_level": "workflow",
      "primary_language": "C++",
      "repo_url": "https://github.com/MetalKnight/woss-ns3",
      "help_website": [
        "http://woss.dei.unipd.it/"
      ],
      "license": null,
      "tags": [
        "underwater-acoustics",
        "simulation",
        "ns-3",
        "bellhop"
      ],
      "id": 186
    },
    {
      "name": "pyMeteo",
      "one_line_profile": "Python library for processing meteorological data",
      "detailed_description": "A collection of Python utilities for handling and processing meteorological datasets, likely including format handling and domain-specific calculations.",
      "domains": [
        "D1",
        "Atmospheric Science"
      ],
      "subtask_category": [
        "data_processing",
        "analysis"
      ],
      "application_level": "library",
      "primary_language": null,
      "repo_url": "https://github.com/Mo-Dabao/pyMeteo",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "meteorology",
        "weather-data",
        "python"
      ],
      "id": 187
    },
    {
      "name": "PyActiveStorage",
      "one_line_profile": "Python implementation of Active Storage for scientific data",
      "detailed_description": "A library implementing Active Storage concepts to push data reduction tasks (like filtering and aggregation) down to the storage layer, optimized for scientific formats like NetCDF/HDF5.",
      "domains": [
        "D1",
        "D1-02",
        "Computer Science"
      ],
      "subtask_category": [
        "io_optimization",
        "data_reduction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NCAS-CMS/PyActiveStorage",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "active-storage",
        "netcdf",
        "hdf5",
        "io-optimization"
      ],
      "id": 188
    },
    {
      "name": "cfdm",
      "one_line_profile": "Python reference implementation of the CF data model",
      "detailed_description": "A complete Python implementation of the Climate and Forecast (CF) data model, used for representing and manipulating earth science data structures and metadata.",
      "domains": [
        "D1",
        "D1-02",
        "Atmospheric Science"
      ],
      "subtask_category": [
        "data_modeling",
        "metadata_handling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NCAS-CMS/cfdm",
      "help_website": [
        "https://ncas-cms.github.io/cfdm/"
      ],
      "license": "MIT",
      "tags": [
        "cf-conventions",
        "climate-data",
        "netcdf",
        "data-model"
      ],
      "id": 189
    },
    {
      "name": "ZarrDAP",
      "one_line_profile": "OPeNDAP interface for Zarr and NetCDF data in object storage",
      "detailed_description": "A FastAPI-based server that provides OPeNDAP access to Zarr and NetCDF datasets stored in remote object storage (S3), facilitating remote scientific data access.",
      "domains": [
        "D1",
        "D1-02",
        "Earth Science"
      ],
      "subtask_category": [
        "data_serving",
        "data_access"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/NCEI-NOAAGov/zarrdap",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "opendap",
        "zarr",
        "netcdf",
        "s3",
        "data-access"
      ],
      "id": 190
    },
    {
      "name": "dicomtk",
      "one_line_profile": "DICOM Toolkit for parsing and exporting medical images",
      "detailed_description": "A Python library that parses DICOM files into an SQLite database for metadata management and supports exporting data to other formats.",
      "domains": [
        "D1",
        "D1-02",
        "Medical Physics"
      ],
      "subtask_category": [
        "io",
        "parsing",
        "metadata_extraction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NKI-AI/dicomtk",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "dicom",
        "medical-imaging",
        "sqlite",
        "parser"
      ],
      "id": 191
    },
    {
      "name": "NCEPLIBS-g2c",
      "one_line_profile": "C decoder/encoder routines for GRIB2 format",
      "detailed_description": "A library containing C routines for decoding and encoding GRIB edition 2 (GRIB2) messages, a standard format for meteorological data.",
      "domains": [
        "D1",
        "D1-02",
        "Atmospheric Science"
      ],
      "subtask_category": [
        "io",
        "encoding",
        "decoding"
      ],
      "application_level": "library",
      "primary_language": "C",
      "repo_url": "https://github.com/NOAA-EMC/NCEPLIBS-g2c",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "grib2",
        "meteorology",
        "noaa",
        "decoder"
      ],
      "id": 192
    },
    {
      "name": "zarr-cesium",
      "one_line_profile": "CesiumJS providers for Zarr data visualization",
      "detailed_description": "A library enabling interactive 2D and 3D visualization of environmental and geospatial data stored in Zarr format within the CesiumJS globe platform.",
      "domains": [
        "D1",
        "Earth Science"
      ],
      "subtask_category": [
        "visualization",
        "web_mapping"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/NOC-OI/zarr-cesium",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "cesiumjs",
        "zarr",
        "visualization",
        "geospatial"
      ],
      "id": 193
    },
    {
      "name": "zarr-vis",
      "one_line_profile": "Browser-based visualization for Zarr multidimensional data",
      "detailed_description": "A JavaScript library for fast, scalable visualization of Zarr-based multidimensional scientific data directly in web browsers.",
      "domains": [
        "D1",
        "Earth Science"
      ],
      "subtask_category": [
        "visualization",
        "data_exploration"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/NOC-OI/zarr-vis",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "zarr",
        "visualization",
        "web",
        "multidimensional-data"
      ],
      "id": 194
    },
    {
      "name": "EDA-Parsers",
      "one_line_profile": "Parsers for EDA standard file formats",
      "detailed_description": "A collection of C++ parsers for standard Electronic Design Automation (EDA) file formats such as Verilog, Liberty, SPEF, VCD, SDF, and SDC.",
      "domains": [
        "D1",
        "D1-02",
        "Electronic Engineering"
      ],
      "subtask_category": [
        "parsing",
        "io"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/OSCC-Project/EDA-Parsers",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "eda",
        "verilog",
        "parser",
        "vcd"
      ],
      "id": 195
    },
    {
      "name": "GDAL",
      "one_line_profile": "Geospatial Data Abstraction Library",
      "detailed_description": "The industry-standard translator library for raster and vector geospatial data formats, providing a single abstract data model for a multitude of formats.",
      "domains": [
        "D1",
        "D1-02",
        "Earth Science"
      ],
      "subtask_category": [
        "io",
        "format_conversion",
        "data_processing"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/OSGeo/gdal",
      "help_website": [
        "https://gdal.org/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "geospatial",
        "gis",
        "raster",
        "vector",
        "conversion"
      ],
      "id": 196
    },
    {
      "name": "echopype",
      "one_line_profile": "Interoperable ocean sonar data analysis in Python",
      "detailed_description": "A Python library built on xarray and zarr to standardize and analyze ocean sonar data, converting proprietary formats into open standards.",
      "domains": [
        "D1",
        "D1-02",
        "Oceanography"
      ],
      "subtask_category": [
        "data_conversion",
        "analysis",
        "standardization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/OSOceanAcoustics/echopype",
      "help_website": [
        "https://echopype.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "sonar",
        "oceanography",
        "netcdf",
        "zarr",
        "conversion"
      ],
      "id": 197
    },
    {
      "name": "PVGeo-HDF5",
      "one_line_profile": "HDF5 and NetCDF4 support for PVGeo",
      "detailed_description": "An extension of the PVGeo library to support reading and visualizing HDF5 and NetCDF4 data formats within the PyVista/ParaView ecosystem.",
      "domains": [
        "D1",
        "D1-02",
        "Geophysics"
      ],
      "subtask_category": [
        "visualization",
        "io"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenGeoVis/PVGeo-HDF5",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "hdf5",
        "netcdf",
        "visualization",
        "paraview",
        "geophysics"
      ],
      "id": 198
    },
    {
      "name": "mesogeos",
      "one_line_profile": "Dataset and models for wildfire modeling in the Mediterranean",
      "detailed_description": "A repository containing a dataset and Deep Learning models for wildfire danger forecasting and burned area prediction in the Mediterranean region.",
      "domains": [
        "Earth Science",
        "Environmental Science"
      ],
      "subtask_category": [
        "modeling",
        "prediction"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/Orion-AI-Lab/mesogeos",
      "help_website": [],
      "license": null,
      "tags": [
        "wildfire",
        "deep-learning",
        "dataset",
        "modeling"
      ],
      "id": 199
    },
    {
      "name": "xclim",
      "one_line_profile": "Library of derived climate variables and indicators",
      "detailed_description": "A Python library based on xarray for calculating climate indicators and derived variables from climate model output and observational data.",
      "domains": [
        "Atmospheric Science",
        "Climate Science"
      ],
      "subtask_category": [
        "data_analysis",
        "calculation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Ouranosinc/xclim",
      "help_website": [
        "https://xclim.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "climate-indicators",
        "xarray",
        "climate-change",
        "analysis"
      ],
      "id": 200
    },
    {
      "name": "PDAL",
      "one_line_profile": "Point Data Abstraction Library",
      "detailed_description": "A C++ library for translating and manipulating point cloud data, functioning as a counterpart to GDAL but for point data (LiDAR, etc.).",
      "domains": [
        "D1",
        "D1-02",
        "Earth Science"
      ],
      "subtask_category": [
        "io",
        "processing",
        "format_conversion"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/PDAL/PDAL",
      "help_website": [
        "https://pdal.io/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "point-cloud",
        "lidar",
        "geospatial",
        "processing"
      ],
      "id": 201
    },
    {
      "name": "E3SM-IO",
      "one_line_profile": "I/O Kernel Benchmark for E3SM",
      "detailed_description": "A benchmark suite designed to evaluate the I/O performance of the Energy Exascale Earth System Model (E3SM), specifically targeting Parallel NetCDF and HDF5 usage.",
      "domains": [
        "D1",
        "Computer Science",
        "Earth Science"
      ],
      "subtask_category": [
        "benchmarking",
        "io_optimization"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/Parallel-NetCDF/E3SM-IO",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "hpc",
        "benchmark",
        "io",
        "e3sm",
        "parallel-netcdf"
      ],
      "id": 202
    },
    {
      "name": "Kaplan",
      "one_line_profile": "Conformer searching package",
      "detailed_description": "A package for searching molecular conformers, useful in computational chemistry and molecular modeling.",
      "domains": [
        "Chemistry"
      ],
      "subtask_category": [
        "molecular_modeling",
        "conformer_search"
      ],
      "application_level": "solver",
      "primary_language": "TeX",
      "repo_url": "https://github.com/PeaWagon/Kaplan",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "chemistry",
        "conformer-search",
        "molecular-modeling"
      ],
      "id": 203
    },
    {
      "name": "vcf-parser",
      "one_line_profile": "Strict streaming parser for VCF 4.1/4.2",
      "detailed_description": "A Java library for strictly parsing Variant Call Format (VCF) files in a streaming manner, ensuring adherence to format specifications.",
      "domains": [
        "D1",
        "D1-02",
        "Bioinformatics"
      ],
      "subtask_category": [
        "parsing",
        "io"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/PharmGKB/vcf-parser",
      "help_website": [],
      "license": "MPL-2.0",
      "tags": [
        "vcf",
        "bioinformatics",
        "parser",
        "genomics"
      ],
      "id": 204
    },
    {
      "name": "PACKMAN",
      "one_line_profile": "Python package for protein structure and dynamics",
      "detailed_description": "A Python package for structural biology analysis, providing tools for handling protein structures and analyzing their dynamics.",
      "domains": [
        "Biology",
        "Structural Biology"
      ],
      "subtask_category": [
        "structure_analysis",
        "dynamics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Pranavkhade/PACKMAN",
      "help_website": [
        "https://py-packman.readthedocs.io/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "protein-structure",
        "bioinformatics",
        "dynamics",
        "structural-biology"
      ],
      "id": 205
    },
    {
      "name": "medio",
      "one_line_profile": "Medical images I/O python package",
      "detailed_description": "A Python package for reading and writing medical images, likely wrapping lower-level libraries like ITK or pydicom for easier access.",
      "domains": [
        "D1",
        "D1-02",
        "Medical Physics"
      ],
      "subtask_category": [
        "io",
        "data_loading"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/RSIP-Vision/medio",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "medical-imaging",
        "io",
        "python"
      ],
      "id": 206
    },
    {
      "name": "UAVProduct",
      "one_line_profile": "UAV image processing and point cloud generation tool",
      "detailed_description": "A C++ tool for processing UAV imagery, implementing geometric correction, image stitching, and point cloud generation using OpenMVG, OpenMVS, and GDAL.",
      "domains": [
        "Remote Sensing",
        "Photogrammetry"
      ],
      "subtask_category": [
        "image_processing",
        "3d_reconstruction"
      ],
      "application_level": "application",
      "primary_language": "C++",
      "repo_url": "https://github.com/RemoteSensingFrank/UAVProduct",
      "help_website": [],
      "license": null,
      "tags": [
        "uav",
        "photogrammetry",
        "point-cloud",
        "gdal"
      ],
      "id": 207
    },
    {
      "name": "ESIO",
      "one_line_profile": "ExaScale IO library for turbulence simulations",
      "detailed_description": "A library providing simple, high-throughput input and output of structured datasets using parallel HDF5, designed for turbulence simulation restart files.",
      "domains": [
        "D1",
        "Physics",
        "Fluid Dynamics"
      ],
      "subtask_category": [
        "io",
        "hpc"
      ],
      "application_level": "library",
      "primary_language": "C",
      "repo_url": "https://github.com/RhysU/ESIO",
      "help_website": [],
      "license": "LGPL-2.1",
      "tags": [
        "hdf5",
        "parallel-io",
        "turbulence",
        "cfd"
      ],
      "id": 208
    },
    {
      "name": "swiftsimio",
      "one_line_profile": "Python library for reading SWIFT simulation data",
      "detailed_description": "A Python library for reading and visualizing data produced by the SWIFT astrophysical simulation code, utilizing unyt for unit handling.",
      "domains": [
        "D1",
        "Astronomy",
        "Astrophysics"
      ],
      "subtask_category": [
        "io",
        "visualization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/SWIFTSIM/swiftsimio",
      "help_website": [
        "https://swiftsimio.readthedocs.io/"
      ],
      "license": "LGPL-3.0",
      "tags": [
        "astrophysics",
        "simulation-data",
        "swift",
        "io"
      ],
      "id": 209
    },
    {
      "name": "variantconvert",
      "one_line_profile": "Customizable genetic variants file format converter",
      "detailed_description": "A Python tool for converting between different genetic variant file formats, facilitating interoperability in bioinformatics workflows.",
      "domains": [
        "D1",
        "D1-02",
        "Bioinformatics"
      ],
      "subtask_category": [
        "format_conversion",
        "data_processing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/SamuelNicaise/variantconvert",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "bioinformatics",
        "variants",
        "converter",
        "genomics"
      ],
      "id": 210
    },
    {
      "name": "Sen2Agri-System",
      "one_line_profile": "Sentinel-2 for Agriculture processing system",
      "detailed_description": "A software system for processing high-resolution satellite images (Sentinel-2) for agricultural monitoring and analysis.",
      "domains": [
        "Remote Sensing",
        "Agriculture"
      ],
      "subtask_category": [
        "image_processing",
        "monitoring"
      ],
      "application_level": "platform",
      "primary_language": "HTML",
      "repo_url": "https://github.com/Sen2Agri/Sen2Agri-System",
      "help_website": [
        "http://www.sen2agri.org/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "sentinel-2",
        "agriculture",
        "remote-sensing",
        "processing"
      ],
      "id": 211
    },
    {
      "name": "fastDFE",
      "one_line_profile": "Inference of distribution of fitness effects",
      "detailed_description": "A Python package for fast and flexible inference of the distribution of fitness effects (DFE) from genomic data, including VCF parsing.",
      "domains": [
        "Biology",
        "Population Genetics"
      ],
      "subtask_category": [
        "inference",
        "analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Sendrowski/fastDFE",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "population-genetics",
        "inference",
        "vcf",
        "dfe"
      ],
      "id": 212
    },
    {
      "name": "vcflib",
      "one_line_profile": "Python library for parsing and manipulation of VCF",
      "detailed_description": "An open-source Python library designed for parsing and manipulating Variant Call Format (VCF) files used in bioinformatics.",
      "domains": [
        "D1",
        "D1-02",
        "Bioinformatics"
      ],
      "subtask_category": [
        "parsing",
        "data_manipulation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Sentieon/vcflib",
      "help_website": [],
      "license": "BSD-2-Clause",
      "tags": [
        "vcf",
        "bioinformatics",
        "parser",
        "python"
      ],
      "id": 213
    },
    {
      "name": "hdfdict",
      "one_line_profile": "Dump and load Python dictionaries to HDF5",
      "detailed_description": "A utility library that simplifies the process of saving and loading Python dictionaries to/from HDF5 files using h5py.",
      "domains": [
        "D1",
        "D1-02",
        "Computer Science"
      ],
      "subtask_category": [
        "io",
        "serialization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/SiggiGue/hdfdict",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "hdf5",
        "python",
        "dictionary",
        "serialization"
      ],
      "id": 214
    },
    {
      "name": "Solcast/netcdf-tiff",
      "one_line_profile": "Utility library for converting NetCDF files to GeoTIFF format",
      "detailed_description": "A Python library designed to convert scientific data stored in NetCDF format into GeoTIFF, facilitating integration with GIS workflows.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "format_conversion",
        "gis_interoperability"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Solcast/netcdf-tiff",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "netcdf",
        "geotiff",
        "converter",
        "gis"
      ],
      "id": 215
    },
    {
      "name": "cheminformatics-microservice",
      "one_line_profile": "Microservices for cheminformatics data processing and analysis",
      "detailed_description": "A set of microservices accessible via API to support cheminformatics tasks, providing modular tools for chemical data handling.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "cheminformatics",
        "data_processing"
      ],
      "application_level": "service",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/Steinbeck-Lab/cheminformatics-microservice",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "cheminformatics",
        "microservice",
        "api"
      ],
      "id": 216
    },
    {
      "name": "AstroBinUploader",
      "one_line_profile": "Parser for astronomical image headers to generate acquisition summaries",
      "detailed_description": "A Python script that parses FITS or XISF file headers to extract acquisition session information and formats it for AstroBin uploads.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "metadata_extraction",
        "data_formatting"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/SteveGreaves/AstroBinUploader",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "astronomy",
        "fits",
        "xisf",
        "metadata"
      ],
      "id": 217
    },
    {
      "name": "mdio-python",
      "one_line_profile": "Python interface for MDIO scalable energy data storage engine",
      "detailed_description": "A cloud-native, scalable storage engine library designed for handling large-scale energy and seismic data in the MDIO format.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_storage",
        "io_library"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/TGSAI/mdio-python",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "seismic-data",
        "energy-data",
        "storage",
        "mdio"
      ],
      "id": 218
    },
    {
      "name": "zarrdataset",
      "one_line_profile": "PyTorch dataset loader for Zarr files in ML pipelines",
      "detailed_description": "A utility library to load data from Zarr files directly into machine learning training pipelines, facilitating the use of large scientific arrays in deep learning.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_loading",
        "machine_learning_io"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/TheJacksonLaboratory/zarrdataset",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "zarr",
        "pytorch",
        "data-loader",
        "bioinformatics"
      ],
      "id": 219
    },
    {
      "name": "OpenAthena-Legacy-Python",
      "one_line_profile": "Geodetic location spotting tool for drones",
      "detailed_description": "A software tool that allows common drones to spot precise geodetic locations using digital elevation models and telemetry data.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "geodesy",
        "spatial_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Theta-Limited/OpenAthena-Legacy-Python",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "drones",
        "geodesy",
        "gis",
        "location-spotting"
      ],
      "id": 220
    },
    {
      "name": "Fiona",
      "one_line_profile": "Python library for reading and writing geographic data files",
      "detailed_description": "Fiona is a Python wrapper around OGR, providing a clean and Pythonic interface for reading and writing vector data formats (GIS).",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_io",
        "gis_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Toblerity/Fiona",
      "help_website": [
        "https://fiona.readthedocs.io/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "gis",
        "vector-data",
        "ogr",
        "geospatial"
      ],
      "id": 221
    },
    {
      "name": "cftime",
      "one_line_profile": "Time-handling library for NetCDF and climate data",
      "detailed_description": "A library providing time-handling functionality for non-standard calendars commonly used in climate modeling and NetCDF files.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "time_processing",
        "climate_modeling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Unidata/cftime",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "netcdf",
        "climate",
        "calendars",
        "time-series"
      ],
      "id": 222
    },
    {
      "name": "GEMPAK",
      "one_line_profile": "Meteorological data analysis and product generation package",
      "detailed_description": "A comprehensive analysis and product generation package for meteorological data, widely used for weather forecasting and research.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "meteorology",
        "data_analysis",
        "visualization"
      ],
      "application_level": "platform",
      "primary_language": "C",
      "repo_url": "https://github.com/Unidata/gempak",
      "help_website": [
        "https://github.com/Unidata/gempak"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "meteorology",
        "weather-analysis",
        "gempak"
      ],
      "id": 223
    },
    {
      "name": "netcdf4-python",
      "one_line_profile": "Python interface to the netCDF C library",
      "detailed_description": "The standard Python interface for the netCDF C library, allowing creation, access, and sharing of array-oriented scientific data.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_io",
        "array_processing"
      ],
      "application_level": "library",
      "primary_language": "Cython",
      "repo_url": "https://github.com/Unidata/netcdf4-python",
      "help_website": [
        "http://unidata.github.io/netcdf4-python/"
      ],
      "license": "MIT",
      "tags": [
        "netcdf",
        "hdf5",
        "climate-data",
        "oceanography"
      ],
      "id": 224
    },
    {
      "name": "hyfo",
      "one_line_profile": "Hydrology and Climate Forecasting R package",
      "detailed_description": "An R package designed for hydrology and climate forecasting, focusing on data processing and visualization of NetCDF and other formats.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "hydrology",
        "climate_forecasting",
        "data_processing"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/Yuanchao-Xu/hyfo",
      "help_website": [],
      "license": null,
      "tags": [
        "hydrology",
        "climate",
        "netcdf",
        "forecasting"
      ],
      "id": 225
    },
    {
      "name": "vcfpp",
      "one_line_profile": "C++ API wrapper for htslib VCF parsing",
      "detailed_description": "A C++ API for htslib designed to be easily integrated and safely used for parsing VCF (Variant Call Format) files in bioinformatics.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "genomics",
        "variant_calling",
        "data_io"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/Zilong-Li/vcfpp",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "vcf",
        "bioinformatics",
        "htslib",
        "cpp"
      ],
      "id": 226
    },
    {
      "name": "vcfppR",
      "one_line_profile": "High-performance VCF/BCF parser for R",
      "detailed_description": "An R package providing a fast interface for parsing VCF and BCF files, leveraging the vcfpp C++ library.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "genomics",
        "data_io"
      ],
      "application_level": "library",
      "primary_language": "C",
      "repo_url": "https://github.com/Zilong-Li/vcfppR",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "vcf",
        "r-package",
        "bioinformatics",
        "genomics"
      ],
      "id": 227
    },
    {
      "name": "pdf2dcm",
      "one_line_profile": "Python package for converting PDF to DICOM",
      "detailed_description": "A utility library to convert PDF documents into DICOM format, facilitating the integration of reports into medical imaging systems.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "format_conversion",
        "medical_imaging"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/a-parida12/pdf2dcm",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "dicom",
        "pdf",
        "medical-imaging",
        "converter"
      ],
      "id": 228
    },
    {
      "name": "bboxfinder.com",
      "one_line_profile": "Web tool for generating bounding box coordinates for GIS",
      "detailed_description": "A web-based utility that helps users find and generate bounding box (bbox) values from a map for use with geospatial tools like GDAL and Leaflet.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "spatial_analysis",
        "utility"
      ],
      "application_level": "solver",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/aaronr/bboxfinder.com",
      "help_website": [
        "http://bboxfinder.com/"
      ],
      "license": null,
      "tags": [
        "gis",
        "bounding-box",
        "coordinates",
        "utility"
      ],
      "id": 229
    },
    {
      "name": "actinia-core",
      "one_line_profile": "REST API for distributed GRASS GIS processing",
      "detailed_description": "An open-source REST API for scalable, distributed, and high-performance processing of geographical data, primarily using GRASS GIS.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "gis_processing",
        "remote_sensing"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/actinia-org/actinia-core",
      "help_website": [
        "https://actinia-org.github.io/actinia-core/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "grass-gis",
        "rest-api",
        "geospatial",
        "cloud-processing"
      ],
      "id": 230
    },
    {
      "name": "lunarsky",
      "one_line_profile": "Astropy extension for lunar surface observations",
      "detailed_description": "An extension to the Astropy library that provides functionality to describe and calculate astronomical observations from the surface of the Moon.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "astronomy",
        "coordinate_systems"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aelanman/lunarsky",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "astronomy",
        "moon",
        "astropy",
        "coordinates"
      ],
      "id": 231
    },
    {
      "name": "gmshparser",
      "one_line_profile": "Parser for Gmsh ASCII file format (.msh)",
      "detailed_description": "A lightweight Python package to parse the Gmsh ASCII file format (.msh), useful for finite element analysis (FEM) workflows.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "mesh_processing",
        "fem"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ahojukka5/gmshparser",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gmsh",
        "fem",
        "mesh",
        "parser"
      ],
      "id": 232
    },
    {
      "name": "aignostics-python-sdk",
      "one_line_profile": "SDK for Aignostics pathology AI platform",
      "detailed_description": "A Python SDK providing access to the Aignostics Platform for computational pathology, including tools for data interaction and analysis.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "pathology",
        "medical_ai",
        "data_access"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aignostics/python-sdk",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pathology",
        "ai",
        "sdk",
        "medical-imaging"
      ],
      "id": 233
    },
    {
      "name": "godal",
      "one_line_profile": "Golang wrapper for GDAL",
      "detailed_description": "A Go programming language wrapper for the GDAL library, enabling geospatial data abstraction and manipulation in Go applications.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "gis_io",
        "data_processing"
      ],
      "application_level": "library",
      "primary_language": "Go",
      "repo_url": "https://github.com/airbusgeo/godal",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "gdal",
        "go",
        "gis",
        "geospatial"
      ],
      "id": 234
    },
    {
      "name": "LinaQA",
      "one_line_profile": "Medical physics toolkit for radiotherapy QA",
      "detailed_description": "A toolkit for medical physics tasks in radiotherapy, diagnostic radiology, and nuclear medicine, built on top of pylinac and pydicom.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "medical_physics",
        "quality_assurance",
        "radiotherapy"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/alanphys/LinaQA",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "medical-physics",
        "radiotherapy",
        "dicom",
        "qa"
      ],
      "id": 235
    },
    {
      "name": "broh5",
      "one_line_profile": "Browser-based GUI HDF5 Viewer",
      "detailed_description": "A browser-based graphical user interface for viewing and inspecting HDF5 files, facilitating data exploration without writing code.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "visualization",
        "data_inspection"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/algotom/broh5",
      "help_website": [
        "https://broh5.readthedocs.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "hdf5",
        "viewer",
        "gui",
        "visualization"
      ],
      "id": 236
    },
    {
      "name": "s2orc-doc2json",
      "one_line_profile": "Parsers for converting scientific papers to structured JSON",
      "detailed_description": "A set of parsers (PDF2JSON, TEX2JSON, JATS2JSON) to convert scientific documents into structured JSON format for text mining and analysis.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "text_mining",
        "document_parsing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/allenai/s2orc-doc2json",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "pdf-parsing",
        "scientific-literature",
        "nlp",
        "json"
      ],
      "id": 237
    },
    {
      "name": "science-parse",
      "one_line_profile": "Parser for extracting structured data from scientific PDFs",
      "detailed_description": "A Java-based tool that parses scientific papers in PDF format and extracts structured information such as titles, authors, and references.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "text_mining",
        "document_parsing"
      ],
      "application_level": "solver",
      "primary_language": "Java",
      "repo_url": "https://github.com/allenai/science-parse",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "pdf-parsing",
        "scientific-literature",
        "metadata-extraction"
      ],
      "id": 238
    },
    {
      "name": "go-eccodes",
      "one_line_profile": "Go wrapper for ecCodes (GRIB/BUFR decoding)",
      "detailed_description": "A Go wrapper for the ecCodes library, enabling the decoding and encoding of meteorological data formats like GRIB and BUFR in Go applications.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "meteorology",
        "data_io"
      ],
      "application_level": "library",
      "primary_language": "C",
      "repo_url": "https://github.com/amsokol/go-eccodes",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "grib",
        "bufr",
        "meteorology",
        "eccodes"
      ],
      "id": 239
    },
    {
      "name": "gdalcubes",
      "one_line_profile": "Tool for creating and analyzing Earth observation data cubes",
      "detailed_description": "A library and tool for processing collections of Earth observation images as regular data cubes, facilitating temporal and spatial analysis.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "remote_sensing",
        "spatiotemporal_analysis"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/appelmar/gdalcubes",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "earth-observation",
        "data-cubes",
        "remote-sensing",
        "gdal"
      ],
      "id": 240
    },
    {
      "name": "picovcf",
      "one_line_profile": "Fast single-header C++ library for VCF parsing",
      "detailed_description": "A lightweight, high-performance C++ library for parsing VCF (Variant Call Format) files with low memory overhead.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "genomics",
        "data_io"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/aprilweilab/picovcf",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "vcf",
        "bioinformatics",
        "cpp",
        "parsing"
      ],
      "id": 241
    },
    {
      "name": "ASDF",
      "one_line_profile": "Advanced Scientific Data Format library",
      "detailed_description": "The reference implementation for ASDF (Advanced Scientific Data Format), a next-generation interchange format for scientific data.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_io",
        "format_specification"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/asdf-format/asdf",
      "help_website": [
        "https://asdf.readthedocs.io/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "asdf",
        "file-format",
        "scientific-data",
        "io"
      ],
      "id": 242
    },
    {
      "name": "fit-decoder",
      "one_line_profile": "JavaScript library for parsing ANT/Garmin .FIT files",
      "detailed_description": "A JavaScript library designed to parse .FIT files generated by Garmin and ANT+ devices, commonly used in sports science and physiology.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "sports_science",
        "data_io"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/ask77nl/fit-decoder",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fit-format",
        "garmin",
        "sports-data",
        "parsing"
      ],
      "id": 243
    },
    {
      "name": "fitsjs",
      "one_line_profile": "JavaScript library for reading FITS astronomical files",
      "detailed_description": "A JavaScript library for reading FITS (Flexible Image Transport System) files in the browser or Node.js, supporting images, data cubes, and tables.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "astronomy",
        "data_io",
        "web_visualization"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/astrojs/fitsjs",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fits",
        "astronomy",
        "javascript",
        "visualization"
      ],
      "id": 244
    },
    {
      "name": "galfit-python-parser",
      "one_line_profile": "Parser for GALFIT output FITS files",
      "detailed_description": "A Python utility to parse the output FITS files generated by GALFIT (galaxy fitting software) and extract fit information.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "astronomy",
        "data_parsing",
        "galaxy_fitting"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/astronomeralex/galfit-python-parser",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "galfit",
        "astronomy",
        "fits",
        "parser"
      ],
      "id": 245
    },
    {
      "name": "asdf-astropy",
      "one_line_profile": "ASDF extension for Astropy data structures",
      "detailed_description": "An extension library that provides support for serializing and deserializing Astropy data structures using the ASDF standard.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "astronomy",
        "data_io",
        "serialization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/astropy/asdf-astropy",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "asdf",
        "astropy",
        "serialization",
        "astronomy"
      ],
      "id": 246
    },
    {
      "name": "Astropy",
      "one_line_profile": "Core library for Astronomy and Astrophysics",
      "detailed_description": "The core Python package for Astronomy, providing common tools such as unit conversions, coordinate systems, and FITS file handling.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "astronomy",
        "data_analysis",
        "data_io"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/astropy/astropy",
      "help_website": [
        "https://www.astropy.org/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "astronomy",
        "fits",
        "coordinates",
        "units"
      ],
      "id": 247
    },
    {
      "name": "astropy-healpix",
      "one_line_profile": "HEALPix implementation for Astropy",
      "detailed_description": "A BSD-licensed implementation of HEALPix (Hierarchical Equal Area isoLatitude Pixelization) for use with the Astropy ecosystem.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "astronomy",
        "spatial_indexing",
        "pixelization"
      ],
      "application_level": "library",
      "primary_language": "C",
      "repo_url": "https://github.com/astropy/astropy-healpix",
      "help_website": [
        "https://astropy-healpix.readthedocs.io/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "healpix",
        "astronomy",
        "sky-map",
        "pixelization"
      ],
      "id": 248
    },
    {
      "name": "astroquery",
      "one_line_profile": "Functions and classes to access online astronomical data resources",
      "detailed_description": "A package that provides a set of tools for querying online astronomical forms and databases such as SIMBAD, VizieR, and others.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_acquisition",
        "database_query"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/astropy/astroquery",
      "help_website": [
        "https://astroquery.readthedocs.io"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "astronomy",
        "data-access",
        "query"
      ],
      "id": 249
    },
    {
      "name": "astrowidgets",
      "one_line_profile": "Jupyter widgets leveraging the Astropy ecosystem for visualization",
      "detailed_description": "A set of Jupyter widgets for interactive visualization and analysis of astronomical data, designed to work with the Astropy ecosystem.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "visualization",
        "interactive_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/astropy/astrowidgets",
      "help_website": [
        "https://astrowidgets.readthedocs.io"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "astronomy",
        "visualization",
        "jupyter-widgets"
      ],
      "id": 250
    },
    {
      "name": "ccdproc",
      "one_line_profile": "Astropy affiliated package for reducing optical/IR CCD data",
      "detailed_description": "A library for basic data reduction of CCD images, providing tools for bias subtraction, flat fielding, and cosmic ray rejection.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_processing",
        "image_reduction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/astropy/ccdproc",
      "help_website": [
        "https://ccdproc.readthedocs.io"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "astronomy",
        "ccd",
        "image-processing"
      ],
      "id": 251
    },
    {
      "name": "photutils",
      "one_line_profile": "Astropy package for source detection and photometry",
      "detailed_description": "A package for performing photometry of astronomical sources, including aperture photometry and PSF-fitting photometry.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_analysis",
        "photometry"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/astropy/photutils",
      "help_website": [
        "https://photutils.readthedocs.io"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "astronomy",
        "photometry",
        "source-detection"
      ],
      "id": 252
    },
    {
      "name": "pyregion",
      "one_line_profile": "DS9 region parser for Python",
      "detailed_description": "A python module to parse ds9 region files, allowing users to read and visualize region files used in astronomical image analysis.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_parsing",
        "visualization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/astropy/pyregion",
      "help_website": [
        "https://pyregion.readthedocs.io"
      ],
      "license": "MIT",
      "tags": [
        "astronomy",
        "ds9",
        "region-files"
      ],
      "id": 253
    },
    {
      "name": "pyvo",
      "one_line_profile": "Access to remote data and services of the Virtual Observatory (VO)",
      "detailed_description": "A package providing access to remote data and services of the Virtual Observatory (VO) using Python, enabling discovery and retrieval of astronomical data.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_acquisition",
        "vo_services"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/astropy/pyvo",
      "help_website": [
        "https://pyvo.readthedocs.io"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "astronomy",
        "virtual-observatory",
        "data-access"
      ],
      "id": 254
    },
    {
      "name": "regions",
      "one_line_profile": "Astropy package for region handling",
      "detailed_description": "A package for handling regions in astronomical images, providing tools to define, manipulate, and visualize regions.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_processing",
        "region_handling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/astropy/regions",
      "help_website": [
        "https://astropy-regions.readthedocs.io"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "astronomy",
        "regions",
        "spatial"
      ],
      "id": 255
    },
    {
      "name": "specreduce",
      "one_line_profile": "Tools for the reduction of spectroscopic observations",
      "detailed_description": "A package providing tools for the reduction of spectroscopic observations from optical and NIR instruments.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_processing",
        "spectroscopy_reduction"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/astropy/specreduce",
      "help_website": [
        "https://specreduce.readthedocs.io"
      ],
      "license": null,
      "tags": [
        "astronomy",
        "spectroscopy",
        "reduction"
      ],
      "id": 256
    },
    {
      "name": "specutils",
      "one_line_profile": "An Astropy coordinated package for astronomical spectroscopy",
      "detailed_description": "A package for representing, manipulating, and analyzing astronomical spectroscopic data.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_analysis",
        "spectroscopy"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/astropy/specutils",
      "help_website": [
        "https://specutils.readthedocs.io"
      ],
      "license": null,
      "tags": [
        "astronomy",
        "spectroscopy",
        "analysis"
      ],
      "id": 257
    },
    {
      "name": "loam",
      "one_line_profile": "Javascript wrapper for GDAL in the browser",
      "detailed_description": "A Javascript wrapper for GDAL (Geospatial Data Abstraction Library) running in the browser via Emscripten, enabling geospatial data processing on the client side.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_processing",
        "format_conversion"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/azavea/loam",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "geospatial",
        "gdal",
        "javascript"
      ],
      "id": 258
    },
    {
      "name": "go-native-netcdf",
      "one_line_profile": "A native Go implementation of NetCDF4",
      "detailed_description": "A library providing a native Go implementation for reading and writing NetCDF4 files, a common format in atmospheric and oceanographic science.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_io",
        "format_parsing"
      ],
      "application_level": "library",
      "primary_language": "Go",
      "repo_url": "https://github.com/batchatco/go-native-netcdf",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "netcdf",
        "go",
        "io"
      ],
      "id": 259
    },
    {
      "name": "jzarr",
      "one_line_profile": "Java implementation of the Zarr API",
      "detailed_description": "A Java implementation of the Zarr chunked, compressed, N-dimensional array storage format, compatible with the Python zarr package.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_io",
        "format_parsing"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/bcdev/jzarr",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "zarr",
        "java",
        "io"
      ],
      "id": 260
    },
    {
      "name": "zappend",
      "one_line_profile": "Robustly creating and updating Zarr data cubes",
      "detailed_description": "A tool for creating and updating Zarr data cubes from smaller subsets, useful for processing large scientific datasets.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_processing",
        "data_management"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/bcdev/zappend",
      "help_website": [
        "https://zappend.readthedocs.io"
      ],
      "license": "MIT",
      "tags": [
        "zarr",
        "data-cubes",
        "python"
      ],
      "id": 261
    },
    {
      "name": "bioconvert",
      "one_line_profile": "Facilitate the interconversion of life science data formats",
      "detailed_description": "A collaborative project providing tools to convert between various life science data formats, streamlining bioinformatics workflows.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "format_conversion",
        "data_processing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/bioconvert/bioconvert",
      "help_website": [
        "http://bioconvert.readthedocs.io"
      ],
      "license": "GPL-3.0",
      "tags": [
        "bioinformatics",
        "format-conversion",
        "data-processing"
      ],
      "id": 262
    },
    {
      "name": "geodot-plugin",
      "one_line_profile": "Godot plugin for loading geospatial data",
      "detailed_description": "A plugin for the Godot game engine that allows loading and visualizing geospatial data, useful for scientific visualization and simulation.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "visualization",
        "simulation"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/boku-ilen/geodot-plugin",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "geospatial",
        "godot",
        "visualization"
      ],
      "id": 263
    },
    {
      "name": "cyvcf2",
      "one_line_profile": "Fast VCF and BCF processing with Cython and htslib",
      "detailed_description": "A high-performance Python wrapper around htslib for parsing and manipulating VCF (Variant Call Format) and BCF files.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_parsing",
        "variant_processing"
      ],
      "application_level": "library",
      "primary_language": "Cython",
      "repo_url": "https://github.com/brentp/cyvcf2",
      "help_website": [
        "https://brentp.github.io/cyvcf2/"
      ],
      "license": "MIT",
      "tags": [
        "bioinformatics",
        "vcf",
        "cython"
      ],
      "id": 264
    },
    {
      "name": "hts-nim",
      "one_line_profile": "Nim wrapper for htslib for parsing genomics data files",
      "detailed_description": "A Nim language wrapper for htslib, enabling high-performance parsing of genomics data formats like BAM, VCF, and BCF.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_parsing",
        "genomics"
      ],
      "application_level": "library",
      "primary_language": "Nim",
      "repo_url": "https://github.com/brentp/hts-nim",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bioinformatics",
        "nim",
        "htslib"
      ],
      "id": 265
    },
    {
      "name": "hts-zig",
      "one_line_profile": "Zig bindings for htslib",
      "detailed_description": "Zig language bindings for htslib, allowing the processing of high-throughput sequencing data in Zig.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_parsing",
        "genomics"
      ],
      "application_level": "library",
      "primary_language": "Zig",
      "repo_url": "https://github.com/brentp/hts-zig",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bioinformatics",
        "zig",
        "htslib"
      ],
      "id": 266
    },
    {
      "name": "gdal3.js",
      "one_line_profile": "Convert raster and vector geospatial data in the browser",
      "detailed_description": "A library to convert raster and vector geospatial data to various formats and coordinate systems entirely in the browser using GDAL.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "format_conversion",
        "geospatial_processing"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/bugra9/gdal3.js",
      "help_website": [],
      "license": "LGPL-2.1",
      "tags": [
        "geospatial",
        "gdal",
        "javascript"
      ],
      "id": 267
    },
    {
      "name": "CaVEMan",
      "one_line_profile": "SNV expectation maximisation based mutation calling algorithm",
      "detailed_description": "A somatic mutation caller for detecting single nucleotide variants (SNVs) in paired tumour/normal cancer samples, supporting BAM and CRAM formats.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "variant_calling",
        "cancer_genomics"
      ],
      "application_level": "solver",
      "primary_language": "C",
      "repo_url": "https://github.com/cancerit/CaVEMan",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "bioinformatics",
        "mutation-calling",
        "cancer"
      ],
      "id": 268
    },
    {
      "name": "cgpBigWig",
      "one_line_profile": "BigWig manipulation tools using libBigWig and htslib",
      "detailed_description": "A set of tools for manipulating BigWig files, including conversion and processing, utilizing libBigWig and htslib.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_processing",
        "genomics"
      ],
      "application_level": "solver",
      "primary_language": "C",
      "repo_url": "https://github.com/cancerit/cgpBigWig",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "bioinformatics",
        "bigwig",
        "genomics"
      ],
      "id": 269
    },
    {
      "name": "ndpyramid",
      "one_line_profile": "Utility for generating ND array pyramids using Xarray and Zarr",
      "detailed_description": "A tool for creating multiscale pyramids of N-dimensional arrays, facilitating efficient visualization and analysis of large scientific datasets.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_processing",
        "visualization_prep"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/carbonplan/ndpyramid",
      "help_website": [
        "https://ndpyramid.readthedocs.io"
      ],
      "license": "MIT",
      "tags": [
        "zarr",
        "xarray",
        "pyramids"
      ],
      "id": 270
    },
    {
      "name": "zarr-gl",
      "one_line_profile": "Custom WebGL Zarr layer for Mapbox and Maplibre",
      "detailed_description": "A WebGL layer for rendering Zarr data on Mapbox and Maplibre maps, enabling interactive visualization of large geospatial datasets.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "visualization",
        "geospatial"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/carderne/zarr-gl",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "zarr",
        "webgl",
        "geospatial"
      ],
      "id": 271
    },
    {
      "name": "vawk",
      "one_line_profile": "Awk-like VCF parser for bioinformatics data",
      "detailed_description": "A command-line tool designed to parse and process VCF (Variant Call Format) files using an awk-like syntax, facilitating filtering and data extraction in bioinformatics workflows.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "parsing",
        "filtering"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/cc2qe/vawk",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "vcf",
        "bioinformatics",
        "genomics",
        "parser"
      ],
      "id": 272
    },
    {
      "name": "fitsrs",
      "one_line_profile": "Pure Rust FITS file reader library",
      "detailed_description": "A library implemented in Rust for reading FITS (Flexible Image Transport System) files, commonly used in astronomy for data storage and transmission.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "parsing",
        "io"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/cds-astro/fitsrs",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "fits",
        "astronomy",
        "rust",
        "io"
      ],
      "id": 273
    },
    {
      "name": "imagecodecs",
      "one_line_profile": "Image transformation and compression codecs for scientific imaging",
      "detailed_description": "A Python library providing block-oriented, in-memory buffer transformation, compression, and decompression functions for use with tifffile and other scientific imaging tools.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "compression",
        "decompression",
        "io"
      ],
      "application_level": "library",
      "primary_language": "Cython",
      "repo_url": "https://github.com/cgohlke/imagecodecs",
      "help_website": [
        "https://pypi.org/project/imagecodecs/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "imaging",
        "compression",
        "codecs",
        "tiff"
      ],
      "id": 274
    },
    {
      "name": "tifffile",
      "one_line_profile": "Reader and writer for scientific TIFF files",
      "detailed_description": "A Python library to read and write TIFF files, specifically designed to handle the complex and multidimensional TIFF formats used in bioimaging and microscopy.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "io",
        "parsing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/cgohlke/tifffile",
      "help_website": [
        "https://pypi.org/project/tifffile/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "tiff",
        "microscopy",
        "bioimaging",
        "io"
      ],
      "id": 275
    },
    {
      "name": "cod-tools",
      "one_line_profile": "Tools for handling Crystallographic Information Framework (CIF) files",
      "detailed_description": "A collection of tools and parsers for handling CIF files, developed for and used by the Crystallography Open Database (COD) for data validation and management.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "parsing",
        "validation",
        "io"
      ],
      "application_level": "library",
      "primary_language": "Perl",
      "repo_url": "https://github.com/cod-developers/cod-tools",
      "help_website": [
        "https://wiki.crystallography.net/cod-tools/"
      ],
      "license": "LGPL-3.0",
      "tags": [
        "cif",
        "crystallography",
        "parsing",
        "cod"
      ],
      "id": 276
    },
    {
      "name": "rio-tiler",
      "one_line_profile": "Rasterio plugin to read and tile raster datasets",
      "detailed_description": "A Python library designed to read and process raster datasets (like Cloud Optimized GeoTIFFs) and create map tiles for web visualization, widely used in geospatial workflows.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "io",
        "tiling",
        "visualization_prep"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/cogeotiff/rio-tiler",
      "help_website": [
        "https://cogeotiff.github.io/rio-tiler/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "geospatial",
        "raster",
        "cog",
        "tiling"
      ],
      "id": 277
    },
    {
      "name": "gdal2tiles-leaflet",
      "one_line_profile": "Generate raster image tiles for Leaflet maps",
      "detailed_description": "A tool to generate raster image tiles from geospatial data compatible with Leaflet, facilitating the visualization of scientific raster data on the web.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "visualization_prep",
        "tiling"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/commenthol/gdal2tiles-leaflet",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gdal",
        "leaflet",
        "tiles",
        "geospatial"
      ],
      "id": 278
    },
    {
      "name": "z5",
      "one_line_profile": "C++ and Python interface for Zarr and N5 formats",
      "detailed_description": "A lightweight C++ library with Python bindings for reading and writing multi-dimensional datasets in Zarr and N5 formats, commonly used in connectomics and bioimaging.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "io",
        "parsing"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/constantinpape/z5",
      "help_website": [
        "https://constantinpape.github.io/z5/"
      ],
      "license": "MIT",
      "tags": [
        "zarr",
        "n5",
        "bioimaging",
        "io"
      ],
      "id": 279
    },
    {
      "name": "dicomParser",
      "one_line_profile": "JavaScript parser for DICOM Part 10 data",
      "detailed_description": "A lightweight JavaScript library for parsing DICOM P10 byte streams in web browsers, enabling web-based medical imaging visualization and analysis applications.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "parsing",
        "io"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/cornerstonejs/dicomParser",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "dicom",
        "medical-imaging",
        "javascript",
        "parser"
      ],
      "id": 280
    },
    {
      "name": "geocube",
      "one_line_profile": "Tool to convert vector data into rasterized xarray data",
      "detailed_description": "A Python tool that simplifies the conversion of geospatial vector data (GeoDataFrame) into rasterized xarray objects, facilitating integration between vector and raster workflows.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "conversion",
        "rasterization"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/corteva/geocube",
      "help_website": [
        "https://corteva.github.io/geocube/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "geospatial",
        "rasterization",
        "xarray",
        "geopandas"
      ],
      "id": 281
    },
    {
      "name": "rioxarray",
      "one_line_profile": "Geospatial xarray extension powered by rasterio",
      "detailed_description": "An extension for xarray that provides geospatial capabilities using rasterio, allowing for easy manipulation, projection, and I/O of raster data in scientific workflows.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "io",
        "processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/corteva/rioxarray",
      "help_website": [
        "https://corteva.github.io/rioxarray/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "xarray",
        "rasterio",
        "geospatial",
        "io"
      ],
      "id": 282
    },
    {
      "name": "agnpy",
      "one_line_profile": "Modelling jetted Active Galactic Nuclei radiative processes",
      "detailed_description": "A Python package for modelling the radiative processes of jetted Active Galactic Nuclei (AGN), enabling numerical calculation of photon spectra for astrophysics research.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "modeling",
        "simulation"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/cosimoNigro/agnpy",
      "help_website": [
        "https://agnpy.readthedocs.io/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "astrophysics",
        "agn",
        "radiative-processes",
        "modeling"
      ],
      "id": 283
    },
    {
      "name": "cubed",
      "one_line_profile": "Scalable array processing with bounded memory",
      "detailed_description": "A library for scalable array processing that operates with bounded memory usage, designed to handle large scientific datasets (like Zarr arrays) in distributed environments.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "computation",
        "processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/cubed-dev/cubed",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "array-processing",
        "distributed-computing",
        "zarr",
        "serverless"
      ],
      "id": 284
    },
    {
      "name": "DVH-Analytics",
      "one_line_profile": "DICOM Database Application for Radiation Oncology",
      "detailed_description": "A software application for building a local database of DICOM Radiation Therapy (RT) data, enabling statistical analysis and visualization of Dose-Volume Histograms (DVH) for clinical research.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "analysis",
        "database_management",
        "visualization"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/cutright/DVH-Analytics",
      "help_website": [
        "http://dvh-analytics.com"
      ],
      "license": "NOASSERTION",
      "tags": [
        "dicom-rt",
        "radiation-oncology",
        "dvh",
        "medical-physics"
      ],
      "id": 285
    },
    {
      "name": "arrow-zarr",
      "one_line_profile": "Rust implementation of Zarr file format for Arrow",
      "detailed_description": "A Rust library providing an implementation of the Zarr storage format, designed to integrate with the Apache Arrow ecosystem for efficient data processing.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "io",
        "parsing"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/datafusion-contrib/arrow-zarr",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "zarr",
        "rust",
        "arrow",
        "io"
      ],
      "id": 286
    },
    {
      "name": "NetCDF4-variable-streamer",
      "one_line_profile": "Streamer for chunked reading/writing of NetCDF4 variables",
      "detailed_description": "A Python utility that extends the netCDF4 package to support streaming read/write operations on variables using chunking, facilitating the handling of large datasets that exceed memory limits.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "io",
        "chunking"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/david-salac/NetCDF4-variable-streamer",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "netcdf",
        "streaming",
        "io",
        "python"
      ],
      "id": 287
    },
    {
      "name": "versioned-hdf5",
      "one_line_profile": "Versioned abstraction on top of HDF5",
      "detailed_description": "A Python library that implements a versioning system on top of HDF5 files, allowing for git-like tracking of data changes within HDF5 containers.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "io",
        "versioning",
        "data_management"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/deshaw/versioned-hdf5",
      "help_website": [
        "https://versioned-hdf5.readthedocs.io/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "hdf5",
        "versioning",
        "data-management",
        "python"
      ],
      "id": 288
    },
    {
      "name": "titiler",
      "one_line_profile": "Dynamic map tile services for raster data",
      "detailed_description": "A modern dynamic tile server built on top of FastAPI and Rasterio, allowing users to create map tile services from Cloud Optimized GeoTIFF (COG) and other raster sources for geospatial analysis and visualization.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "visualization_service",
        "processing"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/developmentseed/titiler",
      "help_website": [
        "https://developmentseed.org/titiler/"
      ],
      "license": "MIT",
      "tags": [
        "geospatial",
        "raster",
        "cog",
        "tile-server"
      ],
      "id": 289
    },
    {
      "name": "titiler-multidim",
      "one_line_profile": "TiTiler application for NetCDF/Zarr datasets",
      "detailed_description": "An extension of TiTiler specifically designed for serving and visualizing multi-dimensional datasets like NetCDF and Zarr, common in climate and meteorological sciences.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "visualization_service",
        "io"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/developmentseed/titiler-multidim",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "netcdf",
        "zarr",
        "multidimensional",
        "geospatial"
      ],
      "id": 290
    },
    {
      "name": "OnkoDICOM",
      "one_line_profile": "Research platform for DICOM-RT and medical imaging",
      "detailed_description": "An open-source software platform for Radiation Oncology research, enabling the viewing and analysis of DICOM standard image sets (CT, MRI, PET, RT) using Python-based technologies.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "analysis",
        "visualization",
        "medical_imaging"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/didymo/OnkoDICOM",
      "help_website": [
        "https://onkodicom.com"
      ],
      "license": "LGPL-2.1",
      "tags": [
        "dicom",
        "radiation-oncology",
        "medical-imaging",
        "research-platform"
      ],
      "id": 291
    },
    {
      "name": "hagelslag",
      "one_line_profile": "Object-based severe weather forecast verification and tracking",
      "detailed_description": "A Python package for object-based tracking and verification of weather fields (like storms), supporting segmentation, tracking, and performance diagram generation for meteorological research.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "tracking",
        "segmentation",
        "verification"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/djgagne/hagelslag",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "meteorology",
        "weather-tracking",
        "verification",
        "machine-learning"
      ],
      "id": 292
    },
    {
      "name": "python-fitparse",
      "one_line_profile": "Parser for ANT/Garmin .FIT files",
      "detailed_description": "A Python library for parsing .FIT files, a format commonly used by Garmin devices and ANT+ sensors, enabling access to sports science and physiological data.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "parsing",
        "io"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/dtcooper/python-fitparse",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fit-format",
        "sports-science",
        "parsing",
        "garmin"
      ],
      "id": 293
    },
    {
      "name": "dask-rasterio",
      "one_line_profile": "Parallel raster I/O using Rasterio and Dask",
      "detailed_description": "A library that integrates Rasterio with Dask to enable parallel reading and writing of raster data, facilitating scalable geospatial data processing.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "io",
        "parallel_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/dymaxionlabs/dask-rasterio",
      "help_website": [
        "https://dask-rasterio.readthedocs.io/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "dask",
        "rasterio",
        "geospatial",
        "parallel-computing"
      ],
      "id": 294
    },
    {
      "name": "icechunk",
      "one_line_profile": "Cloud-native transactional tensor storage engine",
      "detailed_description": "A high-performance, transactional storage engine for tensor data (like Zarr), designed for cloud-native scientific computing and managing large-scale multi-dimensional arrays.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "storage",
        "io",
        "data_management"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/earth-mover/icechunk",
      "help_website": [
        "https://icechunk.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "zarr",
        "tensor-storage",
        "cloud-native",
        "rust"
      ],
      "id": 295
    },
    {
      "name": "lisflood-utilities",
      "one_line_profile": "Utilities for LISFLOOD hydrological model",
      "detailed_description": "A collection of Python utilities for pre-processing and post-processing data for the LISFLOOD hydrological model, maintained by the EC Joint Research Centre.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "preprocessing",
        "postprocessing",
        "hydrology"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/ec-jrc/lisflood-utilities",
      "help_website": [
        "https://ec-jrc.github.io/lisflood-utilities/"
      ],
      "license": "EUPL-1.2",
      "tags": [
        "hydrology",
        "lisflood",
        "environmental-science",
        "utilities"
      ],
      "id": 296
    },
    {
      "name": "climetlab-s2s-ai-challenge",
      "one_line_profile": "Climetlab plugin for S2S AI Challenge data",
      "detailed_description": "A plugin for Climetlab that provides access to the Sub-seasonal to Seasonal (S2S) forecast datasets used in the S2S AI Challenge, facilitating data loading for climate research.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "data_access",
        "io"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/ecmwf-lab/climetlab-s2s-ai-challenge",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "climate",
        "s2s",
        "climetlab",
        "data-access"
      ],
      "id": 297
    },
    {
      "name": "ecCodes",
      "one_line_profile": "ECMWF's library for decoding and encoding GRIB and BUFR meteorological formats",
      "detailed_description": "A package developed by ECMWF which provides an application programming interface and a set of tools for decoding and encoding messages in the following formats: WMO FM-92 GRIB edition 1 and edition 2, WMO FM-94 BUFR edition 3 and edition 4, WMO FM-95 CREX edition 1.",
      "domains": [
        "D1",
        "D1-02",
        "Meteorology"
      ],
      "subtask_category": [
        "parsing",
        "encoding",
        "conversion"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/ecmwf/eccodes",
      "help_website": [
        "https://confluence.ecmwf.int/display/ECC"
      ],
      "license": "Apache-2.0",
      "tags": [
        "grib",
        "bufr",
        "meteorology",
        "weather-data"
      ],
      "id": 298
    },
    {
      "name": "eccodes-python",
      "one_line_profile": "Python interface to the ecCodes GRIB and BUFR decoding/encoding library",
      "detailed_description": "Python 3 interface to the ecCodes library, allowing Python programs to read and write GRIB and BUFR files used in meteorological data.",
      "domains": [
        "D1",
        "D1-02",
        "Meteorology"
      ],
      "subtask_category": [
        "parsing",
        "encoding"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ecmwf/eccodes-python",
      "help_website": [
        "https://confluence.ecmwf.int/display/ECC/Python+bindings"
      ],
      "license": "Apache-2.0",
      "tags": [
        "python",
        "grib",
        "bufr",
        "meteorology"
      ],
      "id": 299
    },
    {
      "name": "pyeccodes",
      "one_line_profile": "Experimental pure Python GRIB decoder",
      "detailed_description": "An experimental, pure Python implementation for decoding GRIB files, serving as a potential alternative to C-binding based parsers for specific use cases.",
      "domains": [
        "D1",
        "D1-02",
        "Meteorology"
      ],
      "subtask_category": [
        "parsing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ecmwf/pyeccodes",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "grib",
        "python",
        "experimental"
      ],
      "id": 300
    },
    {
      "name": "gdal2mbtiles",
      "one_line_profile": "Converter from GDAL-readable datasets to MBTiles format",
      "detailed_description": "A command-line tool and Python library to convert any GDAL-supported geospatial dataset into the MBTiles format, primarily for generating web map tiles.",
      "domains": [
        "D1",
        "D1-02",
        "Geospatial"
      ],
      "subtask_category": [
        "conversion",
        "tiling"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ecometrica/gdal2mbtiles",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "gdal",
        "mbtiles",
        "gis",
        "mapping"
      ],
      "id": 301
    },
    {
      "name": "DicomBrowser",
      "one_line_profile": "Lightweight portable DICOM browser application",
      "detailed_description": "A portable application for viewing and inspecting DICOM medical imaging files, allowing users to browse tags and pixel data.",
      "domains": [
        "D1",
        "D1-02",
        "Medical Imaging"
      ],
      "subtask_category": [
        "visualization",
        "inspection"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ericspod/DicomBrowser",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "dicom",
        "viewer",
        "medical-imaging"
      ],
      "id": 302
    },
    {
      "name": "pandasVCF",
      "one_line_profile": "VCF parser using the Python pandas library",
      "detailed_description": "A tool to parse Variant Call Format (VCF) files into pandas DataFrames, facilitating data analysis and manipulation of genomic variants.",
      "domains": [
        "D1",
        "D1-02",
        "Bioinformatics"
      ],
      "subtask_category": [
        "parsing",
        "data_loading"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/erscott/pandasVCF",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "vcf",
        "pandas",
        "genomics"
      ],
      "id": 303
    },
    {
      "name": "fitsio",
      "one_line_profile": "Python package for FITS input/output wrapping cfitsio",
      "detailed_description": "A Python wrapper around the cfitsio library, providing a full-featured interface for reading and writing FITS (Flexible Image Transport System) files used in astronomy.",
      "domains": [
        "D1",
        "D1-02",
        "Astronomy"
      ],
      "subtask_category": [
        "parsing",
        "io"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/esheldon/fitsio",
      "help_website": [],
      "license": "GPL-2.0",
      "tags": [
        "fits",
        "astronomy",
        "cfitsio"
      ],
      "id": 304
    },
    {
      "name": "astropysics",
      "one_line_profile": "Astrophysics utilities and libraries for Python",
      "detailed_description": "A library containing a variety of utilities for astrophysics, including coordinate transformations, cosmological calculations, and FITS file handling.",
      "domains": [
        "D1",
        "D1-02",
        "Astronomy"
      ],
      "subtask_category": [
        "data_processing",
        "calculation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/eteq/astropysics",
      "help_website": [
        "http://pythonhosted.org/Astropysics/"
      ],
      "license": null,
      "tags": [
        "astrophysics",
        "fits",
        "photometry"
      ],
      "id": 305
    },
    {
      "name": "grid_map_geo",
      "one_line_profile": "Geolocalization for grid maps using GDAL",
      "detailed_description": "A C++ library that provides geolocalization capabilities for grid maps by interfacing with GDAL, allowing conversion between grid coordinates and geo-referenced coordinates.",
      "domains": [
        "D1",
        "D1-02",
        "Robotics",
        "Geospatial"
      ],
      "subtask_category": [
        "georeferencing",
        "conversion"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/ethz-asl/grid_map_geo",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "gdal",
        "grid-map",
        "robotics",
        "gis"
      ],
      "id": 306
    },
    {
      "name": "VCF-Simplify",
      "one_line_profile": "Python parser to simplify and build VCF files",
      "detailed_description": "A tool designed to simplify the parsing and construction of VCF (Variant Call Format) files, making it easier to handle genomic variant data programmatically.",
      "domains": [
        "D1",
        "D1-02",
        "Bioinformatics"
      ],
      "subtask_category": [
        "parsing",
        "simplification"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/everestial/VCF-Simplify",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "vcf",
        "genomics",
        "parser"
      ],
      "id": 307
    },
    {
      "name": "GeoDataFrames.jl",
      "one_line_profile": "Geographical vector interaction for Julia built on ArchGDAL",
      "detailed_description": "A Julia package that provides functionality for working with geospatial vector data, leveraging ArchGDAL for I/O and operations.",
      "domains": [
        "D1",
        "D1-02",
        "Geospatial"
      ],
      "subtask_category": [
        "io",
        "data_manipulation"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/evetion/GeoDataFrames.jl",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "julia",
        "gis",
        "gdal",
        "vector-data"
      ],
      "id": 308
    },
    {
      "name": "SGS",
      "one_line_profile": "Browser for visualizing single-cell and spatial multiomics data",
      "detailed_description": "A user-friendly, collaborative browser tool for visualizing and exploring single-cell and spatial multiomics datasets.",
      "domains": [
        "D1",
        "D1-02",
        "Bioinformatics"
      ],
      "subtask_category": [
        "visualization",
        "exploration"
      ],
      "application_level": "solver",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/fanglu0411/sgs",
      "help_website": [],
      "license": null,
      "tags": [
        "single-cell",
        "spatial-omics",
        "visualization"
      ],
      "id": 309
    },
    {
      "name": "ngff-zarr",
      "one_line_profile": "OME Next Generation File Format (NGFF) Zarr implementation",
      "detailed_description": "A lightweight Python implementation of the Open Microscopy Environment (OME) Next Generation File Format (NGFF) based on Zarr, for bioimaging data.",
      "domains": [
        "D1",
        "D1-02",
        "Bioimaging"
      ],
      "subtask_category": [
        "io",
        "format_implementation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/fideus-labs/ngff-zarr",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ome-zarr",
        "ngff",
        "microscopy",
        "bioimaging"
      ],
      "id": 310
    },
    {
      "name": "vcf-reformatter",
      "one_line_profile": "High-performance VCF file parser and reformatter",
      "detailed_description": "A Rust-based tool to parse VCF files and reformat them into analyzable TSV formats, with support for VEP annotations and transcript handling.",
      "domains": [
        "D1",
        "D1-02",
        "Bioinformatics"
      ],
      "subtask_category": [
        "conversion",
        "parsing",
        "formatting"
      ],
      "application_level": "solver",
      "primary_language": "Rust",
      "repo_url": "https://github.com/flalom/vcf-reformatter",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "vcf",
        "rust",
        "genomics",
        "bioinformatics"
      ],
      "id": 311
    },
    {
      "name": "Gammapy",
      "one_line_profile": "Python package for gamma-ray astronomy",
      "detailed_description": "A community-developed, open-source Python package for gamma-ray astronomy, providing tools for data analysis, modeling, and format handling.",
      "domains": [
        "D1",
        "D1-02",
        "Astronomy"
      ],
      "subtask_category": [
        "data_analysis",
        "modeling",
        "io"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/gammapy/gammapy",
      "help_website": [
        "https://gammapy.org/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "gamma-ray",
        "astronomy",
        "fits",
        "data-analysis"
      ],
      "id": 312
    },
    {
      "name": "h5pyViewer",
      "one_line_profile": "Viewer for HDF5 files",
      "detailed_description": "A graphical user interface tool for viewing and inspecting the contents of HDF5 files, built using Python and h5py.",
      "domains": [
        "D1",
        "D1-02",
        "General Science"
      ],
      "subtask_category": [
        "visualization",
        "inspection"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ganymede42/h5pyViewer",
      "help_website": [],
      "license": "BSD-2-Clause",
      "tags": [
        "hdf5",
        "viewer",
        "gui"
      ],
      "id": 313
    },
    {
      "name": "hidefix",
      "one_line_profile": "Concurrent HDF5 and NetCDF4 reader",
      "detailed_description": "A Rust library for concurrent reading of HDF5 and NetCDF4 files, optimized for performance in scientific data pipelines.",
      "domains": [
        "D1",
        "D1-02",
        "General Science"
      ],
      "subtask_category": [
        "io",
        "parsing"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/gauteh/hidefix",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "hdf5",
        "netcdf",
        "rust",
        "concurrency"
      ],
      "id": 314
    },
    {
      "name": "SDFormat",
      "one_line_profile": "Simulation Description Format (SDFormat) parser",
      "detailed_description": "The official parser and description files for SDFormat (SDF), an XML format that describes objects and environments for robot simulators, visualization, and control.",
      "domains": [
        "D1",
        "D1-02",
        "Robotics"
      ],
      "subtask_category": [
        "parsing",
        "simulation_description"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/gazebosim/sdformat",
      "help_website": [
        "http://sdformat.org"
      ],
      "license": "Apache-2.0",
      "tags": [
        "robotics",
        "simulation",
        "sdf",
        "xml"
      ],
      "id": 315
    },
    {
      "name": "Geofileops",
      "one_line_profile": "Toolbox to process large geospatial vector files",
      "detailed_description": "A Python toolbox designed to process large geospatial vector files efficiently, utilizing multiprocessing and spatial indexing.",
      "domains": [
        "D1",
        "D1-02",
        "Geospatial"
      ],
      "subtask_category": [
        "data_processing",
        "optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/geofileops/geofileops",
      "help_website": [
        "https://geofileops.readthedocs.io/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "gis",
        "vector-data",
        "geospatial",
        "processing"
      ],
      "id": 316
    },
    {
      "name": "gdal-rust",
      "one_line_profile": "Rust bindings for GDAL",
      "detailed_description": "Idiomatic Rust bindings for the GDAL (Geospatial Data Abstraction Library), enabling Rust applications to read and write geospatial raster and vector data.",
      "domains": [
        "D1",
        "D1-02",
        "Geospatial"
      ],
      "subtask_category": [
        "io",
        "bindings"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/georust/gdal",
      "help_website": [
        "https://docs.rs/gdal/"
      ],
      "license": "MIT",
      "tags": [
        "rust",
        "gdal",
        "gis",
        "geospatial"
      ],
      "id": 317
    },
    {
      "name": "nc4fortran",
      "one_line_profile": "Object-oriented Fortran NetCDF4 interface",
      "detailed_description": "A lightweight, object-oriented Fortran interface for reading and writing NetCDF4 files, simplifying scientific I/O in Fortran applications.",
      "domains": [
        "D1",
        "D1-02",
        "General Science"
      ],
      "subtask_category": [
        "io",
        "interface"
      ],
      "application_level": "library",
      "primary_language": "CMake",
      "repo_url": "https://github.com/geospace-code/nc4fortran",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fortran",
        "netcdf",
        "io"
      ],
      "id": 318
    },
    {
      "name": "LabCD",
      "one_line_profile": "Remote sensing change detection annotation tool",
      "detailed_description": "A specialized annotation tool for remote sensing change detection tasks, facilitating the creation of ground truth datasets for scientific research.",
      "domains": [
        "D1",
        "D1-02",
        "Remote Sensing"
      ],
      "subtask_category": [
        "annotation",
        "data_generation"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/geoyee/LabCD",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "remote-sensing",
        "annotation",
        "change-detection"
      ],
      "id": 319
    },
    {
      "name": "DINCAE",
      "one_line_profile": "Data-Interpolating Convolutional Auto-Encoder for satellite data reconstruction",
      "detailed_description": "A neural network-based tool (DINCAE) designed to reconstruct missing data (e.g., due to clouds) in satellite observations, particularly for oceanographic data.",
      "domains": [
        "D1",
        "D1-02",
        "Oceanography",
        "Remote Sensing"
      ],
      "subtask_category": [
        "reconstruction",
        "interpolation",
        "modeling"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/gher-uliege/DINCAE",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "satellite",
        "reconstruction",
        "neural-network",
        "oceanography"
      ],
      "id": 320
    },
    {
      "name": "dmrpp-generator",
      "one_line_profile": "Generator for DMR++ files from NetCDF4 and HDF5",
      "detailed_description": "A tool developed by NASA GHRC DAAC to generate DMR++ (OPeNDAP Hyrax) files from NetCDF4 and HDF5 data, enabling efficient cloud-based data access.",
      "domains": [
        "D1",
        "D1-02",
        "Earth Science"
      ],
      "subtask_category": [
        "conversion",
        "metadata_generation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ghrcdaac/dmrpp-generator",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "opendap",
        "netcdf",
        "hdf5",
        "nasa"
      ],
      "id": 321
    },
    {
      "name": "geoserver-rest",
      "one_line_profile": "Python library for managing geospatial data in GeoServer",
      "detailed_description": "A Python client library for interacting with the GeoServer REST API, allowing programmatic management of geospatial data layers, styles, and workspaces.",
      "domains": [
        "D1",
        "D1-02",
        "Geospatial"
      ],
      "subtask_category": [
        "data_management",
        "api_client"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/gicait/geoserver-rest",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "geoserver",
        "gis",
        "python"
      ],
      "id": 322
    },
    {
      "name": "go-dicom",
      "one_line_profile": "DICOM Medical Image Parser in Go",
      "detailed_description": "A Go library for parsing and processing DICOM medical imaging files, supporting reading and writing of DICOM elements.",
      "domains": [
        "D1",
        "D1-02",
        "Medical Imaging"
      ],
      "subtask_category": [
        "parsing",
        "io"
      ],
      "application_level": "library",
      "primary_language": "Go",
      "repo_url": "https://github.com/gillesdemey/go-dicom",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "dicom",
        "go",
        "medical-imaging"
      ],
      "id": 323
    },
    {
      "name": "dans-gdal-scripts",
      "one_line_profile": "Utilities for use in conjunction with GDAL",
      "detailed_description": "A collection of command-line utilities and scripts that extend or complement GDAL for geospatial data processing tasks.",
      "domains": [
        "D1",
        "D1-02",
        "Geospatial"
      ],
      "subtask_category": [
        "data_processing",
        "utilities"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/gina-alaska/dans-gdal-scripts",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "gdal",
        "gis",
        "utilities"
      ],
      "id": 324
    },
    {
      "name": "gdal2cesium",
      "one_line_profile": "Python+GDAL Cesium heightmap/terrain generator",
      "detailed_description": "A tool to generate Cesium-compatible heightmaps and terrain tiles from GDAL-supported raster data sources.",
      "domains": [
        "D1",
        "D1-02",
        "Geospatial"
      ],
      "subtask_category": [
        "conversion",
        "terrain_generation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/giohappy/gdal2cesium",
      "help_website": [],
      "license": "GPL-2.0",
      "tags": [
        "cesium",
        "gdal",
        "terrain",
        "heightmap"
      ],
      "id": 325
    },
    {
      "name": "ChemmineOB",
      "one_line_profile": "OpenBabel wrapper package for R",
      "detailed_description": "An R package that provides an interface to the OpenBabel chemistry library, enabling chemical file format conversion and molecular processing within R.",
      "domains": [
        "D1",
        "D1-02",
        "Cheminformatics"
      ],
      "subtask_category": [
        "conversion",
        "processing"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/girke-lab/ChemmineOB",
      "help_website": [
        "https://bioconductor.org/packages/ChemmineOB/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "r",
        "openbabel",
        "chemistry",
        "cheminformatics"
      ],
      "id": 326
    },
    {
      "name": "gnparser",
      "one_line_profile": "Scientific name parser and normalizer",
      "detailed_description": "A high-performance tool written in Go to parse, normalize, and extract semantic elements from scientific names of organisms.",
      "domains": [
        "D1",
        "D1-02",
        "Biodiversity"
      ],
      "subtask_category": [
        "parsing",
        "normalization"
      ],
      "application_level": "solver",
      "primary_language": "Go",
      "repo_url": "https://github.com/gnames/gnparser",
      "help_website": [
        "https://parser.globalnames.org/"
      ],
      "license": "MIT",
      "tags": [
        "taxonomy",
        "biodiversity",
        "parser",
        "scientific-names"
      ],
      "id": 327
    },
    {
      "name": "arco-era5",
      "one_line_profile": "Recipes for reproducing Analysis-Ready & Cloud Optimized (ARCO) ERA5 datasets",
      "detailed_description": "A collection of pipelines and tools to process ERA5 climate data into Analysis-Ready, Cloud Optimized (ARCO) formats like Zarr, facilitating large-scale climate research.",
      "domains": [
        "D1",
        "D1-02",
        "Climate Science"
      ],
      "subtask_category": [
        "data_processing",
        "workflow"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/google-research/arco-era5",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "era5",
        "climate",
        "zarr",
        "cloud-optimized"
      ],
      "id": 328
    },
    {
      "name": "Google Earth Enterprise",
      "one_line_profile": "Open Source version of Google Earth Enterprise",
      "detailed_description": "A geospatial application platform that allows organizations to build and host their own private globes and maps using their own geospatial data.",
      "domains": [
        "D1",
        "D1-02",
        "Geospatial"
      ],
      "subtask_category": [
        "visualization",
        "platform"
      ],
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/google/earthenterprise",
      "help_website": [
        "http://www.opengee.org/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "gis",
        "earth",
        "mapping",
        "platform"
      ],
      "id": 329
    },
    {
      "name": "xarray-beam",
      "one_line_profile": "Distributed Xarray with Apache Beam",
      "detailed_description": "A Python library that adapts Xarray data structures for distributed processing using Apache Beam, enabling scalable analysis of large scientific datasets.",
      "domains": [
        "D1",
        "D1-02",
        "General Science"
      ],
      "subtask_category": [
        "data_processing",
        "distributed_computing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/google/xarray-beam",
      "help_website": [
        "https://xarray-beam.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "xarray",
        "apache-beam",
        "distributed-computing",
        "big-data"
      ],
      "id": 330
    },
    {
      "name": "gradienthealth/dicom",
      "one_line_profile": "High Performance DICOM Medical Image Parser in Go",
      "detailed_description": "A high-performance Go library for parsing and processing DICOM files, designed for efficiency in medical imaging applications.",
      "domains": [
        "D1",
        "D1-02",
        "Medical Imaging"
      ],
      "subtask_category": [
        "parsing",
        "io"
      ],
      "application_level": "library",
      "primary_language": "Go",
      "repo_url": "https://github.com/gradienthealth/dicom",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "dicom",
        "go",
        "medical-imaging"
      ],
      "id": 331
    },
    {
      "name": "grailbio/go-dicom",
      "one_line_profile": "DICOM parser for Golang",
      "detailed_description": "A Go library for parsing DICOM files, providing functionality to read and extract information from medical images.",
      "domains": [
        "D1",
        "D1-02",
        "Medical Imaging"
      ],
      "subtask_category": [
        "parsing",
        "io"
      ],
      "application_level": "library",
      "primary_language": "Go",
      "repo_url": "https://github.com/grailbio/go-dicom",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "dicom",
        "go",
        "medical-imaging"
      ],
      "id": 332
    },
    {
      "name": "embedded-pydicom-react-viewer",
      "one_line_profile": "Web-based DICOM viewer component using Pydicom and WebAssembly",
      "detailed_description": "A medical DICOM file viewer that runs in the browser by using Pyodide to run Pydicom (Python) within a React application. It allows for parsing and viewing medical imaging data directly in web interfaces.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "visualization",
        "parsing"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/grimmerk/embedded-pydicom-react-viewer",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "dicom",
        "medical-imaging",
        "visualization",
        "webassembly"
      ],
      "id": 333
    },
    {
      "name": "node-dicom",
      "one_line_profile": "DICOM parser and utility library for Node.js",
      "detailed_description": "A JavaScript/CoffeeScript library for parsing and handling DICOM medical imaging files in Node.js environments. It provides utilities to read tags and extract data from DICOM files.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "parsing",
        "data_extraction"
      ],
      "application_level": "library",
      "primary_language": "CoffeeScript",
      "repo_url": "https://github.com/grmble/node-dicom",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "dicom",
        "medical-imaging",
        "parser",
        "nodejs"
      ],
      "id": 334
    },
    {
      "name": "zarr.js",
      "one_line_profile": "JavaScript implementation of the Zarr storage format",
      "detailed_description": "A JavaScript library for reading and writing Zarr arrays, a format widely used for chunked, compressed, N-dimensional arrays in scientific computing (e.g., genomics, bioimaging).",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "io",
        "data_access"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/gzuidhof/zarr.js",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "zarr",
        "array-storage",
        "io",
        "javascript"
      ],
      "id": 335
    },
    {
      "name": "h5netcdf",
      "one_line_profile": "Pythonic interface to netCDF4 via h5py",
      "detailed_description": "A Python library that provides a netCDF4-compatible interface using h5py as the backend. It allows reading and writing netCDF4 files (which are based on HDF5) without relying on the official netCDF4-python C bindings.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "io",
        "data_access"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/h5netcdf/h5netcdf",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "netcdf",
        "hdf5",
        "geoscience",
        "io"
      ],
      "id": 336
    },
    {
      "name": "h5py",
      "one_line_profile": "Pythonic interface to the HDF5 binary data format",
      "detailed_description": "The standard Python package for interacting with HDF5 binary data format. It lets you store huge amounts of numerical data, and easily manipulate that data from NumPy.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "io",
        "data_storage"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/h5py/h5py",
      "help_website": [
        "https://www.h5py.org"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "hdf5",
        "io",
        "data-storage",
        "numpy"
      ],
      "id": 337
    },
    {
      "name": "IOH5Write",
      "one_line_profile": "Library to write OpenFOAM cases as HDF5 archives",
      "detailed_description": "A C++ library designed to convert and write OpenFOAM (Computational Fluid Dynamics) simulation cases into HDF5 archives, facilitating data management and analysis.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "conversion",
        "io"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/hakostra/IOH5Write",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "openfoam",
        "cfd",
        "hdf5",
        "conversion"
      ],
      "id": 338
    },
    {
      "name": "vcf.js",
      "one_line_profile": "VCF parser and variant record model in JavaScript",
      "detailed_description": "A JavaScript library for parsing Variant Call Format (VCF) files, commonly used in bioinformatics for storing gene sequence variations.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "parsing",
        "genomics"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/hammerlab/vcf.js",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "vcf",
        "bioinformatics",
        "genomics",
        "parser"
      ],
      "id": 339
    },
    {
      "name": "clusttraj",
      "one_line_profile": "Trajectory clustering tool for molecular dynamics",
      "detailed_description": "A Python script that performs agglomerative clustering on molecular dynamics or Monte Carlo trajectories to classify similar molecular structures.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "clustering",
        "structure_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/hmcezar/clusttraj",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "molecular-dynamics",
        "clustering",
        "trajectory-analysis"
      ],
      "id": 340
    },
    {
      "name": "viv",
      "one_line_profile": "Multiscale visualization library for bioimaging data",
      "detailed_description": "A JavaScript library for high-resolution multiplexed bioimaging data visualization on the web. It supports direct rendering of Zarr and OME-TIFF formats.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "visualization",
        "bioimaging"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/hms-dbmi/viv",
      "help_website": [
        "http://viv.gehlenborglab.org/"
      ],
      "license": "MIT",
      "tags": [
        "bioimaging",
        "visualization",
        "zarr",
        "ome-tiff"
      ],
      "id": 341
    },
    {
      "name": "vizarr",
      "one_line_profile": "Minimal Zarr image viewer",
      "detailed_description": "A lightweight viewer for Zarr images based on the Viv library, enabling quick visualization of OME-Zarr data.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "visualization",
        "bioimaging"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/hms-dbmi/vizarr",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "zarr",
        "viewer",
        "bioimaging",
        "ome-zarr"
      ],
      "id": 342
    },
    {
      "name": "ceramic",
      "one_line_profile": "R tool for loading map tiles using GDAL",
      "detailed_description": "An R package to obtain web map tiles directly as raster objects using GDAL, facilitating the integration of geospatial imagery into R workflows.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_access",
        "geospatial"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/hypertidy/ceramic",
      "help_website": [],
      "license": null,
      "tags": [
        "gdal",
        "geospatial",
        "mapping",
        "r"
      ],
      "id": 343
    },
    {
      "name": "vcf-rs",
      "one_line_profile": "Rust implementation of VCF parser",
      "detailed_description": "A Rust library for parsing Variant Call Format (VCF) files, providing high-performance handling of genomic variation data.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "parsing",
        "genomics"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/informationsea/vcf-rs",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "vcf",
        "rust",
        "bioinformatics",
        "parser"
      ],
      "id": 344
    },
    {
      "name": "POAP",
      "one_line_profile": "Parallelized Open Babel & Autodock suite Pipeline",
      "detailed_description": "A pipeline tool that parallelizes ligand preparation (using Open Babel) and molecular docking (using Autodock suite) for virtual screening and drug discovery.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "molecular_docking",
        "virtual_screening"
      ],
      "application_level": "workflow",
      "primary_language": "Shell",
      "repo_url": "https://github.com/inpacdb/POAP",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "molecular-docking",
        "drug-discovery",
        "pipeline",
        "autodock"
      ],
      "id": 345
    },
    {
      "name": "cellmap-segmentation-challenge",
      "one_line_profile": "Workflows for CellMap segmentation challenge",
      "detailed_description": "A collection of scripts and workflows from Janelia CellMap for training 2D/3D models, prediction, and post-processing of large-scale EM segmentation data.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "segmentation",
        "workflow"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/janelia-cellmap/cellmap-segmentation-challenge",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "segmentation",
        "em",
        "deep-learning",
        "janelia"
      ],
      "id": 346
    },
    {
      "name": "wasm-dicom-parser",
      "one_line_profile": "WebAssembly DICOM parser",
      "detailed_description": "A C-based DICOM parser compiled to WebAssembly, enabling high-performance parsing of medical images directly in web browsers.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "parsing",
        "webassembly"
      ],
      "application_level": "library",
      "primary_language": "C",
      "repo_url": "https://github.com/jodogne/wasm-dicom-parser",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "dicom",
        "webassembly",
        "parser",
        "medical-imaging"
      ],
      "id": 347
    },
    {
      "name": "pyroSAR",
      "one_line_profile": "Framework for large-scale SAR satellite data processing",
      "detailed_description": "A Python framework for processing Synthetic Aperture Radar (SAR) satellite data, providing workflows for data retrieval, organization, and processing using tools like SNAP and GAMMA.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "remote_sensing",
        "processing"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/johntruckenbrodt/pyroSAR",
      "help_website": [
        "https://pyrosar.readthedocs.io"
      ],
      "license": "MIT",
      "tags": [
        "sar",
        "remote-sensing",
        "satellite",
        "processing"
      ],
      "id": 348
    },
    {
      "name": "thorsky",
      "one_line_profile": "Astronomy time-and-sky calculation tools",
      "detailed_description": "A Python package built on Astropy for astronomical calculations related to time and sky positions, useful for observational planning.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "astronomy",
        "calculation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/jrthorstensen/thorsky",
      "help_website": [],
      "license": "BSD-2-Clause",
      "tags": [
        "astronomy",
        "astropy",
        "observational-planning"
      ],
      "id": 349
    },
    {
      "name": "dust_extinction",
      "one_line_profile": "Astronomical dust extinction models",
      "detailed_description": "A Python package that provides various models for interstellar dust extinction, used in astronomy to correct observations for dust effects.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "modeling",
        "correction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/karllark/dust_extinction",
      "help_website": [
        "https://dust-extinction.readthedocs.io"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "astronomy",
        "dust-extinction",
        "modeling"
      ],
      "id": 350
    },
    {
      "name": "pizzarr",
      "one_line_profile": "Zarr array slicing for R",
      "detailed_description": "An R library that provides functionality to slice and access Zarr arrays, enabling R users to work with this cloud-native scientific data format.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "io",
        "data_access"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/keller-mark/pizzarr",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "zarr",
        "r",
        "io",
        "array-slicing"
      ],
      "id": 351
    },
    {
      "name": "fit",
      "one_line_profile": "ANT+ FIT file parser for R",
      "detailed_description": "An R package for parsing ANT+ FIT files, which are commonly used in sports science and health monitoring devices (e.g., Garmin).",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "parsing",
        "sports_science"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/kuperov/fit",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fit",
        "ant+",
        "r",
        "sports-science"
      ],
      "id": 352
    },
    {
      "name": "CuteVCF",
      "one_line_profile": "Simple viewer for VCF files",
      "detailed_description": "A C++ based viewer for Variant Call Format (VCF) files using htslib, providing a graphical interface to inspect genomic variants.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "visualization",
        "genomics"
      ],
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/labsquare/CuteVCF",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "vcf",
        "viewer",
        "bioinformatics",
        "gui"
      ],
      "id": 353
    },
    {
      "name": "3d-data-precrocessing",
      "one_line_profile": "Preprocessing toolkit for 3D medical images (NIfTI/DICOM)",
      "detailed_description": "A Python library for 3D medical image preprocessing, supporting conversion between NIfTI and HDF5, DICOM to NIfTI conversion, and 3D patch extraction/ROI cropping.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "image_preprocessing",
        "format_conversion"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/li-pengcheng/3d-data-precrocessing",
      "help_website": [],
      "license": null,
      "tags": [
        "medical-imaging",
        "nifti",
        "dicom",
        "preprocessing"
      ],
      "id": 354
    },
    {
      "name": "mesh-data-synthesizer",
      "one_line_profile": "Synthetic dataset generator from 3D meshes for ML",
      "detailed_description": "A tool using Unreal Engine and Cesium to generate large synthetic datasets from 3D meshes, enabling machine learning tasks like Visual Place Recognition.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_generation",
        "simulation"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/lolleko/mesh-data-synthesizer",
      "help_website": [
        "https://meshvpr.github.io"
      ],
      "license": "NOASSERTION",
      "tags": [
        "synthetic-data",
        "3d-mesh",
        "machine-learning",
        "simulation"
      ],
      "id": 355
    },
    {
      "name": "go-gdal",
      "one_line_profile": "Go wrapper for the Geospatial Data Abstraction Library (GDAL)",
      "detailed_description": "A Go programming language wrapper for GDAL, providing access to geospatial data translation and processing capabilities for raster and vector formats.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_io",
        "format_conversion"
      ],
      "application_level": "library",
      "primary_language": "Go",
      "repo_url": "https://github.com/lukeroth/gdal",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gdal",
        "geospatial",
        "gis",
        "go"
      ],
      "id": 356
    },
    {
      "name": "zarrita.js",
      "one_line_profile": "JavaScript toolkit for Zarr array storage",
      "detailed_description": "A JavaScript toolkit for working with chunked, compressed, n-dimensional arrays in the Zarr format, enabling web-based scientific data visualization and processing.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_io",
        "web_visualization"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/manzt/zarrita.js",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "zarr",
        "javascript",
        "web-science",
        "array-storage"
      ],
      "id": 357
    },
    {
      "name": "ncvue",
      "one_line_profile": "Minimal GUI for viewing NetCDF files",
      "detailed_description": "A Python-based GUI tool for quick visualization and inspection of NetCDF files, designed as a modern replacement for ncview.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "visualization",
        "data_inspection"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/mcuntz/ncvue",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "netcdf",
        "visualization",
        "gui",
        "climate-data"
      ],
      "id": 358
    },
    {
      "name": "vcf-go",
      "one_line_profile": "Variant Call Format (VCF) parser for Go",
      "detailed_description": "A Go library for parsing Variant Call Format (VCF) files, used in bioinformatics for storing gene sequence variations.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_io",
        "genomics"
      ],
      "application_level": "library",
      "primary_language": "Go",
      "repo_url": "https://github.com/mendelics/vcf",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "vcf",
        "bioinformatics",
        "genomics",
        "go"
      ],
      "id": 359
    },
    {
      "name": "openbabel-node",
      "one_line_profile": "Node.js bindings for OpenBabel chemistry library",
      "detailed_description": "Node.js bindings for OpenBabel, enabling chemical file format conversion and chemical data processing within JavaScript environments.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "format_conversion",
        "cheminformatics"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/mohebifar/openbabel-node",
      "help_website": [],
      "license": null,
      "tags": [
        "cheminformatics",
        "openbabel",
        "chemistry",
        "node"
      ],
      "id": 360
    },
    {
      "name": "vcf_parser",
      "one_line_profile": "Simple VCF parser for Python",
      "detailed_description": "A Python library for parsing VCF (Variant Call Format) files, providing a simple interface for accessing genomic variant data.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_io",
        "genomics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/moonso/vcf_parser",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "vcf",
        "bioinformatics",
        "python",
        "genomics"
      ],
      "id": 361
    },
    {
      "name": "reducer",
      "one_line_profile": "Astronomical data reduction and calibration tool",
      "detailed_description": "A Python package for reducing and calibrating astronomical images (CCD/CMOS), handling bias, dark, and flat field corrections.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "image_processing",
        "calibration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/mwcraig/reducer",
      "help_website": [
        "https://reducer.readthedocs.io/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "astronomy",
        "image-reduction",
        "calibration",
        "fits"
      ],
      "id": 362
    },
    {
      "name": "harmony-netcdf-to-zarr",
      "one_line_profile": "NASA Harmony service for NetCDF to Zarr conversion",
      "detailed_description": "A service module for NASA's Harmony platform that transforms NetCDF4 files into Zarr format, facilitating cloud-native access to Earth observation data.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "format_conversion",
        "cloud_data"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/nasa/harmony-netcdf-to-zarr",
      "help_website": [
        "https://github.com/nasa/harmony"
      ],
      "license": "NOASSERTION",
      "tags": [
        "nasa",
        "netcdf",
        "zarr",
        "earth-science"
      ],
      "id": 363
    },
    {
      "name": "radbelt",
      "one_line_profile": "Wrapper for AE-8/AP-8 Van Allen radiation belt models",
      "detailed_description": "A Python wrapper for the AE-8 and AP-8 models of the Van Allen radiation belts, allowing calculation of trapped particle fluxes.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "modeling",
        "space_physics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/nasa/radbelt",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "space-weather",
        "radiation-belts",
        "nasa",
        "physics"
      ],
      "id": 364
    },
    {
      "name": "stitchee",
      "one_line_profile": "NetCDF4 file concatenation service",
      "detailed_description": "A NASA Harmony service that concatenates multiple NetCDF4 data files along an existing dimension, useful for merging time-series or spatial data chunks.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_processing",
        "concatenation"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/nasa/stitchee",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "netcdf",
        "nasa",
        "data-processing",
        "earth-science"
      ],
      "id": 365
    },
    {
      "name": "node-gdal",
      "one_line_profile": "Node.js bindings for GDAL",
      "detailed_description": "Node.js bindings for the Geospatial Data Abstraction Library (GDAL), enabling reading and writing of raster and vector geospatial data formats in JavaScript applications.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_io",
        "format_conversion"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/naturalatlas/node-gdal",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "gdal",
        "gis",
        "node",
        "geospatial"
      ],
      "id": 366
    },
    {
      "name": "pygdal",
      "one_line_profile": "Virtualenv-friendly GDAL Python bindings",
      "detailed_description": "A packaging of the standard GDAL Python bindings designed to install easily into virtual environments via pip, solving common version matching issues.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_io",
        "environment_management"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/nextgis/pygdal",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "gdal",
        "python",
        "gis",
        "bindings"
      ],
      "id": 367
    },
    {
      "name": "nibabel",
      "one_line_profile": "Neuroimaging file format access library",
      "detailed_description": "A comprehensive Python library for reading and writing neuroimaging file formats, including NIfTI, GIFTI, MINC, and ANALYZE.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_io",
        "neuroimaging"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/nipy/nibabel",
      "help_website": [
        "https://nipy.org/nibabel/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "neuroimaging",
        "nifti",
        "mri",
        "medical-imaging"
      ],
      "id": 368
    },
    {
      "name": "nitransforms",
      "one_line_profile": "Geometric transformations for neuroimaging",
      "detailed_description": "A library for applying and manipulating geometric transformations (affine, non-linear) on neuroimaging data, facilitating spatial normalization and alignment.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "image_registration",
        "spatial_transformation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/nipy/nitransforms",
      "help_website": [
        "https://nitransforms.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "neuroimaging",
        "registration",
        "transformation",
        "mri"
      ],
      "id": 369
    },
    {
      "name": "neuropythy",
      "one_line_profile": "Neuroscience library for cortical surface analysis",
      "detailed_description": "A Python library for neuroscience research, providing tools for cortical surface analysis, retinotopy, and integration with FreeSurfer and Human Connectome Project data.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "neuroscience_analysis",
        "cortical_mapping"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/noahbenson/neuropythy",
      "help_website": [
        "https://github.com/noahbenson/neuropythy/wiki"
      ],
      "license": "AGPL-3.0",
      "tags": [
        "neuroscience",
        "cortex",
        "retinotopy",
        "freesurfer"
      ],
      "id": 370
    },
    {
      "name": "ODDT",
      "one_line_profile": "Open Drug Discovery Toolkit",
      "detailed_description": "A modular and comprehensive toolkit for cheminformatics and drug discovery, providing unified interfaces for various toolkits (OpenBabel, RDKit) and implementing scoring functions for molecular docking.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "drug_discovery",
        "molecular_docking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/oddt/oddt",
      "help_website": [
        "http://oddt.readthedocs.org"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "cheminformatics",
        "drug-discovery",
        "docking",
        "molecular-modeling"
      ],
      "id": 371
    },
    {
      "name": "go2com",
      "one_line_profile": "DICOM file parser for Go",
      "detailed_description": "A Go library for parsing DICOM files, enabling the reading and processing of medical imaging data within Go applications.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_io",
        "medical_imaging"
      ],
      "application_level": "library",
      "primary_language": "Go",
      "repo_url": "https://github.com/okieraised/go2com",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "dicom",
        "medical-imaging",
        "go",
        "parser"
      ],
      "id": 372
    },
    {
      "name": "ome-zarr-models-py",
      "one_line_profile": "OME-Zarr metadata models for Python",
      "detailed_description": "A Python package for reading, writing, and validating OME-Zarr metadata models, supporting the Next Generation File Format (NGFF) for bioimaging.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "metadata_management",
        "validation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ome-zarr-models/ome-zarr-models-py",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ome-zarr",
        "bioimaging",
        "metadata",
        "ngff"
      ],
      "id": 373
    },
    {
      "name": "napari-ome-zarr",
      "one_line_profile": "Napari plugin for OME-Zarr images",
      "detailed_description": "A plugin for the napari image viewer that enables reading and visualization of OME-Zarr (NGFF) bioimaging data directly from local or remote sources.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "visualization",
        "plugin"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ome/napari-ome-zarr",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "napari",
        "ome-zarr",
        "bioimaging",
        "visualization"
      ],
      "id": 374
    },
    {
      "name": "ome-zarr-py",
      "one_line_profile": "Python implementation of OME-NGFF for bioimaging",
      "detailed_description": "A Python library implementing the OME-NGFF (Next Generation File Format) specification for storing and accessing bioimaging data in the cloud using Zarr.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_io",
        "cloud_storage"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ome/ome-zarr-py",
      "help_website": [
        "https://ome-zarr.readthedocs.io/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "bioimaging",
        "ome-zarr",
        "ngff",
        "microscopy"
      ],
      "id": 375
    },
    {
      "name": "dicom_parser",
      "one_line_profile": "Python library for parsing and accessing DICOM medical imaging data",
      "detailed_description": "A Python library designed to facilitate access to DICOM data, providing parsing capabilities for medical imaging files.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "parsing",
        "data_access"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/open-dicom/dicom_parser",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "dicom",
        "medical-imaging",
        "parsing"
      ],
      "id": 376
    },
    {
      "name": "Open Babel",
      "one_line_profile": "Chemical toolbox for converting and processing chemical data formats",
      "detailed_description": "A chemical toolbox designed to speak the many languages of chemical data. It allows searching, converting, analyzing, and storing data from molecular modeling, chemistry, solid-state materials, biochemistry, or related areas.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "format_conversion",
        "molecular_modeling"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/openbabel/openbabel",
      "help_website": [
        "http://openbabel.org/"
      ],
      "license": "GPL-2.0",
      "tags": [
        "chemistry",
        "cheminformatics",
        "file-conversion",
        "molecular-data"
      ],
      "id": 377
    },
    {
      "name": "Open Data Cube Core",
      "one_line_profile": "Framework for analysing continental scale Earth Observation data",
      "detailed_description": "Open Data Cube (ODC) is an open source geospatial data management and analysis software project that helps you harness the power of Satellite data. It provides a Python API for high-performance access and analysis of geospatial data.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_management",
        "geospatial_analysis"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/opendatacube/datacube-core",
      "help_website": [
        "https://www.opendatacube.org/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "earth-observation",
        "geospatial",
        "satellite-data",
        "datacube"
      ],
      "id": 378
    },
    {
      "name": "pangeo-forge-recipes",
      "one_line_profile": "ETL framework for building analysis-ready cloud-optimized scientific datasets",
      "detailed_description": "A Python library for building Pangeo Forge recipes, which are used to transform archival data into cloud-optimized formats (like Zarr) for scientific analysis.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "etl",
        "data_processing"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/pangeo-forge/pangeo-forge-recipes",
      "help_website": [
        "https://pangeo-forge.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "geoscience",
        "etl",
        "zarr",
        "pangeo"
      ],
      "id": 379
    },
    {
      "name": "node-netcdf4",
      "one_line_profile": "NodeJS addon to read and write NetCDF4 files",
      "detailed_description": "A Node.js addon that provides bindings to the NetCDF C library, allowing reading and writing of NetCDF4 files, commonly used in atmospheric and oceanographic science.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "io",
        "parsing"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/parro-it/netcdf4",
      "help_website": [],
      "license": "ISC",
      "tags": [
        "netcdf",
        "nodejs",
        "geoscience",
        "io"
      ],
      "id": 380
    },
    {
      "name": "PLIP",
      "one_line_profile": "Protein-Ligand Interaction Profiler for PDB files",
      "detailed_description": "A tool to analyze and visualize non-covalent protein-ligand interactions in PDB files. It detects interactions such as hydrogen bonds, hydrophobic contacts, and pi-stacking.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "interaction_analysis",
        "structural_biology"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/pharmai/plip",
      "help_website": [
        "https://plip-tool.biotec.tu-dresden.de/"
      ],
      "license": "GPL-2.0",
      "tags": [
        "protein-ligand",
        "bioinformatics",
        "pdb",
        "interaction-profiling"
      ],
      "id": 381
    },
    {
      "name": "easy-fit",
      "one_line_profile": "JavaScript library to parse FIT files",
      "detailed_description": "A library to parse .FIT files (Flexible and Interoperable Data Transfer), commonly used in sports science and fitness tracking devices, directly in JavaScript.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "parsing",
        "data_processing"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/pierremtb/easy-fit",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "fit-file",
        "sports-science",
        "parsing",
        "javascript"
      ],
      "id": 382
    },
    {
      "name": "radseq",
      "one_line_profile": "Scripts for parsing and analyzing RAD-seq data",
      "detailed_description": "A collection of Python scripts designed for the parsing and analysis of Restriction site Associated DNA sequencing (RAD-seq) data.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "genomics_analysis",
        "parsing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pimbongaerts/radseq",
      "help_website": [],
      "license": null,
      "tags": [
        "rad-seq",
        "genomics",
        "bioinformatics",
        "scripts"
      ],
      "id": 383
    },
    {
      "name": "fitdecode",
      "one_line_profile": "FIT file parsing and decoding library",
      "detailed_description": "A Python library for parsing and decoding FIT (Flexible and Interoperable Data Transfer) files, used in sports science and device data logging.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "parsing",
        "decoding"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/polyvertex/fitdecode",
      "help_website": [
        "https://fitdecode.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "fit-file",
        "parsing",
        "python",
        "sports-data"
      ],
      "id": 384
    },
    {
      "name": "simple-tiles",
      "one_line_profile": "Library for generating map tiles from geospatial data",
      "detailed_description": "A C library for generating map tiles from shapefiles and other geospatial data sources, useful for scientific visualization in GIS.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "visualization",
        "gis"
      ],
      "application_level": "library",
      "primary_language": "C",
      "repo_url": "https://github.com/propublica/simple-tiles",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gis",
        "mapping",
        "visualization",
        "shapefile"
      ],
      "id": 385
    },
    {
      "name": "pydicom-deid",
      "one_line_profile": "Best effort anonymization for medical images",
      "detailed_description": "A Python library for anonymizing DICOM medical images, essential for data privacy in medical research and dataset sharing.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "anonymization",
        "data_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pydicom/deid",
      "help_website": [
        "https://pydicom.github.io/deid/"
      ],
      "license": "MIT",
      "tags": [
        "dicom",
        "anonymization",
        "medical-imaging",
        "privacy"
      ],
      "id": 386
    },
    {
      "name": "pydicom",
      "one_line_profile": "Standard Python library for DICOM medical image file IO",
      "detailed_description": "A pure Python package for reading, modifying, and writing DICOM files. It is the de facto standard library for handling medical imaging data in Python.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "io",
        "parsing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pydicom/pydicom",
      "help_website": [
        "https://pydicom.github.io/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "dicom",
        "medical-imaging",
        "io",
        "python"
      ],
      "id": 387
    },
    {
      "name": "pylibjpeg",
      "one_line_profile": "Framework for decoding JPEG images in DICOM",
      "detailed_description": "A Python framework for decoding JPEG images, specifically designed to support pydicom and the various JPEG transfer syntaxes used in medical imaging.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "decoding",
        "image_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pydicom/pylibjpeg",
      "help_website": [
        "https://github.com/pydicom/pylibjpeg"
      ],
      "license": "MIT",
      "tags": [
        "jpeg",
        "dicom",
        "decoding",
        "medical-imaging"
      ],
      "id": 388
    },
    {
      "name": "pylibjpeg-openjpeg",
      "one_line_profile": "JPEG 2000 decoder plugin for pylibjpeg",
      "detailed_description": "A plugin for pylibjpeg that provides JPEG 2000 (J2K, JP2, HTJ2K) decoding support using OpenJPEG, critical for handling compressed DICOM medical images.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "decoding",
        "image_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pydicom/pylibjpeg-openjpeg",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "jpeg2000",
        "dicom",
        "decoding",
        "plugin"
      ],
      "id": 389
    },
    {
      "name": "pynetdicom",
      "one_line_profile": "Python implementation of the DICOM networking protocol",
      "detailed_description": "A Python implementation of the DICOM networking protocol (Service Class User and Service Class Provider), allowing Python applications to communicate with medical imaging devices and PACS.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "networking",
        "data_transfer"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pydicom/pynetdicom",
      "help_website": [
        "https://pydicom.github.io/pynetdicom/"
      ],
      "license": "MIT",
      "tags": [
        "dicom",
        "networking",
        "pacs",
        "medical-imaging"
      ],
      "id": 390
    },
    {
      "name": "pysam",
      "one_line_profile": "Python library for reading and manipulating genomics data (SAM/BAM/VCF)",
      "detailed_description": "A Python module for reading, manipulating, and writing genomic data sets. It is a lightweight wrapper of the HTSlib C-API, supporting SAM, BAM, CRAM, VCF, and BCF formats.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "io",
        "genomics_processing"
      ],
      "application_level": "library",
      "primary_language": "Cython",
      "repo_url": "https://github.com/pysam-developers/pysam",
      "help_website": [
        "https://pysam.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "genomics",
        "bioinformatics",
        "sam",
        "bam",
        "vcf"
      ],
      "id": 391
    },
    {
      "name": "feets",
      "one_line_profile": "Feature extractor for time series data in astronomy",
      "detailed_description": "A library for extracting features from time series data, primarily designed for astronomical light curves.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "feature_extraction",
        "time_series_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/quatrope/feets",
      "help_website": [
        "https://feets.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "astronomy",
        "time-series",
        "feature-extraction",
        "light-curves"
      ],
      "id": 392
    },
    {
      "name": "rt-utils",
      "one_line_profile": "Library for creating and manipulating DICOM RTStructs",
      "detailed_description": "A minimal Python library to facilitate the creation and manipulation of DICOM RTStruct (Radiotherapy Structure Set) files, used in medical physics and radiation therapy.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_manipulation",
        "radiotherapy"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/qurit/rt-utils",
      "help_website": [
        "https://rt-utils.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "dicom",
        "rtstruct",
        "radiotherapy",
        "medical-physics"
      ],
      "id": 393
    },
    {
      "name": "sf",
      "one_line_profile": "Simple Features for R - Spatial data analysis",
      "detailed_description": "An R package that provides support for simple features, a standardized way to encode spatial vector data. It allows for manipulation, analysis, and visualization of spatial data.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "spatial_analysis",
        "gis"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/r-spatial/sf",
      "help_website": [
        "https://r-spatial.github.io/sf/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "gis",
        "spatial-analysis",
        "r-stats",
        "simple-features"
      ],
      "id": 394
    },
    {
      "name": "RADTorch",
      "one_line_profile": "Medical Imaging Machine Learning Framework",
      "detailed_description": "A framework designed for medical imaging machine learning tasks, facilitating data loading, processing, and model training for radiology applications.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "machine_learning",
        "medical_imaging"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/radtorch/radtorch",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "medical-imaging",
        "machine-learning",
        "radiology",
        "pytorch"
      ],
      "id": 395
    },
    {
      "name": "Rasters.jl",
      "one_line_profile": "Raster data manipulation for Julia",
      "detailed_description": "A Julia package for manipulating and analyzing raster data, supporting various file formats and geospatial operations.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "raster_analysis",
        "geospatial"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/rafaqz/Rasters.jl",
      "help_website": [
        "https://rafaqz.github.io/Rasters.jl/"
      ],
      "license": "MIT",
      "tags": [
        "julia",
        "raster",
        "geospatial",
        "gis"
      ],
      "id": 396
    },
    {
      "name": "rasterio",
      "one_line_profile": "Python library for reading and writing geospatial raster datasets",
      "detailed_description": "Rasterio reads and writes geospatial raster datasets (like GeoTIFF) using GDAL, providing a Pythonic API for raster data processing.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "io",
        "geospatial_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/rasterio/rasterio",
      "help_website": [
        "https://rasterio.readthedocs.io/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "geospatial",
        "raster",
        "gdal",
        "python"
      ],
      "id": 397
    },
    {
      "name": "pydicom-seg",
      "one_line_profile": "Library for DICOM-SEG medical segmentation file IO",
      "detailed_description": "A Python package for reading and writing DICOM-SEG (Segmentation) objects, facilitating the handling of medical image segmentation results.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "io",
        "segmentation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/razorx89/pydicom-seg",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "dicom",
        "segmentation",
        "medical-imaging",
        "io"
      ],
      "id": 398
    },
    {
      "name": "NIFTI-Reader-JS",
      "one_line_profile": "JavaScript NIfTI file format reader",
      "detailed_description": "A JavaScript library for reading NIfTI (Neuroimaging Informatics Technology Initiative) file formats, enabling web-based visualization and processing of neuroimaging data.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "parsing",
        "visualization"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/rii-mango/NIFTI-Reader-JS",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nifti",
        "neuroimaging",
        "javascript",
        "parsing"
      ],
      "id": 399
    },
    {
      "name": "segmentation-eval",
      "one_line_profile": "Radiomics evaluation for liver cancer tumors from DICOM",
      "detailed_description": "A tool to extract and evaluate radiomics features for liver cancer tumors using DICOM segmentation masks, integrating SimpleITK and PyRadiomics.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "radiomics",
        "evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/rmsandu/segmentation-eval",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "radiomics",
        "medical-imaging",
        "liver-cancer",
        "evaluation"
      ],
      "id": 400
    },
    {
      "name": "clisops",
      "one_line_profile": "Library for subsetting, averaging and regridding climate simulation data",
      "detailed_description": "A python library for running data-reduction operations on climate simulation data (NetCDF), typically used in the ROOCS (Remote Operations On Climate Simulations) framework.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_processing",
        "subsetting"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/roocs/clisops",
      "help_website": [
        "https://clisops.readthedocs.io/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "climate-science",
        "netcdf",
        "roocs"
      ],
      "id": 401
    },
    {
      "name": "MODIStsp",
      "one_line_profile": "Automated download and preprocessing of MODIS Land Products",
      "detailed_description": "An R package that automates the creation of time series of rasters derived from MODIS Land Products data, performing download, mosaicking, reprojection and resizing.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_acquisition",
        "preprocessing"
      ],
      "application_level": "workflow",
      "primary_language": "R",
      "repo_url": "https://github.com/ropensci/MODIStsp",
      "help_website": [
        "https://docs.ropensci.org/MODIStsp/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "modis",
        "remote-sensing",
        "time-series"
      ],
      "id": 402
    },
    {
      "name": "dcm2niix",
      "one_line_profile": "Robust DICOM to NIfTI converter for medical imaging",
      "detailed_description": "A popular tool to convert DICOM images from medical scanners (MRI, CT) to the NIfTI format used by scientific analysis tools like FSL, SPM, and AFNI.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "format_conversion",
        "medical_imaging"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/rordenlab/dcm2niix",
      "help_website": [
        "https://www.nitrc.org/plugins/mwiki/index.php/dcm2nii:MainPage"
      ],
      "license": "NOASSERTION",
      "tags": [
        "dicom",
        "nifti",
        "neuroimaging"
      ],
      "id": 403
    },
    {
      "name": "FitFileParser",
      "one_line_profile": "Swift library for parsing Garmin FIT files",
      "detailed_description": "A Swift package that provides functionality to parse FIT (Flexible and Interoperable Data Transfer) files, commonly used in sports science and physiological data logging.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_parsing"
      ],
      "application_level": "library",
      "primary_language": "Swift",
      "repo_url": "https://github.com/roznet/FitFileParser",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fit-file",
        "garmin",
        "sports-science"
      ],
      "id": 404
    },
    {
      "name": "terra",
      "one_line_profile": "Spatial data analysis and manipulation in R",
      "detailed_description": "A comprehensive R package for spatial data analysis, replacing the 'raster' package. It supports raster and vector data manipulation, including reading/writing various geospatial formats.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "spatial_analysis",
        "data_io"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/rspatial/terra",
      "help_website": [
        "https://rspatial.github.io/terra/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "geospatial",
        "raster",
        "gis"
      ],
      "id": 405
    },
    {
      "name": "rust-htslib",
      "one_line_profile": "High-level Rust bindings for HTSlib",
      "detailed_description": "Provides a high-level Rust API for reading and writing high-throughput sequencing data formats (BAM, BCF, VCF, etc.) via HTSlib.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_io",
        "bioinformatics_core"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/rust-bio/rust-htslib",
      "help_website": [
        "https://docs.rs/rust-htslib"
      ],
      "license": "MIT",
      "tags": [
        "bioinformatics",
        "bam",
        "vcf"
      ],
      "id": 406
    },
    {
      "name": "htslib",
      "one_line_profile": "C library for high-throughput sequencing data formats",
      "detailed_description": "The core C library for parsing and manipulating high-throughput sequencing data formats such as SAM, BAM, CRAM, and VCF. It is the foundation for samtools and bcftools.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_io",
        "format_parsing"
      ],
      "application_level": "library",
      "primary_language": "C",
      "repo_url": "https://github.com/samtools/htslib",
      "help_website": [
        "http://www.htslib.org/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "bioinformatics",
        "ngs",
        "sam"
      ],
      "id": 407
    },
    {
      "name": "samtools",
      "one_line_profile": "Utilities for manipulating NGS data (SAM/BAM/CRAM)",
      "detailed_description": "A suite of programs for interacting with high-throughput sequencing data. It provides tools for converting, sorting, indexing, and viewing data in SAM/BAM/CRAM formats.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_processing",
        "format_conversion"
      ],
      "application_level": "solver",
      "primary_language": "C",
      "repo_url": "https://github.com/samtools/samtools",
      "help_website": [
        "http://www.htslib.org/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "bioinformatics",
        "ngs",
        "alignment"
      ],
      "id": 408
    },
    {
      "name": "mo_netcdf",
      "one_line_profile": "Modern Fortran wrapper for NetCDF",
      "detailed_description": "A library providing an object-oriented, modern Fortran interface to the NetCDF C library, simplifying IO operations in climate and weather models.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_io"
      ],
      "application_level": "library",
      "primary_language": "Fortran",
      "repo_url": "https://github.com/schaefed/mo_netcdf",
      "help_website": [],
      "license": null,
      "tags": [
        "fortran",
        "netcdf",
        "climate-modeling"
      ],
      "id": 409
    },
    {
      "name": "zarr-rust",
      "one_line_profile": "Rust implementation of the Zarr storage format",
      "detailed_description": "A pure Rust implementation of the Zarr V3 specification, enabling high-performance N-dimensional array storage and retrieval, critical for cloud-native scientific data.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_io",
        "storage"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/sci-rs/zarr",
      "help_website": [
        "https://docs.rs/zarrs/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "zarr",
        "rust",
        "cloud-native"
      ],
      "id": 410
    },
    {
      "name": "scippnexus",
      "one_line_profile": "NeXus file handling with Scipp integration",
      "detailed_description": "A Python library that provides an h5py-like interface for reading NeXus files (common in neutron/X-ray scattering) directly into Scipp data structures.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_io",
        "format_parsing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/scipp/scippnexus",
      "help_website": [
        "https://scipp.github.io/scippnexus/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "nexus",
        "neutron-scattering",
        "scipp"
      ],
      "id": 411
    },
    {
      "name": "astrolibpy",
      "one_line_profile": "Python port of IDL astronomy library",
      "detailed_description": "A collection of astronomical utility codes ported from the classic IDL astrolib, providing various calculations and data handling functions for astronomers.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_analysis",
        "utility"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/segasai/astrolibpy",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "astronomy",
        "idl",
        "utility"
      ],
      "id": 412
    },
    {
      "name": "SeqAn",
      "one_line_profile": "C++ template library for sequence analysis",
      "detailed_description": "A high-performance C++ library for the analysis of biological sequences, providing algorithms and data structures for string matching, alignment, and IO.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "sequence_analysis",
        "data_io"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/seqan/seqan",
      "help_website": [
        "http://www.seqan.de"
      ],
      "license": "NOASSERTION",
      "tags": [
        "bioinformatics",
        "sequence-alignment",
        "cpp"
      ],
      "id": 413
    },
    {
      "name": "bio2zarr",
      "one_line_profile": "Converter for bioinformatics formats to Zarr",
      "detailed_description": "A tool to convert standard bioinformatics file formats (like VCF and PLINK) into Zarr archives, facilitating cloud-based analysis and efficient storage.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "format_conversion"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/sgkit-dev/bio2zarr",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "bioinformatics",
        "zarr",
        "vcf"
      ],
      "id": 414
    },
    {
      "name": "h5grove",
      "one_line_profile": "Utilities for serving HDF5 file content",
      "detailed_description": "A Python package providing utilities to design backends for serving HDF5 file content (attributes, metadata, data) efficiently, often used for web-based visualization.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_service",
        "visualization_backend"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/silx-kit/h5grove",
      "help_website": [
        "https://silx-kit.github.io/h5grove/"
      ],
      "license": "MIT",
      "tags": [
        "hdf5",
        "web-service",
        "visualization"
      ],
      "id": 415
    },
    {
      "name": "hdf5plugin",
      "one_line_profile": "HDF5 compression filters for h5py",
      "detailed_description": "Provides a set of HDF5 compression filters (Blosc, LZ4, Zstandard, etc.) for use with h5py, enabling reading and writing of compressed HDF5 data.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_compression",
        "data_io"
      ],
      "application_level": "library",
      "primary_language": "C",
      "repo_url": "https://github.com/silx-kit/hdf5plugin",
      "help_website": [
        "http://www.silx.org/doc/hdf5plugin/latest/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "hdf5",
        "compression",
        "h5py"
      ],
      "id": 416
    },
    {
      "name": "rust-fitsio",
      "one_line_profile": "Rust bindings for CFITSIO",
      "detailed_description": "A Rust library providing a wrapper around the CFITSIO C library, allowing Rust programs to read and write FITS files used in astronomy.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_io"
      ],
      "application_level": "library",
      "primary_language": "C",
      "repo_url": "https://github.com/simonrw/rust-fitsio",
      "help_website": [
        "https://docs.rs/fitsio"
      ],
      "license": "Apache-2.0",
      "tags": [
        "astronomy",
        "fits",
        "rust"
      ],
      "id": 417
    },
    {
      "name": "garmin_wrapped",
      "one_line_profile": "Analysis and visualization scripts for Garmin FIT data",
      "detailed_description": "A collection of scripts to parse, analyze, and visualize running data from Garmin .fit and .txt files, enabling fine-grained performance analysis.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_analysis",
        "visualization"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/sivaprakasaman/garmin_wrapped",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "fit-file",
        "sports-analytics",
        "visualization"
      ],
      "id": 418
    },
    {
      "name": "astroimtools",
      "one_line_profile": "Astronomical image convenience tools",
      "detailed_description": "A Python package providing convenience tools for processing astronomical images, including cutouts and simple arithmetic, developed by STScI.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "image_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/spacetelescope/astroimtools",
      "help_website": [
        "https://astroimtools.readthedocs.io/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "astronomy",
        "image-processing",
        "stsci"
      ],
      "id": 419
    },
    {
      "name": "gwcs",
      "one_line_profile": "Generalized World Coordinate System",
      "detailed_description": "A library for managing the Generalized World Coordinate System (GWCS), providing a flexible way to map pixel coordinates to world coordinates in astronomy.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "coordinate_transformation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/spacetelescope/gwcs",
      "help_website": [
        "https://gwcs.readthedocs.io/"
      ],
      "license": null,
      "tags": [
        "astronomy",
        "wcs",
        "coordinates"
      ],
      "id": 420
    },
    {
      "name": "imexam",
      "one_line_profile": "Interactive image examination and plotting",
      "detailed_description": "A Python tool for simple interactive image examination and plotting, providing functionality similar to IRAF's imexamine task.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "image_analysis",
        "visualization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/spacetelescope/imexam",
      "help_website": [
        "https://imexam.readthedocs.io/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "astronomy",
        "visualization",
        "iraf"
      ],
      "id": 421
    },
    {
      "name": "jwql",
      "one_line_profile": "James Webb Space Telescope Quicklook Application",
      "detailed_description": "A software suite for monitoring the health and stability of the JWST instruments and data products, providing quicklook visualization and analysis.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "quality_control",
        "data_visualization"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/spacetelescope/jwql",
      "help_website": [
        "https://jwql.readthedocs.io/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "jwst",
        "astronomy",
        "quality-control"
      ],
      "id": 422
    },
    {
      "name": "stsynphot",
      "one_line_profile": "Synthetic photometry for HST and JWST",
      "detailed_description": "An extension of synphot providing synthetic photometry utilities specifically for Hubble Space Telescope (HST) and James Webb Space Telescope (JWST).",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "simulation",
        "photometry"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/spacetelescope/stsynphot_refactor",
      "help_website": [
        "https://stsynphot.readthedocs.io/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "astronomy",
        "photometry",
        "hst"
      ],
      "id": 423
    },
    {
      "name": "synphot",
      "one_line_profile": "Synthetic photometry using Astropy",
      "detailed_description": "A general-purpose package for synthetic photometry in astronomy, allowing users to simulate observations and calculate photometric properties.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "simulation",
        "photometry"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/spacetelescope/synphot_refactor",
      "help_website": [
        "https://synphot.readthedocs.io/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "astronomy",
        "photometry",
        "simulation"
      ],
      "id": 424
    },
    {
      "name": "JGribX",
      "one_line_profile": "Java GRIB-1/GRIB-2 decoder",
      "detailed_description": "A Java library for decoding GRIB-1 and GRIB-2 files, which are standard formats for meteorological data.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_parsing"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/spidru/JGribX",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "meteorology",
        "grib",
        "java"
      ],
      "id": 425
    },
    {
      "name": "fitparse-rs",
      "one_line_profile": "Rust library to parse FIT formatted files",
      "detailed_description": "A Rust library for parsing FIT (Flexible and Interoperable Data Transfer) files, providing access to data records and definitions.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_parsing"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/stadelmanma/fitparse-rs",
      "help_website": [
        "https://docs.rs/fitparse"
      ],
      "license": "MIT",
      "tags": [
        "fit-file",
        "rust",
        "parser"
      ],
      "id": 426
    },
    {
      "name": "Optika",
      "one_line_profile": "Python library for simulating and designing optical systems",
      "detailed_description": "A Python library for simulating optical systems, offering functionality similar to Zemax for ray tracing and optical design analysis.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "simulation",
        "optical_design"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/sun-data/optika",
      "help_website": [],
      "license": null,
      "tags": [
        "optics",
        "simulation",
        "ray-tracing"
      ],
      "id": 427
    },
    {
      "name": "ndcube",
      "one_line_profile": "Library for handling multi-dimensional coordinate-aware arrays in solar physics",
      "detailed_description": "A base package for multi-dimensional contiguous and non-contiguous coordinate-aware arrays, primarily designed for solar physics data analysis.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_processing",
        "coordinate_handling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/sunpy/ndcube",
      "help_website": [
        "https://docs.sunpy.org/projects/ndcube/"
      ],
      "license": "BSD-2-Clause",
      "tags": [
        "solar-physics",
        "coordinates",
        "data-cubes"
      ],
      "id": 428
    },
    {
      "name": "SunPy",
      "one_line_profile": "Core library for Solar Physics data analysis",
      "detailed_description": "An open-source software library for solar physics data analysis, providing tools for querying, downloading, and analyzing solar data.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_analysis",
        "data_acquisition"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/sunpy/sunpy",
      "help_website": [
        "https://sunpy.org/"
      ],
      "license": "BSD-2-Clause",
      "tags": [
        "solar-physics",
        "astronomy",
        "data-analysis"
      ],
      "id": 429
    },
    {
      "name": "sunraster",
      "one_line_profile": "Tools to analyze solar spectral data",
      "detailed_description": "A SunPy-affiliated package providing tools to analyze spectral data from various solar missions.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_analysis",
        "spectroscopy"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/sunpy/sunraster",
      "help_website": [],
      "license": "BSD-2-Clause",
      "tags": [
        "solar-physics",
        "spectroscopy",
        "raster-data"
      ],
      "id": 430
    },
    {
      "name": "dicom (Go)",
      "one_line_profile": "High-performance DICOM medical image parser in Go",
      "detailed_description": "A high-performance library written in Go for parsing and processing DICOM medical image files.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_parsing",
        "medical_imaging"
      ],
      "application_level": "library",
      "primary_language": "Go",
      "repo_url": "https://github.com/suyashkumar/dicom",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "dicom",
        "medical-imaging",
        "go"
      ],
      "id": 431
    },
    {
      "name": "AGStoShapefile",
      "one_line_profile": "Tool to convert ArcGIS Server Map Services to GeoJSON/Shapefile",
      "detailed_description": "A utility to query ArcGIS Server Dynamic Map Services and convert the output into standard geospatial formats like GeoJSON and Shapefile.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "format_conversion",
        "data_acquisition"
      ],
      "application_level": "solver",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/tannerjt/AGStoShapefile",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gis",
        "arcgis",
        "geojson",
        "shapefile"
      ],
      "id": 432
    },
    {
      "name": "gdal2tiles",
      "one_line_profile": "Library for generating map tiles from GDAL-supported rasters",
      "detailed_description": "A Python library for generating map tiles (TMS) based on the gdal2tiles.py utility from the GDAL project, facilitating geospatial data visualization.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_processing",
        "visualization_prep"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/tehamalab/gdal2tiles",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gdal",
        "gis",
        "map-tiles"
      ],
      "id": 433
    },
    {
      "name": "scipdf_parser",
      "one_line_profile": "Parser for extracting content and figures from scientific PDF publications",
      "detailed_description": "A Python tool to parse scientific PDF articles, extracting metadata, text content, and figures, useful for scientific text mining and analysis.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_mining",
        "text_extraction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/titipata/scipdf_parser",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pdf-parsing",
        "scientific-literature",
        "nlp"
      ],
      "id": 434
    },
    {
      "name": "Argos",
      "one_line_profile": "GUI viewer for HDF5, NetCDF4, and other scientific data formats",
      "detailed_description": "A data viewer application capable of reading and inspecting HDF5, NetCDF4, and other common scientific file formats.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "visualization",
        "data_inspection"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/titusjan/argos",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "hdf5",
        "netcdf",
        "data-viewer"
      ],
      "id": 435
    },
    {
      "name": "nii2dcm",
      "one_line_profile": "Tool for converting NIfTI medical images to DICOM format",
      "detailed_description": "A Python tool designed to convert NIfTI (Neuroimaging Informatics Technology Initiative) files into DICOM series.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "format_conversion",
        "medical_imaging"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/tomaroberts/nii2dcm",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "nifti",
        "dicom",
        "medical-imaging"
      ],
      "id": 436
    },
    {
      "name": "bio-vcf",
      "one_line_profile": "DSL and parser for filtering and processing VCF files",
      "detailed_description": "A smart parser and Domain Specific Language (DSL) for processing, filtering, and rewriting VCF (Variant Call Format) files in bioinformatics.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_processing",
        "variant_filtering"
      ],
      "application_level": "solver",
      "primary_language": "Ruby",
      "repo_url": "https://github.com/vcflib/bio-vcf",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "vcf",
        "bioinformatics",
        "dsl"
      ],
      "id": 437
    },
    {
      "name": "virtual-tiff",
      "one_line_profile": "Tool to create virtual Zarr stores from TIFF files",
      "detailed_description": "A utility to produce and explore virtual Zarr stores backed by TIFF files, enabling cloud-optimized access patterns without data duplication.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_io",
        "format_interoperability"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/virtual-zarr/virtual-tiff",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "zarr",
        "tiff",
        "cloud-native"
      ],
      "id": 438
    },
    {
      "name": "Vitessce",
      "one_line_profile": "Visual integration tool for spatial single-cell experiments",
      "detailed_description": "A visual integration tool for exploration of spatial single-cell experiment data, supporting various bio-imaging formats.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "visualization",
        "single_cell_analysis"
      ],
      "application_level": "solver",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/vitessce/vitessce",
      "help_website": [
        "http://vitessce.io/"
      ],
      "license": "MIT",
      "tags": [
        "single-cell",
        "visualization",
        "spatial-biology"
      ],
      "id": 439
    },
    {
      "name": "SeqLib",
      "one_line_profile": "C++ interface for querying and manipulating sequence data",
      "detailed_description": "A C++ library providing an interface to htslib, bwa-mem, and fermi for efficient interrogation and manipulation of sequencing data (BAM/CRAM/FASTQ).",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_analysis",
        "sequence_alignment"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/walaj/SeqLib",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "bioinformatics",
        "htslib",
        "sequencing"
      ],
      "id": 440
    },
    {
      "name": "ogr2ogr-wrapper",
      "one_line_profile": "Wrapper library for the ogr2ogr geospatial conversion tool",
      "detailed_description": "A TypeScript/JavaScript wrapper around the ogr2ogr command-line tool, facilitating geospatial data format conversion within Node.js applications.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "format_conversion",
        "geospatial_processing"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/wavded/ogr2ogr",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gis",
        "gdal",
        "ogr2ogr"
      ],
      "id": 441
    },
    {
      "name": "pydl",
      "one_line_profile": "Library of IDL astronomy routines converted to Python",
      "detailed_description": "A Python library implementing common astronomy routines originally written in IDL, facilitating migration and data analysis in Python.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_analysis",
        "migration_tools"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/weaverba137/pydl",
      "help_website": [
        "http://pydl.readthedocs.org/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "astronomy",
        "idl",
        "python"
      ],
      "id": 442
    },
    {
      "name": "zen3geo",
      "one_line_profile": "Library for geospatial data IO and processing",
      "detailed_description": "A Python library designed to simplify geospatial data input/output and processing tasks.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_io",
        "geospatial_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/weiji14/zen3geo",
      "help_website": [],
      "license": "LGPL-3.0",
      "tags": [
        "geospatial",
        "gis",
        "data-science"
      ],
      "id": 443
    },
    {
      "name": "wradlib",
      "one_line_profile": "Library for weather radar data processing",
      "detailed_description": "A Python library designed for processing weather radar data, including reading, correcting, and visualizing radar data formats.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_processing",
        "meteorology"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/wradlib/wradlib",
      "help_website": [
        "https://wradlib.org/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "weather-radar",
        "meteorology",
        "remote-sensing"
      ],
      "id": 444
    },
    {
      "name": "xcube",
      "one_line_profile": "Toolkit for generating and exploiting Earth observation data cubes",
      "detailed_description": "A Python package for generating, manipulating, and analyzing data cubes from Earth observation data, leveraging xarray, dask, and zarr.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_generation",
        "data_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/xcube-dev/xcube",
      "help_website": [
        "https://xcube.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "earth-observation",
        "data-cubes",
        "xarray"
      ],
      "id": 445
    },
    {
      "name": "xtensor-zarr",
      "one_line_profile": "C++ implementation of the Zarr core protocol",
      "detailed_description": "A C++ library implementing the Zarr core protocol (v2 and v3) based on the xtensor library, enabling efficient multidimensional array storage.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_io",
        "storage"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/xtensor-stack/xtensor-zarr",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "zarr",
        "c++",
        "xtensor"
      ],
      "id": 446
    },
    {
      "name": "ArchGDAL.jl",
      "one_line_profile": "High-level Julia API for GDAL",
      "detailed_description": "A Julia package providing a high-level interface to the GDAL (Geospatial Data Abstraction Library) for reading and writing geospatial data formats.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_io",
        "geospatial_processing"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/yeesian/ArchGDAL.jl",
      "help_website": [
        "https://yeesian.com/ArchGDAL.jl/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "julia",
        "gdal",
        "gis"
      ],
      "id": 447
    },
    {
      "name": "OvertureMapsDownloader",
      "one_line_profile": "Tool for downloading and processing Overture Maps data",
      "detailed_description": "A tool that simplifies the acquisition and manipulation of geospatial data from Overture Maps using DuckDB, Dask, and GDAL.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_acquisition",
        "geospatial_processing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/yharby/OvertureMapsDownloader",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "gis",
        "overture-maps",
        "data-download"
      ],
      "id": 448
    },
    {
      "name": "VirtualiZarr",
      "one_line_profile": "Tool to cloud-optimize scientific data as Virtual Zarr stores",
      "detailed_description": "A Python library to create virtual Zarr stores from existing data files, enabling cloud-optimized access via xarray syntax without data duplication.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_optimization",
        "data_io"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/zarr-developers/VirtualiZarr",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "zarr",
        "cloud-native",
        "xarray"
      ],
      "id": 449
    },
    {
      "name": "pydantic-zarr",
      "one_line_profile": "Pydantic models for Zarr data structures",
      "detailed_description": "A Python library providing Pydantic models for validating and defining Zarr data structures and metadata.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_validation",
        "metadata_management"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/zarr-developers/pydantic-zarr",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "zarr",
        "pydantic",
        "validation"
      ],
      "id": 450
    },
    {
      "name": "zarr-java",
      "one_line_profile": "Java implementation of the Zarr Specification for N-dimensional arrays",
      "detailed_description": "A Java library providing an implementation of the Zarr specification, enabling storage and retrieval of chunked, compressed, N-dimensional arrays. It serves as a core IO library for Java-based scientific applications requiring Zarr format support.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_io",
        "format_parsing"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/zarr-developers/zarr-java",
      "help_website": [
        "https://github.com/zarr-developers/zarr-java"
      ],
      "license": "MIT",
      "tags": [
        "zarr",
        "java",
        "multidimensional-arrays",
        "data-storage"
      ],
      "id": 451
    },
    {
      "name": "zarr-python",
      "one_line_profile": "Reference Python implementation of chunked, compressed, N-dimensional arrays",
      "detailed_description": "The official Python implementation of the Zarr format, providing an interface for chunked, compressed, N-dimensional arrays. It is widely used in genomics, bioimaging, and geospatial sciences for handling large-scale scientific datasets.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_io",
        "array_manipulation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/zarr-developers/zarr-python",
      "help_website": [
        "https://zarr.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "zarr",
        "python",
        "numpy",
        "scientific-data",
        "io"
      ],
      "id": 452
    },
    {
      "name": "zarrs",
      "one_line_profile": "Rust library for the Zarr storage format",
      "detailed_description": "A high-performance Rust library for creating, reading, and manipulating Zarr V3 and V2 data. It provides core IO capabilities for multidimensional arrays and metadata in the Rust ecosystem.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_io",
        "format_parsing"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/zarrs/zarrs",
      "help_website": [
        "https://docs.rs/zarrs/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "rust",
        "zarr",
        "data-storage",
        "multidimensional-arrays"
      ],
      "id": 453
    },
    {
      "name": "zarrs-python",
      "one_line_profile": "Python bindings for the zarrs Rust crate",
      "detailed_description": "Provides a high-performance CodecPipeline for zarr-python, backed by the zarrs Rust library. It accelerates Zarr data processing in Python environments by leveraging Rust's performance.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_io",
        "performance_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/zarrs/zarrs-python",
      "help_website": [
        "https://github.com/zarrs/zarrs-python"
      ],
      "license": "MIT",
      "tags": [
        "python",
        "rust",
        "zarr",
        "optimization"
      ],
      "id": 454
    },
    {
      "name": "ZnH5MD",
      "one_line_profile": "High Performance Interface for H5MD Trajectories",
      "detailed_description": "A Python interface designed for high-performance reading and writing of H5MD (HDF5 for Molecular Dynamics) files. It facilitates the handling of large-scale molecular dynamics trajectory data.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_io",
        "trajectory_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/zincware/ZnH5MD",
      "help_website": [
        "https://znh5md.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "molecular-dynamics",
        "hdf5",
        "h5md",
        "trajectory"
      ],
      "id": 455
    },
    {
      "name": "zarr-ml",
      "one_line_profile": "OCaml implementation of the Zarr storage format",
      "detailed_description": "An OCaml library implementing the Zarr specification for chunked and compressed multidimensional arrays, enabling scientific data IO in the OCaml ecosystem.",
      "domains": [
        "D1",
        "D1-02"
      ],
      "subtask_category": [
        "data_io",
        "format_parsing"
      ],
      "application_level": "library",
      "primary_language": "OCaml",
      "repo_url": "https://github.com/zoj613/zarr-ml",
      "help_website": [
        "https://zoj613.github.io/zarr-ml/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "ocaml",
        "zarr",
        "data-storage",
        "io"
      ],
      "id": 456
    },
    {
      "name": "MungeSumstats",
      "one_line_profile": "Rapid standardisation and quality control of GWAS or QTL summary statistics",
      "detailed_description": "A Bioconductor package designed to facilitate the standardization and quality control of Genome-Wide Association Study (GWAS) summary statistics. It handles formatting, filtering, and correcting inconsistencies to ensure data integrity for downstream analysis.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "standardization",
        "gwas_analysis"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/Al-Murphy/MungeSumstats",
      "help_website": [],
      "license": null,
      "tags": [
        "gwas",
        "bioinformatics",
        "quality-control",
        "statistics"
      ],
      "id": 457
    },
    {
      "name": "SOAPnuke",
      "one_line_profile": "Integrated Quality Control and Preprocessing tool for FASTQ or BAM/CRAM sequencing data",
      "detailed_description": "A software tool developed by BGI for integrated quality control and preprocessing of high-throughput sequencing data. It supports filtering low-quality reads, trimming adapters, and generating statistics for FASTQ, BAM, and CRAM files.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "preprocessing",
        "filtering"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/BGI-flexlab/SOAPnuke",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "bioinformatics",
        "ngs",
        "quality-control",
        "fastq"
      ],
      "id": 458
    },
    {
      "name": "SQANTI3",
      "one_line_profile": "Tool for the Quality Control of Long-Read Defined Transcriptomes",
      "detailed_description": "A pipeline for the structural classification and quality control of isoforms defined by long-read sequencing technologies (PacBio, Oxford Nanopore). It characterizes transcripts based on their splice junctions and compares them to a reference annotation.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "isoform_classification",
        "transcriptomics"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ConesaLab/SQANTI3",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "transcriptomics",
        "long-read-sequencing",
        "quality-control",
        "isoforms"
      ],
      "id": 459
    },
    {
      "name": "blobtools",
      "one_line_profile": "Modular command-line solution for visualisation, quality control and taxonomic partitioning of genome datasets",
      "detailed_description": "A toolset for the visualization, quality control, and taxonomic partitioning of genome assemblies. It allows researchers to identify contamination and assess the quality of genomic datasets using GC content, coverage, and taxonomy.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "visualization",
        "taxonomic_partitioning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/DRL/blobtools",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "genomics",
        "visualization",
        "quality-control",
        "taxonomy"
      ],
      "id": 460
    },
    {
      "name": "CheckM",
      "one_line_profile": "Assess the quality of microbial genomes recovered from isolates, single cells, and metagenomes",
      "detailed_description": "A tool for assessing the quality of microbial genomes recovered from isolates, single cells, and metagenomes. It provides robust estimates of genome completeness and contamination by using collocated sets of genes that are ubiquitous and single-copy within a phylogenetic lineage.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_assessment",
        "genome_recovery",
        "metagenomics"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Ecogenomics/CheckM",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "microbiology",
        "genomics",
        "quality-assessment",
        "metagenomics"
      ],
      "id": 461
    },
    {
      "name": "TrimGalore",
      "one_line_profile": "A wrapper around Cutadapt and FastQC for consistent adapter and quality trimming of FastQ files",
      "detailed_description": "Trim Galore is a wrapper script to automate quality and adapter trimming as well as quality control, with some added functionality to remove Galore-specific RRBS sequence diversity bias when processing RRBS libraries.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "adapter_trimming",
        "preprocessing"
      ],
      "application_level": "workflow",
      "primary_language": "Perl",
      "repo_url": "https://github.com/FelixKrueger/TrimGalore",
      "help_website": [
        "https://github.com/FelixKrueger/TrimGalore/blob/master/Docs/Trim_Galore_User_Guide.md"
      ],
      "license": "GPL-3.0",
      "tags": [
        "bioinformatics",
        "fastq",
        "quality-control",
        "ngs"
      ],
      "id": 462
    },
    {
      "name": "dropSeqPipe",
      "one_line_profile": "A Snakemake workflow for SingleCell RNASeq pre-processing",
      "detailed_description": "dropSeqPipe is a comprehensive pipeline for processing Drop-Seq data, handling steps from raw reads to expression matrices, including quality control and alignment.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "preprocessing",
        "quality_control",
        "alignment"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/Hoohm/dropSeqPipe",
      "help_website": [
        "https://github.com/Hoohm/dropSeqPipe"
      ],
      "license": "CC-BY-SA-4.0",
      "tags": [
        "bioinformatics",
        "rnaseq",
        "single-cell",
        "snakemake"
      ],
      "id": 463
    },
    {
      "name": "kgcl",
      "one_line_profile": "Data model library for the Knowledge Graph Change Language (KGCL)",
      "detailed_description": "Provides the data model and implementation for KGCL, a language designed for defining and executing changes in ontologies and knowledge graphs, widely used in biomedical informatics.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "ontology_management",
        "data_modeling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/INCATools/kgcl",
      "help_website": [
        "https://github.com/INCATools/kgcl"
      ],
      "license": "MIT",
      "tags": [
        "ontology",
        "knowledge-graph",
        "bioinformatics",
        "metadata"
      ],
      "id": 464
    },
    {
      "name": "kgcl-rdflib",
      "one_line_profile": "RDFLib-based tools for manipulating ontologies using KGCL",
      "detailed_description": "A Python library that provides functionality to apply Knowledge Graph Change Language (KGCL) operations on RDF graphs using RDFLib, facilitating ontology curation and evolution.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "ontology_manipulation",
        "graph_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/INCATools/kgcl-rdflib",
      "help_website": [
        "https://github.com/INCATools/kgcl-rdflib"
      ],
      "license": "MIT",
      "tags": [
        "rdflib",
        "ontology",
        "kgcl",
        "bioinformatics"
      ],
      "id": 465
    },
    {
      "name": "semantic-sql",
      "one_line_profile": "Library to create SQL and SQLite builds from OWL ontologies",
      "detailed_description": "A tool designed to convert OWL ontologies into SQL/SQLite formats, enabling efficient querying and integration of semantic data within relational database systems, commonly used in biomedical ontology pipelines.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "format_conversion",
        "ontology_querying"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/INCATools/semantic-sql",
      "help_website": [
        "https://github.com/INCATools/semantic-sql"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "ontology",
        "sql",
        "owl",
        "bioinformatics"
      ],
      "id": 466
    },
    {
      "name": "bombcell",
      "one_line_profile": "Automated quality control and curation tool for spike-sorted electrophysiology data",
      "detailed_description": "A toolbox for automated quality control and curation of spike-sorted data, allowing researchers to classify neurons and assess recording quality in electrophysiology experiments.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "spike_sorting_curation",
        "classification"
      ],
      "application_level": "solver",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/Julie-Fabre/bombcell",
      "help_website": [
        "https://github.com/Julie-Fabre/bombcell/wiki"
      ],
      "license": "GPL-3.0",
      "tags": [
        "neuroscience",
        "electrophysiology",
        "quality-control",
        "spike-sorting"
      ],
      "id": 467
    },
    {
      "name": "MegaQC",
      "one_line_profile": "Longitudinal quality control monitoring platform for MultiQC reports",
      "detailed_description": "MegaQC is a web application designed to aggregate, store, and visualize data from multiple MultiQC runs over time. It enables sequencing facilities and bioinformatics cores to track quality metrics across projects and monitor trends in sequencing performance.",
      "domains": [
        "D1",
        "D1-03",
        "Bioinformatics"
      ],
      "subtask_category": [
        "quality_control",
        "qc_reporting",
        "longitudinal_analysis"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/MultiQC/MegaQC",
      "help_website": [
        "https://megaqc.info/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "bioinformatics",
        "quality-control",
        "visualization",
        "dashboard"
      ],
      "id": 468
    },
    {
      "name": "MultiQC",
      "one_line_profile": "Aggregate bioinformatics analysis reports across many samples",
      "detailed_description": "MultiQC searches a given directory for analysis logs and compiles a HTML report. It's a general use tool, perfect for summarizing the output from numerous bioinformatics tools.",
      "domains": [
        "D1",
        "D1-03",
        "Bioinformatics"
      ],
      "subtask_category": [
        "quality_control",
        "qc_reporting",
        "data_aggregation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/MultiQC/MultiQC",
      "help_website": [
        "https://multiqc.info/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "bioinformatics",
        "ngs",
        "quality-control",
        "reporting"
      ],
      "id": 469
    },
    {
      "name": "MultiQC_SAV",
      "one_line_profile": "MultiQC plugin for Illumina Sequencing Analysis Viewer (SAV) data",
      "detailed_description": "A plugin for MultiQC that parses and visualizes InterOp data from Illumina sequencers, replicating plots found in the Illumina Sequencing Analysis Viewer (SAV) for quality control purposes.",
      "domains": [
        "D1",
        "D1-03",
        "Bioinformatics"
      ],
      "subtask_category": [
        "quality_control",
        "sequencing_qc"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/MultiQC/MultiQC_SAV",
      "help_website": [
        "https://github.com/MultiQC/MultiQC_SAV"
      ],
      "license": "MIT",
      "tags": [
        "illumina",
        "sequencing",
        "quality-control",
        "multiqc-plugin"
      ],
      "id": 470
    },
    {
      "name": "MultiQC_bcbio",
      "one_line_profile": "MultiQC plugin for bcbio-nextgen pipeline metrics",
      "detailed_description": "A plugin for MultiQC that incorporates metrics and logs specifically generated by the bcbio-nextgen bioinformatics analysis pipeline into the aggregate QC report.",
      "domains": [
        "D1",
        "D1-03",
        "Bioinformatics"
      ],
      "subtask_category": [
        "quality_control",
        "pipeline_reporting"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/MultiQC/MultiQC_bcbio",
      "help_website": [
        "https://github.com/MultiQC/MultiQC_bcbio"
      ],
      "license": "MIT",
      "tags": [
        "bcbio",
        "bioinformatics",
        "quality-control",
        "multiqc-plugin"
      ],
      "id": 471
    },
    {
      "name": "AfterQC",
      "one_line_profile": "Automatic filtering, trimming, error removing and quality control for FASTQ data",
      "detailed_description": "AfterQC is a tool for automatic quality control of NGS data. It performs filtering, trimming, error correction, and quality visualization for FASTQ files, designed to be faster and more automated than traditional tools.",
      "domains": [
        "D1",
        "D1-03",
        "Bioinformatics"
      ],
      "subtask_category": [
        "quality_control",
        "read_trimming",
        "error_correction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenGene/AfterQC",
      "help_website": [
        "https://github.com/OpenGene/AfterQC"
      ],
      "license": "MIT",
      "tags": [
        "fastq",
        "ngs",
        "quality-control",
        "trimming"
      ],
      "id": 472
    },
    {
      "name": "fastplong",
      "one_line_profile": "Ultra-fast preprocessing and quality control for long-read sequencing data",
      "detailed_description": "fastplong is a high-performance tool designed for the preprocessing and quality control of long-read sequencing data (e.g., Nanopore, PacBio). It offers functions for filtering, trimming, and generating QC statistics.",
      "domains": [
        "D1",
        "D1-03",
        "Bioinformatics"
      ],
      "subtask_category": [
        "quality_control",
        "read_preprocessing",
        "long_read_sequencing"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/OpenGene/fastplong",
      "help_website": [
        "https://github.com/OpenGene/fastplong"
      ],
      "license": "MIT",
      "tags": [
        "long-read",
        "nanopore",
        "pacbio",
        "quality-control"
      ],
      "id": 473
    },
    {
      "name": "rnaseq-pipeline",
      "one_line_profile": "RNA-seq pipeline for raw sequence alignment and quantification",
      "detailed_description": "A bioinformatics pipeline for processing RNA-seq data, handling steps from raw sequence alignment to transcript and gene quantification. Developed by the Pavlidis Lab.",
      "domains": [
        "D1",
        "Bioinformatics"
      ],
      "subtask_category": [
        "workflow",
        "alignment",
        "quantification"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/PavlidisLab/rnaseq-pipeline",
      "help_website": [
        "https://github.com/PavlidisLab/rnaseq-pipeline"
      ],
      "license": "Unlicense",
      "tags": [
        "rna-seq",
        "bioinformatics",
        "pipeline",
        "quantification"
      ],
      "id": 474
    },
    {
      "name": "Qoala-T",
      "one_line_profile": "Supervised-learning tool for quality control of FreeSurfer segmented MRI data",
      "detailed_description": "Qoala-T is a supervised learning tool designed to assess the quality of FreeSurfer-segmented MRI data. It helps in automatically identifying poor quality segmentations in neuroimaging datasets.",
      "domains": [
        "D1",
        "D1-03",
        "Neuroscience"
      ],
      "subtask_category": [
        "quality_control",
        "image_segmentation_qc",
        "neuroimaging"
      ],
      "application_level": "solver",
      "primary_language": "R",
      "repo_url": "https://github.com/Qoala-T/QC",
      "help_website": [
        "https://qoala-t.shinyapps.io/qoala-t_app/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "mri",
        "freesurfer",
        "quality-control",
        "neuroscience"
      ],
      "id": 475
    },
    {
      "name": "Schematic",
      "one_line_profile": "Biomedical data model and metadata management ingress tool",
      "detailed_description": "A package developed by Sage Bionetworks for managing biomedical data models and metadata ingress. It facilitates the validation and submission of data according to defined schemas, supporting data curation workflows in biomedical research.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "metadata_management",
        "schema_validation",
        "data_curation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Sage-Bionetworks/schematic",
      "help_website": [
        "https://schematic.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "biomedical",
        "metadata",
        "schema",
        "validation",
        "data-model"
      ],
      "id": 476
    },
    {
      "name": "Ketupa",
      "one_line_profile": "Deep learning framework for RF signal integrity analysis and simulation",
      "detailed_description": "A deep learning project applied to signal integrity and RF analysis. It automates modeling, simulation, and data storage of HFSS for patch antennas, transmission lines, vias, and connectors, training S-parameter models on simulation data.",
      "domains": [
        "Physics",
        "Engineering"
      ],
      "subtask_category": [
        "simulation",
        "modeling",
        "rf_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Shallot-2009/Ketupa",
      "help_website": [],
      "license": "BSD-2-Clause",
      "tags": [
        "deep-learning",
        "rf-engineering",
        "electromagnetics",
        "simulation"
      ],
      "id": 477
    },
    {
      "name": "3D-MedDiffusion",
      "one_line_profile": "3D medical diffusion model for controllable image generation",
      "detailed_description": "A 3D Medical Diffusion Model designed for controllable and high-quality medical image generation. It addresses the challenges of generating realistic 3D volumetric medical data for research and analysis.",
      "domains": [
        "Medical Imaging",
        "AI4S"
      ],
      "subtask_category": [
        "image_generation",
        "generative_modeling"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ShanghaiTech-IMPACT/3D-MedDiffusion",
      "help_website": [],
      "license": null,
      "tags": [
        "medical-imaging",
        "diffusion-models",
        "3d-generation",
        "deep-learning"
      ],
      "id": 478
    },
    {
      "name": "Cerberus",
      "one_line_profile": "Visual-Inertial-Leg Odometry (VILO) for legged robots",
      "detailed_description": "A state estimation framework for legged robots that fuses visual, inertial, and leg kinematics data to provide precise odometry. It is used in robotics research for localization and mapping.",
      "domains": [
        "Robotics",
        "Control Systems"
      ],
      "subtask_category": [
        "odometry",
        "state_estimation",
        "localization"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/ShuoYangRobotics/Cerberus",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "robotics",
        "odometry",
        "slam",
        "visual-inertial"
      ],
      "id": 479
    },
    {
      "name": "Cerberus2.0",
      "one_line_profile": "Low-drift Visual-Inertial-Leg Odometry for legged robots",
      "detailed_description": "The second version of the Cerberus VILO framework, offering improved precision and low-drift performance for state estimation in legged robotics research.",
      "domains": [
        "Robotics",
        "Control Systems"
      ],
      "subtask_category": [
        "odometry",
        "state_estimation",
        "localization"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/ShuoYangRobotics/Cerberus2.0",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "robotics",
        "odometry",
        "slam",
        "visual-inertial"
      ],
      "id": 480
    },
    {
      "name": "Cerberus (Histology)",
      "one_line_profile": "Multi-task learning model for histology image segmentation and classification",
      "detailed_description": "A deep learning model that enables simultaneous histology image segmentation and classification. It uses multi-task learning to improve performance on pathology image analysis tasks.",
      "domains": [
        "Bioinformatics",
        "Medical Imaging"
      ],
      "subtask_category": [
        "image_segmentation",
        "classification",
        "histology_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/TissueImageAnalytics/cerberus",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "histology",
        "pathology",
        "deep-learning",
        "segmentation"
      ],
      "id": 481
    },
    {
      "name": "Crowd-Kit",
      "one_line_profile": "Quality control and aggregation for crowdsourced data labeling",
      "detailed_description": "A Python library for computational quality control and aggregation of crowdsourced data. It provides efficient implementations of aggregation algorithms (like Dawid-Skene, GLAD) to ensure high-quality labeled datasets for machine learning research.",
      "domains": [
        "D1",
        "D1-03",
        "Machine Learning"
      ],
      "subtask_category": [
        "quality_control",
        "data_aggregation",
        "annotation_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Toloka/crowd-kit",
      "help_website": [
        "https://crowd-kit.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "crowdsourcing",
        "data-labeling",
        "quality-control",
        "aggregation"
      ],
      "id": 482
    },
    {
      "name": "FluLINE",
      "one_line_profile": "Comprehensive pipeline for influenza virus sequencing analysis",
      "detailed_description": "A bioinformatics pipeline for analyzing influenza sequencing data. It performs filtering, species identification, consensus genome generation, mapping, and SNV identification.",
      "domains": [
        "Bioinformatics",
        "Genomics"
      ],
      "subtask_category": [
        "pipeline",
        "variant_calling",
        "genome_assembly"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/UmaSangumathi/FluLINE",
      "help_website": [],
      "license": null,
      "tags": [
        "influenza",
        "ngs",
        "pipeline",
        "genomics"
      ],
      "id": 483
    },
    {
      "name": "openclean",
      "one_line_profile": "Data cleaning and profiling library for scientific data workflows",
      "detailed_description": "A Python library for data cleaning and profiling, developed by VIDA-NYU. It provides a declarative framework for detecting and fixing data quality issues, suitable for preparing scientific datasets.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_cleaning",
        "data_profiling",
        "quality_control"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/VIDA-NYU/openclean",
      "help_website": [
        "https://github.com/VIDA-NYU/openclean/tree/master/docs"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "data-cleaning",
        "profiling",
        "quality-control",
        "python"
      ],
      "id": 484
    },
    {
      "name": "ModelDB",
      "one_line_profile": "Open source ML model versioning and metadata management",
      "detailed_description": "A system for managing machine learning models, including versioning, metadata, and experiment tracking. Originally developed at MIT CSAIL, it supports scientific reproducibility in ML workflows.",
      "domains": [
        "D1",
        "Machine Learning"
      ],
      "subtask_category": [
        "experiment_tracking",
        "metadata_management",
        "reproducibility"
      ],
      "application_level": "platform",
      "primary_language": "Java",
      "repo_url": "https://github.com/VertaAI/modeldb",
      "help_website": [
        "https://modeldb.ai/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "mlops",
        "metadata",
        "experiment-tracking",
        "reproducibility"
      ],
      "id": 485
    },
    {
      "name": "cfDNApipe",
      "one_line_profile": "Quality control and analysis pipeline for cell-free DNA sequencing data",
      "detailed_description": "A comprehensive pipeline for the quality control and analysis of cell-free DNA (cfDNA) high-throughput sequencing data. It handles tasks specific to liquid biopsy data analysis.",
      "domains": [
        "Bioinformatics",
        "Genomics"
      ],
      "subtask_category": [
        "pipeline",
        "quality_control",
        "data_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/XWangLabTHU/cfDNApipe",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "cfdna",
        "ngs",
        "pipeline",
        "quality-control"
      ],
      "id": 486
    },
    {
      "name": "QuickNAT",
      "one_line_profile": "Fast brain MRI segmentation framework with uncertainty-based quality control",
      "detailed_description": "A PyTorch implementation of QuickNAT and Bayesian QuickNAT for fast and accurate segmentation of neuroanatomy from brain MRI scans. It features a structure-wise uncertainty estimation mechanism that serves as a quality control measure for the segmentation results.",
      "domains": [
        "D3",
        "D1-03"
      ],
      "subtask_category": [
        "segmentation",
        "quality_control"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ai-med/quickNAT_pytorch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mri",
        "neuroimaging",
        "segmentation",
        "quality-control",
        "uncertainty-estimation"
      ],
      "id": 487
    },
    {
      "name": "AGR Curation Schema",
      "one_line_profile": "Data schema and validation specifications for the Alliance of Genome Resources",
      "detailed_description": "The official schema repository for the Alliance of Genome Resources (AGR), defining the data models and validation rules for persistent data storage and curation of genomic data across multiple model organisms.",
      "domains": [
        "D1-03"
      ],
      "subtask_category": [
        "schema_definition",
        "data_validation"
      ],
      "application_level": "library",
      "primary_language": "Makefile",
      "repo_url": "https://github.com/alliance-genome/agr_curation_schema",
      "help_website": [
        "https://www.alliancegenome.org/"
      ],
      "license": "MIT",
      "tags": [
        "genomics",
        "schema",
        "data-curation",
        "model-organisms"
      ],
      "id": 488
    },
    {
      "name": "faster",
      "one_line_profile": "High-performance FASTQ file statistics and quality metrics calculator",
      "detailed_description": "A fast, Rust-based command-line tool for calculating statistics and quality metrics from FASTQ files, designed to provide rapid insights into sequencing data quality.",
      "domains": [
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "statistics"
      ],
      "application_level": "solver",
      "primary_language": "Rust",
      "repo_url": "https://github.com/angelovangel/faster",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fastq",
        "bioinformatics",
        "quality-control",
        "rust"
      ],
      "id": 489
    },
    {
      "name": "partialsmiles",
      "one_line_profile": "Validating SMILES parser with support for incomplete strings",
      "detailed_description": "A Python library for parsing and validating SMILES strings (Simplified Molecular Input Line Entry System), specifically designed to handle and validate incomplete or partial SMILES strings, useful for interactive chemistry applications and data cleaning.",
      "domains": [
        "D1-03"
      ],
      "subtask_category": [
        "parsing",
        "data_validation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/baoilleach/partialsmiles",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "cheminformatics",
        "smiles",
        "validation",
        "chemistry"
      ],
      "id": 490
    },
    {
      "name": "fastqc-viz",
      "one_line_profile": "Enhanced visualization for FastQC reports",
      "detailed_description": "A tool to generate improved, modernized visualizations from FastQC reports, aiding in the interpretation of quality control metrics for high-throughput sequencing data.",
      "domains": [
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "visualization"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/barreiro-r/fastqc-viz",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "fastqc",
        "visualization",
        "ngs",
        "bioinformatics"
      ],
      "id": 491
    },
    {
      "name": "pmultiqc",
      "one_line_profile": "Proteomics quality control reporting library based on MultiQC",
      "detailed_description": "A library designed to generate quality control reports for proteomics data by extending the MultiQC framework, enabling aggregated visualization of metrics from various proteomics analysis tools.",
      "domains": [
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "qc_report"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/bigbio/pmultiqc",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "proteomics",
        "quality-control",
        "multiqc",
        "bioinformatics"
      ],
      "id": 492
    },
    {
      "name": "KneadData",
      "one_line_profile": "Quality control and decontamination tool for metagenomic data",
      "detailed_description": "A tool designed to perform quality control on metagenomic and metatranscriptomic sequencing data. It separates host reads from bacterial reads and performs trimming/filtering to ensure high-quality data for downstream analysis.",
      "domains": [
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "filtering"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/biobakery/kneaddata",
      "help_website": [
        "http://huttenhower.sph.harvard.edu/kneaddata"
      ],
      "license": "NOASSERTION",
      "tags": [
        "metagenomics",
        "quality-control",
        "decontamination",
        "bioinformatics"
      ],
      "id": 493
    },
    {
      "name": "bioio",
      "one_line_profile": "Standardized image reading and metadata management for microscopy",
      "detailed_description": "A Python library providing a standardized interface for reading, writing, and managing metadata of microscopy images, supporting various bioimaging formats and facilitating seamless integration into analysis workflows.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "parsing",
        "metadata_extraction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/bioio-devs/bioio",
      "help_website": [
        "https://github.com/bioio-devs/bioio"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "microscopy",
        "bioimaging",
        "metadata",
        "io"
      ],
      "id": 494
    },
    {
      "name": "Biolink Model",
      "one_line_profile": "Data model and schema for biological entities and relationships",
      "detailed_description": "The Biolink Model is a high-level data model that defines a set of classes, slots, and relationships for representing biological knowledge. It serves as a schema for standardizing data exchange in knowledge graphs and translational science.",
      "domains": [
        "D1-03"
      ],
      "subtask_category": [
        "schema_definition",
        "data_modeling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/biolink/biolink-model",
      "help_website": [
        "https://biolink.github.io/biolink-model/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "knowledge-graph",
        "schema",
        "biology",
        "standardization"
      ],
      "id": 495
    },
    {
      "name": "biolinkml",
      "one_line_profile": "Modeling language and framework for biological entities (predecessor to LinkML)",
      "detailed_description": "A framework for defining data models and schemas for biological entities, facilitating the generation of JSON-Schema, SHACL, and other artifacts. It has been largely superseded by LinkML but remains relevant for legacy biological data models.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "schema_definition",
        "metadata_management"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/biolink/biolinkml",
      "help_website": [
        "https://biolink.github.io/biolinkml/"
      ],
      "license": "CC0-1.0",
      "tags": [
        "biolink",
        "schema",
        "metadata",
        "biology"
      ],
      "id": 496
    },
    {
      "name": "ccdhmodel",
      "one_line_profile": "LinkML schema definitions for Cancer Data Harmonization",
      "detailed_description": "Contains the LinkML model definitions for the Center for Cancer Data Harmonization (CCDH), used to harmonize and validate data across various cancer research programs.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "schema_definition",
        "data_harmonization"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/cancerDHC/ccdhmodel",
      "help_website": [
        "https://cancerdhc.github.io/ccdhmodel/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "cancer",
        "data-harmonization",
        "linkml",
        "schema"
      ],
      "id": 497
    },
    {
      "name": "chemrof",
      "one_line_profile": "Schema definitions for chemistry ontology classes",
      "detailed_description": "Provides schema definitions and structures for chemistry ontology classes, facilitating the organization and validation of chemical knowledge graph data.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "schema_definition",
        "ontology_management"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/chemkg/chemrof",
      "help_website": [],
      "license": "CC0-1.0",
      "tags": [
        "chemistry",
        "ontology",
        "schema",
        "knowledge-graph"
      ],
      "id": 498
    },
    {
      "name": "CheckM2",
      "one_line_profile": "Machine learning tool for assessing metagenome bin quality",
      "detailed_description": "A tool that uses machine learning to assess the quality (completeness and contamination) of metagenome-derived genome bins, improving upon previous lineage-specific marker set methods.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "metagenomics"
      ],
      "application_level": "solver",
      "primary_language": "Scilab",
      "repo_url": "https://github.com/chklovski/CheckM2",
      "help_website": [
        "https://github.com/chklovski/CheckM2"
      ],
      "license": "GPL-3.0",
      "tags": [
        "metagenomics",
        "quality-control",
        "genome-binning",
        "machine-learning"
      ],
      "id": 499
    },
    {
      "name": "HistoQC",
      "one_line_profile": "Quality control tool for digital pathology slides",
      "detailed_description": "An open-source quality control tool designed for digital pathology slides. It automatically identifies artifacts and assesses the suitability of slides for computational analysis.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "image_processing"
      ],
      "application_level": "solver",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/choosehappy/HistoQC",
      "help_website": [
        "https://github.com/choosehappy/HistoQC"
      ],
      "license": "BSD-3-Clause-Clear",
      "tags": [
        "pathology",
        "quality-control",
        "histology",
        "medical-imaging"
      ],
      "id": 500
    },
    {
      "name": "DataHarmonizer",
      "one_line_profile": "Standardized spreadsheet editor and validator for public health data",
      "detailed_description": "A browser-based spreadsheet editor and validator that facilitates the harmonization of data according to defined schemas (e.g., LinkML). It includes templates for SARS-CoV-2 and Monkeypox sampling data.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_harmonization",
        "data_validation",
        "data_entry"
      ],
      "application_level": "platform",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/cidgoh/DataHarmonizer",
      "help_website": [
        "https://github.com/cidgoh/DataHarmonizer"
      ],
      "license": "MIT",
      "tags": [
        "epidemiology",
        "data-harmonization",
        "validation",
        "genomics"
      ],
      "id": 501
    },
    {
      "name": "Epsilon",
      "one_line_profile": "GPS data integrity verification algorithm suite",
      "detailed_description": "A suite of algorithms designed to verify the integrity of received GPS data and ranging signals, improving resiliency against signal loss and spoofing for navigation and timing applications.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "signal_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/cisagov/Epsilon",
      "help_website": [],
      "license": "CC0-1.0",
      "tags": [
        "gps",
        "signal-integrity",
        "pnt",
        "navigation"
      ],
      "id": 502
    },
    {
      "name": "PNT-Integrity",
      "one_line_profile": "Library for verifying GPS/PNT signal integrity",
      "detailed_description": "Provides methods to verify the integrity of received GPS data and ranging signals, serving as a reference implementation for detecting anomalies in Positioning, Navigation, and Timing (PNT) data.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "signal_processing"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/cisagov/PNT-Integrity",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "gps",
        "pnt",
        "integrity-check",
        "signal-processing"
      ],
      "id": 503
    },
    {
      "name": "Clowder",
      "one_line_profile": "Research data management and analysis platform",
      "detailed_description": "A data management system that enables users to share, annotate, organize, and analyze large collections of datasets. It supports extensible metadata annotation (JSON-LD) and automated extraction pipelines.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_management",
        "metadata_management",
        "workflow_automation"
      ],
      "application_level": "platform",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/clowder-framework/clowder",
      "help_website": [
        "https://clowderframework.org/"
      ],
      "license": "NCSA",
      "tags": [
        "data-management",
        "metadata",
        "curation",
        "research-data"
      ],
      "id": 504
    },
    {
      "name": "json-flattener",
      "one_line_profile": "Utility for denormalizing nested JSON objects to tables",
      "detailed_description": "A Python library used in bioinformatics pipelines to denormalize nested dictionaries or JSON objects (such as ontology terms) into tabular formats and vice versa.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_conversion",
        "data_wrangling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/cmungall/json-flattener",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "json",
        "conversion",
        "bioinformatics",
        "flattening"
      ],
      "id": 505
    },
    {
      "name": "linkml-phenopackets",
      "one_line_profile": "LinkML rendering and tools for Phenopackets",
      "detailed_description": "Provides tools and schema definitions for working with the Phenopackets standard using the LinkML modeling framework, facilitating phenotypic data exchange.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "schema_definition",
        "data_conversion"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/cmungall/linkml-phenopackets",
      "help_website": [],
      "license": null,
      "tags": [
        "phenopackets",
        "linkml",
        "phenotype",
        "schema"
      ],
      "id": 506
    },
    {
      "name": "semantic-llama",
      "one_line_profile": "LLM-based scientific knowledge extraction tool",
      "detailed_description": "A tool that leverages Large Language Models (LLMs) to extract structured semantic information and knowledge from unstructured text, tailored for scientific curation tasks.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "knowledge_extraction",
        "nlp"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/cmungall/semantic-llama",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "llm",
        "knowledge-extraction",
        "semantics",
        "curation"
      ],
      "id": 507
    },
    {
      "name": "GrandQC",
      "one_line_profile": "Quality control and tissue detection tool for digital pathology slides",
      "detailed_description": "GrandQC is a tool designed for the quality control of digital pathology images. It performs tissue detection and identifies artifacts or quality issues in whole slide images (WSIs), facilitating the preprocessing steps required for computational pathology and AI model training in medical research.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "image_processing",
        "tissue_detection"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/cpath-ukk/grandqc",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "pathology",
        "quality-control",
        "wsi",
        "tissue-detection"
      ],
      "id": 508
    },
    {
      "name": "FastQC (CSF Fork)",
      "one_line_profile": "Quality control tool for high throughput sequence data (CSF fork)",
      "detailed_description": "This is a fork of the standard FastQC tool, adapted for usage on selected reads of unaligned BAM files. It provides a modular set of analyses which can be used to give a quick impression of whether your data has any problems of which you should be aware before doing any further analysis.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "ngs_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Java",
      "repo_url": "https://github.com/csf-ngs/fastqc",
      "help_website": [
        "http://www.bioinformatics.babraham.ac.uk/projects/fastqc/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "fastqc",
        "ngs",
        "quality-control",
        "bam"
      ],
      "id": 509
    },
    {
      "name": "PLUTON",
      "one_line_profile": "Automated shell pipeline for NIPT data preprocessing and aneuploidy prediction",
      "detailed_description": "An automated pipeline wrapping tools like FastQC, SAMtools, and WisecondorX to execute pre-processing of aligned reads and predict fetal gender and chromosomal aneuploidies.",
      "domains": [
        "D1-03",
        "D2"
      ],
      "subtask_category": [
        "quality_control",
        "variant_calling",
        "workflow"
      ],
      "application_level": "workflow",
      "primary_language": "Shell",
      "repo_url": "https://github.com/epigen-bioinfolab/PLUTON",
      "help_website": [],
      "license": null,
      "tags": [
        "nipt",
        "bioinformatics",
        "pipeline",
        "aneuploidy"
      ],
      "id": 510
    },
    {
      "name": "nanoq",
      "one_line_profile": "Ultra-fast quality control and filtering for Nanopore sequencing reads",
      "detailed_description": "A minimal but speedy quality control tool for nanopore reads written in Rust, providing read filtering and summary statistics.",
      "domains": [
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "filtering"
      ],
      "application_level": "solver",
      "primary_language": "Rust",
      "repo_url": "https://github.com/esteinig/nanoq",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nanopore",
        "quality-control",
        "bioinformatics",
        "fastq"
      ],
      "id": 511
    },
    {
      "name": "FAIR4Health Data Curation Tool",
      "one_line_profile": "Desktop tool for cleaning, validating and harmonizing health datasets for FAIR compliance",
      "detailed_description": "A tool developed by the FAIR4Health project to assist researchers in curating and validating health data to ensure it meets FAIR principles.",
      "domains": [
        "D1-03"
      ],
      "subtask_category": [
        "data_curation",
        "validation",
        "harmonization"
      ],
      "application_level": "solver",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/fair4health/data-curation-tool",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "fair-data",
        "health-informatics",
        "data-curation"
      ],
      "id": 512
    },
    {
      "name": "fastqc-rs",
      "one_line_profile": "High-performance Rust implementation of the FastQC quality control tool",
      "detailed_description": "A quality control tool for FASTQ files written in Rust, offering a faster alternative to the original Java-based FastQC.",
      "domains": [
        "D1-03"
      ],
      "subtask_category": [
        "quality_control"
      ],
      "application_level": "solver",
      "primary_language": "Rust",
      "repo_url": "https://github.com/fastqc-rs/fastqc-rs",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fastq",
        "quality-control",
        "bioinformatics",
        "rust"
      ],
      "id": 513
    },
    {
      "name": "frictionless-ci",
      "one_line_profile": "Continuous integration service for validating tabular data packages in repositories",
      "detailed_description": "A data management service that brings continuous data validation to tabular data in your repository via Github Action, ensuring data quality in scientific workflows.",
      "domains": [
        "D1-03"
      ],
      "subtask_category": [
        "validation",
        "quality_control"
      ],
      "application_level": "service",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/frictionlessdata/frictionless-ci",
      "help_website": [
        "https://frictionlessdata.io"
      ],
      "license": "MIT",
      "tags": [
        "data-validation",
        "continuous-integration",
        "frictionless-data"
      ],
      "id": 514
    },
    {
      "name": "frictionless-darwin-core",
      "one_line_profile": "Converter and validator for Darwin Core Archive biodiversity data using Frictionless standards",
      "detailed_description": "A tool to handle DarwinCore Archives as Frictionless Data Packages, facilitating the validation and management of biodiversity data.",
      "domains": [
        "D1-03",
        "D1"
      ],
      "subtask_category": [
        "data_conversion",
        "validation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/frictionlessdata/frictionless-darwin-core",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "biodiversity",
        "darwin-core",
        "frictionless-data",
        "metadata"
      ],
      "id": 515
    },
    {
      "name": "Frictionless Framework (Python)",
      "one_line_profile": "Data management framework to describe, extract, validate, and transform tabular data",
      "detailed_description": "A comprehensive framework for Python that provides functionality to describe, extract, validate, and transform tabular data, widely used in research data management.",
      "domains": [
        "D1-03",
        "D1"
      ],
      "subtask_category": [
        "validation",
        "metadata_management",
        "data_cleaning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/frictionlessdata/frictionless-py",
      "help_website": [
        "https://framework.frictionlessdata.io"
      ],
      "license": "MIT",
      "tags": [
        "data-validation",
        "metadata",
        "tabular-data",
        "frictionless-data"
      ],
      "id": 516
    },
    {
      "name": "frictionless-r",
      "one_line_profile": "R interface for reading, writing, and validating Frictionless Data Packages",
      "detailed_description": "An R package to read and write Frictionless Data Packages, allowing R users to integrate Frictionless validation and metadata standards into their analysis pipelines.",
      "domains": [
        "D1-03",
        "D1"
      ],
      "subtask_category": [
        "validation",
        "data_io"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/frictionlessdata/frictionless-r",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "r-package",
        "data-validation",
        "frictionless-data"
      ],
      "id": 517
    },
    {
      "name": "RNA-SeQC",
      "one_line_profile": "Efficient tool for computing quality control metrics for RNA-seq data",
      "detailed_description": "Fast, efficient RNA-Seq metrics for quality control and process optimization, providing essential metrics for transcriptomics data analysis.",
      "domains": [
        "D1-03"
      ],
      "subtask_category": [
        "quality_control"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/getzlab/rnaseqc",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "rna-seq",
        "quality-control",
        "bioinformatics",
        "genomics"
      ],
      "id": 518
    },
    {
      "name": "tableschema-to-template",
      "one_line_profile": "Tool to generate Excel data entry templates from Frictionless Table Schemas for HuBMAP data submission",
      "detailed_description": "Developed by the HuBMAP Consortium, this tool converts Frictionless Table Schema definitions into Excel spreadsheets with built-in validation, facilitating standardized metadata collection and data submission in biomedical research.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "metadata_management",
        "schema_validation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/hubmapconsortium/tableschema-to-template",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "metadata",
        "hubmap",
        "frictionless-data",
        "biomedical-data"
      ],
      "id": 519
    },
    {
      "name": "compliance-checker",
      "one_line_profile": "Tool to check oceanographic datasets against metadata compliance standards",
      "detailed_description": "A Python tool developed by IOOS to check local or remote datasets (NetCDF, OPeNDAP) against various compliance standards such as CF (Climate and Forecast), ACDD, and ISO metadata standards, ensuring data interoperability in earth sciences.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "metadata_validation",
        "standard_compliance"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ioos/compliance-checker",
      "help_website": [
        "https://ioos.github.io/compliance-checker/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "oceanography",
        "netcdf",
        "metadata-validation",
        "cf-conventions"
      ],
      "id": 520
    },
    {
      "name": "iRODS",
      "one_line_profile": "Open source data management software for data virtualization and workflow automation",
      "detailed_description": "The Integrated Rule-Oriented Data System (iRODS) is a data management software used in high-performance computing and scientific research (genomics, physics, etc.) to manage data lifecycle, metadata, and provenance across distributed storage resources.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_management",
        "workflow_automation",
        "metadata_management"
      ],
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/irods/irods",
      "help_website": [
        "https://irods.org/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "data-management",
        "hpc",
        "metadata",
        "workflow"
      ],
      "id": 521
    },
    {
      "name": "TaxTriage",
      "one_line_profile": "Nextflow workflow for taxonomic classification and triage of metagenomic NGS data",
      "detailed_description": "A comprehensive bioinformatics workflow designed to identify and classify microbial organisms within short- or long-read metagenomic Next-Generation Sequencing (NGS) data. It performs quality control, host removal, and taxonomic classification to triage samples for further analysis.",
      "domains": [
        "D1",
        "D6"
      ],
      "subtask_category": "taxonomic_classification",
      "application_level": "workflow",
      "primary_language": "Java",
      "repo_url": "https://github.com/jhuapl-bio/taxtriage",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "metagenomics",
        "ngs",
        "microbiome",
        "nextflow",
        "bioinformatics"
      ],
      "id": 522
    },
    {
      "name": "fastqcr",
      "one_line_profile": "Quality control and reporting for sequencing data",
      "detailed_description": "An R package designed for quality control (QC) of high-throughput sequencing data. It facilitates running FastQC from R, aggregating reports, and generating summaries to assess the quality of raw sequencing reads (FASTQ files).",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": "quality_control",
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/kassambara/fastqcr",
      "help_website": [
        "http://www.sthda.com/english/wiki/fastqcr-an-r-package-facilitating-quality-controls-of-sequencing-data"
      ],
      "license": "GPL-3.0",
      "tags": [
        "fastq",
        "quality-control",
        "ngs",
        "bioinformatics",
        "r-package"
      ],
      "id": 523
    },
    {
      "name": "FastQt",
      "one_line_profile": "Qt5 port of FastQC for high throughput sequence data quality control",
      "detailed_description": "A quality control tool for high throughput sequence data, serving as a C++/Qt5 port of the popular FastQC tool. It provides interactive visualization and analysis of FASTQ files to ensure data quality before downstream analysis.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "sequence_analysis"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/labsquare/FastQt",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "fastqc",
        "ngs",
        "quality-control",
        "bioinformatics"
      ],
      "id": 524
    },
    {
      "name": "FastqCleaner",
      "one_line_profile": "Shiny application for FASTQ quality control and filtering",
      "detailed_description": "A Shiny application and R package designed for quality control, filtering, and trimming of FASTQ files, enabling interactive data cleaning for NGS workflows.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "data_filtering"
      ],
      "application_level": "solver",
      "primary_language": "HTML",
      "repo_url": "https://github.com/leandroroser/FastqCleaner",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "fastq",
        "quality-control",
        "shiny",
        "bioinformatics"
      ],
      "id": 525
    },
    {
      "name": "LinkML",
      "one_line_profile": "Linked Open Data Modeling Language and toolkit",
      "detailed_description": "A general-purpose modeling language and toolkit for defining data schemas (ontologies, data models) that can be compiled into various artifacts (JSON-Schema, SHACL, SQL DDL, Python dataclasses), widely used in scientific data standardization.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "schema_definition",
        "data_modeling",
        "validation"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/linkml/linkml",
      "help_website": [
        "https://linkml.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "schema",
        "metadata",
        "linked-data",
        "modeling"
      ],
      "id": 526
    },
    {
      "name": "linkml-arrays",
      "one_line_profile": "N-dimensional array support for LinkML",
      "detailed_description": "An extension for LinkML to support loading, dumping, and validating N-dimensional arrays, facilitating the integration of complex scientific data structures into LinkML models.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_modeling",
        "serialization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/linkml/linkml-arrays",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "arrays",
        "linkml",
        "scientific-data"
      ],
      "id": 527
    },
    {
      "name": "linkml-datalog",
      "one_line_profile": "Datalog translation and inference for LinkML schemas",
      "detailed_description": "A tool that translates LinkML schemas into Datalog programs and executes them using Souffle, enabling advanced validation and logical inference over scientific instance data.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "validation",
        "inference"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/linkml/linkml-datalog",
      "help_website": [],
      "license": null,
      "tags": [
        "datalog",
        "inference",
        "validation",
        "linkml"
      ],
      "id": 528
    },
    {
      "name": "linkml-dataops",
      "one_line_profile": "Data manipulation API for LinkML instances",
      "detailed_description": "A library providing a data API for manipulating, querying, and validating LinkML instance data, supporting operations like patching and diffing.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_processing",
        "data_manipulation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/linkml/linkml-dataops",
      "help_website": [],
      "license": null,
      "tags": [
        "data-ops",
        "linkml",
        "api"
      ],
      "id": 529
    },
    {
      "name": "linkml-map",
      "one_line_profile": "Schema mapping tool for LinkML",
      "detailed_description": "A tool for defining and executing mappings between different LinkML schemas, facilitating data transformation and interoperability between scientific data standards.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_transformation",
        "schema_mapping"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/linkml/linkml-map",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "mapping",
        "transformation",
        "linkml"
      ],
      "id": 530
    },
    {
      "name": "linkml-owl",
      "one_line_profile": "LinkML to OWL converter",
      "detailed_description": "An extension of the LinkML runtime for converting instances of LinkML classes to OWL (Web Ontology Language) classes or instances, bridging data modeling with semantic web ontologies.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_conversion",
        "ontology_mapping"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/linkml/linkml-owl",
      "help_website": [],
      "license": null,
      "tags": [
        "owl",
        "ontology",
        "semantic-web",
        "linkml"
      ],
      "id": 531
    },
    {
      "name": "linkml-reference-validator",
      "one_line_profile": "Validator for text quotes in scientific data",
      "detailed_description": "A tool to validate that supporting text quotes in data actually appear in their cited references, useful for data curation and quality control in scientific databases.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "validation",
        "quality_control"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/linkml/linkml-reference-validator",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "validation",
        "curation",
        "text-mining"
      ],
      "id": 532
    },
    {
      "name": "linkml-renderer",
      "one_line_profile": "Renderer for LinkML instance data",
      "detailed_description": "A library and tool for rendering LinkML instance data into various formats such as HTML, Markdown, and Mermaid diagrams, aiding in data visualization and documentation.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "visualization",
        "reporting"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/linkml/linkml-renderer",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rendering",
        "visualization",
        "documentation"
      ],
      "id": 533
    },
    {
      "name": "linkml-runtime",
      "one_line_profile": "Runtime library for LinkML models",
      "detailed_description": "The core runtime support library for working with LinkML generated models in Python, providing utilities for data loading, dumping, and manipulation.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_processing",
        "serialization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/linkml/linkml-runtime",
      "help_website": [],
      "license": "CC0-1.0",
      "tags": [
        "runtime",
        "linkml",
        "serialization"
      ],
      "id": 534
    },
    {
      "name": "linkml-runtime.js",
      "one_line_profile": "JavaScript runtime for LinkML",
      "detailed_description": "The JavaScript implementation of the LinkML runtime, enabling the use of LinkML models and validation in web and Node.js environments.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_processing",
        "validation"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/linkml/linkml-runtime.js",
      "help_website": [],
      "license": null,
      "tags": [
        "javascript",
        "linkml",
        "runtime"
      ],
      "id": 535
    },
    {
      "name": "linkml-solr",
      "one_line_profile": "Solr integration for LinkML",
      "detailed_description": "A wrapper and utility for using Apache Solr with LinkML schemas, facilitating the indexing and querying of scientific data modeled with LinkML.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_indexing",
        "data_storage"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/linkml/linkml-solr",
      "help_website": [],
      "license": null,
      "tags": [
        "solr",
        "search",
        "indexing",
        "linkml"
      ],
      "id": 536
    },
    {
      "name": "linkml-sparql",
      "one_line_profile": "LinkML to SPARQL mapper",
      "detailed_description": "A library that maps LinkML queries to SPARQL and back, enabling semantic queries over LinkML-compliant data stored in RDF stores.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_querying",
        "semantic_web"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/linkml/linkml-sparql",
      "help_website": [],
      "license": null,
      "tags": [
        "sparql",
        "rdf",
        "querying"
      ],
      "id": 537
    },
    {
      "name": "linkml-store",
      "one_line_profile": "Storage abstraction for LinkML",
      "detailed_description": "A wrapper library providing a unified interface for multiple storage engines (e.g., SQL, MongoDB, Solr) for LinkML data.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_storage",
        "database_interface"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/linkml/linkml-store",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "storage",
        "database",
        "abstraction"
      ],
      "id": 538
    },
    {
      "name": "prefixmaps",
      "one_line_profile": "Semantic prefix map registry and library",
      "detailed_description": "A Python library and registry for managing semantic prefix maps (CURIEs), essential for resolving identifiers in scientific data and semantic web applications.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "identifier_resolution",
        "metadata_management"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/linkml/prefixmaps",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "curie",
        "prefixes",
        "semantic-web"
      ],
      "id": 539
    },
    {
      "name": "schema-automator",
      "one_line_profile": "Automated schema induction tool",
      "detailed_description": "A toolkit for automating the schema development lifecycle, including inducing LinkML schemas from structured data sources like TSV, JSON, or other schema formats.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "schema_induction",
        "data_modeling"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/linkml/schema-automator",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "automation",
        "schema-inference",
        "modeling"
      ],
      "id": 540
    },
    {
      "name": "schemasheets",
      "one_line_profile": "Schema management via spreadsheets",
      "detailed_description": "A tool that allows users to structure and define data schemas using Google Sheets or TSVs, which are then converted to LinkML and other formats, promoting FAIR data practices.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "schema_definition",
        "data_curation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/linkml/schemasheets",
      "help_website": [],
      "license": null,
      "tags": [
        "spreadsheets",
        "fair-data",
        "schema-conversion"
      ],
      "id": 541
    },
    {
      "name": "semantic-dsl",
      "one_line_profile": "Domain Specific Language creator for schemas",
      "detailed_description": "A library for creating easy-to-use Domain Specific Languages (DSLs) for scientific schemas, simplifying the process of defining complex data models.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "schema_definition",
        "dsl"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/linkml/semantic-dsl",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "dsl",
        "modeling",
        "semantics"
      ],
      "id": 542
    },
    {
      "name": "sparqlfun",
      "one_line_profile": "SPARQL templating library",
      "detailed_description": "A library for managing and executing SPARQL templates, providing a Python wrapper to simplify querying RDF data in scientific workflows.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_querying",
        "sparql"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/linkml/sparqlfun",
      "help_website": [],
      "license": "CC0-1.0",
      "tags": [
        "sparql",
        "templates",
        "rdf"
      ],
      "id": 543
    },
    {
      "name": "sssom-py",
      "one_line_profile": "Python toolkit for working with SSSOM (Simple Standard for Sharing Ontology Mappings) metadata",
      "detailed_description": "A Python library and command-line toolkit for manipulating, validating, and converting ontology mapping sets in the SSSOM format, facilitating interoperability between scientific ontologies.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "metadata_management",
        "ontology_mapping",
        "data_conversion"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/mapping-commons/sssom-py",
      "help_website": [
        "https://mapping-commons.github.io/sssom-py/"
      ],
      "license": "MIT",
      "tags": [
        "ontology",
        "metadata",
        "mapping",
        "sssom",
        "bioinformatics"
      ],
      "id": 544
    },
    {
      "name": "nmdc-schema",
      "one_line_profile": "Unified data model and validation schema for the National Microbiome Data Collaborative",
      "detailed_description": "Provides the schema definitions and generated Python libraries (Pydantic models) for validating and structuring microbiome data within the NMDC ecosystem, ensuring data interoperability and quality.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "metadata_schema",
        "data_validation",
        "data_modeling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/microbiomedata/nmdc-schema",
      "help_website": [
        "https://microbiomedata.github.io/nmdc-schema/"
      ],
      "license": "CC0-1.0",
      "tags": [
        "microbiome",
        "schema",
        "metadata",
        "linkml",
        "validation"
      ],
      "id": 545
    },
    {
      "name": "NMDC Sample Annotator",
      "one_line_profile": "Tool for annotating microbiome samples with NMDC-compliant metadata",
      "detailed_description": "A tool designed to assist researchers in annotating biosamples with standardized metadata according to the National Microbiome Data Collaborative (NMDC) schema.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "metadata_curation",
        "sample_annotation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/microbiomedata/sample-annotator",
      "help_website": [],
      "license": null,
      "tags": [
        "microbiome",
        "metadata",
        "annotation",
        "nmdc"
      ],
      "id": 546
    },
    {
      "name": "Koza",
      "one_line_profile": "Data transformation framework for ingesting data into LinkML-compliant knowledge graphs",
      "detailed_description": "A data transformation framework used to transform source data (CSV, JSON, etc.) into a target data model (LinkML), primarily used for building biological knowledge graphs.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_transformation",
        "knowledge_graph_construction",
        "etl"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/monarch-initiative/koza",
      "help_website": [
        "https://koza.monarchinitiative.org/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "etl",
        "knowledge-graph",
        "linkml",
        "data-transformation"
      ],
      "id": 547
    },
    {
      "name": "OntoGPT",
      "one_line_profile": "LLM-based tool for extracting structured ontological information from unstructured text",
      "detailed_description": "A Python package that uses Large Language Models (LLMs) to extract structured information from text, conforming to LinkML schemas, useful for knowledge base curation and ontology population.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "knowledge_extraction",
        "ontology_population",
        "text_mining"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/monarch-initiative/ontogpt",
      "help_website": [
        "https://monarch-initiative.github.io/ontogpt/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "llm",
        "ontology",
        "knowledge-extraction",
        "nlp",
        "bioinformatics"
      ],
      "id": 548
    },
    {
      "name": "TidyMultiqc",
      "one_line_profile": "R package to convert MultiQC reports into tidy data frames for downstream analysis",
      "detailed_description": "Provides functions to parse the output of MultiQC (a common bioinformatics QC tool) into tidy data frames, enabling easy statistical analysis and visualization in R.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "qc_reporting",
        "data_parsing",
        "data_conversion"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/multimeric/TidyMultiqc",
      "help_website": [
        "https://multimeric.github.io/TidyMultiqc/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "multiqc",
        "bioinformatics",
        "r",
        "tidy-data",
        "quality-control"
      ],
      "id": 549
    },
    {
      "name": "NASA MMT",
      "one_line_profile": "Web-based tool for managing metadata in the NASA Common Metadata Repository (CMR)",
      "detailed_description": "The Metadata Management Tool (MMT) allows users to create, update, and manage metadata records within NASA's Common Metadata Repository, supporting Earth Science data stewardship.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "metadata_management",
        "data_curation"
      ],
      "application_level": "platform",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/nasa/mmt",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "nasa",
        "metadata",
        "cmr",
        "earth-science",
        "data-management"
      ],
      "id": 550
    },
    {
      "name": "nf-core/hgtseq",
      "one_line_profile": "Bioinformatics pipeline for investigating horizontal gene transfer from NGS data",
      "detailed_description": "A Nextflow pipeline designed to detect and analyze horizontal gene transfer events using Next Generation Sequencing data, facilitating evolutionary biology research.",
      "domains": [
        "D1",
        "Bioinformatics"
      ],
      "subtask_category": [
        "sequence_analysis",
        "horizontal_gene_transfer_detection"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/nf-core/hgtseq",
      "help_website": [
        "https://nf-co.re/hgtseq"
      ],
      "license": "MIT",
      "tags": [
        "bioinformatics",
        "ngs",
        "hgt",
        "nextflow"
      ],
      "id": 551
    },
    {
      "name": "nf-core/rnaseq",
      "one_line_profile": "Comprehensive RNA sequencing analysis pipeline",
      "detailed_description": "A standard bioinformatics pipeline for RNA-seq data analysis, integrating aligners (STAR, HISAT2) and quantification tools (Salmon) with extensive quality control reporting.",
      "domains": [
        "D1",
        "Bioinformatics"
      ],
      "subtask_category": [
        "rna_seq_analysis",
        "quality_control",
        "quantification"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/nf-core/rnaseq",
      "help_website": [
        "https://nf-co.re/rnaseq"
      ],
      "license": "MIT",
      "tags": [
        "bioinformatics",
        "rna-seq",
        "gene-expression",
        "nextflow"
      ],
      "id": 552
    },
    {
      "name": "nf-core/seqinspector",
      "one_line_profile": "Dedicated quality control pipeline for sequencing data",
      "detailed_description": "A pipeline focused solely on generating comprehensive quality control reports for large-scale sequencing datasets, suitable for core facilities and high-throughput research.",
      "domains": [
        "D1",
        "D1-03",
        "Bioinformatics"
      ],
      "subtask_category": [
        "quality_control",
        "qc_report"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/nf-core/seqinspector",
      "help_website": [
        "https://nf-co.re/seqinspector"
      ],
      "license": "MIT",
      "tags": [
        "bioinformatics",
        "qc",
        "ngs",
        "multiqc"
      ],
      "id": 553
    },
    {
      "name": "datapackage-m",
      "one_line_profile": "Power Query M functions for Frictionless Data Packages",
      "detailed_description": "A library enabling Power BI and Excel to directly ingest and parse Tabular Data Packages, a standard format in Open Science and research data management.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_parsing",
        "data_import"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/nimblelearn/datapackage-m",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "frictionless-data",
        "power-bi",
        "data-package",
        "open-data"
      ],
      "id": 554
    },
    {
      "name": "MRIQC",
      "one_line_profile": "Automated Quality Control for structural and functional MRI",
      "detailed_description": "A tool for extracting quality measures from structural (T1w, T2w) and functional MRI data, generating visual reports to assess data integrity in neuroimaging studies.",
      "domains": [
        "D1",
        "D1-03",
        "Neuroscience"
      ],
      "subtask_category": [
        "quality_control",
        "image_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/nipreps/mriqc",
      "help_website": [
        "https://mriqc.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "neuroimaging",
        "mri",
        "quality-control",
        "bids"
      ],
      "id": 555
    },
    {
      "name": "Cosmos-Transfer2.5",
      "one_line_profile": "World simulation model for physical AI applications",
      "detailed_description": "A foundation model for generating high-quality physical world simulations conditioned on spatial inputs, used for robotics and physical AI research.",
      "domains": [
        "Physics",
        "Robotics",
        "Simulation"
      ],
      "subtask_category": [
        "simulation",
        "generative_modeling"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/nvidia-cosmos/cosmos-transfer2.5",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "simulation",
        "physical-ai",
        "world-model",
        "generative-ai"
      ],
      "id": 556
    },
    {
      "name": "Open Data Editor",
      "one_line_profile": "No-code application to explore and validate tabular research data",
      "detailed_description": "A desktop application powered by the Frictionless Framework for validating, cleaning, and exploring tabular data, designed to support Open Data standards in research.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_validation",
        "data_cleaning"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/okfn/opendataeditor",
      "help_website": [
        "https://opendataeditor.okfn.org/"
      ],
      "license": "MIT",
      "tags": [
        "frictionless-data",
        "data-validation",
        "open-science",
        "csv"
      ],
      "id": 557
    },
    {
      "name": "Onedata",
      "one_line_profile": "Distributed scientific data management platform",
      "detailed_description": "A high-performance, distributed global data management system that provides transparent data access and POSIX-compliant virtual filesystems for research infrastructures.",
      "domains": [
        "D1",
        "Data_Infrastructure"
      ],
      "subtask_category": [
        "data_management",
        "storage_virtualization"
      ],
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/onedata/onedata",
      "help_website": [
        "https://onedata.org/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "distributed-storage",
        "research-data-management",
        "posix",
        "grid-computing"
      ],
      "id": 558
    },
    {
      "name": "Open Semantic Search",
      "one_line_profile": "Research tool for analyzing and searching large document collections",
      "detailed_description": "An integrated search and analysis platform for research document collections, featuring text mining, named entity recognition, and metadata management.",
      "domains": [
        "D1",
        "Text_Mining"
      ],
      "subtask_category": [
        "text_mining",
        "information_retrieval",
        "document_analysis"
      ],
      "application_level": "platform",
      "primary_language": "Shell",
      "repo_url": "https://github.com/opensemanticsearch/open-semantic-search",
      "help_website": [
        "https://www.opensemanticsearch.org/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "semantic-search",
        "text-mining",
        "ner",
        "research-tool"
      ],
      "id": 559
    },
    {
      "name": "Atlas Checks",
      "one_line_profile": "Integrity checks for geospatial data (OpenStreetMap)",
      "detailed_description": "A framework for running data integrity checks on geospatial map data, utilizing the Atlas library to identify quality issues in OpenStreetMap datasets.",
      "domains": [
        "D1",
        "D1-03",
        "Geospatial"
      ],
      "subtask_category": [
        "data_validation",
        "quality_control"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/osmlab/atlas-checks",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "geospatial",
        "osm",
        "quality-control",
        "map-data"
      ],
      "id": 560
    },
    {
      "name": "pytest-pandera",
      "one_line_profile": "Data testing plugin for scientific dataframes",
      "detailed_description": "A pytest plugin for Pandera, enabling statistical data validation and schema enforcement for pandas dataframes, widely used in scientific data processing pipelines.",
      "domains": [
        "D1",
        "D1-03",
        "Data_Science"
      ],
      "subtask_category": [
        "data_validation",
        "schema_enforcement"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pandera-dev/pytest-pandera",
      "help_website": [
        "https://pandera.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "data-validation",
        "pandas",
        "testing",
        "quality-control"
      ],
      "id": 561
    },
    {
      "name": "Nabu",
      "one_line_profile": "Digital media management system for ethnographic research",
      "detailed_description": "A catalog and workflow management system for audio/video items and metadata, developed by PARADISEC for archiving endangered cultures' linguistic data.",
      "domains": [
        "D1",
        "Humanities"
      ],
      "subtask_category": [
        "data_management",
        "archiving",
        "metadata_management"
      ],
      "application_level": "platform",
      "primary_language": "Ruby",
      "repo_url": "https://github.com/paradisec-archive/nabu",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "archival-science",
        "linguistics",
        "metadata",
        "workflow"
      ],
      "id": 562
    },
    {
      "name": "pointblank",
      "one_line_profile": "Data validation toolkit for R",
      "detailed_description": "A comprehensive toolkit for data validation and quality monitoring in R, allowing scientists to define and verify data quality rules within analysis pipelines.",
      "domains": [
        "D1",
        "D1-03",
        "Statistics"
      ],
      "subtask_category": [
        "data_validation",
        "quality_assessment"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/posit-dev/pointblank",
      "help_website": [
        "https://pointblank.posit.co/"
      ],
      "license": "MIT",
      "tags": [
        "r-stats",
        "data-validation",
        "quality-control",
        "data-science"
      ],
      "id": 563
    },
    {
      "name": "pvanalytics",
      "one_line_profile": "Quality control and feature labeling tools for photovoltaic system data",
      "detailed_description": "A Python library providing functions for quality control, filtering, and feature labeling of data from photovoltaic energy systems. It supports tasks like clipping detection, daytime identification, and sensor data validation.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "data_filtering",
        "feature_labeling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pvlib/pvanalytics",
      "help_website": [
        "https://pvanalytics.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "photovoltaic",
        "energy",
        "quality-control",
        "time-series"
      ],
      "id": 564
    },
    {
      "name": "geoflow",
      "one_line_profile": "Orchestrator for geospatial metadata management and FAIR services",
      "detailed_description": "An R package to orchestrate geospatial metadata management workflows. It facilitates the management of FAIR services and metadata for geospatial data, supporting various standards and formats.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "metadata_management",
        "workflow_orchestration"
      ],
      "application_level": "workflow",
      "primary_language": "R",
      "repo_url": "https://github.com/r-geoflow/geoflow",
      "help_website": [
        "https://github.com/r-geoflow/geoflow"
      ],
      "license": "NOASSERTION",
      "tags": [
        "geospatial",
        "metadata",
        "fair-data",
        "workflow"
      ],
      "id": 565
    },
    {
      "name": "MetaCerberus",
      "one_line_profile": "Functional ontology assignments for metagenomes using HMM",
      "detailed_description": "A Python tool for versatile functional ontology assignments for metagenomes via Hidden Markov Models (HMM), focusing on environmental shotgun metaomics data.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "annotation",
        "ontology_mapping",
        "metagenomics_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/raw-lab/MetaCerberus",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "metagenomics",
        "hmm",
        "ontology",
        "bioinformatics"
      ],
      "id": 566
    },
    {
      "name": "Refinery Platform",
      "one_line_profile": "Data management, analysis, and visualization system for bioinformatics",
      "detailed_description": "A platform for bioinformatics and computational biology applications consisting of a data repository with rich metadata capabilities, a Galaxy-based workflow engine, and visualization tools.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_management",
        "workflow_orchestration",
        "visualization"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/refinery-platform/refinery-platform",
      "help_website": [
        "http://refinery-platform.org/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "bioinformatics",
        "data-management",
        "galaxy",
        "metadata"
      ],
      "id": 567
    },
    {
      "name": "fastq.bio",
      "one_line_profile": "Interactive web tool for quality control of DNA sequencing data",
      "detailed_description": "A web-based interactive tool designed for performing quality control on DNA sequencing data (FASTQ files), running entirely in the browser.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "visualization"
      ],
      "application_level": "solver",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/robertaboukhalil/fastq.bio",
      "help_website": [
        "http://fastq.bio/"
      ],
      "license": "MIT",
      "tags": [
        "fastq",
        "quality-control",
        "dna-sequencing",
        "web-tool"
      ],
      "id": 568
    },
    {
      "name": "minion_multiQC",
      "one_line_profile": "Multi-tool quality control wrapper for MinION sequencing data",
      "detailed_description": "A shell-based wrapper tool to perform quality control on MinION sequencing data using multiple underlying QC tools.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control"
      ],
      "application_level": "workflow",
      "primary_language": "Shell",
      "repo_url": "https://github.com/roblanf/minion_multiQC",
      "help_website": [],
      "license": null,
      "tags": [
        "minion",
        "nanopore",
        "quality-control"
      ],
      "id": 569
    },
    {
      "name": "minion_qc",
      "one_line_profile": "Quality control tool for MinION sequencing data",
      "detailed_description": "An R script/tool for generating quality control plots and statistics from MinION sequencing data.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "visualization"
      ],
      "application_level": "solver",
      "primary_language": "R",
      "repo_url": "https://github.com/roblanf/minion_qc",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "minion",
        "nanopore",
        "quality-control",
        "r"
      ],
      "id": 570
    },
    {
      "name": "skimr",
      "one_line_profile": "Compact and flexible summary statistics for data analysis",
      "detailed_description": "An R package from rOpenSci that provides a frictionless approach to dealing with summary statistics, commonly used for data profiling and initial quality assessment in scientific workflows.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "data_profiling",
        "statistics"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/ropensci/skimr",
      "help_website": [
        "https://docs.ropensci.org/skimr/"
      ],
      "license": null,
      "tags": [
        "statistics",
        "data-summary",
        "r-package",
        "ropensci"
      ],
      "id": 571
    },
    {
      "name": "FastQC",
      "one_line_profile": "Standard quality control analysis tool for high throughput sequencing data",
      "detailed_description": "A quality control tool for high throughput sequence data. It provides a modular set of analyses which you can use to give a quick impression of whether your data has any problems of which you should be aware before doing any further analysis.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "qc_report"
      ],
      "application_level": "solver",
      "primary_language": "Java",
      "repo_url": "https://github.com/s-andrews/FastQC",
      "help_website": [
        "http://www.bioinformatics.babraham.ac.uk/projects/fastqc/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "ngs",
        "quality-control",
        "fastq",
        "bioinformatics"
      ],
      "id": 572
    },
    {
      "name": "sequana_fastqc",
      "one_line_profile": "Sequana pipeline wrapper for parallel FastQC execution",
      "detailed_description": "A pipeline based on Sequana to perform FastQC analysis in parallel and summarize results using MultiQC, designed for efficient quality control of sequencing data.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "workflow"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/sequana/fastqc",
      "help_website": [
        "https://sequana.readthedocs.io"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "pipeline",
        "fastqc",
        "sequana",
        "bioinformatics"
      ],
      "id": 573
    },
    {
      "name": "Falco",
      "one_line_profile": "High-performance C++ quality control tool for sequencing data, compatible with FastQC",
      "detailed_description": "Falco is a C++ implementation of FastQC, designed to assess the quality of high-throughput sequencing data (NGS). It provides faster processing speeds while maintaining compatibility with FastQC reports, enabling efficient quality control in large-scale bioinformatics pipelines.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "qc_report"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/smithlabcode/falco",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "bioinformatics",
        "ngs",
        "quality-control",
        "fastq"
      ],
      "id": 574
    },
    {
      "name": "iterative-stratification",
      "one_line_profile": "Cross-validators for iterative stratification of multilabel data in scikit-learn",
      "detailed_description": "A Python library providing scikit-learn compatible cross-validators for multilabel data, ensuring balanced label distribution across splits, which is essential for rigorous scientific machine learning experiments.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "data_splitting",
        "model_validation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/trent-b/iterative-stratification",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "scikit-learn",
        "cross-validation",
        "multilabel",
        "machine-learning"
      ],
      "id": 575
    },
    {
      "name": "Tropy",
      "one_line_profile": "Research photo and metadata management tool for scientists and archivists",
      "detailed_description": "A desktop application designed for researchers to organize, describe, and annotate photos of research material (e.g., from archives), managing metadata and facilitating the research workflow.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "metadata_management",
        "data_organization"
      ],
      "application_level": "platform",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/tropy/tropy",
      "help_website": [
        "https://tropy.org"
      ],
      "license": "NOASSERTION",
      "tags": [
        "metadata",
        "research-data-management",
        "archival-science"
      ],
      "id": 576
    },
    {
      "name": "SVI-Quality-Checker",
      "one_line_profile": "Quality control tool for Street View Imagery datasets",
      "detailed_description": "A tool developed by the Urban Analytics Lab to examine and validate the quality of street view imagery (SVI) datasets, supporting urban science and geospatial research.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "image_validation"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/ualsg/SVI-Quality-Checker",
      "help_website": [],
      "license": null,
      "tags": [
        "urban-science",
        "geospatial",
        "quality-control",
        "street-view"
      ],
      "id": 577
    },
    {
      "name": "Checkmate",
      "one_line_profile": "Checkpoint management tool for TensorFlow models",
      "detailed_description": "A lightweight utility for managing and saving the best TensorFlow checkpoints during model training, ensuring model integrity and preventing data loss in scientific ML workflows.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "workflow_management",
        "artifact_management"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/vonclites/checkmate",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "tensorflow",
        "checkpointing",
        "ml-ops"
      ],
      "id": 578
    },
    {
      "name": "nanoQC",
      "one_line_profile": "Quality control tool for nanopore sequencing data",
      "detailed_description": "A bioinformatics tool specifically designed for quality control of Oxford Nanopore sequencing data, generating plots and statistics to assess read quality.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "sequencing_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/wdecoster/nanoQC",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "nanopore",
        "bioinformatics",
        "quality-control",
        "long-read"
      ],
      "id": 579
    },
    {
      "name": "ydata-quality",
      "one_line_profile": "Data quality assessment library for data science",
      "detailed_description": "A Python library for comprehensive data quality assessment, providing metrics and visualizations to identify issues in datasets prior to analysis or modeling.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_assessment",
        "data_profiling"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/ydataai/ydata-quality",
      "help_website": [
        "https://github.com/ydataai/ydata-quality"
      ],
      "license": "MIT",
      "tags": [
        "data-quality",
        "data-science",
        "profiling"
      ],
      "id": 580
    },
    {
      "name": "ZotaData",
      "one_line_profile": "Metadata management plugin for Zotero",
      "detailed_description": "A tool (likely a plugin or extension) for Zotero that enhances metadata management capabilities, facilitating the organization of scientific literature and citations.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "metadata_management",
        "literature_management"
      ],
      "application_level": "solver",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/ydeng11/zotero-zotadata",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "zotero",
        "metadata",
        "bibliography"
      ],
      "id": 581
    },
    {
      "name": "LongQC",
      "one_line_profile": "Quality control tool for PacBio and ONT long read data",
      "detailed_description": "A tool designed for the quality control of long-read sequencing data (PacBio and Oxford Nanopore), addressing specific error profiles and artifacts associated with these technologies.",
      "domains": [
        "D1",
        "D1-03"
      ],
      "subtask_category": [
        "quality_control",
        "sequencing_analysis"
      ],
      "application_level": "solver",
      "primary_language": "C",
      "repo_url": "https://github.com/yfukasawa/LongQC",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "long-read",
        "pacbio",
        "ont",
        "bioinformatics",
        "quality-control"
      ],
      "id": 582
    },
    {
      "name": "Tibanna",
      "one_line_profile": "Workflow execution manager for running genomic pipelines on AWS",
      "detailed_description": "Tibanna is a software tool designed to execute genomic pipelines on the Amazon Web Services (AWS) cloud. It supports Common Workflow Language (CWL), Workflow Description Language (WDL), and Snakemake, integrating with Docker for containerization. It is specifically used by the 4D Nucleome Data Coordination and Integration Center (4DN DCIC) for processing large-scale genomic data.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_orchestration",
        "genomic_pipeline_execution"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/4dn-dcic/tibanna",
      "help_website": [
        "https://tibanna.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "genomics",
        "aws",
        "pipeline",
        "cwl",
        "wdl"
      ],
      "id": 583
    },
    {
      "name": "Covalent",
      "one_line_profile": "Workflow orchestration tool for HPC and quantum computing",
      "detailed_description": "Covalent is a Pythonic workflow orchestration tool designed for research scientists and engineers to manage and execute complex workflows on heterogeneous compute environments, including High Performance Computing (HPC) clusters and quantum computers. It simplifies the deployment of machine learning and experimental simulations across diverse hardware backends.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_orchestration",
        "hpc_job_management",
        "quantum_computing"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/AgnostiqHQ/covalent",
      "help_website": [
        "https://covalent.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "hpc",
        "quantum-computing",
        "workflow",
        "orchestration"
      ],
      "id": 584
    },
    {
      "name": "Cromwell Frontend",
      "one_line_profile": "Web interface for managing Cromwell workflows",
      "detailed_description": "A web-based frontend application for the Cromwell Workflow Management System, which is widely used in bioinformatics for scientific workflow execution. It provides capabilities for job monitoring, authentication, and management of workflow definitions, facilitating the operation of genomic pipelines.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_management",
        "job_monitoring"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/BiRG/cromwell-frontend",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bioinformatics",
        "cromwell",
        "workflow-management",
        "gui"
      ],
      "id": 585
    },
    {
      "name": "AiiDA-GROMACS",
      "one_line_profile": "AiiDA plugin for GROMACS molecular dynamics simulations",
      "detailed_description": "A plugin for the AiiDA workflow engine that enables the automation and provenance tracking of GROMACS molecular dynamics simulations. It allows researchers to integrate GROMACS calculations into complex scientific workflows for computational chemistry and materials science.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "molecular_dynamics",
        "workflow_automation"
      ],
      "application_level": "connector",
      "primary_language": "Python",
      "repo_url": "https://github.com/CCPBioSim/aiida-gromacs",
      "help_website": [
        "https://aiida-gromacs.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "molecular-dynamics",
        "aiida",
        "gromacs",
        "workflow",
        "chemistry"
      ],
      "id": 586
    },
    {
      "name": "CalliNGS-NF",
      "one_line_profile": "Nextflow pipeline for GATK RNA-Seq variant calling",
      "detailed_description": "CalliNGS-NF is a bioinformatics pipeline implemented in Nextflow for performing variant calling on RNA-Seq data using the GATK Best Practices. It automates the process of read alignment, processing, and variant discovery, ensuring reproducibility and scalability in genomic research.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "variant_calling",
        "rna_seq_analysis",
        "pipeline_execution"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/CRG-CNAG/CalliNGS-NF",
      "help_website": [],
      "license": "MPL-2.0",
      "tags": [
        "bioinformatics",
        "nextflow",
        "variant-calling",
        "rna-seq",
        "gatk"
      ],
      "id": 587
    },
    {
      "name": "ClarityNLP",
      "one_line_profile": "NLP framework for clinical phenotyping and medical data extraction",
      "detailed_description": "ClarityNLP is a Natural Language Processing (NLP) framework specifically designed for clinical phenotyping and extracting information from unstructured medical notes. It integrates with OMOP Common Data Model and Solr to enable researchers to build queries and pipelines for identifying patient cohorts and clinical features.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "clinical_nlp",
        "phenotyping",
        "information_extraction"
      ],
      "application_level": "platform",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/ClarityNLP/ClarityNLP",
      "help_website": [
        "http://claritynlp.readthedocs.io/en/latest/"
      ],
      "license": "MPL-2.0",
      "tags": [
        "clinical-nlp",
        "phenotyping",
        "omop",
        "healthcare",
        "text-mining"
      ],
      "id": 588
    },
    {
      "name": "Toil",
      "one_line_profile": "Scalable, efficient, cross-platform workflow engine for scientific pipelines",
      "detailed_description": "Toil is a workflow engine written in pure Python that supports WDL, CWL, and Python workflows. It is designed for scalability and is widely used in genomics and bioinformatics for orchestrating complex analysis pipelines across cloud and local environments.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_orchestration",
        "pipeline_management"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/DataBiosphere/toil",
      "help_website": [
        "https://toil.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "workflow-engine",
        "wdl",
        "cwl",
        "bioinformatics",
        "genomics"
      ],
      "id": 589
    },
    {
      "name": "ETL-Connector-for-ODK",
      "one_line_profile": "QGIS plugin to connect and retrieve data from ODK instances",
      "detailed_description": "A QGIS plugin that serves as a connector to Open Data Kit (ODK) instances, allowing researchers to retrieve field survey forms and load them directly into QGIS as shapefiles, geopackages, or CSVs for geospatial analysis.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "data_ingestion",
        "geospatial_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/DemevengDerrick/ETL-Connector-for-ODK",
      "help_website": [
        "https://plugins.qgis.org/plugins/odkconnector2/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "qgis-plugin",
        "odk",
        "field-data",
        "gis",
        "etl"
      ],
      "id": 590
    },
    {
      "name": "fold_tree",
      "one_line_profile": "Snakemake pipeline for creating phylogenetic trees from sequences",
      "detailed_description": "A bioinformatics pipeline implemented in Snakemake that automates the process of creating phylogenetic trees from sets of biological sequences, facilitating evolutionary biology research.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "phylogenetics",
        "sequence_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/DessimozLab/fold_tree",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "snakemake",
        "phylogeny",
        "bioinformatics",
        "pipeline"
      ],
      "id": 591
    },
    {
      "name": "VIRify",
      "one_line_profile": "Pipeline for detection of phages and eukaryotic viruses",
      "detailed_description": "A bioinformatics pipeline designed for the detection and classification of bacteriophages and eukaryotic viruses from metagenomic and metatranscriptomic assembly data.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "viral_detection",
        "metagenomics"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/EBI-Metagenomics/emg-viral-pipeline",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "metagenomics",
        "virology",
        "bioinformatics",
        "pipeline"
      ],
      "id": 592
    },
    {
      "name": "REAT",
      "one_line_profile": "Robust Eukaryotic Annotation Toolkit",
      "detailed_description": "A toolkit designed for the robust annotation of eukaryotic genomes, providing workflows and utilities to improve the quality and consistency of genomic feature annotation.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "genome_annotation",
        "genomics"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/EI-CoreBioinformatics/reat",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "genome-annotation",
        "eukaryotes",
        "bioinformatics"
      ],
      "id": 593
    },
    {
      "name": "caper",
      "one_line_profile": "Python wrapper and manager for Cromwell workflow engine",
      "detailed_description": "A Python wrapper for the Cromwell workflow engine, developed by the ENCODE DCC. It simplifies the running of WDL workflows on various backends (Cloud, HPC) and manages workflow configurations.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_management",
        "pipeline_execution"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/ENCODE-DCC/caper",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "cromwell",
        "wdl",
        "encode",
        "workflow-manager"
      ],
      "id": 594
    },
    {
      "name": "croo",
      "one_line_profile": "Output organizer for Cromwell workflows",
      "detailed_description": "A utility tool designed to organize and structure the outputs generated by Cromwell workflows, making it easier to manage and interpret results from large-scale genomic analyses.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "data_organization",
        "workflow_utility"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ENCODE-DCC/croo",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "cromwell",
        "wdl",
        "encode",
        "data-management"
      ],
      "id": 595
    },
    {
      "name": "precisionFDA",
      "one_line_profile": "Platform for benchmarking and validating NGS analysis pipelines",
      "detailed_description": "A cloud-based platform and environment that enables the community to test, pilot, and benchmark new approaches for validating next-generation sequencing (NGS) analysis pipelines and bioinformatics tools.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "benchmarking",
        "ngs_analysis"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/FDA/precisionFDA",
      "help_website": [
        "https://precision.fda.gov/"
      ],
      "license": "CC0-1.0",
      "tags": [
        "ngs",
        "benchmarking",
        "bioinformatics",
        "fda"
      ],
      "id": 596
    },
    {
      "name": "MAG_Snakemake_wf",
      "one_line_profile": "Workflow for recovery of prokaryotic genomes from metagenomes",
      "detailed_description": "A Snakemake workflow designed for the recovery of prokaryotic genomes (Metagenome-Assembled Genomes, MAGs) from shotgun metagenomic sequencing data.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "genome_assembly",
        "metagenomics"
      ],
      "application_level": "workflow",
      "primary_language": "HTML",
      "repo_url": "https://github.com/Finn-Lab/MAG_Snakemake_wf",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "snakemake",
        "metagenomics",
        "mag",
        "bioinformatics"
      ],
      "id": 597
    },
    {
      "name": "redundans",
      "one_line_profile": "Pipeline for assembly of heterozygous/polymorphic genomes",
      "detailed_description": "A pipeline that assists in the assembly of heterozygous or polymorphic genomes by reducing redundancy and scaffolding contigs, particularly useful for complex eukaryotic genomes.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "genome_assembly",
        "genomics"
      ],
      "application_level": "workflow",
      "primary_language": "C++",
      "repo_url": "https://github.com/Gabaldonlab/redundans",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "genome-assembly",
        "heterozygosity",
        "bioinformatics"
      ],
      "id": 598
    },
    {
      "name": "sv-callers",
      "one_line_profile": "Snakemake workflow for detecting structural variants",
      "detailed_description": "A Snakemake-based workflow that integrates multiple tools for detecting structural variants in genomic data, developed by the GooglingTheCancerGenome project.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "variant_calling",
        "genomics"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/GooglingTheCancerGenome/sv-callers",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "structural-variants",
        "snakemake",
        "cancer-genomics",
        "bioinformatics"
      ],
      "id": 599
    },
    {
      "name": "aiida-fleur",
      "one_line_profile": "AiiDA plugin for the FLEUR DFT code",
      "detailed_description": "A plugin for the AiiDA workflow engine that interfaces with the FLEUR code, enabling high-throughput density functional theory (DFT) calculations and electronic structure analysis.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "material_science",
        "dft_calculation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/JuDFTteam/aiida-fleur",
      "help_website": [
        "https://aiida-fleur.readthedocs.io/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "aiida",
        "dft",
        "material-science",
        "fleur",
        "workflow-plugin"
      ],
      "id": 600
    },
    {
      "name": "aiida-kkr",
      "one_line_profile": "AiiDA plugin for the JuKKR KKR code",
      "detailed_description": "A plugin for the AiiDA workflow engine that interfaces with the JuKKR (Korringa-Kohn-Rostoker) code, facilitating high-throughput electronic structure calculations in materials science.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "material_science",
        "dft_calculation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/JuDFTteam/aiida-kkr",
      "help_website": [
        "https://aiida-kkr.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "aiida",
        "kkr",
        "material-science",
        "dft",
        "workflow-plugin"
      ],
      "id": 601
    },
    {
      "name": "pytest-workflow",
      "one_line_profile": "Test framework for bioinformatics workflows and pipelines",
      "detailed_description": "A pytest plugin designed to test computational pipelines (e.g., Nextflow, Snakemake, WDL) by running them and verifying their outputs, widely used in bioinformatics engineering.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "pipeline_testing",
        "workflow_verification"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/LUMC/pytest-workflow",
      "help_website": [
        "https://pytest-workflow.readthedocs.io/"
      ],
      "license": "AGPL-3.0",
      "tags": [
        "testing",
        "bioinformatics",
        "workflow",
        "pipeline"
      ],
      "id": 602
    },
    {
      "name": "BioJupies",
      "one_line_profile": "Automated generation of bioinformatics Jupyter Notebooks",
      "detailed_description": "A web-based tool and framework that automatically generates tailored Jupyter Notebooks for RNA-seq data analysis, facilitating reproducible bioinformatics research.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_generation",
        "rna_seq_analysis"
      ],
      "application_level": "platform",
      "primary_language": "HTML",
      "repo_url": "https://github.com/MaayanLab/biojupies",
      "help_website": [
        "https://biojupies.cloud/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "bioinformatics",
        "jupyter-notebook",
        "rna-seq",
        "automation"
      ],
      "id": 603
    },
    {
      "name": "MetONTIIME",
      "one_line_profile": "Meta-barcoding pipeline for ONT data analysis",
      "detailed_description": "A bioinformatics pipeline built with Nextflow for analyzing Oxford Nanopore Technologies (ONT) meta-barcoding data within the QIIME2 framework.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "metabarcoding",
        "sequence_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/MaestSi/MetONTIIME",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "bioinformatics",
        "nextflow",
        "qiime2",
        "nanopore"
      ],
      "id": 604
    },
    {
      "name": "MapGIS Client for JavaScript",
      "one_line_profile": "Web client SDK for MapGIS geospatial platform",
      "detailed_description": "A JavaScript development platform for Cloud GIS, integrating visualization libraries like Echarts and D3 for efficient visual expression and analysis of geospatial big data.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "geospatial_visualization",
        "gis_analysis"
      ],
      "application_level": "library",
      "primary_language": "HTML",
      "repo_url": "https://github.com/MapGIS/WebClient-JavaScript",
      "help_website": [
        "http://develop.smaryun.com/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "gis",
        "visualization",
        "geospatial",
        "web-client"
      ],
      "id": 605
    },
    {
      "name": "Express.jl",
      "one_line_profile": "Workflow framework for ab initio materials science calculations",
      "detailed_description": "A high-level, extensible workflow framework written in Julia for automating and accelerating ab initio calculations in materials science, specifically designed for Quantum ESPRESSO.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "ab_initio_calculation",
        "workflow_automation"
      ],
      "application_level": "workflow",
      "primary_language": "Julia",
      "repo_url": "https://github.com/MineralsCloud/Express.jl",
      "help_website": [
        "https://mineralscloud.github.io/Express.jl/dev/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "materials-science",
        "workflow",
        "julia",
        "quantum-espresso"
      ],
      "id": 606
    },
    {
      "name": "pgsc_calc",
      "one_line_profile": "Polygenic Score Catalog Calculator pipeline",
      "detailed_description": "A Nextflow pipeline for calculating polygenic scores (PGS) using the PGS Catalog, handling data download, harmonization, and scoring.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "polygenic_scoring",
        "genomics_pipeline"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/PGScatalog/pgsc_calc",
      "help_website": [
        "https://pgsc-calc.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "bioinformatics",
        "genomics",
        "polygenic-scores",
        "nextflow"
      ],
      "id": 607
    },
    {
      "name": "HiFi-16S-workflow",
      "one_line_profile": "PacBio HiFi 16S rRNA analysis pipeline",
      "detailed_description": "A Nextflow pipeline specifically designed for analyzing PacBio HiFi full-length 16S rRNA sequencing data, including quality control and taxonomy assignment.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "16s_rrna_analysis",
        "metagenomics"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/PacificBiosciences/HiFi-16S-workflow",
      "help_website": [],
      "license": "BSD-3-Clause-Clear",
      "tags": [
        "bioinformatics",
        "pacbio",
        "16s",
        "microbiome"
      ],
      "id": 608
    },
    {
      "name": "bionix",
      "one_line_profile": "Reproducible bioinformatics pipelines using Nix",
      "detailed_description": "A library for building functional, highly reproducible bioinformatics pipelines using the Nix package manager, ensuring consistent environments and execution.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_management",
        "reproducibility"
      ],
      "application_level": "workflow",
      "primary_language": "Nix",
      "repo_url": "https://github.com/PapenfussLab/bionix",
      "help_website": [
        "https://bionix.readthedocs.io/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "bioinformatics",
        "nix",
        "reproducibility",
        "pipeline"
      ],
      "id": 609
    },
    {
      "name": "ReAdW",
      "one_line_profile": "Thermo Raw to mzXML converter",
      "detailed_description": "A command-line tool for converting Thermo Finnigan RAW mass spectrometry data files into the open mzXML format, facilitating downstream proteomics analysis.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "format_conversion",
        "mass_spectrometry"
      ],
      "application_level": "solver",
      "primary_language": "C",
      "repo_url": "https://github.com/PedrioliLab/ReAdW",
      "help_website": [],
      "license": "LGPL-3.0",
      "tags": [
        "mass-spectrometry",
        "proteomics",
        "file-conversion",
        "mzxml"
      ],
      "id": 610
    },
    {
      "name": "PyPSA-Eur",
      "one_line_profile": "Sector-coupled open optimisation model of the European energy system",
      "detailed_description": "PyPSA-Eur is an open model dataset and workflow of the European power system at the transmission network level. It covers the full ENTSO-E area and is designed for energy system optimization and modeling.",
      "domains": [
        "D1-04"
      ],
      "subtask_category": [
        "energy_system_modeling",
        "optimization"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/PyPSA/pypsa-eur",
      "help_website": [
        "https://pypsa-eur.readthedocs.io"
      ],
      "license": null,
      "tags": [
        "energy-systems",
        "optimization",
        "snakemake",
        "power-grid"
      ],
      "id": 611
    },
    {
      "name": "VesselExpress",
      "one_line_profile": "Automated blood vasculature analysis of 3D light-sheet image volumes",
      "detailed_description": "A pipeline for the automated analysis of blood vasculature in 3D Light-Sheet Microscopy image volumes, facilitating the study of vascular structures in biological samples.",
      "domains": [
        "D1-04"
      ],
      "subtask_category": [
        "image_analysis",
        "vasculature_segmentation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/RUB-Bioinf/VesselExpress",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "bioimaging",
        "light-sheet-microscopy",
        "vasculature",
        "3d-imaging"
      ],
      "id": 612
    },
    {
      "name": "TlseHypDataSet",
      "one_line_profile": "Data loader for the Toulouse Hyperspectral Data Set",
      "detailed_description": "A Python library designed to flexibly load PyTorch datasets and facilitate machine learning experiments specifically on the Toulouse Hyperspectral Data Set, aiding in remote sensing research.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "data_loading",
        "hyperspectral_imaging"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Romain3Ch216/TlseHypDataSet",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "hyperspectral",
        "remote-sensing",
        "pytorch",
        "data-loader"
      ],
      "id": 613
    },
    {
      "name": "inphared",
      "one_line_profile": "Phage genome databases and bioinformatics pipeline utilities",
      "detailed_description": "Provides up-to-date phage genome databases, metrics, and input files to support various bioinformatic pipelines focused on bacteriophage research.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "genome_analysis",
        "database_construction"
      ],
      "application_level": "workflow",
      "primary_language": "Perl",
      "repo_url": "https://github.com/RyanCook94/inphared",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "bacteriophage",
        "genomics",
        "bioinformatics",
        "database"
      ],
      "id": 614
    },
    {
      "name": "reticulatus",
      "one_line_profile": "Pipeline for assembling and polishing long genomes from nanopore reads",
      "detailed_description": "A Snakemake-based pipeline designed for the assembly and polishing of long genomes using long nanopore sequencing reads, streamlining the genomic assembly workflow.",
      "domains": [
        "D1-04"
      ],
      "subtask_category": [
        "genome_assembly",
        "nanopore_sequencing"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/SamStudio8/reticulatus",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "genome-assembly",
        "nanopore",
        "snakemake",
        "bioinformatics"
      ],
      "id": 615
    },
    {
      "name": "tf-WSI-dataset-utils",
      "one_line_profile": "Pipeline for Whole Slide Image (WSI) data in Tensorflow",
      "detailed_description": "An optimized pipeline and utility set for working with Whole Slide Image (WSI) data within the Tensorflow framework, facilitating digital pathology and bioimaging analysis.",
      "domains": [
        "D1-04"
      ],
      "subtask_category": [
        "image_processing",
        "whole_slide_imaging"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/SarderLab/tf-WSI-dataset-utils",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "digital-pathology",
        "wsi",
        "tensorflow",
        "bioimaging"
      ],
      "id": 616
    },
    {
      "name": "ilus",
      "one_line_profile": "Variant calling pipeline generator for WGS/WES data",
      "detailed_description": "A lightweight pipeline generator for Whole Genome Sequencing (WGS) and Whole Exome Sequencing (WES) data analysis, utilizing GATK and Sentieon for variant calling.",
      "domains": [
        "D1-04"
      ],
      "subtask_category": [
        "variant_calling",
        "pipeline_generation"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/ShujiaHuang/ilus",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "variant-calling",
        "gatk",
        "wgs",
        "wes",
        "bioinformatics"
      ],
      "id": 617
    },
    {
      "name": "SingleRust",
      "one_line_profile": "High-throughput single-cell analysis pipeline in Rust",
      "detailed_description": "A tool for single-cell analysis leveraging Rust's concurrency for scalable, high-throughput pipelines, aiming to improve performance in single-cell genomics workflows.",
      "domains": [
        "D1-04"
      ],
      "subtask_category": [
        "single_cell_analysis",
        "clustering"
      ],
      "application_level": "solver",
      "primary_language": "Rust",
      "repo_url": "https://github.com/SingleRust/SingleRust",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "single-cell",
        "rust",
        "bioinformatics",
        "high-throughput"
      ],
      "id": 618
    },
    {
      "name": "ACES",
      "one_line_profile": "Workflow for querying small sequences in large genome sets with phylogenetic analysis",
      "detailed_description": "ACES is a bioinformatics workflow designed to query small sequences against a large set of genomes. It automates the process of BLAST searching, multiple sequence alignment, fragment assembly, and phylogenetic tree construction.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "sequence_alignment",
        "phylogenetic_analysis",
        "workflow_orchestration"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/TNTurnerLab/ACES",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bioinformatics",
        "genomics",
        "phylogenetics",
        "blast"
      ],
      "id": 619
    },
    {
      "name": "aiida-champ",
      "one_line_profile": "AiiDA plugin for the CHAMP Quantum Monte Carlo code",
      "detailed_description": "A plugin for the AiiDA workflow engine that interfaces with CHAMP (Cornell-Holland Ab-initio Materials Package), enabling the automation and provenance tracking of Quantum Monte Carlo simulations.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "simulation_interface",
        "workflow_orchestration",
        "quantum_monte_carlo"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/TREX-CoE/aiida-champ",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "aiida",
        "materials-science",
        "qmc",
        "workflow"
      ],
      "id": 620
    },
    {
      "name": "aiida-qp2",
      "one_line_profile": "AiiDA plugin for Quantum Package 2.0",
      "detailed_description": "An AiiDA plugin designed to interface with Quantum Package 2.0, facilitating the orchestration of quantum chemistry calculations and managing data provenance within the AiiDA ecosystem.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "simulation_interface",
        "workflow_orchestration",
        "quantum_chemistry"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/TREX-CoE/aiida-qp2",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "aiida",
        "quantum-chemistry",
        "workflow"
      ],
      "id": 621
    },
    {
      "name": "Earl Grey",
      "one_line_profile": "Fully automated transposable element curation and annotation pipeline",
      "detailed_description": "Earl Grey is a comprehensive pipeline for the automated curation and annotation of transposable elements (TEs) in eukaryotic genomes, streamlining the identification and classification process.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "genome_annotation",
        "transposable_elements",
        "pipeline"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/TobyBaril/EarlGrey",
      "help_website": [
        "https://github.com/TobyBaril/EarlGrey"
      ],
      "license": "NOASSERTION",
      "tags": [
        "bioinformatics",
        "genomics",
        "transposable-elements",
        "annotation"
      ],
      "id": 622
    },
    {
      "name": "snk",
      "one_line_profile": "CLI generation tool for Snakemake workflows",
      "detailed_description": "Snk is a utility that automatically generates Command Line Interfaces (CLIs) for Snakemake workflows, making scientific pipelines easier to distribute, install, and run as standalone applications.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_management",
        "pipeline_deployment"
      ],
      "application_level": "tool",
      "primary_language": "Python",
      "repo_url": "https://github.com/Wytamma/snk",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "snakemake",
        "workflow",
        "cli",
        "reproducibility"
      ],
      "id": 623
    },
    {
      "name": "cLoops",
      "one_line_profile": "Accurate and flexible loops calling tool for 3D genomic data",
      "detailed_description": "cLoops is a tool designed for calling loops in 3D genomic data (such as ChIA-PET, Hi-C, and HiChIP). It uses a clustering-based approach to identify significant interactions in chromatin structure.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "3d_genomics",
        "loop_calling",
        "data_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/YaqiangCao/cLoops",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bioinformatics",
        "hi-c",
        "chromatin-loops",
        "genomics"
      ],
      "id": 624
    },
    {
      "name": "HemTools",
      "one_line_profile": "Collection of NGS pipelines and bioinformatic analyses",
      "detailed_description": "HemTools provides a suite of pipelines and tools for Next-Generation Sequencing (NGS) data analysis, specifically tailored for hematology and general bioinformatics tasks, including ChIP-seq, RNA-seq, and CRISPR screening analysis.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "ngs_analysis",
        "pipeline",
        "data_processing"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/YichaoOU/HemTools",
      "help_website": [
        "https://hemtools.readthedocs.io/"
      ],
      "license": null,
      "tags": [
        "bioinformatics",
        "ngs",
        "pipeline",
        "crispr"
      ],
      "id": 625
    },
    {
      "name": "veridical-flow",
      "one_line_profile": "Framework for building trustworthy data-science pipelines based on PCS",
      "detailed_description": "Veridical Flow is a Python tool that implements the PCS (Predictability, Computability, Stability) framework to help researchers build stable and trustworthy data science pipelines, facilitating rigorous data analysis and validation.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "data_validation",
        "pipeline_stability",
        "reproducibility"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/Yu-Group/veridical-flow",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "data-science",
        "pcs-framework",
        "pipeline",
        "statistics"
      ],
      "id": 626
    },
    {
      "name": "ProteinFlow",
      "one_line_profile": "Pipeline for processing protein structure data for deep learning",
      "detailed_description": "ProteinFlow is a versatile computational pipeline designed to process protein structure data (e.g., from PDB) into formats suitable for deep learning applications. It handles filtering, clustering, and feature extraction for protein modeling.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "data_preprocessing",
        "protein_structure",
        "deep_learning_prep"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/adaptyvbio/ProteinFlow",
      "help_website": [
        "https://proteinflow.readthedocs.io/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "protein-structure",
        "deep-learning",
        "bioinformatics",
        "dataset"
      ],
      "id": 627
    },
    {
      "name": "aiida-cusp",
      "one_line_profile": "Custodian based VASP Plugin for AiiDA",
      "detailed_description": "AiiDA-CUSP is a plugin for the AiiDA workflow engine that integrates VASP calculations with Custodian-based error handling and recovery, designed for high-throughput materials science simulations.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "simulation_interface",
        "error_correction",
        "workflow_orchestration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aiida-cusp/aiida-cusp",
      "help_website": [
        "https://aiida-cusp.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "aiida",
        "vasp",
        "materials-science",
        "custodian"
      ],
      "id": 628
    },
    {
      "name": "aiida-phonopy",
      "one_line_profile": "AiiDA plugin for phonon calculations using Phonopy",
      "detailed_description": "A plugin for AiiDA that interfaces with Phonopy to automate phonon calculations. It enables the calculation of vibrational properties of materials within a reproducible workflow environment.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "simulation_interface",
        "phonon_calculation",
        "workflow_orchestration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aiida-phonopy/aiida-phonopy",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "aiida",
        "phonopy",
        "materials-science",
        "phonons"
      ],
      "id": 629
    },
    {
      "name": "aiida-trains-pot",
      "one_line_profile": "AiiDA workflow for training neural network interatomic potentials",
      "detailed_description": "An AiiDA workflow that implements an automated active learning scheme to train neural network interatomic potentials, facilitating the development of machine learning force fields for materials simulations.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "ml_potential_training",
        "active_learning",
        "workflow_orchestration"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/aiida-trieste-developers/aiida-trains-pot",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "aiida",
        "machine-learning",
        "interatomic-potentials",
        "materials-science"
      ],
      "id": 630
    },
    {
      "name": "aiida-vasp",
      "one_line_profile": "AiiDA plugin for running VASP simulations",
      "detailed_description": "The official AiiDA plugin for the Vienna Ab initio Simulation Package (VASP). It provides a robust interface for setting up, running, and parsing VASP calculations within the AiiDA provenance graph.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "simulation_interface",
        "workflow_orchestration",
        "dft"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aiida-vasp/aiida-vasp",
      "help_website": [
        "https://aiida-vasp.readthedocs.io/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "aiida",
        "vasp",
        "dft",
        "materials-science"
      ],
      "id": 631
    },
    {
      "name": "AiiDAlab",
      "one_line_profile": "Web platform for AiiDA workflows and applications",
      "detailed_description": "AiiDAlab is a web-based platform that provides a user-friendly environment for running AiiDA workflows. It allows users to access computational science tools through a graphical interface in Jupyter notebooks.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_platform",
        "gui",
        "scientific_computing"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/aiidalab/aiidalab",
      "help_website": [
        "https://aiidalab.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "aiida",
        "jupyter",
        "workflow-platform",
        "materials-science"
      ],
      "id": 632
    },
    {
      "name": "aiidalab-qe",
      "one_line_profile": "AiiDAlab application for Quantum ESPRESSO",
      "detailed_description": "An application for the AiiDAlab platform that provides a graphical user interface for running Quantum ESPRESSO calculations, making DFT simulations accessible to non-experts.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "simulation_gui",
        "dft",
        "workflow_interface"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/aiidalab/aiidalab-qe",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "aiidalab",
        "quantum-espresso",
        "dft",
        "gui"
      ],
      "id": 633
    },
    {
      "name": "aiida-ase",
      "one_line_profile": "AiiDA plugin for the Atomic Simulation Environment (ASE)",
      "detailed_description": "A plugin that integrates the Atomic Simulation Environment (ASE) with AiiDA, allowing users to use ASE calculators and structures within AiiDA workflows for materials science simulations.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "simulation_interface",
        "structure_manipulation",
        "workflow_orchestration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aiidaplugins/aiida-ase",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "aiida",
        "ase",
        "materials-science",
        "interoperability"
      ],
      "id": 634
    },
    {
      "name": "aiida-lammps",
      "one_line_profile": "AiiDA plugin for LAMMPS molecular dynamics",
      "detailed_description": "A plugin for AiiDA to interface with LAMMPS, enabling the automation of Molecular Dynamics simulations and the management of force fields and trajectories within AiiDA.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "simulation_interface",
        "molecular_dynamics",
        "workflow_orchestration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aiidaplugins/aiida-lammps",
      "help_website": [
        "https://aiida-lammps.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "aiida",
        "lammps",
        "molecular-dynamics",
        "materials-science"
      ],
      "id": 635
    },
    {
      "name": "aiida-common-workflows",
      "one_line_profile": "Common workflow interfaces for materials science codes in AiiDA",
      "detailed_description": "A library that defines and implements common interfaces for workflows across different quantum engines (like VASP, QE, Siesta) in AiiDA, enabling code-agnostic simulation protocols.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_standardization",
        "interoperability",
        "simulation_interface"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aiidateam/aiida-common-workflows",
      "help_website": [
        "https://aiida-common-workflows.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "aiida",
        "workflows",
        "standardization",
        "materials-science"
      ],
      "id": 636
    },
    {
      "name": "AiiDA",
      "one_line_profile": "Automated Interactive Infrastructure and Database for Computational Science",
      "detailed_description": "AiiDA is a flexible and scalable informatics infrastructure to manage, preserve, and disseminate computational data and workflows. It provides a workflow engine with provenance tracking for high-throughput computational science.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_management",
        "provenance_tracking",
        "high_throughput_computing"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/aiidateam/aiida-core",
      "help_website": [
        "https://www.aiida.net/"
      ],
      "license": "MIT",
      "tags": [
        "workflow-engine",
        "materials-science",
        "provenance",
        "hpc"
      ],
      "id": 637
    },
    {
      "name": "AiiDA-CP2K",
      "one_line_profile": "AiiDA plugin for the CP2K quantum chemistry and solid state physics software",
      "detailed_description": "A plugin connecting the AiiDA workflow engine with CP2K, enabling automated atomistic simulations and electronic structure calculations with provenance tracking.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "simulation_connector",
        "electronic_structure"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aiidateam/aiida-cp2k",
      "help_website": [
        "https://aiida-cp2k.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "cp2k",
        "dft",
        "molecular-dynamics",
        "aiida-plugin"
      ],
      "id": 638
    },
    {
      "name": "AiiDA-Hubbard",
      "one_line_profile": "Workflows for self-consistent Hubbard parameters from first-principles",
      "detailed_description": "A plugin for AiiDA that provides workflows to calculate onsite and intersite Hubbard U and V parameters using linear response theory (DFPT), primarily for Quantum ESPRESSO.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "parameter_estimation",
        "electronic_structure"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/aiidateam/aiida-hubbard",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "dft+u",
        "hubbard-parameters",
        "quantum-espresso",
        "materials-science"
      ],
      "id": 639
    },
    {
      "name": "AiiDA-HyperQueue",
      "one_line_profile": "AiiDA plugin for the HyperQueue metascheduler",
      "detailed_description": "A scheduler plugin for AiiDA that interfaces with HyperQueue, allowing for efficient task scheduling on HPC clusters within scientific workflows.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "job_scheduling",
        "hpc_connector"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aiidateam/aiida-hyperqueue",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "hpc",
        "scheduling",
        "hyperqueue",
        "aiida-plugin"
      ],
      "id": 640
    },
    {
      "name": "AiiDA-Project",
      "one_line_profile": "Project management utility for AiiDA",
      "detailed_description": "A command-line tool to manage AiiDA projects, facilitating the organization and switching between different scientific research contexts and datasets.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "project_management",
        "workflow_utility"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/aiidateam/aiida-project",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "project-management",
        "cli",
        "aiida-utility"
      ],
      "id": 641
    },
    {
      "name": "AiiDA-QuantumESPRESSO",
      "one_line_profile": "Official AiiDA plugin for Quantum ESPRESSO",
      "detailed_description": "The official plugin to interface AiiDA with the Quantum ESPRESSO suite, enabling automated, reproducible density functional theory (DFT) calculations and workflows.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "simulation_connector",
        "electronic_structure"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aiidateam/aiida-quantumespresso",
      "help_website": [
        "https://aiida-quantumespresso.readthedocs.io/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "quantum-espresso",
        "dft",
        "materials-science",
        "aiida-plugin"
      ],
      "id": 642
    },
    {
      "name": "AiiDA-Shell",
      "one_line_profile": "Plugin to run arbitrary shell commands within AiiDA workflows",
      "detailed_description": "A utility plugin that allows users to run any shell command or executable as an AiiDA process, automatically capturing provenance and outputs without writing a full plugin.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_utility",
        "provenance_tracking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aiidateam/aiida-shell",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "shell",
        "provenance",
        "automation",
        "aiida-plugin"
      ],
      "id": 643
    },
    {
      "name": "AiiDA-Submission-Controller",
      "one_line_profile": "Utility for managing large-scale AiiDA process submissions",
      "detailed_description": "A library providing classes to manage and throttle the submission of large numbers of processes in AiiDA, useful for high-throughput screening studies.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "high_throughput_computing",
        "workflow_control"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aiidateam/aiida-submission-controller",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "high-throughput",
        "batch-processing",
        "aiida-utility"
      ],
      "id": 644
    },
    {
      "name": "AiiDA-Wannier90",
      "one_line_profile": "AiiDA plugin for the Wannier90 code",
      "detailed_description": "A plugin connecting AiiDA with Wannier90, enabling the calculation of Maximally Localized Wannier Functions (MLWFs) within automated workflows.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "simulation_connector",
        "electronic_structure"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aiidateam/aiida-wannier90",
      "help_website": [
        "https://aiida-wannier90.readthedocs.io/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "wannier90",
        "wannier-functions",
        "materials-science",
        "aiida-plugin"
      ],
      "id": 645
    },
    {
      "name": "AiiDA-Wannier90-Workflows",
      "one_line_profile": "Automated workflows for Wannier90 calculations",
      "detailed_description": "A collection of advanced, automated workflows for computing Wannier functions and related properties using AiiDA and Wannier90, designed to minimize user intervention.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_automation",
        "electronic_structure"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/aiidateam/aiida-wannier90-workflows",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "wannier90",
        "automation",
        "workflows",
        "materials-science"
      ],
      "id": 646
    },
    {
      "name": "AiiDA-WorkGraph",
      "one_line_profile": "Interactive GUI and workflow manager for AiiDA",
      "detailed_description": "A tool to design and manage flexible AiiDA workflows efficiently, featuring an interactive GUI, checkpoints, and remote execution capabilities.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_design",
        "gui"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/aiidateam/aiida-workgraph",
      "help_website": [
        "https://aiida-workgraph.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "gui",
        "workflow-management",
        "visual-programming",
        "aiida-plugin"
      ],
      "id": 647
    },
    {
      "name": "BindFlow",
      "one_line_profile": "Snakemake workflow for FEP and MM(PB/GB)SA calculations",
      "detailed_description": "A Snakemake-based automated workflow for performing Free Energy Perturbation (FEP) and MM/PBSA or MM/GBSA calculations using GROMACS.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "molecular_dynamics",
        "free_energy_calculation"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/ale94mleon/BindFlow",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "gromacs",
        "snakemake",
        "free-energy",
        "molecular-dynamics"
      ],
      "id": 648
    },
    {
      "name": "scispacy",
      "one_line_profile": "spaCy pipeline and models for scientific/biomedical documents",
      "detailed_description": "A Python package containing spaCy pipelines and models specifically designed for processing biomedical, scientific, and clinical text, enabling entity recognition and linking in scientific literature.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "natural_language_processing",
        "text_mining"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/allenai/scispacy",
      "help_website": [
        "https://allenai.github.io/scispacy/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "nlp",
        "biomedical",
        "spacy",
        "text-mining"
      ],
      "id": 649
    },
    {
      "name": "FlowCraft",
      "one_line_profile": "Component-based pipeline composer for omics analysis using Nextflow",
      "detailed_description": "FlowCraft is a tool for building Nextflow pipelines specifically for omics data analysis. It allows researchers to assemble various bioinformatics components into a coherent workflow for processing biological data.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "pipeline_composition",
        "omics_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/assemblerflow/flowcraft",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "nextflow",
        "omics",
        "bioinformatics",
        "pipeline-builder"
      ],
      "id": 650
    },
    {
      "name": "Bactopia",
      "one_line_profile": "A flexible pipeline for complete analysis of bacterial genomes",
      "detailed_description": "Bactopia is an extensive workflow for processing bacterial genome data. It integrates numerous bioinformatics tools to perform quality control, assembly, annotation, and variant calling, specifically tailored for bacterial genomics.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "genome_assembly",
        "variant_calling",
        "bacterial_genomics"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/bactopia/bactopia",
      "help_website": [
        "https://bactopia.github.io/"
      ],
      "license": "MIT",
      "tags": [
        "bacteria",
        "genomics",
        "nextflow",
        "bioinformatics-pipeline"
      ],
      "id": 651
    },
    {
      "name": "BEDOPS",
      "one_line_profile": "High-performance genomic feature operations and set statistics",
      "detailed_description": "A highly scalable and fast toolkit for performing set operations (intersection, union, difference) on genomic interval data (BED files), essential for genomic analysis pipelines.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "genomic_interval_manipulation",
        "data_processing"
      ],
      "application_level": "solver",
      "primary_language": "C",
      "repo_url": "https://github.com/bedops/bedops",
      "help_website": [
        "https://bedops.readthedocs.io/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "genomics",
        "bioinformatics",
        "bed-files",
        "set-operations"
      ],
      "id": 652
    },
    {
      "name": "TOmicsVis",
      "one_line_profile": "Transcriptomics visualization and analysis R package",
      "detailed_description": "An R package designed for the visualization and analysis of transcriptomics data, providing ready-to-use plotting functions for scientific publications.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "visualization",
        "transcriptomics_analysis"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/benben-miao/TOmicsVis",
      "help_website": [],
      "license": "GPL-2.0",
      "tags": [
        "transcriptomics",
        "visualization",
        "r-package",
        "bioinformatics"
      ],
      "id": 653
    },
    {
      "name": "kraken2_classification",
      "one_line_profile": "Snakemake workflow for metagenomic classification using Kraken2",
      "detailed_description": "A reproducible Snakemake workflow designed to perform taxonomic classification of metagenomic sequences using the Kraken2 engine.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "metagenomics",
        "taxonomic_classification"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/bhattlab/kraken2_classification",
      "help_website": [],
      "license": null,
      "tags": [
        "metagenomics",
        "snakemake",
        "kraken2",
        "bioinformatics"
      ],
      "id": 654
    },
    {
      "name": "ABFE_workflow",
      "one_line_profile": "High-throughput Absolute Binding Free Energy calculation workflow",
      "detailed_description": "A Snakemake-based workflow for automating Absolute Binding Free Energy (ABFE) calculations, designed for scalability on HPC clusters.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "molecular_dynamics",
        "free_energy_calculation"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/bigginlab/ABFE_workflow",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "computational-chemistry",
        "molecular-dynamics",
        "snakemake",
        "drug-discovery"
      ],
      "id": 655
    },
    {
      "name": "Master of Pores",
      "one_line_profile": "Nextflow pipeline for direct RNA Nanopore sequencing analysis",
      "detailed_description": "A comprehensive Nextflow pipeline for processing and analyzing direct RNA sequencing data from Oxford Nanopore Technologies, including preprocessing, mapping, and modification detection.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "rna_sequencing_analysis",
        "nanopore_processing"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/biocorecrg/master_of_pores",
      "help_website": [
        "https://biocorecrg.github.io/master_of_pores/"
      ],
      "license": "MIT",
      "tags": [
        "nanopore",
        "rna-seq",
        "nextflow",
        "bioinformatics"
      ],
      "id": 656
    },
    {
      "name": "Taska",
      "one_line_profile": "Workflow management system for biomedical exploration",
      "detailed_description": "A workflow management tool specifically designed to support biomedical research and exploration tasks.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_management",
        "biomedical_research"
      ],
      "application_level": "platform",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/bioinformatics-ua/taska",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "workflow-manager",
        "biomedical",
        "bioinformatics"
      ],
      "id": 657
    },
    {
      "name": "Cromshell",
      "one_line_profile": "CLI for interacting with Cromwell scientific workflow servers",
      "detailed_description": "A command-line interface tool for submitting workflows, checking status, and retrieving metadata from a Cromwell server, facilitating scientific workflow management.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_management",
        "job_submission"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/broadinstitute/cromshell",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "cromwell",
        "wdl",
        "bioinformatics",
        "workflow-cli"
      ],
      "id": 658
    },
    {
      "name": "Cromwell",
      "one_line_profile": "Scientific workflow engine for WDL and CWL",
      "detailed_description": "A workflow management system geared towards scientific workflows, supporting WDL (Workflow Description Language) and CWL (Common Workflow Language), widely used in genomics and bioinformatics.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_execution",
        "pipeline_orchestration"
      ],
      "application_level": "platform",
      "primary_language": "Scala",
      "repo_url": "https://github.com/broadinstitute/cromwell",
      "help_website": [
        "https://cromwell.readthedocs.io/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "workflow-engine",
        "wdl",
        "cwl",
        "bioinformatics",
        "genomics"
      ],
      "id": 659
    },
    {
      "name": "Cromwell-tools",
      "one_line_profile": "Python clients and utilities for Cromwell workflow engine",
      "detailed_description": "A collection of Python libraries and scripts to interact with the Cromwell workflow engine, enabling programmatic submission and monitoring of scientific workflows.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_management",
        "api_client"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/broadinstitute/cromwell-tools",
      "help_website": [
        "https://cromwell-tools.readthedocs.io/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "cromwell",
        "python-client",
        "bioinformatics",
        "workflow"
      ],
      "id": 660
    },
    {
      "name": "GATK-SV",
      "one_line_profile": "Structural variation discovery pipeline for short-read sequencing",
      "detailed_description": "A comprehensive pipeline for detecting structural variations (SV) from short-read sequencing data, developed by the Broad Institute.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "structural_variation_calling",
        "genomics_pipeline"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/broadinstitute/gatk-sv",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "genomics",
        "structural-variation",
        "gatk",
        "bioinformatics"
      ],
      "id": 661
    },
    {
      "name": "Widdler",
      "one_line_profile": "CLI for managing WDL workflows on Cromwell",
      "detailed_description": "A command-line tool for executing, validating, and querying WDL workflows on Cromwell servers, providing an alternative interface for workflow management.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_management",
        "wdl_execution"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/broadinstitute/widdler",
      "help_website": [],
      "license": null,
      "tags": [
        "wdl",
        "cromwell",
        "bioinformatics",
        "cli"
      ],
      "id": 662
    },
    {
      "name": "V-pipe",
      "one_line_profile": "Bioinformatics pipeline for viral genome analysis",
      "detailed_description": "A bioinformatics pipeline integrating various tools for the analysis of next-generation sequencing (NGS) data from short viral genomes, used for variant calling and haplotype reconstruction.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "viral_genomics",
        "variant_calling",
        "haplotype_reconstruction"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/cbg-ethz/V-pipe",
      "help_website": [
        "https://cbg-ethz.github.io/V-pipe/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "virology",
        "ngs",
        "bioinformatics",
        "pipeline"
      ],
      "id": 663
    },
    {
      "name": "GraffiTE",
      "one_line_profile": "Pipeline for detecting polymorphic transposable elements in genome assemblies",
      "detailed_description": "GraffiTE is a computational pipeline designed to identify polymorphic transposable elements (TEs) in genome assemblies and long reads. It genotypes discovered polymorphisms using genome-graphs, facilitating the study of structural variations in genomics.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "variant_calling",
        "genome_assembly_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "R",
      "repo_url": "https://github.com/cgroza/GraffiTE",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "genomics",
        "transposable-elements",
        "structural-variation",
        "pipeline"
      ],
      "id": 664
    },
    {
      "name": "wdl-cell-ranger",
      "one_line_profile": "WDL workflows for running Cell Ranger single-cell analysis pipelines",
      "detailed_description": "A collection of Workflow Description Language (WDL) tasks and workflows for running 10x Genomics Cell Ranger pipelines. It enables scalable execution of single-cell RNA-seq data processing on Cromwell-compatible infrastructure.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "scRNA-seq_processing",
        "pipeline_orchestration"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/chanzuckerberg/wdl-cell-ranger",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "single-cell",
        "wdl",
        "cell-ranger",
        "bioinformatics-workflow"
      ],
      "id": 665
    },
    {
      "name": "CompareM2",
      "one_line_profile": "Microbial genomes-to-report pipeline for comparative genomics",
      "detailed_description": "A comprehensive pipeline for processing microbial genomes to generate comparative reports. It automates steps likely including quality control, taxonomic classification, and functional annotation for microbial research.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "comparative_genomics",
        "microbial_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/cmkobel/CompareM2",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "microbiology",
        "genomics",
        "pipeline",
        "comparative-analysis"
      ],
      "id": 666
    },
    {
      "name": "ncov2019-artic-nf",
      "one_line_profile": "Nextflow pipeline for SARS-CoV-2 ARTIC field bioinformatics",
      "detailed_description": "A Nextflow pipeline implementing the ARTIC network's field bioinformatics protocol for SARS-CoV-2 sequencing data. It handles alignment, variant calling, and consensus sequence generation for viral genomics.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "viral_genomics",
        "variant_calling",
        "consensus_generation"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/connor-lab/ncov2019-artic-nf",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "covid-19",
        "nextflow",
        "bioinformatics",
        "artic-network"
      ],
      "id": 667
    },
    {
      "name": "pyflow-ATACseq",
      "one_line_profile": "Snakemake pipeline for ATAC-seq data analysis",
      "detailed_description": "A Snakemake-based bioinformatics pipeline for processing ATAC-seq data. It automates steps from raw reads to peak calling and quality control, facilitating chromatin accessibility studies.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "atac-seq_analysis",
        "pipeline_orchestration"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/crazyhottommy/pyflow-ATACseq",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "atac-seq",
        "snakemake",
        "bioinformatics",
        "epigenetics"
      ],
      "id": 668
    },
    {
      "name": "pyflow-ChIPseq",
      "one_line_profile": "Snakemake pipeline for ChIP-seq data analysis",
      "detailed_description": "A Snakemake workflow for analyzing ChIP-seq data, handling alignment, peak calling, and QC. Designed to process data from GEO or local sources for epigenomic research.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "chip-seq_analysis",
        "pipeline_orchestration"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/crazyhottommy/pyflow-ChIPseq",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "chip-seq",
        "snakemake",
        "bioinformatics",
        "epigenetics"
      ],
      "id": 669
    },
    {
      "name": "WISC_MVPA",
      "one_line_profile": "Workflow for Whole-brain Imaging with Sparse Correlations (MVPA)",
      "detailed_description": "A High Throughput Computing (HTC) workflow for performing Multi-Voxel Pattern Analysis (MVPA) on whole-brain imaging data. It is designed for neuroscience research involving fMRI data analysis.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "neuroimaging_analysis",
        "mvpa"
      ],
      "application_level": "workflow",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/crcox/WISC_MVPA",
      "help_website": [],
      "license": null,
      "tags": [
        "neuroscience",
        "fmri",
        "mvpa",
        "matlab"
      ],
      "id": 670
    },
    {
      "name": "ARMOR",
      "one_line_profile": "Automated Reproducible MOdular RNA-seq workflow",
      "detailed_description": "A light-weight Snakemake workflow for preprocessing and statistical analysis of RNA-seq data. It integrates tools like Salmon, edgeR, and DRIMSeq to provide a complete analysis pipeline from reads to results.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "rna-seq_analysis",
        "differential_expression"
      ],
      "application_level": "workflow",
      "primary_language": "R",
      "repo_url": "https://github.com/csoneson/ARMOR",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rna-seq",
        "snakemake",
        "bioinformatics",
        "transcriptomics"
      ],
      "id": 671
    },
    {
      "name": "Cylc",
      "one_line_profile": "Workflow engine for cycling systems in meteorology and climate science",
      "detailed_description": "Cylc is a workflow engine designed specifically for cycling systems, widely used in operational weather forecasting and climate modeling (e.g., by the Met Office). It handles complex dependencies in infinite cycling workflows.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_orchestration",
        "climate_modeling"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/cylc/cylc-flow",
      "help_website": [
        "https://cylc.org/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "meteorology",
        "workflow-engine",
        "climate-science",
        "hpc"
      ],
      "id": 672
    },
    {
      "name": "DeepChem",
      "one_line_profile": "Deep learning library for drug discovery, materials science, and quantum chemistry",
      "detailed_description": "DeepChem provides a high-quality open-source toolchain that democratizes the use of deep learning in drug discovery, materials science, quantum chemistry, and biology. It includes molecular featurizers (ETL), data loaders, and specialized model architectures.",
      "domains": [
        "D4",
        "D1"
      ],
      "subtask_category": [
        "drug_discovery",
        "molecular_modeling",
        "property_prediction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/deepchem/deepchem",
      "help_website": [
        "https://deepchem.io"
      ],
      "license": "MIT",
      "tags": [
        "drug-discovery",
        "cheminformatics",
        "deep-learning",
        "molecular-modeling"
      ],
      "id": 673
    },
    {
      "name": "Dockstore",
      "one_line_profile": "Platform for sharing scientific tools and workflows",
      "detailed_description": "Dockstore is an open platform used by the GA4GH for sharing Docker-based scientific tools and workflows (CWL, WDL, Nextflow). It enables reproducibility and discovery of bioinformatics pipelines.",
      "domains": [
        "D1-04"
      ],
      "subtask_category": [
        "workflow_sharing",
        "reproducibility",
        "tool_discovery"
      ],
      "application_level": "platform",
      "primary_language": "Java",
      "repo_url": "https://github.com/dockstore/dockstore",
      "help_website": [
        "https://dockstore.org"
      ],
      "license": "Apache-2.0",
      "tags": [
        "bioinformatics",
        "workflow",
        "cwl",
        "wdl",
        "reproducibility"
      ],
      "id": 674
    },
    {
      "name": "WDL Workspace",
      "one_line_profile": "Web-based UI for running WDL bioinformatics workflows via Cromwell",
      "detailed_description": "A web interface designed to facilitate the execution and management of WDL-based bioinformatics workflows using the Cromwell server backend.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_execution",
        "bioinformatics_pipeline"
      ],
      "application_level": "platform",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/epam/wdl-workspace",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "wdl",
        "bioinformatics",
        "cromwell",
        "workflow-ui"
      ],
      "id": 675
    },
    {
      "name": "aiida-defects",
      "one_line_profile": "AiiDA plugin for point defect calculations in materials science",
      "detailed_description": "A plugin for the AiiDA workflow engine that provides automated workflows for calculating properties of point defects in materials.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "defect_calculation",
        "materials_modeling"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/epfl-theos/aiida-defects",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "aiida",
        "materials-science",
        "defects",
        "workflow"
      ],
      "id": 676
    },
    {
      "name": "MrBiomics",
      "one_line_profile": "Modular bioinformatics pipeline framework for multi-omics analysis",
      "detailed_description": "A framework providing composable modules and recipes to automate bioinformatics analyses for multi-omics data, built on top of Snakemake.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "multi_omics_analysis",
        "pipeline_framework"
      ],
      "application_level": "workflow",
      "primary_language": "R",
      "repo_url": "https://github.com/epigen/MrBiomics",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bioinformatics",
        "multi-omics",
        "snakemake",
        "pipeline"
      ],
      "id": 677
    },
    {
      "name": "atacseq_pipeline",
      "one_line_profile": "Snakemake workflow for ATAC-seq data processing and quantification",
      "detailed_description": "A comprehensive bioinformatics pipeline for processing, quantifying, and annotating ATAC-seq data, implemented as a MrBiomics module.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "atac_seq_analysis",
        "genomics_pipeline"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/epigen/atacseq_pipeline",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "atac-seq",
        "snakemake",
        "bioinformatics",
        "ngs"
      ],
      "id": 678
    },
    {
      "name": "dea_limma",
      "one_line_profile": "Differential expression analysis workflow using Limma",
      "detailed_description": "A Snakemake workflow and MrBiomics module for performing differential expression analyses on NGS data using the R package limma.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "differential_expression",
        "transcriptomics"
      ],
      "application_level": "workflow",
      "primary_language": "R",
      "repo_url": "https://github.com/epigen/dea_limma",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "limma",
        "rna-seq",
        "differential-expression",
        "snakemake"
      ],
      "id": 679
    },
    {
      "name": "enrichment_analysis",
      "one_line_profile": "Genomic region and gene set enrichment analysis workflow",
      "detailed_description": "A Snakemake workflow for performing enrichment analyses on genomic regions and gene sets using tools like LOLA, GREAT, and GSEApy.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "enrichment_analysis",
        "genomics"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/epigen/enrichment_analysis",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gsea",
        "enrichment",
        "bioinformatics",
        "snakemake"
      ],
      "id": 680
    },
    {
      "name": "genome_tracks",
      "one_line_profile": "Workflow for visualization of genome browser tracks",
      "detailed_description": "A Snakemake workflow for generating and visualizing genome browser tracks from aligned BAM files (RNA-seq, ATAC-seq, etc.) using pyGenomeTracks and IGV.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "genome_visualization",
        "track_generation"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/epigen/genome_tracks",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "visualization",
        "genome-browser",
        "bam",
        "snakemake"
      ],
      "id": 681
    },
    {
      "name": "scrnaseq_processing_seurat",
      "one_line_profile": "Single-cell RNA-seq processing workflow using Seurat",
      "detailed_description": "A Snakemake workflow for processing and visualizing single-cell/nuclei RNA-seq data (10X or MTX format) using the Seurat R package.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "single_cell_analysis",
        "scrnaseq"
      ],
      "application_level": "workflow",
      "primary_language": "R",
      "repo_url": "https://github.com/epigen/scrnaseq_processing_seurat",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "scRNA-seq",
        "seurat",
        "bioinformatics",
        "snakemake"
      ],
      "id": 682
    },
    {
      "name": "unsupervised_analysis",
      "one_line_profile": "Workflow for unsupervised dimensionality reduction and clustering",
      "detailed_description": "A general purpose Snakemake workflow for performing unsupervised analyses such as dimensionality reduction and cluster analysis on high-dimensional biological data.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "dimensionality_reduction",
        "clustering"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/epigen/unsupervised_analysis",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "clustering",
        "pca",
        "bioinformatics",
        "snakemake"
      ],
      "id": 683
    },
    {
      "name": "ClusterFlow",
      "one_line_profile": "Pipeline tool for bioinformatics analyses on cluster environments",
      "detailed_description": "A command-line tool designed to automate and standardize bioinformatics analyses on cluster environments, managing job submissions and dependencies.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_management",
        "bioinformatics_pipeline"
      ],
      "application_level": "workflow",
      "primary_language": "Perl",
      "repo_url": "https://github.com/ewels/clusterflow",
      "help_website": [
        "http://clusterflow.io/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "bioinformatics",
        "pipeline",
        "hpc",
        "cluster"
      ],
      "id": 684
    },
    {
      "name": "aiida-orca",
      "one_line_profile": "AiiDA plugin for the ORCA quantum chemistry package",
      "detailed_description": "A plugin for the AiiDA workflow engine that interfaces with the ORCA software for quantum chemistry calculations.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "quantum_chemistry",
        "workflow_connector"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ezpzbz/aiida-orca",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "aiida",
        "orca",
        "quantum-chemistry",
        "plugin"
      ],
      "id": 685
    },
    {
      "name": "flowr",
      "one_line_profile": "R-based workflow system for bioinformatics",
      "detailed_description": "A robust and efficient workflow management system implemented in R, designed to streamline complex bioinformatics pipelines.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_management",
        "bioinformatics_pipeline"
      ],
      "application_level": "workflow",
      "primary_language": "R",
      "repo_url": "https://github.com/flow-r/flowr",
      "help_website": [
        "http://docs.flowr.space"
      ],
      "license": "NOASSERTION",
      "tags": [
        "r",
        "bioinformatics",
        "workflow",
        "pipeline"
      ],
      "id": 686
    },
    {
      "name": "bacannot",
      "one_line_profile": "Pipeline for prokaryotic genome annotation",
      "detailed_description": "A comprehensive Nextflow pipeline for prokaryotic genome annotation and interrogation, including interactive reports.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "genome_annotation",
        "prokaryotic_genomics"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/fmalmeida/bacannot",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "genome-annotation",
        "nextflow",
        "bacteria",
        "bioinformatics"
      ],
      "id": 687
    },
    {
      "name": "metaGEM",
      "one_line_profile": "Workflow for generating genome-scale metabolic models from metagenomes",
      "detailed_description": "A pipeline for generating context-specific genome-scale metabolic models and predicting metabolic interactions within microbial communities directly from metagenomic data.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "metabolic_modeling",
        "metagenomics"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/franciscozorrilla/metaGEM",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "metagenomics",
        "metabolic-modeling",
        "systems-biology",
        "workflow"
      ],
      "id": 688
    },
    {
      "name": "dagr",
      "one_line_profile": "Scala DSL for bioinformatics pipelines",
      "detailed_description": "A Scala-based DSL and framework for writing and executing bioinformatics pipelines as Directed Acyclic Graphs (DAGs).",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_definition",
        "bioinformatics_pipeline"
      ],
      "application_level": "library",
      "primary_language": "Scala",
      "repo_url": "https://github.com/fulcrumgenomics/dagr",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "scala",
        "bioinformatics",
        "pipeline",
        "dsl"
      ],
      "id": 689
    },
    {
      "name": "Galaxy",
      "one_line_profile": "Open web-based platform for data intensive biomedical research",
      "detailed_description": "A scientific workflow, data integration, and data and analysis persistence and publishing platform that aims to make computational biology accessible to research scientists that do not have computer programming experience.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_platform",
        "bioinformatics_analysis"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/galaxyproject/galaxy",
      "help_website": [
        "https://galaxyproject.org/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "bioinformatics",
        "workflow",
        "platform",
        "reproducibility"
      ],
      "id": 690
    },
    {
      "name": "hybracter",
      "one_line_profile": "Automated long-read bacterial genome assembly pipeline",
      "detailed_description": "A Snakemake pipeline for automated long-read first bacterial genome assembly, utilizing Snaketool for workflow management.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "genome_assembly",
        "bacterial_genomics"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/gbouras13/hybracter",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "genome-assembly",
        "bacteria",
        "snakemake",
        "long-read"
      ],
      "id": 691
    },
    {
      "name": "Cloudgene",
      "one_line_profile": "Framework for building SaaS platforms for bioinformatics pipelines",
      "detailed_description": "A framework to build Software As A Service (SaaS) platforms for data analysis pipelines, widely used for genetic imputation servers.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "platform_building",
        "genetics_pipeline"
      ],
      "application_level": "platform",
      "primary_language": "Java",
      "repo_url": "https://github.com/genepi/cloudgene",
      "help_website": [
        "http://cloudgene.uibk.ac.at/"
      ],
      "license": "AGPL-3.0",
      "tags": [
        "saas",
        "bioinformatics",
        "pipeline",
        "genetics"
      ],
      "id": 692
    },
    {
      "name": "nanopype",
      "one_line_profile": "Snakemake pipelines for nanopore sequencing data archiving and processing",
      "detailed_description": "A collection of Snakemake pipelines designed for the processing and archiving of Oxford Nanopore Technologies (ONT) sequencing data, facilitating reproducible bioinformatics analysis.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "sequencing_processing",
        "bioinformatics_pipeline"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/giesselmann/nanopype",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "snakemake",
        "nanopore",
        "bioinformatics"
      ],
      "id": 693
    },
    {
      "name": "dataset_grouper",
      "one_line_profile": "Libraries for efficient and scalable group-structured dataset pipelines",
      "detailed_description": "A Python library for creating efficient and scalable data pipelines for group-structured datasets, commonly used in machine learning research such as Federated Learning.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "data_loading",
        "ml_pipeline"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/google-parfait/dataset_grouper",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "data-pipeline",
        "machine-learning",
        "federated-learning"
      ],
      "id": 694
    },
    {
      "name": "DeepSomatic",
      "one_line_profile": "Deep learning-based somatic variant caller",
      "detailed_description": "An analysis pipeline that uses a deep neural network to call somatic variants from tumor-normal and tumor-only sequencing data, developed by Google.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "variant_calling",
        "genomics"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/google/deepsomatic",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "genomics",
        "deep-learning",
        "variant-calling"
      ],
      "id": 695
    },
    {
      "name": "DeepVariant",
      "one_line_profile": "Deep learning-based genetic variant caller",
      "detailed_description": "An analysis pipeline that uses a deep neural network to call genetic variants from next-generation DNA sequencing data with high accuracy.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "variant_calling",
        "genomics"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/google/deepvariant",
      "help_website": [
        "https://github.com/google/deepvariant"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "genomics",
        "deep-learning",
        "variant-calling"
      ],
      "id": 696
    },
    {
      "name": "kedro-dagster",
      "one_line_profile": "Kedro plugin to support running pipelines on Dagster",
      "detailed_description": "A plugin that integrates Kedro data science pipelines with the Dagster orchestration engine, facilitating the deployment and management of scientific data workflows.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_orchestration",
        "pipeline_integration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/gtauzin/kedro-dagster",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "kedro",
        "dagster",
        "data-science-pipeline"
      ],
      "id": 697
    },
    {
      "name": "atomate",
      "one_line_profile": "Pre-built workflows for computational materials science",
      "detailed_description": "A Python library containing pre-built workflows for computational materials science, designed to run with the FireWorks workflow software.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "materials_science_workflow",
        "computational_chemistry"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/hackingmaterials/atomate",
      "help_website": [
        "https://atomate.org"
      ],
      "license": "NOASSERTION",
      "tags": [
        "materials-science",
        "workflows",
        "fireworks"
      ],
      "id": 698
    },
    {
      "name": "snpArcher",
      "one_line_profile": "Snakemake workflow for variant calling in non-model organisms",
      "detailed_description": "A reproducible and scalable Snakemake workflow for variant calling, specifically designed for ease-of-use in non-model organisms.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "variant_calling",
        "bioinformatics_pipeline"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/harvardinformatics/snpArcher",
      "help_website": [
        "https://snparcher.readthedocs.io"
      ],
      "license": "MIT",
      "tags": [
        "snakemake",
        "variant-calling",
        "genomics"
      ],
      "id": 699
    },
    {
      "name": "rnaflow",
      "one_line_profile": "RNA-Seq differential gene expression pipeline using Nextflow",
      "detailed_description": "A simple and reproducible RNA-Seq differential gene expression analysis pipeline implemented in Nextflow.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "rna-seq",
        "differential_expression"
      ],
      "application_level": "workflow",
      "primary_language": "HTML",
      "repo_url": "https://github.com/hoelzer-lab/rnaflow",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "nextflow",
        "rna-seq",
        "bioinformatics"
      ],
      "id": 700
    },
    {
      "name": "htmap",
      "one_line_profile": "High-Throughput Computing in Python powered by HTCondor",
      "detailed_description": "A library that enables high-throughput computing in Python by wrapping HTCondor, allowing users to map Python functions over inputs and run them on a cluster.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "high_throughput_computing",
        "job_scheduling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/htcondor/htmap",
      "help_website": [
        "https://htmap.readthedocs.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "htcondor",
        "distributed-computing",
        "python"
      ],
      "id": 701
    },
    {
      "name": "httk",
      "one_line_profile": "High-Throughput Toolkit for materials science calculations",
      "detailed_description": "A toolkit for preparing and running calculations, analyzing results, and storing outcomes in databases, primarily for computational materials science.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "materials_science",
        "workflow_management"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/httk/httk",
      "help_website": [
        "http://httk.openmaterialsdb.se/"
      ],
      "license": "AGPL-3.0",
      "tags": [
        "materials-science",
        "high-throughput",
        "simulation"
      ],
      "id": 702
    },
    {
      "name": "IQA-Dataset",
      "one_line_profile": "Interface for downloading and loading Image Quality Assessment datasets",
      "detailed_description": "A unified interface for downloading and loading popular Image Quality Assessment (IQA) datasets, facilitating research in computer vision and image processing.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "dataset_loading",
        "image_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/icbcbicc/IQA-Dataset",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "iqa",
        "dataset-loader",
        "computer-vision"
      ],
      "id": 703
    },
    {
      "name": "nextNEOpi",
      "one_line_profile": "Comprehensive pipeline for computational neoantigen prediction",
      "detailed_description": "A Nextflow pipeline for computational neoantigen prediction from sequencing data, used in cancer immunology research.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "neoantigen_prediction",
        "immunoinformatics"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/icbi-lab/nextNEOpi",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "nextflow",
        "cancer-immunology",
        "neoantigen"
      ],
      "id": 704
    },
    {
      "name": "sns",
      "one_line_profile": "Analysis pipelines for genomic sequencing data",
      "detailed_description": "A collection of analysis pipelines for processing genomic sequencing data, including RNA-seq, ChIP-seq, and others.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "genomics_pipeline",
        "sequencing_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "Shell",
      "repo_url": "https://github.com/igordot/sns",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "genomics",
        "pipeline",
        "ngs"
      ],
      "id": 705
    },
    {
      "name": "redun",
      "one_line_profile": "Scientific workflow engine by Insitro",
      "detailed_description": "A workflow engine developed by Insitro, designed to handle complex scientific workflows with features like caching, provenance tracking, and execution on various backends.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_engine",
        "scientific_computing"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/insitro/redun",
      "help_website": [
        "https://insitro.github.io/redun/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "workflow-engine",
        "bioinformatics",
        "reproducibility"
      ],
      "id": 706
    },
    {
      "name": "apple-health-ingester",
      "one_line_profile": "Ingestion tool for Apple Health export data to time-series databases",
      "detailed_description": "A server and utility to ingest Apple Health XML export data and store it into databases like InfluxDB for analysis, enabling personal health data research and quantification.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "data_ingestion",
        "format_conversion"
      ],
      "application_level": "service",
      "primary_language": "Go",
      "repo_url": "https://github.com/irvinlim/apple-health-ingester",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "apple-health",
        "etl",
        "quantified-self",
        "health-data"
      ],
      "id": 707
    },
    {
      "name": "ATMOSPEC (aiidalab-ispg)",
      "one_line_profile": "AiiDA lab application for ab initio UV/vis spectroscopy",
      "detailed_description": "An application for the AiiDA lab platform that provides workflows for computing UV/vis spectroscopy properties using ab initio methods, developed by the Interdisciplinary Surface Physics Group.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "spectroscopy",
        "workflow_automation"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/ispg-group/aiidalab-ispg",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "aiida",
        "spectroscopy",
        "computational-chemistry",
        "materials-science"
      ],
      "id": 708
    },
    {
      "name": "smk-simple-slurm",
      "one_line_profile": "Snakemake profile for Slurm job scheduling",
      "detailed_description": "A configuration profile to enable Snakemake scientific workflows to run on Slurm-managed High Performance Computing (HPC) clusters, acting as a connector between the workflow engine and the scheduler.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_scheduling",
        "hpc_connector"
      ],
      "application_level": "workflow",
      "primary_language": "Shell",
      "repo_url": "https://github.com/jdblischak/smk-simple-slurm",
      "help_website": [],
      "license": "CC0-1.0",
      "tags": [
        "snakemake",
        "slurm",
        "hpc",
        "bioinformatics-workflow"
      ],
      "id": 709
    },
    {
      "name": "scDataLoader",
      "one_line_profile": "DataLoader for large single-cell datasets from LaminDB",
      "detailed_description": "A specialized data loader designed to efficiently fetch and process large-scale single-cell biological datasets stored in LaminDB for machine learning workflows.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "data_loading",
        "single_cell_analysis"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/jkobject/scDataLoader",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "single-cell",
        "lamindb",
        "dataloader",
        "biology"
      ],
      "id": 710
    },
    {
      "name": "multiPrime",
      "one_line_profile": "Mismatch-tolerant minimal primer set design tool",
      "detailed_description": "A tool for designing minimal primer sets that are tolerant to mismatches, specifically optimized for large and diverse viral sequences.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "primer_design",
        "sequence_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/joybio/multiPrime",
      "help_website": [
        "http://multiPrime.cn"
      ],
      "license": "MIT",
      "tags": [
        "bioinformatics",
        "primer-design",
        "virology",
        "genomics"
      ],
      "id": 711
    },
    {
      "name": "era5_in_gee",
      "one_line_profile": "Ingestion tools for ERA5 climate data into Google Earth Engine",
      "detailed_description": "A collection of Python scripts and functions designed to facilitate the ingestion of ERA5 climate reanalysis data into Google Earth Engine (GEE) for geospatial analysis.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "data_ingestion",
        "climate_data_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/jwagemann/era5_in_gee",
      "help_website": [],
      "license": null,
      "tags": [
        "era5",
        "google-earth-engine",
        "climate-data",
        "etl"
      ],
      "id": 712
    },
    {
      "name": "landsat_ingestor",
      "one_line_profile": "Ingestion scripts for Landsat satellite data",
      "detailed_description": "A set of tools and scripts for processing and ingesting Landsat satellite imagery data into Amazon S3 for public hosting and analysis.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "data_ingestion",
        "remote_sensing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/landsat-pds/landsat_ingestor",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "landsat",
        "satellite-imagery",
        "aws",
        "ingestion"
      ],
      "id": 713
    },
    {
      "name": "tempo",
      "one_line_profile": "Self-hosted weather API and processing pipeline",
      "detailed_description": "A self-hosted weather data processing tool that ingests ECMWF data and serves it as colorized WebP maps and GeoJSON contours for GIS integration.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "meteorological_data_processing",
        "visualization_service"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/leoneljdias/tempo",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "weather-api",
        "ecmwf",
        "gis",
        "meteorology"
      ],
      "id": 714
    },
    {
      "name": "fermikit",
      "one_line_profile": "De novo assembly based variant calling pipeline",
      "detailed_description": "A bioinformatics pipeline for Illumina short reads that performs de novo assembly-based variant calling, integrating assembly, mapping, and calling steps.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "variant_calling",
        "sequence_assembly"
      ],
      "application_level": "workflow",
      "primary_language": "C",
      "repo_url": "https://github.com/lh3/fermikit",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "bioinformatics",
        "genomics",
        "variant-calling",
        "assembly"
      ],
      "id": 715
    },
    {
      "name": "LncPipe",
      "one_line_profile": "Nextflow pipeline for lncRNA analysis",
      "detailed_description": "A comprehensive Nextflow-based pipeline for the analysis of long non-coding RNAs (lncRNAs) from RNA-seq datasets, including identification and differential expression analysis.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "rna_seq_analysis",
        "lncrna_identification"
      ],
      "application_level": "workflow",
      "primary_language": "Groovy",
      "repo_url": "https://github.com/likelet/LncPipe",
      "help_website": [],
      "license": "LGPL-3.0",
      "tags": [
        "bioinformatics",
        "nextflow",
        "lncrna",
        "rna-seq"
      ],
      "id": 716
    },
    {
      "name": "altocumulus",
      "one_line_profile": "CLI for submitting WDL jobs to Terra/Cromwell",
      "detailed_description": "A command-line tool designed to facilitate the submission and management of WDL-based scientific workflows on Terra or Cromwell servers.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_submission",
        "cloud_computing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/lilab-bcb/altocumulus",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "bioinformatics",
        "wdl",
        "terra",
        "cromwell"
      ],
      "id": 717
    },
    {
      "name": "aiida-lsmo",
      "one_line_profile": "AiiDA workflows for materials science at LSMO",
      "detailed_description": "A collection of AiiDA workflows and calculation plugins tailored for computational materials science research, specifically developed for the LSMO laboratory.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "materials_modeling",
        "workflow_orchestration"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/lsmo-epfl/aiida-lsmo",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "aiida",
        "materials-science",
        "workflow",
        "epfl"
      ],
      "id": 718
    },
    {
      "name": "aiida-raspa",
      "one_line_profile": "AiiDA plugin for RASPA molecular simulations",
      "detailed_description": "An AiiDA plugin that interfaces with the RASPA software, enabling the orchestration of molecular simulations for adsorption and diffusion in nanoporous materials.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "molecular_simulation",
        "materials_modeling"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/lsmo-epfl/aiida-raspa",
      "help_website": [
        "http://www.aiida.net"
      ],
      "license": "NOASSERTION",
      "tags": [
        "aiida",
        "raspa",
        "molecular-simulation",
        "materials-science"
      ],
      "id": 719
    },
    {
      "name": "aiida-zeopp",
      "one_line_profile": "AiiDA plugin for Zeo++ porous materials analysis",
      "detailed_description": "An AiiDA plugin for Zeo++, facilitating high-throughput geometric analysis and characterization of porous materials within the AiiDA workflow engine.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "materials_analysis",
        "geometric_characterization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/lsmo-epfl/aiida-zeopp",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "aiida",
        "zeopp",
        "porous-materials",
        "materials-science"
      ],
      "id": 720
    },
    {
      "name": "aiida-graph-render",
      "one_line_profile": "Visualization tool for AiiDA provenance graphs using GePhi",
      "detailed_description": "A utility to export and render complex provenance graphs from the AiiDA materials science workflow engine into GePhi format for visualization and analysis.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "visualization",
        "provenance_tracking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ltalirz/aiida-graph-render",
      "help_website": [],
      "license": null,
      "tags": [
        "aiida",
        "visualization",
        "materials-science",
        "graph"
      ],
      "id": 721
    },
    {
      "name": "AiiDA Explorer",
      "one_line_profile": "Interactive provenance browser for AiiDA databases",
      "detailed_description": "A web-based application to explore and visualize the provenance graph and data stored in AiiDA databases, facilitating the analysis of computational materials science workflows.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "visualization",
        "data_exploration"
      ],
      "application_level": "platform",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/materialscloud-org/aiida-explorer",
      "help_website": [
        "https://aiida-explorer.materialscloud.org"
      ],
      "license": "MIT",
      "tags": [
        "aiida",
        "materials-science",
        "provenance",
        "visualization"
      ],
      "id": 722
    },
    {
      "name": "atomate2",
      "one_line_profile": "Library of computational materials science workflows",
      "detailed_description": "A library of computational materials science workflows for the Materials Project stack, designed to run complex simulations using engines like VASP and CP2K.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_management",
        "simulation"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/materialsproject/atomate2",
      "help_website": [
        "https://materialsproject.github.io/atomate2/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "materials-science",
        "workflow",
        "vasp",
        "cp2k"
      ],
      "id": 723
    },
    {
      "name": "FireWorks",
      "one_line_profile": "Workflow management system for high-throughput computing",
      "detailed_description": "A workflow management system designed for running high-throughput calculations at supercomputing centers, widely used in the Materials Project for managing computational materials science tasks.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_management",
        "high_throughput_computing"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/materialsproject/fireworks",
      "help_website": [
        "https://materialsproject.github.io/fireworks/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "workflow",
        "hpc",
        "materials-science",
        "high-throughput"
      ],
      "id": 724
    },
    {
      "name": "MPmorph",
      "one_line_profile": "Tools for ab-initio molecular dynamics (AIMD) analysis",
      "detailed_description": "A collection of tools to run and analyze ab-initio molecular dynamics (AIMD) calculations, integrating with the Materials Project stack (pymatgen, fireworks).",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "simulation_analysis",
        "molecular_dynamics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/materialsproject/mpmorph",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "aimd",
        "molecular-dynamics",
        "materials-science",
        "vasp"
      ],
      "id": 725
    },
    {
      "name": "voxceleb-luigi",
      "one_line_profile": "Pipeline for processing VoxCeleb audio datasets",
      "detailed_description": "A Luigi-based workflow pipeline to download, extract, and process the VoxCeleb audio dataset for speaker recognition research.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "data_preparation",
        "audio_processing"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/maxhollmann/voxceleb-luigi",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "audio",
        "dataset-pipeline",
        "luigi",
        "speaker-recognition"
      ],
      "id": 726
    },
    {
      "name": "snakePipes",
      "one_line_profile": "Snakemake workflows for NGS data analysis",
      "detailed_description": "A set of flexible and customizable workflows based on Snakemake for the analysis of Next-Generation Sequencing (NGS) data, including DNA-seq, RNA-seq, and ChIP-seq.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "bioinformatics_pipeline",
        "ngs_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/maxplanck-ie/snakepipes",
      "help_website": [
        "https://snakepipes.readthedocs.io"
      ],
      "license": null,
      "tags": [
        "bioinformatics",
        "ngs",
        "snakemake",
        "workflow"
      ],
      "id": 727
    },
    {
      "name": "ATLAS",
      "one_line_profile": "Metagenome data analysis pipeline",
      "detailed_description": "A workflow pipeline for the assembly, annotation, and quantification of metagenomic data, designed to be easy to use and reproducible.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "metagenomics",
        "sequence_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/metagenome-atlas/atlas",
      "help_website": [
        "https://metagenome-atlas.readthedocs.io"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "metagenomics",
        "bioinformatics",
        "snakemake",
        "assembly"
      ],
      "id": 728
    },
    {
      "name": "CromwellOnAzure",
      "one_line_profile": "Cromwell workflow engine deployment for Azure",
      "detailed_description": "Microsoft Genomics implementation of the Broad Institute's Cromwell workflow engine, enabling large-scale bioinformatics workflows on Azure cloud infrastructure.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_orchestration",
        "cloud_computing"
      ],
      "application_level": "platform",
      "primary_language": "C#",
      "repo_url": "https://github.com/microsoft/CromwellOnAzure",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bioinformatics",
        "cromwell",
        "azure",
        "workflow"
      ],
      "id": 729
    },
    {
      "name": "aiida-autocas",
      "one_line_profile": "AiiDA plugin for automatic active space selection",
      "detailed_description": "An AiiDA plugin that automates the selection of active spaces for multireference quantum chemistry calculations.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "computational_chemistry",
        "workflow_automation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/aiida-autocas",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "aiida",
        "quantum-chemistry",
        "active-space",
        "automation"
      ],
      "id": 730
    },
    {
      "name": "aiida-dynamic-workflows",
      "one_line_profile": "Dynamic workflow definition for AiiDA",
      "detailed_description": "An AiiDA plugin allowing the definition of dynamic workflows using Python functions, enhancing flexibility for computational science simulations.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_management",
        "computational_science"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/aiida-dynamic-workflows",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "aiida",
        "workflow",
        "dynamic-execution"
      ],
      "id": 731
    },
    {
      "name": "aiida-pyscf",
      "one_line_profile": "AiiDA plugin for PySCF",
      "detailed_description": "An AiiDA plugin to interface with the PySCF (Python-based Simulations of Chemistry Framework) code, enabling automated quantum chemistry calculations.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "computational_chemistry",
        "workflow_connector"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/aiida-pyscf",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "aiida",
        "pyscf",
        "quantum-chemistry",
        "plugin"
      ],
      "id": 732
    },
    {
      "name": "biomedica-etl",
      "one_line_profile": "ETL pipeline for BIOMEDICA image-caption dataset",
      "detailed_description": "Data processing pipeline used to create the BIOMEDICA dataset (Biomedical Image-Caption Archive) from scientific literature, supporting vision-language model research.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "data_generation",
        "biomedical_mining"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/minwoosun/biomedica-etl",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "etl",
        "biomedical",
        "dataset-creation",
        "vision-language"
      ],
      "id": 733
    },
    {
      "name": "MToolBox",
      "one_line_profile": "Bioinformatics pipeline for mtDNA analysis from NGS data",
      "detailed_description": "A highly automated pipeline for the reconstruction and analysis of human mitochondrial DNA from High Throughput Sequencing data, including variant calling, haplogroup classification, and heteroplasmy assessment.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "variant_calling",
        "haplogroup_classification",
        "mtDNA_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/mitoNGS/MToolBox",
      "help_website": [
        "https://github.com/mitoNGS/MToolBox/wiki"
      ],
      "license": "GPL-3.0",
      "tags": [
        "bioinformatics",
        "mtDNA",
        "ngs",
        "pipeline"
      ],
      "id": 734
    },
    {
      "name": "grenepipe",
      "one_line_profile": "Automated variant calling pipeline for raw sequence reads",
      "detailed_description": "A flexible, scalable, and reproducible Snakemake pipeline to automate variant calling from raw sequence reads, supporting both sampled individuals and pool sequencing.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "variant_calling",
        "sequence_processing"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/moiexpositoalonsolab/grenepipe",
      "help_website": [
        "https://github.com/moiexpositoalonsolab/grenepipe"
      ],
      "license": "GPL-3.0",
      "tags": [
        "bioinformatics",
        "variant-calling",
        "snakemake",
        "genomics"
      ],
      "id": 735
    },
    {
      "name": "pipeliner",
      "one_line_profile": "Nextflow-based framework for sequencing data processing",
      "detailed_description": "A flexible Nextflow-based framework for the definition and execution of sequencing data processing pipelines, facilitating reproducible bioinformatics workflows.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_management",
        "sequencing_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/montilab/pipeliner",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "bioinformatics",
        "nextflow",
        "pipeline",
        "ngs"
      ],
      "id": 736
    },
    {
      "name": "aiida-gaussian",
      "one_line_profile": "AiiDA plugin for Gaussian quantum chemistry software",
      "detailed_description": "A plugin to interface the Gaussian quantum chemistry software with the AiiDA workflow engine, enabling automated and reproducible computational chemistry simulations.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_connector",
        "quantum_chemistry"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/nanotech-empa/aiida-gaussian",
      "help_website": [
        "https://aiida-gaussian.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "aiida",
        "computational-chemistry",
        "gaussian",
        "workflow"
      ],
      "id": 737
    },
    {
      "name": "aiidalab-empa-surfaces",
      "one_line_profile": "AiiDAlab app for on-surface chemistry simulations",
      "detailed_description": "An AiiDAlab application designed for running and analyzing on-surface chemistry simulations, developed at Empa.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "surface_chemistry",
        "simulation_workflow"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/nanotech-empa/aiidalab-empa-surfaces",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "aiidalab",
        "surface-science",
        "chemistry",
        "simulation"
      ],
      "id": 738
    },
    {
      "name": "GeneLab Data Processing",
      "one_line_profile": "NASA GeneLab bioinformatics pipelines for spaceflight omics data",
      "detailed_description": "Standardized bioinformatics pipelines used by NASA GeneLab for processing and analyzing omics data from spaceflight and space-relevant experiments.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "omics_processing",
        "bioinformatics_pipeline"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/nasa/GeneLab_Data_Processing",
      "help_website": [
        "https://genelab.nasa.gov/"
      ],
      "license": null,
      "tags": [
        "nasa",
        "genelab",
        "omics",
        "space-biology"
      ],
      "id": 739
    },
    {
      "name": "MayomicsVC",
      "one_line_profile": "Variant Calling Pipeline in Cromwell/WDL",
      "detailed_description": "A variant calling pipeline implemented in WDL (Workflow Description Language) for execution with Cromwell, designed for genomic data analysis.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "variant_calling",
        "genomics_workflow"
      ],
      "application_level": "workflow",
      "primary_language": "Shell",
      "repo_url": "https://github.com/ncsa/MayomicsVC",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "wdl",
        "cromwell",
        "variant-calling",
        "genomics"
      ],
      "id": 740
    },
    {
      "name": "paperetl",
      "one_line_profile": "ETL processes for medical and scientific papers",
      "detailed_description": "A library for processing medical and scientific papers, converting them into structured data for downstream analysis, indexing, and machine learning tasks.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "scientific_literature_mining",
        "etl"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/neuml/paperetl",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "etl",
        "nlp",
        "scientific-literature",
        "cord-19"
      ],
      "id": 741
    },
    {
      "name": "Nextflow",
      "one_line_profile": "Data-driven computational pipeline framework",
      "detailed_description": "A workflow management system and DSL that enables scalable and reproducible scientific workflows using software containers. It is widely used in bioinformatics and other data-intensive scientific fields.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_orchestration",
        "pipeline_management"
      ],
      "application_level": "platform",
      "primary_language": "Groovy",
      "repo_url": "https://github.com/nextflow-io/nextflow",
      "help_website": [
        "https://www.nextflow.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "workflow-engine",
        "bioinformatics",
        "reproducibility",
        "pipeline"
      ],
      "id": 742
    },
    {
      "name": "rnaseq-nf",
      "one_line_profile": "Proof of concept RNA-seq pipeline using Nextflow",
      "detailed_description": "A basic RNA-seq analysis pipeline implemented in Nextflow, often used as a reference implementation or educational example for building scientific workflows.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "rnaseq_analysis",
        "workflow_example"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/nextflow-io/rnaseq-nf",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rnaseq",
        "nextflow",
        "bioinformatics",
        "pipeline"
      ],
      "id": 743
    },
    {
      "name": "nf-core/ampliseq",
      "one_line_profile": "Amplicon sequencing analysis workflow",
      "detailed_description": "A bioinformatics pipeline for amplicon sequencing analysis, utilizing DADA2 and QIIME2 for processing and analyzing microbial community data.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "amplicon_sequencing",
        "microbiome_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/nf-core/ampliseq",
      "help_website": [
        "https://nf-co.re/ampliseq"
      ],
      "license": "MIT",
      "tags": [
        "nf-core",
        "microbiome",
        "16s",
        "bioinformatics"
      ],
      "id": 744
    },
    {
      "name": "nf-core/atacseq",
      "one_line_profile": "ATAC-seq peak-calling and QC analysis pipeline",
      "detailed_description": "A reproducible analysis pipeline for ATAC-seq data, including quality control, alignment, and peak calling steps.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "atac_seq",
        "peak_calling",
        "epigenetics"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/nf-core/atacseq",
      "help_website": [
        "https://nf-co.re/atacseq"
      ],
      "license": "MIT",
      "tags": [
        "nf-core",
        "atac-seq",
        "genomics",
        "pipeline"
      ],
      "id": 745
    },
    {
      "name": "nf-core/bacass",
      "one_line_profile": "Bacterial assembly and annotation pipeline",
      "detailed_description": "A pipeline for the assembly and annotation of bacterial genomes from NGS data.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "genome_assembly",
        "genome_annotation",
        "bacterial_genomics"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/nf-core/bacass",
      "help_website": [
        "https://nf-co.re/bacass"
      ],
      "license": "MIT",
      "tags": [
        "nf-core",
        "bacteria",
        "assembly",
        "annotation"
      ],
      "id": 746
    },
    {
      "name": "nf-core/chipseq",
      "one_line_profile": "ChIP-seq peak-calling and differential analysis pipeline",
      "detailed_description": "A comprehensive pipeline for ChIP-seq data analysis, covering QC, alignment, peak calling, and differential binding analysis.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "chip_seq",
        "peak_calling",
        "epigenetics"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/nf-core/chipseq",
      "help_website": [
        "https://nf-co.re/chipseq"
      ],
      "license": "MIT",
      "tags": [
        "nf-core",
        "chip-seq",
        "genomics",
        "pipeline"
      ],
      "id": 747
    },
    {
      "name": "nf-core/cutandrun",
      "one_line_profile": "Analysis pipeline for CUT&RUN and CUT&TAG experiments",
      "detailed_description": "A pipeline designed for the analysis of CUT&RUN and CUT&TAG chromatin profiling data, including QC, spike-in normalization, and peak calling.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "cut_and_run",
        "epigenetics",
        "chromatin_profiling"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/nf-core/cutandrun",
      "help_website": [
        "https://nf-co.re/cutandrun"
      ],
      "license": "MIT",
      "tags": [
        "nf-core",
        "cut-and-run",
        "epigenetics",
        "pipeline"
      ],
      "id": 748
    },
    {
      "name": "nf-core/differentialabundance",
      "one_line_profile": "Differential abundance analysis pipeline",
      "detailed_description": "A pipeline for performing differential abundance analysis on feature matrices (e.g., from RNA-seq), generating plots and reports.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "differential_expression",
        "statistical_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/nf-core/differentialabundance",
      "help_website": [
        "https://nf-co.re/differentialabundance"
      ],
      "license": "MIT",
      "tags": [
        "nf-core",
        "rnaseq",
        "statistics",
        "visualization"
      ],
      "id": 749
    },
    {
      "name": "nf-core/eager",
      "one_line_profile": "Ancient DNA analysis pipeline",
      "detailed_description": "A fully reproducible pipeline for the analysis of ancient DNA (aDNA) data, handling specific characteristics like damage patterns and contamination.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "ancient_dna",
        "paleogenomics",
        "sequencing_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/nf-core/eager",
      "help_website": [
        "https://nf-co.re/eager"
      ],
      "license": "MIT",
      "tags": [
        "nf-core",
        "adna",
        "paleogenomics",
        "pipeline"
      ],
      "id": 750
    },
    {
      "name": "nf-core/fetchngs",
      "one_line_profile": "Pipeline to fetch metadata and raw FastQ files",
      "detailed_description": "A utility pipeline to retrieve sequencing metadata and raw FastQ files from public databases like SRA, ENA, and DDBJ for downstream analysis.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "data_retrieval",
        "metadata_fetching"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/nf-core/fetchngs",
      "help_website": [
        "https://nf-co.re/fetchngs"
      ],
      "license": "MIT",
      "tags": [
        "nf-core",
        "sra",
        "ena",
        "data-download"
      ],
      "id": 751
    },
    {
      "name": "nf-core/funcscan",
      "one_line_profile": "Screening for functional and natural product gene sequences",
      "detailed_description": "A pipeline for screening (meta-)genomes for functional genes and biosynthetic gene clusters (BGCs), aiding in natural product discovery.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "functional_screening",
        "biosynthetic_gene_clusters",
        "metagenomics"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/nf-core/funcscan",
      "help_website": [
        "https://nf-co.re/funcscan"
      ],
      "license": "MIT",
      "tags": [
        "nf-core",
        "metagenomics",
        "natural-products",
        "pipeline"
      ],
      "id": 752
    },
    {
      "name": "nf-core/hic",
      "one_line_profile": "Analysis of Chromosome Conformation Capture data (Hi-C)",
      "detailed_description": "A pipeline for processing Hi-C data, including mapping, filtering, matrix generation, and quality control to study 3D genome organization.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "hic_analysis",
        "3d_genome",
        "chromatin_structure"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/nf-core/hic",
      "help_website": [
        "https://nf-co.re/hic"
      ],
      "license": "MIT",
      "tags": [
        "nf-core",
        "hi-c",
        "genomics",
        "pipeline"
      ],
      "id": 753
    },
    {
      "name": "nf-core/mag",
      "one_line_profile": "Assembly and binning of metagenomes",
      "detailed_description": "A pipeline for the assembly, binning, and taxonomic classification of metagenomic data to reconstruct Metagenome-Assembled Genomes (MAGs).",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "metagenomics",
        "genome_assembly",
        "binning"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/nf-core/mag",
      "help_website": [
        "https://nf-co.re/mag"
      ],
      "license": "MIT",
      "tags": [
        "nf-core",
        "metagenomics",
        "mag",
        "assembly"
      ],
      "id": 754
    },
    {
      "name": "nf-core/methylseq",
      "one_line_profile": "Methylation (Bisulfite-Sequencing) analysis pipeline",
      "detailed_description": "A pipeline for the analysis of Bisulfite-Sequencing data to study DNA methylation, supporting various alignment and methylation calling tools.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "methylation_analysis",
        "epigenetics",
        "bisulfite_sequencing"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/nf-core/methylseq",
      "help_website": [
        "https://nf-co.re/methylseq"
      ],
      "license": "MIT",
      "tags": [
        "nf-core",
        "methylation",
        "epigenetics",
        "pipeline"
      ],
      "id": 755
    },
    {
      "name": "nf-core/nanoseq",
      "one_line_profile": "Nanopore demultiplexing, QC and alignment pipeline",
      "detailed_description": "A pipeline for analyzing Oxford Nanopore sequencing data, including demultiplexing, quality control, and alignment steps.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "nanopore_sequencing",
        "long_read_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/nf-core/nanoseq",
      "help_website": [
        "https://nf-co.re/nanoseq"
      ],
      "license": "MIT",
      "tags": [
        "nf-core",
        "nanopore",
        "long-read",
        "pipeline"
      ],
      "id": 756
    },
    {
      "name": "nf-core/oncoanalyser",
      "one_line_profile": "Comprehensive cancer DNA/RNA analysis and reporting pipeline",
      "detailed_description": "A bioinformatics pipeline for the analysis of cancer DNA and RNA sequencing data, providing comprehensive reporting for oncology research.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "cancer_genomics",
        "variant_calling",
        "rnaseq_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/nf-core/oncoanalyser",
      "help_website": [
        "https://nf-co.re/oncoanalyser"
      ],
      "license": "MIT",
      "tags": [
        "cancer",
        "genomics",
        "pipeline",
        "nextflow"
      ],
      "id": 757
    },
    {
      "name": "nf-core/pangenome",
      "one_line_profile": "Pipeline to render sequences into a pangenome graph",
      "detailed_description": "A bioinformatics pipeline that constructs pangenome graphs from a collection of genomic sequences, facilitating comparative genomics analysis.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "pangenome_construction",
        "graph_generation"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/nf-core/pangenome",
      "help_website": [
        "https://nf-co.re/pangenome"
      ],
      "license": "MIT",
      "tags": [
        "pangenome",
        "graph",
        "genomics",
        "nextflow"
      ],
      "id": 758
    },
    {
      "name": "nf-core/proteinfold",
      "one_line_profile": "Protein 3D structure prediction pipeline",
      "detailed_description": "A pipeline for predicting protein 3D structures from amino acid sequences, integrating tools like AlphaFold2 and ColabFold for structural biology research.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "structure_prediction",
        "protein_folding"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/nf-core/proteinfold",
      "help_website": [
        "https://nf-co.re/proteinfold"
      ],
      "license": "MIT",
      "tags": [
        "protein-structure",
        "alphafold",
        "colabfold",
        "nextflow"
      ],
      "id": 759
    },
    {
      "name": "nf-core/raredisease",
      "one_line_profile": "Variant calling pipeline for rare disease analysis",
      "detailed_description": "A pipeline designed to call and score variants from Whole Genome Sequencing (WGS) or Whole Exome Sequencing (WES) data specifically for rare disease patients.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "variant_calling",
        "rare_disease_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/nf-core/raredisease",
      "help_website": [
        "https://nf-co.re/raredisease"
      ],
      "license": "MIT",
      "tags": [
        "rare-disease",
        "variant-calling",
        "wgs",
        "wes"
      ],
      "id": 760
    },
    {
      "name": "nf-core/rnafusion",
      "one_line_profile": "RNA-seq analysis pipeline for gene fusion detection",
      "detailed_description": "A specialized RNA-seq analysis pipeline focused on the detection of gene fusions, utilizing multiple fusion callers for robust identification.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "gene_fusion_detection",
        "rnaseq_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/nf-core/rnafusion",
      "help_website": [
        "https://nf-co.re/rnafusion"
      ],
      "license": "MIT",
      "tags": [
        "rna-seq",
        "gene-fusion",
        "genomics",
        "nextflow"
      ],
      "id": 761
    },
    {
      "name": "nf-core/sarek",
      "one_line_profile": "Variant calling pipeline for germline and somatic variants",
      "detailed_description": "A comprehensive pipeline for detecting germline or somatic variants from Whole Genome Sequencing (WGS) or targeted sequencing data, including preprocessing and annotation.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "variant_calling",
        "germline_analysis",
        "somatic_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/nf-core/sarek",
      "help_website": [
        "https://nf-co.re/sarek"
      ],
      "license": "MIT",
      "tags": [
        "variant-calling",
        "wgs",
        "cancer",
        "genomics"
      ],
      "id": 762
    },
    {
      "name": "nf-core/scdownstream",
      "one_line_profile": "Single-cell transcriptomics downstream analysis pipeline",
      "detailed_description": "A pipeline for the downstream analysis of single-cell transcriptomics data, handling quality control, data integration, and visualization.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "single_cell_analysis",
        "transcriptomics",
        "data_integration"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/nf-core/scdownstream",
      "help_website": [
        "https://nf-co.re/scdownstream"
      ],
      "license": "MIT",
      "tags": [
        "single-cell",
        "rna-seq",
        "downstream-analysis",
        "nextflow"
      ],
      "id": 763
    },
    {
      "name": "nf-core/scrnaseq",
      "one_line_profile": "Single-cell RNA-Seq processing pipeline",
      "detailed_description": "A pipeline for processing single-cell RNA-Seq data from barcode-based protocols (10x, DropSeq, SmartSeq), supporting alignment and empty-droplet detection.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "single_cell_analysis",
        "expression_quantification"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/nf-core/scrnaseq",
      "help_website": [
        "https://nf-co.re/scrnaseq"
      ],
      "license": "MIT",
      "tags": [
        "single-cell",
        "rna-seq",
        "10x-genomics",
        "nextflow"
      ],
      "id": 764
    },
    {
      "name": "nf-core/smrnaseq",
      "one_line_profile": "Small-RNA sequencing analysis pipeline",
      "detailed_description": "A bioinformatics pipeline dedicated to the analysis of small-RNA sequencing data, including miRNA identification and quantification.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "small_rna_analysis",
        "mirna_profiling"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/nf-core/smrnaseq",
      "help_website": [
        "https://nf-co.re/smrnaseq"
      ],
      "license": "MIT",
      "tags": [
        "small-rna",
        "mirna",
        "sequencing",
        "nextflow"
      ],
      "id": 765
    },
    {
      "name": "nf-core/spatialvi",
      "one_line_profile": "Spatial transcriptomics processing pipeline",
      "detailed_description": "A pipeline for processing spatially-resolved gene counts alongside image data, specifically designed for 10x Genomics Visium transcriptomics.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "spatial_transcriptomics",
        "image_processing"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/nf-core/spatialvi",
      "help_website": [
        "https://nf-co.re/spatialvi"
      ],
      "license": "MIT",
      "tags": [
        "spatial-transcriptomics",
        "visium",
        "10x",
        "nextflow"
      ],
      "id": 766
    },
    {
      "name": "nf-core/taxprofiler",
      "one_line_profile": "Multi-taxonomic profiling pipeline for metagenomic data",
      "detailed_description": "A highly parallelized pipeline for taxonomic profiling of shotgun short- and long-read metagenomic data using multiple profiling tools.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "metagenomic_profiling",
        "taxonomic_classification"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/nf-core/taxprofiler",
      "help_website": [
        "https://nf-co.re/taxprofiler"
      ],
      "license": "MIT",
      "tags": [
        "metagenomics",
        "taxonomy",
        "profiling",
        "nextflow"
      ],
      "id": 767
    },
    {
      "name": "nf-core/viralrecon",
      "one_line_profile": "Viral assembly and variant calling pipeline",
      "detailed_description": "A bioinformatics pipeline for the assembly of viral genomes and intrahost/low-frequency variant calling, useful for viral surveillance.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "viral_genomics",
        "assembly",
        "variant_calling"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/nf-core/viralrecon",
      "help_website": [
        "https://nf-co.re/viralrecon"
      ],
      "license": "MIT",
      "tags": [
        "viral-genomics",
        "covid-19",
        "assembly",
        "nextflow"
      ],
      "id": 768
    },
    {
      "name": "b2luigi",
      "one_line_profile": "Workflow scheduling library for Belle II analysis (basf2)",
      "detailed_description": "A Python library extending Luigi to handle task scheduling and batch job execution specifically for the Belle II Analysis Software Framework (basf2) in high-energy physics.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_scheduling",
        "hep_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/nils-braun/b2luigi",
      "help_website": [
        "https://b2luigi.readthedocs.io/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "high-energy-physics",
        "workflow",
        "luigi",
        "basf2"
      ],
      "id": 769
    },
    {
      "name": "distiller-nf",
      "one_line_profile": "Modular Hi-C mapping pipeline",
      "detailed_description": "A Nextflow-based data processing pipeline for mapping and processing Hi-C (chromosome conformation capture) data.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "hic_mapping",
        "genome_structure_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "Groovy",
      "repo_url": "https://github.com/open2c/distiller-nf",
      "help_website": [
        "https://github.com/open2c/distiller-nf"
      ],
      "license": "MIT",
      "tags": [
        "hi-c",
        "genomics",
        "mapping",
        "nextflow"
      ],
      "id": 770
    },
    {
      "name": "OpenMOLE",
      "one_line_profile": "Workflow engine for exploration of simulation models",
      "detailed_description": "A platform designed for the exploration, calibration, and sensitivity analysis of scientific simulation models using high-throughput computing.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "simulation_exploration",
        "model_calibration",
        "sensitivity_analysis"
      ],
      "application_level": "platform",
      "primary_language": "Scala",
      "repo_url": "https://github.com/openmole/openmole",
      "help_website": [
        "https://openmole.org/"
      ],
      "license": "AGPL-3.0",
      "tags": [
        "simulation",
        "workflow",
        "hpc",
        "model-exploration"
      ],
      "id": 771
    },
    {
      "name": "neoantigen-vaccine-pipeline",
      "one_line_profile": "Pipeline for patient-specific cancer neoantigen vaccine selection",
      "detailed_description": "A bioinformatics pipeline that processes sequencing data to identify and select patient-specific cancer neoantigens for vaccine development.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "neoantigen_prediction",
        "vaccine_design",
        "immunotherapy"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/openvax/neoantigen-vaccine-pipeline",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "cancer-immunotherapy",
        "neoantigen",
        "bioinformatics",
        "pipeline"
      ],
      "id": 772
    },
    {
      "name": "ASAP",
      "one_line_profile": "Automated bacterial genome assembly and analysis pipeline",
      "detailed_description": "A scalable pipeline for the assembly, annotation, and analysis of bacterial genomes, integrating multiple bioinformatics tools.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "genome_assembly",
        "genome_annotation",
        "bacterial_genomics"
      ],
      "application_level": "workflow",
      "primary_language": "Groovy",
      "repo_url": "https://github.com/oschwengers/asap",
      "help_website": [
        "https://github.com/oschwengers/asap"
      ],
      "license": "GPL-3.0",
      "tags": [
        "bacteria",
        "assembly",
        "annotation",
        "pipeline"
      ],
      "id": 773
    },
    {
      "name": "widget-bandsplot",
      "one_line_profile": "Jupyter widget for plotting electronic band structures",
      "detailed_description": "A Jupyter widget designed for the visualization of electronic band structures and density of states (DOS) in materials science and physics.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "electronic_structure_visualization",
        "band_structure_analysis"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/osscar-org/widget-bandsplot",
      "help_website": [
        "https://osscar-org.github.io/widget-bandsplot/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "materials-science",
        "physics",
        "visualization",
        "jupyter-widget"
      ],
      "id": 774
    },
    {
      "name": "SciLuigi",
      "one_line_profile": "Helper library for writing scientific workflows in Luigi",
      "detailed_description": "A lightweight wrapper around Spotify's Luigi workflow library, designed to make writing scientific workflows more fluent, flexible, and modular. It addresses specific needs in bioinformatics and computational science pipelines.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_management",
        "pipeline_orchestration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pharmbio/sciluigi",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "luigi",
        "workflow",
        "bioinformatics",
        "pipeline"
      ],
      "id": 775
    },
    {
      "name": "Python Workflow Definition",
      "one_line_profile": "Interoperability layer for materials science workflows",
      "detailed_description": "A project defining common python interfaces for workflows in materials science, enabling interoperability between different workflow engines like AiiDA, Jobflow, and PyIron.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_interoperability",
        "materials_informatics"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/pythonworkflow/python-workflow-definition",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "materials-science",
        "workflow",
        "interoperability",
        "aiida",
        "pyiron"
      ],
      "id": 776
    },
    {
      "name": "What the Phage",
      "one_line_profile": "Phage identification and annotation pipeline",
      "detailed_description": "A Nextflow-based bioinformatics pipeline for the identification and annotation of bacteriophage sequences from metagenomic data. It integrates multiple phage detection tools into a reproducible workflow.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "phage_identification",
        "metagenomics",
        "pipeline"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/replikation/What_the_Phage",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "bioinformatics",
        "nextflow",
        "phage",
        "metagenomics"
      ],
      "id": 777
    },
    {
      "name": "Aviary",
      "one_line_profile": "Metagenomics hybrid assembly and binning pipeline",
      "detailed_description": "A comprehensive pipeline for hybrid assembly (using long and short reads) and recovery of Metagenome-Assembled Genomes (MAGs). It automates the workflow from raw reads to high-quality bins.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "genome_assembly",
        "metagenomics",
        "pipeline"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/rhysnewell/aviary",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "metagenomics",
        "assembly",
        "mag-recovery",
        "bioinformatics"
      ],
      "id": 778
    },
    {
      "name": "law",
      "one_line_profile": "Luigi Analysis Workflow for High Energy Physics",
      "detailed_description": "A python package to build complex and large-scale task workflows. It extends Luigi with features specifically useful for High Energy Physics (HEP) analyses, such as remote job submission (HTCondor, Slurm) and environment sandboxing.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_management",
        "physics_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/riga/law",
      "help_website": [
        "https://law.readthedocs.io"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "high-energy-physics",
        "luigi",
        "workflow",
        "batch-processing"
      ],
      "id": 779
    },
    {
      "name": "NetworkCommons",
      "one_line_profile": "Platform for inferring context-specific protein interaction networks",
      "detailed_description": "A community-driven platform designed to simplify access to tools and resources for inferring context-specific protein interaction networks by integrating context-agnostic prior knowledge with omics data.",
      "domains": [
        "D1",
        "D4"
      ],
      "subtask_category": [
        "network_inference",
        "omics_integration"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/saezlab/networkcommons",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "protein-interaction",
        "network-inference",
        "omics",
        "bioinformatics"
      ],
      "id": 780
    },
    {
      "name": "scDataset",
      "one_line_profile": "Scalable data loading library for large-scale single-cell omics",
      "detailed_description": "A Python library designed for efficient and scalable data loading of large-scale single-cell omics data, facilitating deep learning applications in bioinformatics.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "data_loading",
        "single_cell_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/scDataset/scDataset",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "single-cell",
        "omics",
        "data-loader",
        "deep-learning"
      ],
      "id": 781
    },
    {
      "name": "SciPipe",
      "one_line_profile": "Workflow library for building complex scientific pipelines in Go",
      "detailed_description": "A robust and flexible workflow library written in Go, designed for building complex, resource-efficient scientific pipelines with support for provenance tracking and command-line execution.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_orchestration",
        "pipeline_management"
      ],
      "application_level": "workflow",
      "primary_language": "Go",
      "repo_url": "https://github.com/scipipe/scipipe",
      "help_website": [
        "http://scipipe.org"
      ],
      "license": "MIT",
      "tags": [
        "workflow",
        "pipeline",
        "go",
        "reproducibility",
        "bioinformatics"
      ],
      "id": 782
    },
    {
      "name": "PyDESeq2",
      "one_line_profile": "Python implementation of the DESeq2 pipeline for bulk RNA-seq analysis",
      "detailed_description": "A Python library implementing the DESeq2 statistical method for differential expression analysis of bulk RNA-seq data, enabling integration with Python-based data science workflows.",
      "domains": [
        "D4",
        "D1-04"
      ],
      "subtask_category": [
        "differential_expression_analysis",
        "rna_seq_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/scverse/PyDESeq2",
      "help_website": [
        "https://pydeseq2.readthedocs.io"
      ],
      "license": "MIT",
      "tags": [
        "rna-seq",
        "deseq2",
        "differential-expression",
        "bioinformatics"
      ],
      "id": 783
    },
    {
      "name": "wdlRunR",
      "one_line_profile": "R interface for running WDL workflows for genomic data science",
      "detailed_description": "A tool enabling the execution and management of WDL (Workflow Description Language) workflows directly from R, facilitating reproducible genomic data science backed by cloud resources.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_execution",
        "genomics_pipeline"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/seandavi/wdlRunR",
      "help_website": [],
      "license": null,
      "tags": [
        "wdl",
        "r",
        "genomics",
        "workflow",
        "reproducibility"
      ],
      "id": 784
    },
    {
      "name": "Nextflow Tower",
      "one_line_profile": "Monitoring and management platform for Nextflow pipelines",
      "detailed_description": "A centralized platform for monitoring, logging, and managing Nextflow data analysis pipelines, providing visibility and control over distributed scientific workflows.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_management",
        "pipeline_monitoring"
      ],
      "application_level": "platform",
      "primary_language": "Groovy",
      "repo_url": "https://github.com/seqeralabs/nf-tower",
      "help_website": [
        "https://tower.nf"
      ],
      "license": "MPL-2.0",
      "tags": [
        "nextflow",
        "workflow-management",
        "bioinformatics",
        "cloud-computing"
      ],
      "id": 785
    },
    {
      "name": "Sequana",
      "one_line_profile": "Collection of Snakemake pipelines for NGS analysis",
      "detailed_description": "A set of Snakemake pipelines and Python tools dedicated to the analysis of Next Generation Sequencing (NGS) data, covering various tasks such as quality control, variant calling, and coverage analysis.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "ngs_pipeline",
        "quality_control"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/sequana/sequana",
      "help_website": [
        "http://sequana.readthedocs.io"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "snakemake",
        "ngs",
        "pipeline",
        "bioinformatics"
      ],
      "id": 786
    },
    {
      "name": "hecatomb",
      "one_line_profile": "Virome analysis pipeline for Illumina sequence data",
      "detailed_description": "A bioinformatics pipeline designed for the analysis of viral metagenomes (viromes) from Illumina sequencing data, handling processing, assembly, and taxonomic assignment.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "virome_analysis",
        "metagenomics"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/shandley/hecatomb",
      "help_website": [
        "https://hecatomb.readthedocs.io"
      ],
      "license": "MIT",
      "tags": [
        "virome",
        "metagenomics",
        "snakemake",
        "bioinformatics"
      ],
      "id": 787
    },
    {
      "name": "AiiDA-Siesta",
      "one_line_profile": "AiiDA plugin and workflows for the SIESTA DFT code",
      "detailed_description": "A plugin for the AiiDA workflow engine that interfaces with the SIESTA Density Functional Theory (DFT) code, enabling automated and reproducible materials science simulations.",
      "domains": [
        "D1",
        "D1-04",
        "D4"
      ],
      "subtask_category": [
        "material_simulation",
        "workflow_automation"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/siesta-project/aiida_siesta_plugin",
      "help_website": [
        "https://aiida-siesta-plugin.readthedocs.io"
      ],
      "license": null,
      "tags": [
        "aiida",
        "siesta",
        "dft",
        "materials-science",
        "workflow"
      ],
      "id": 788
    },
    {
      "name": "CeleScope",
      "one_line_profile": "Single-cell analysis pipelines for Singleron data",
      "detailed_description": "A suite of bioinformatics pipelines for processing single-cell sequencing data generated by Singleron platforms, covering various assays like RNA-seq and VDJ.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "single_cell_analysis",
        "pipeline_processing"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/singleron-RD/CeleScope",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "single-cell",
        "pipeline",
        "bioinformatics",
        "rna-seq"
      ],
      "id": 789
    },
    {
      "name": "snakefiles",
      "one_line_profile": "Collection of Snakemake workflows for RNA-seq analysis",
      "detailed_description": "A repository of reusable Snakemake workflow definitions (Snakefiles) for common RNA-seq data analysis tasks, including alignment with STAR and quantification with Kallisto.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "rna_seq_workflow",
        "alignment"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/slowkow/snakefiles",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "snakemake",
        "rna-seq",
        "workflow",
        "bioinformatics"
      ],
      "id": 790
    },
    {
      "name": "cookiecutter-snakemake-workflow",
      "one_line_profile": "Standard template for creating Snakemake workflows",
      "detailed_description": "A Cookiecutter template designed to standardize the structure of Snakemake workflows, promoting best practices and reproducibility in scientific data analysis pipelines.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_development",
        "reproducibility"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/snakemake-workflows/cookiecutter-snakemake-workflow",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "snakemake",
        "cookiecutter",
        "template",
        "workflow"
      ],
      "id": 791
    },
    {
      "name": "dna-seq-gatk-variant-calling",
      "one_line_profile": "Snakemake pipeline implementing GATK best-practices for DNA-seq",
      "detailed_description": "A standardized Snakemake workflow that implements the GATK best-practices for DNA-seq variant calling, ensuring reproducible and scalable genomic analysis.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "variant_calling",
        "genomics_pipeline"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/snakemake-workflows/dna-seq-gatk-variant-calling",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gatk",
        "snakemake",
        "variant-calling",
        "dna-seq",
        "bioinformatics"
      ],
      "id": 792
    },
    {
      "name": "dna-seq-varlociraptor",
      "one_line_profile": "Snakemake workflow for small and structural variant calling using Varlociraptor",
      "detailed_description": "A comprehensive Snakemake workflow designed for calling small and structural variants across various scenarios (tumor/normal, germline, pedigree, etc.) using the unified statistical model of Varlociraptor.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "variant_calling",
        "genomic_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/snakemake-workflows/dna-seq-varlociraptor",
      "help_website": [
        "https://snakemake.github.io/snakemake-workflow-catalog/"
      ],
      "license": "MIT",
      "tags": [
        "snakemake",
        "bioinformatics",
        "variant-calling",
        "dna-seq"
      ],
      "id": 793
    },
    {
      "name": "rna-seq-kallisto-sleuth",
      "one_line_profile": "Snakemake workflow for RNA-seq differential expression analysis",
      "detailed_description": "A Snakemake workflow that performs differential expression analysis of RNA-seq data utilizing Kallisto for pseudo-alignment and Sleuth for statistical analysis.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "differential_expression_analysis",
        "rna_seq"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/snakemake-workflows/rna-seq-kallisto-sleuth",
      "help_website": [
        "https://snakemake.github.io/snakemake-workflow-catalog/"
      ],
      "license": "MIT",
      "tags": [
        "snakemake",
        "rna-seq",
        "kallisto",
        "sleuth"
      ],
      "id": 794
    },
    {
      "name": "rna-seq-star-deseq2",
      "one_line_profile": "RNA-seq analysis workflow using STAR and DESeq2",
      "detailed_description": "A standard Snakemake workflow for RNA-sequencing analysis that employs STAR for alignment and DESeq2 for differential expression analysis.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "rna_seq",
        "alignment",
        "differential_expression"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/snakemake-workflows/rna-seq-star-deseq2",
      "help_website": [
        "https://snakemake.github.io/snakemake-workflow-catalog/"
      ],
      "license": "MIT",
      "tags": [
        "snakemake",
        "star",
        "deseq2",
        "bioinformatics"
      ],
      "id": 795
    },
    {
      "name": "Snakemake",
      "one_line_profile": "Workflow management system for reproducible data analysis",
      "detailed_description": "A workflow management system that aims to reduce the complexity of creating facsimiles of data analysis to create reproducible and scalable data analyses. It is widely used in bioinformatics and scientific computing.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_orchestration",
        "reproducibility"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/snakemake/snakemake",
      "help_website": [
        "https://snakemake.readthedocs.io"
      ],
      "license": "MIT",
      "tags": [
        "workflow-engine",
        "pipeline",
        "reproducibility",
        "bioinformatics"
      ],
      "id": 796
    },
    {
      "name": "Bpipe",
      "one_line_profile": "Platform for running and managing bioinformatics pipelines",
      "detailed_description": "Bpipe is a tool for running and managing bioinformatics pipelines. It provides a domain specific language for defining pipelines and handles task execution, parallelism, and reporting.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_orchestration",
        "bioinformatics_pipeline"
      ],
      "application_level": "platform",
      "primary_language": "Groovy",
      "repo_url": "https://github.com/ssadedin/bpipe",
      "help_website": [
        "http://docs.bpipe.org/"
      ],
      "license": "MIT",
      "tags": [
        "bioinformatics",
        "pipeline",
        "workflow"
      ],
      "id": 797
    },
    {
      "name": "aiida-mlip",
      "one_line_profile": "AiiDA plugin for machine learning interatomic potentials",
      "detailed_description": "A plugin for the AiiDA workflow platform that interfaces with Machine Learning Interatomic Potentials (MLIP), enabling automated calculations and simulations in materials science.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "molecular_dynamics",
        "materials_modeling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/stfc/aiida-mlip",
      "help_website": [
        "https://aiida-mlip.readthedocs.io/"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "aiida",
        "materials-science",
        "mlip",
        "interatomic-potentials"
      ],
      "id": 798
    },
    {
      "name": "St. Jude Cloud Workflows",
      "one_line_profile": "Bioinformatics workflows for St. Jude Cloud",
      "detailed_description": "A collection of bioinformatics workflows written in WDL, developed for and used on the St. Jude Cloud project for genomic analysis.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "genomic_analysis",
        "bioinformatics_pipeline"
      ],
      "application_level": "workflow",
      "primary_language": "WDL",
      "repo_url": "https://github.com/stjudecloud/workflows",
      "help_website": [
        "https://stjude.cloud"
      ],
      "license": "MIT",
      "tags": [
        "wdl",
        "genomics",
        "bioinformatics",
        "workflow"
      ],
      "id": 799
    },
    {
      "name": "Sunbeam",
      "one_line_profile": "Extensible metagenomics pipeline",
      "detailed_description": "A robust and extensible pipeline for metagenomics analysis, handling quality control, assembly, and classification of sequencing data.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "metagenomics_analysis",
        "sequence_processing"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/sunbeam-labs/sunbeam",
      "help_website": [
        "https://sunbeam.readthedocs.io/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "metagenomics",
        "pipeline",
        "bioinformatics",
        "snakemake"
      ],
      "id": 800
    },
    {
      "name": "bio-pipeline",
      "one_line_profile": "Collection of bioinformatics analysis pipelines",
      "detailed_description": "A collection of lightweight bioinformatics analysis pipelines and scripts for specific tasks such as assembly, mapping, and annotation, developed by Tang Haibao.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "bioinformatics_analysis",
        "sequence_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/tanghaibao/bio-pipeline",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bioinformatics",
        "pipeline",
        "scripts"
      ],
      "id": 801
    },
    {
      "name": "scib-pipeline",
      "one_line_profile": "Snakemake pipeline for benchmarking single-cell data integration methods",
      "detailed_description": "A reproducible pipeline built with Snakemake to benchmark various single-cell data integration methods using the scIB (Single-Cell Integration Benchmarking) package. It automates the execution of integration tasks and metric evaluation.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "benchmarking",
        "data_integration",
        "single_cell_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/theislab/scib-pipeline",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "single-cell",
        "benchmarking",
        "snakemake",
        "bioinformatics"
      ],
      "id": 802
    },
    {
      "name": "UGENE",
      "one_line_profile": "Integrated bioinformatics software for sequence analysis and visualization",
      "detailed_description": "A free open-source cross-platform bioinformatics software suite. It integrates dozens of popular bioinformatics tools and algorithms for sequence alignment, assembly, and analysis into a single graphical interface, facilitating complex workflows.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "sequence_analysis",
        "visualization",
        "alignment",
        "workflow_management"
      ],
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/ugeneunipro/ugene",
      "help_website": [
        "http://ugene.net"
      ],
      "license": "GPL-2.0",
      "tags": [
        "bioinformatics",
        "gui",
        "sequence-analysis",
        "ngs"
      ],
      "id": 803
    },
    {
      "name": "pyrpipe",
      "one_line_profile": "Python framework for creating reproducible bioinformatics pipelines",
      "detailed_description": "A Python package that enables the creation of reproducible bioinformatics pipelines by providing a wrapper around Unix tools and commands. It facilitates the integration of various bioinformatics tools into Python scripts with logging and report generation.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_management",
        "pipeline_creation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/urmi-21/pyrpipe",
      "help_website": [
        "https://pyrpipe.readthedocs.io"
      ],
      "license": "MIT",
      "tags": [
        "bioinformatics",
        "pipeline",
        "reproducibility",
        "python"
      ],
      "id": 804
    },
    {
      "name": "seq2science",
      "one_line_profile": "Automated preprocessing and analysis workflows for NGS data",
      "detailed_description": "A collection of automated and customizable workflows for preprocessing Next-Generation Sequencing (NGS) data, including ATAC-seq, ChIP-seq, and RNA-seq. It handles downloading, alignment, and quality control.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "preprocessing",
        "alignment",
        "quality_control",
        "rna_seq"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/vanheeringen-lab/seq2science",
      "help_website": [
        "https://vanheeringen-lab.github.io/seq2science/"
      ],
      "license": "MIT",
      "tags": [
        "ngs",
        "rna-seq",
        "chip-seq",
        "atac-seq",
        "snakemake"
      ],
      "id": 805
    },
    {
      "name": "vsn-pipelines",
      "one_line_profile": "Collection of Nextflow pipelines for single-cell data analysis",
      "detailed_description": "A repository of Nextflow DSL2 pipelines specifically designed for processing and analyzing single-cell sequencing data, maintained by the VIB Single Cell Core.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "single_cell_analysis",
        "pipeline",
        "workflow_management"
      ],
      "application_level": "workflow",
      "primary_language": "Nextflow",
      "repo_url": "https://github.com/vib-singlecell-nf/vsn-pipelines",
      "help_website": [
        "https://vib-singlecell-nf.github.io/vsn-pipelines"
      ],
      "license": "GPL-3.0",
      "tags": [
        "single-cell",
        "nextflow",
        "bioinformatics",
        "vib"
      ],
      "id": 806
    },
    {
      "name": "UAVS",
      "one_line_profile": "Intelligent UAV path planning and simulation platform",
      "detailed_description": "A comprehensive simulation system for Unmanned Aerial Vehicles (UAVs) focusing on path planning, formation control, and mission simulation in complex environments. It integrates model building, automated control, and route verification.",
      "domains": [
        "Robotics",
        "Simulation"
      ],
      "subtask_category": [
        "path_planning",
        "simulation",
        "control_system"
      ],
      "application_level": "platform",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/wangwei39120157028/UAVS",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "uav",
        "path-planning",
        "simulation",
        "robotics"
      ],
      "id": 807
    },
    {
      "name": "scATAC-pro",
      "one_line_profile": "Comprehensive workflow for single-cell ATAC-seq data analysis",
      "detailed_description": "A tool for processing, analyzing, and visualizing single-cell chromatin accessibility sequencing (scATAC-seq) data. It provides a complete workflow from raw data processing to downstream analysis like clustering and motif enrichment.",
      "domains": [
        "Bioinformatics",
        "Genomics"
      ],
      "subtask_category": [
        "scatac-seq",
        "data_analysis",
        "visualization"
      ],
      "application_level": "workflow",
      "primary_language": "R",
      "repo_url": "https://github.com/wbaopaul/scATAC-pro",
      "help_website": [
        "https://github.com/wbaopaul/scATAC-pro"
      ],
      "license": "MIT",
      "tags": [
        "single-cell",
        "atac-seq",
        "chromatin-accessibility",
        "bioinformatics"
      ],
      "id": 808
    },
    {
      "name": "nano-snakemake",
      "one_line_profile": "Snakemake pipeline for structural variant analysis from Nanopore data",
      "detailed_description": "A bioinformatics pipeline built with Snakemake for analyzing structural variants (SV) using data from Oxford Nanopore sequencing. It automates the workflow for long-read genomic analysis.",
      "domains": [
        "Bioinformatics",
        "Genomics",
        "D1-04"
      ],
      "subtask_category": [
        "structural_variant_calling",
        "pipeline",
        "nanopore_sequencing"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/wdecoster/nano-snakemake",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "snakemake",
        "nanopore",
        "structural-variants",
        "genomics"
      ],
      "id": 809
    },
    {
      "name": "flo",
      "one_line_profile": "Pipeline for lifting over genome annotations within the same species",
      "detailed_description": "A Ruby-based pipeline designed to lift over genome annotations from one assembly to another within the same species, facilitating genomic data migration and comparison.",
      "domains": [
        "Bioinformatics",
        "Genomics",
        "D1-04"
      ],
      "subtask_category": [
        "annotation_liftover",
        "genome_assembly",
        "pipeline"
      ],
      "application_level": "workflow",
      "primary_language": "Ruby",
      "repo_url": "https://github.com/wurmlab/flo",
      "help_website": [],
      "license": null,
      "tags": [
        "genomics",
        "annotation",
        "liftover",
        "pipeline"
      ],
      "id": 810
    },
    {
      "name": "aiida-yambo",
      "one_line_profile": "AiiDA plugin for Yambo many-body perturbation theory code",
      "detailed_description": "A plugin for the AiiDA workflow engine that interfaces with the Yambo code. It enables automated high-throughput calculations for many-body perturbation theory (MBPT) in materials science.",
      "domains": [
        "Materials Science",
        "Physics",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_connector",
        "electronic_structure",
        "mbpt"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/yambo-code/aiida-yambo",
      "help_website": [
        "https://aiida-yambo.readthedocs.io/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "aiida",
        "yambo",
        "materials-science",
        "workflow"
      ],
      "id": 811
    },
    {
      "name": "aiida-castep",
      "one_line_profile": "AiiDA plugin for CASTEP DFT code",
      "detailed_description": "A plugin for the AiiDA workflow engine that provides an interface to CASTEP, a leading code for calculating the properties of materials from first principles. It facilitates automated DFT calculations and provenance tracking.",
      "domains": [
        "Materials Science",
        "Physics",
        "D1-04"
      ],
      "subtask_category": [
        "workflow_connector",
        "dft",
        "electronic_structure"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/zhubonan/aiida-castep",
      "help_website": [
        "https://aiida-castep.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "aiida",
        "castep",
        "dft",
        "materials-science"
      ],
      "id": 812
    },
    {
      "name": "rgbd360",
      "one_line_profile": "Omnidirectional RGB-D sensor data acquisition and SLAM tool",
      "detailed_description": "A tool for image acquisition, localization, and mapping using an omnidirectional RGB-D sensor. It supports data serialization, frame registration, loop closure detection, and PbMap-based hybrid SLAM (Simultaneous Localization and Mapping).",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "data_acquisition",
        "slam",
        "localization"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/EduFdez/rgbd360",
      "help_website": [],
      "license": null,
      "tags": [
        "slam",
        "rgb-d",
        "robotics",
        "computer-vision"
      ],
      "id": 813
    },
    {
      "name": "open-parse",
      "one_line_profile": "Improved file parsing library for LLM data preparation",
      "detailed_description": "A library designed to parse complex documents (PDFs, etc.) into structured chunks suitable for Large Language Models (LLMs), facilitating RAG (Retrieval-Augmented Generation) workflows in scientific literature analysis.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "document_parsing",
        "data_chunking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Filimoa/open-parse",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pdf-parsing",
        "llm",
        "rag",
        "document-processing"
      ],
      "id": 814
    },
    {
      "name": "ArUCo-Markers-Pose-Estimation-Generation-Python",
      "one_line_profile": "Pose estimation tool using ArUCo markers",
      "detailed_description": "A Python tool for generating ArUCo markers and estimating pose, commonly used in robotics, computer vision research, and laboratory automation for object tracking.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "pose_estimation",
        "computer_vision"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/GSNCodes/ArUCo-Markers-Pose-Estimation-Generation-Python",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "aruco",
        "pose-estimation",
        "computer-vision",
        "robotics"
      ],
      "id": 815
    },
    {
      "name": "detectron2-publaynet",
      "one_line_profile": "Document layout analysis models trained on PubLayNet",
      "detailed_description": "Provides trained Detectron2 models for document layout analysis, specifically optimized for scientific publications (PubLayNet dataset). Essential for extracting structure from scientific PDFs.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "layout_analysis",
        "document_parsing"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/JPLeoRX/detectron2-publaynet",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "layout-analysis",
        "publaynet",
        "detectron2",
        "pdf-parsing"
      ],
      "id": 816
    },
    {
      "name": "YuzuMarker.FontDetection",
      "one_line_profile": "CJK font recognition and style extraction model",
      "detailed_description": "A deep learning model and tool for recognizing Chinese, Japanese, and Korean (CJK) fonts and extracting styles from document images, aiding in detailed document layout analysis and OCR.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "font_recognition",
        "document_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/JeffersonQin/YuzuMarker.FontDetection",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "font-recognition",
        "ocr",
        "document-analysis",
        "cjk"
      ],
      "id": 817
    },
    {
      "name": "TopOpt.jl",
      "one_line_profile": "Topology optimization library for Julia",
      "detailed_description": "A Julia package for binary and continuous topology optimization on unstructured meshes using automatic differentiation. Used for structural design and physics simulations.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "topology_optimization",
        "simulation"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/JuliaTopOpt/TopOpt.jl",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "topology-optimization",
        "julia",
        "fem",
        "automatic-differentiation"
      ],
      "id": 818
    },
    {
      "name": "parsetron",
      "one_line_profile": "Natural language semantic parsing library",
      "detailed_description": "A library for semantic parsing of natural language, capable of mapping text to structured representations, applicable in scientific text mining and command parsing.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "semantic_parsing",
        "nlp"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Kitt-AI/parsetron",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "semantic-parsing",
        "nlp",
        "grammar"
      ],
      "id": 819
    },
    {
      "name": "markify",
      "one_line_profile": "File to Markdown converter for RAG/LLM",
      "detailed_description": "A tool to convert various file formats (PDF, etc.) into Markdown, specifically optimized to help RAG systems and LLMs understand document structure and content.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "document_conversion",
        "rag_preparation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/KylinMountain/markify",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "markdown",
        "pdf-conversion",
        "rag",
        "llm"
      ],
      "id": 820
    },
    {
      "name": "layout-model-training",
      "one_line_profile": "Training scripts for LayoutParser models",
      "detailed_description": "Scripts and utilities for training Detectron2-based document layout analysis models, supporting the LayoutParser ecosystem for custom scientific document parsing.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "model_training",
        "layout_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/Layout-Parser/layout-model-training",
      "help_website": [],
      "license": null,
      "tags": [
        "layout-analysis",
        "training-scripts",
        "detectron2",
        "document-parsing"
      ],
      "id": 821
    },
    {
      "name": "layout-parser",
      "one_line_profile": "Unified toolkit for deep learning based document image analysis",
      "detailed_description": "A comprehensive toolkit for document image analysis, providing pre-trained models and tools for layout detection, character recognition, and structural analysis of scientific documents.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "layout_analysis",
        "ocr",
        "document_parsing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Layout-Parser/layout-parser",
      "help_website": [
        "https://layout-parser.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "document-analysis",
        "deep-learning",
        "ocr",
        "layout-parsing"
      ],
      "id": 822
    },
    {
      "name": "MetaConfigurator",
      "one_line_profile": "A schema editor and form generator for JSON schemas and research data",
      "detailed_description": "MetaConfigurator is a web-based tool designed to facilitate the creation and editing of JSON schemas and configuration files. It is particularly useful for managing metadata and structured research data, allowing researchers to define and validate data formats visually.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "schema_management",
        "data_formatting"
      ],
      "application_level": "solver",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/MetaConfigurator/meta-configurator",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "json-schema",
        "metadata",
        "configuration-management"
      ],
      "id": 823
    },
    {
      "name": "Arabic Nougat",
      "one_line_profile": "Fine-tuned Nougat model for parsing Arabic scientific documents",
      "detailed_description": "This tool provides an implementation of the Nougat (Neural Optical Understanding for Academic Documents) model, specifically fine-tuned for Arabic text. It converts PDF documents into Markdown, enabling the extraction of structured text and layout information from Arabic scientific literature.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "document_parsing",
        "ocr"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/MohamedAliRashad/arabic-nougat",
      "help_website": [],
      "license": null,
      "tags": [
        "pdf-parsing",
        "ocr",
        "arabic-nlp",
        "nougat"
      ],
      "id": 824
    },
    {
      "name": "NanoNets docext",
      "one_line_profile": "Toolkit for OCR-free unstructured data extraction and benchmarking",
      "detailed_description": "docext is a toolkit designed for extracting structured data from unstructured documents without relying on traditional OCR pipelines. It supports converting documents to Markdown and includes benchmarking tools to evaluate extraction performance, suitable for processing scientific literature and reports.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "document_parsing",
        "layout_analysis",
        "benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/NanoNets/docext",
      "help_website": [
        "https://idp-leaderboard.org/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "document-extraction",
        "pdf-to-markdown",
        "unstructured-data"
      ],
      "id": 825
    },
    {
      "name": "NeurboParser",
      "one_line_profile": "Neural TurboSemanticParser for semantic dependency parsing",
      "detailed_description": "NeurboParser is a C++ implementation of a neural semantic dependency parser. It is used in Natural Language Processing (NLP) to extract semantic structures from text, which is a critical step in mining structured information from scientific literature.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "semantic_parsing",
        "nlp"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/Noahs-ARK/NeurboParser",
      "help_website": [],
      "license": "LGPL-3.0",
      "tags": [
        "semantic-parsing",
        "dependency-parsing",
        "nlp"
      ],
      "id": 826
    },
    {
      "name": "Layout2Graph",
      "one_line_profile": "GNN-based framework for document layout analysis",
      "detailed_description": "Layout2Graph is an implementation of the Paragraph2Graph framework, which uses Graph Neural Networks (GNNs) to perform language-independent layout analysis. This tool is essential for understanding the structure of scientific documents and extracting logical sections.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "layout_analysis",
        "document_parsing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/NormXU/Layout2Graph",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "layout-analysis",
        "gnn",
        "document-structure"
      ],
      "id": 827
    },
    {
      "name": "nougat-latex-ocr",
      "one_line_profile": "Nougat-based image to LaTeX generation tool",
      "detailed_description": "This repository provides code for fine-tuning and evaluating Nougat-based models specifically for the task of converting images (such as mathematical formulas and scientific text) into LaTeX code, facilitating the digitization of scientific content.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "ocr",
        "latex_generation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/NormXU/nougat-latex-ocr",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "image-to-latex",
        "nougat",
        "ocr",
        "scientific-notation"
      ],
      "id": 828
    },
    {
      "name": "LAREX",
      "one_line_profile": "Layout Analysis and Region Extraction tool for early printed books",
      "detailed_description": "LAREX is a semi-automatic tool designed for the layout analysis and region extraction of early printed books and historical documents. It helps researchers in digital humanities and history of science to digitize and structure complex historical layouts.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "layout_analysis",
        "document_parsing"
      ],
      "application_level": "solver",
      "primary_language": "Java",
      "repo_url": "https://github.com/OCR4all/LAREX",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "layout-analysis",
        "historical-documents",
        "ocr"
      ],
      "id": 829
    },
    {
      "name": "General-Documents-Layout-parser",
      "one_line_profile": "General document layout analysis and parsing tool",
      "detailed_description": "A tool for general document layout analysis, capable of parsing the structure of documents (including Chinese documents). It aids in extracting content from diverse document formats, supporting downstream scientific data mining tasks.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "layout_analysis",
        "document_parsing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/OKC13/General-Documents-Layout-parser",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "layout-analysis",
        "document-parsing",
        "chinese-nlp"
      ],
      "id": 830
    },
    {
      "name": "Oxen",
      "one_line_profile": "Data version control system for machine learning datasets",
      "detailed_description": "Oxen is a fast data version control system optimized for structured and unstructured machine learning datasets. It enables researchers to version control large scientific datasets (images, text, audio) similarly to how code is versioned, ensuring reproducibility in AI4S workflows.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "data_versioning",
        "workflow_management"
      ],
      "application_level": "solver",
      "primary_language": "Rust",
      "repo_url": "https://github.com/Oxen-AI/Oxen",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "data-version-control",
        "machine-learning",
        "dataset-management"
      ],
      "id": 831
    },
    {
      "name": "MegaParse",
      "one_line_profile": "File parser optimized for LLM ingestion of documents",
      "detailed_description": "MegaParse is a robust file parsing tool designed to convert unstructured documents (PDF, Docx, PPTx) into clean, structured formats ideal for Large Language Model (LLM) ingestion. It is highly relevant for processing scientific literature and reports in AI4S pipelines.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "document_parsing",
        "data_conversion"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/QuivrHQ/MegaParse",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "pdf-parsing",
        "llm-ingestion",
        "unstructured-data"
      ],
      "id": 832
    },
    {
      "name": "RapidLayout",
      "one_line_profile": "Layout analysis tool for Chinese and English documents",
      "detailed_description": "A tool for analyzing document layouts, specifically optimized for Chinese and English, capable of identifying regions like text, headers, and figures, which is essential for parsing unstructured scientific documents.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "layout_analysis",
        "document_parsing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/RapidAI/RapidLayout",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "layout-analysis",
        "ocr",
        "document-parsing"
      ],
      "id": 833
    },
    {
      "name": "TableStructureRec",
      "one_line_profile": "Table structure recognition and extraction tool",
      "detailed_description": "A collection of optimized models for table structure recognition, including pre- and post-processing pipelines and ONNX conversion, facilitating the extraction of structured data from scientific papers and reports.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "table_extraction",
        "structure_recognition"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/RapidAI/TableStructureRec",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "table-recognition",
        "ocr",
        "onnx"
      ],
      "id": 834
    },
    {
      "name": "Spotlight",
      "one_line_profile": "Interactive visualization and curation tool for unstructured data",
      "detailed_description": "A tool for interactively exploring and curating unstructured datasets (images, audio, text) directly from dataframes, supporting data-centric AI workflows in scientific research.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "data_visualization",
        "data_curation",
        "exploratory_analysis"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/Renumics/spotlight",
      "help_website": [
        "https://renumics.com/"
      ],
      "license": "MIT",
      "tags": [
        "data-curation",
        "visualization",
        "unstructured-data"
      ],
      "id": 835
    },
    {
      "name": "SCOREC Core",
      "one_line_profile": "Parallel unstructured mesh management library",
      "detailed_description": "A set of C++ libraries for managing parallel unstructured meshes, supporting adaptive mesh refinement and partition, essential for finite element simulations in physics and engineering.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "mesh_generation",
        "simulation",
        "finite_element_analysis"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/SCOREC/core",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "mesh",
        "fem",
        "hpc"
      ],
      "id": 836
    },
    {
      "name": "SPECFEM3D Cartesian",
      "one_line_profile": "Seismic wave propagation simulation software",
      "detailed_description": "Simulates acoustic, elastic, coupled acoustic/elastic, or poroelastic seismic wave propagation in any type of conforming mesh of hexahedra, widely used in geophysics and seismology.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "simulation",
        "seismic_modeling",
        "wave_propagation"
      ],
      "application_level": "solver",
      "primary_language": "Fortran",
      "repo_url": "https://github.com/SPECFEM/specfem3d",
      "help_website": [
        "https://specfem3d.readthedocs.io/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "seismology",
        "geophysics",
        "simulation"
      ],
      "id": 837
    },
    {
      "name": "GravitasML",
      "one_line_profile": "XML parser optimized for LLM data ingestion",
      "detailed_description": "A lightweight XML parsing library designed to simplify the processing of XML data for Large Language Models, facilitating the ingestion of structured scientific data into AI workflows.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "xml_parsing",
        "data_ingestion",
        "llm_preprocessing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Significant-Gravitas/gravitasml",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "xml",
        "llm",
        "parsing"
      ],
      "id": 838
    },
    {
      "name": "Merlin",
      "one_line_profile": "3D Vision-Language Model for medical imaging",
      "detailed_description": "A 3D Vision-Language Model (VLM) specifically designed for computed tomography (CT) scans, leveraging structured EHR and unstructured radiology reports for pretraining to assist in medical diagnosis and analysis.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "medical_imaging",
        "structure_prediction",
        "multimodal_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/StanfordMIMI/Merlin",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "medical-ai",
        "ct-scan",
        "vlm"
      ],
      "id": 839
    },
    {
      "name": "Table Transformer",
      "one_line_profile": "OCR and vision-based table extraction tool",
      "detailed_description": "An open-source tool combining OCR and computer vision to extract structured tabular data from images, suitable for preprocessing scientific documents for LLMs and data analysis.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "table_extraction",
        "ocr",
        "document_parsing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Sudhanshu1304/table-transformer",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "table-extraction",
        "ocr",
        "computer-vision"
      ],
      "id": 840
    },
    {
      "name": "OCR Correction",
      "one_line_profile": "Seq2seq model for post-processing OCR errors",
      "detailed_description": "A tool using sequence-to-sequence models to correct errors in Optical Character Recognition (OCR) output, improving the quality of digitized scientific texts and historical documents.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "ocr_correction",
        "text_processing",
        "data_cleaning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/TurkuNLP/ocr-correction",
      "help_website": [],
      "license": null,
      "tags": [
        "ocr",
        "nlp",
        "seq2seq"
      ],
      "id": 841
    },
    {
      "name": "UXarray",
      "one_line_profile": "Xarray extension for unstructured climate data",
      "detailed_description": "An extension to Xarray that provides analysis and visualization capabilities for unstructured grid data, specifically designed for climate and global weather datasets.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "climate_analysis",
        "data_visualization",
        "unstructured_grid"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/UXARRAY/uxarray",
      "help_website": [
        "https://uxarray.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "climate",
        "weather",
        "xarray"
      ],
      "id": 842
    },
    {
      "name": "Fiducials",
      "one_line_profile": "Simultaneous localization and mapping using fiducial markers",
      "detailed_description": "A system for simultaneous localization and mapping (SLAM) using fiducial markers, enabling robots to determine their position and orientation in an environment, widely used in robotics research.",
      "domains": [
        "D1"
      ],
      "subtask_category": [
        "slam",
        "localization",
        "robotics"
      ],
      "application_level": "solver",
      "primary_language": "C",
      "repo_url": "https://github.com/UbiquityRobotics/fiducials",
      "help_website": [
        "http://wiki.ros.org/fiducials"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "slam",
        "robotics",
        "localization"
      ],
      "id": 843
    },
    {
      "name": "Unstructured",
      "one_line_profile": "Open-source ETL library for ingesting and processing unstructured documents for LLMs",
      "detailed_description": "A library that provides components for ingesting and processing unstructured documents (PDF, HTML, Word, etc.) into structured formats suitable for Large Language Models (LLMs). It handles partitioning, cleaning, and extracting text/metadata from complex document layouts.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "document_parsing",
        "etl",
        "data_cleaning"
      ],
      "application_level": "library",
      "primary_language": "HTML",
      "repo_url": "https://github.com/Unstructured-IO/unstructured",
      "help_website": [
        "https://unstructured.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "etl",
        "pdf-parsing",
        "llm-preprocessing",
        "unstructured-data"
      ],
      "id": 844
    },
    {
      "name": "ViTLP",
      "one_line_profile": "Visually Guided Generative Text-Layout Pre-training for Document Intelligence",
      "detailed_description": "An implementation of a visually guided generative text-layout pre-training framework for document intelligence. It is designed to handle document understanding tasks by leveraging both visual and layout information.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "document_intelligence",
        "layout_analysis",
        "pretraining"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Veason-silverbullet/ViTLP",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "document-understanding",
        "layout-analysis",
        "generative-model"
      ],
      "id": 845
    },
    {
      "name": "Document-Layout-Analysis",
      "one_line_profile": "Tools for extracting figures, tables, and text from PDF documents",
      "detailed_description": "A collection of tools designed to analyze the layout of PDF documents and extract structured components such as figures, tables, and text blocks, facilitating downstream data processing.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "pdf_extraction",
        "layout_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Wild-Rift/Document-Layout-Analysis",
      "help_website": [],
      "license": null,
      "tags": [
        "pdf-extraction",
        "layout-analysis",
        "data-mining"
      ],
      "id": 846
    },
    {
      "name": "csv-schema-inference",
      "one_line_profile": "Automatic schema and data type inference for CSV files",
      "detailed_description": "A tool that automatically infers column data types and generates schemas for CSV files, aiding in the preprocessing and validation of tabular data.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "schema_inference",
        "data_profiling"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/Wittline/csv-schema-inference",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "csv",
        "schema-inference",
        "data-preprocessing"
      ],
      "id": 847
    },
    {
      "name": "GEM",
      "one_line_profile": "Online globally consistent dense elevation mapping for unstructured terrain",
      "detailed_description": "A system for creating globally consistent dense elevation maps of unstructured terrain in real-time, useful for robotics navigation and geological surface analysis.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "mapping",
        "slam",
        "terrain_analysis"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/ZJU-Robotics-Lab/GEM",
      "help_website": [],
      "license": null,
      "tags": [
        "robotics",
        "elevation-mapping",
        "slam",
        "unstructured-terrain"
      ],
      "id": 848
    },
    {
      "name": "llmwhisperer-table-extraction",
      "one_line_profile": "PDF table extraction pipeline using LLMWhisperer and Langchain",
      "detailed_description": "A workflow for extracting tabular data from PDFs using LLMWhisperer and structuring the information using Langchain, specifically targeting the conversion of unstructured document tables into structured formats.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "table_extraction",
        "pdf_parsing"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/Zipstack/llmwhisperer-table-extraction",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "table-extraction",
        "pdf",
        "llm",
        "structured-data"
      ],
      "id": 849
    },
    {
      "name": "Unstract",
      "one_line_profile": "No-code LLM platform for structuring unstructured documents via ETL pipelines",
      "detailed_description": "A platform that enables the creation of ETL pipelines to extract structured data from unstructured documents using Large Language Models, providing APIs and a no-code interface for document processing workflows.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "etl",
        "document_structuring",
        "pipeline_orchestration"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/Zipstack/unstract",
      "help_website": [
        "https://unstract.com"
      ],
      "license": "AGPL-3.0",
      "tags": [
        "etl",
        "llm",
        "document-processing",
        "no-code"
      ],
      "id": 850
    },
    {
      "name": "OmniParse",
      "one_line_profile": "Universal data ingestion and parsing tool for GenAI compatibility",
      "detailed_description": "A tool designed to ingest, parse, and optimize various unstructured data formats (documents, multimedia) into structured formats optimized for Generative AI frameworks.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "data_ingestion",
        "parsing",
        "multimodal_processing"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/adithya-s-k/omniparse",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "data-ingestion",
        "parsing",
        "genai",
        "unstructured-data"
      ],
      "id": 851
    },
    {
      "name": "CCTag",
      "one_line_profile": "Library for detecting concentric circle markers for photogrammetry",
      "detailed_description": "A computer vision library for the detection and identification of CCTag markers, which are used for reliable tracking and calibration in photogrammetry and 3D reconstruction workflows.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "marker_detection",
        "photogrammetry",
        "calibration"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/alicevision/CCTag",
      "help_website": [
        "https://alicevision.org"
      ],
      "license": "MPL-2.0",
      "tags": [
        "computer-vision",
        "photogrammetry",
        "fiducial-markers"
      ],
      "id": 852
    },
    {
      "name": "AllenNLP Semparse",
      "one_line_profile": "Framework for building semantic parsers with AllenNLP",
      "detailed_description": "A library built on AllenNLP for developing semantic parsers, which convert natural language into logical forms or executable code, facilitating the extraction of structured meaning from text.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "semantic_parsing",
        "nlp"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/allenai/allennlp-semparse",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "semantic-parsing",
        "nlp",
        "allennlp"
      ],
      "id": 853
    },
    {
      "name": "ROSGPT",
      "one_line_profile": "Interface for controlling robots using natural language via ChatGPT",
      "detailed_description": "A tool that integrates ChatGPT with ROS (Robot Operating System) to convert unstructured human language commands into actionable robotic instructions, enabling natural language interaction with robots.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "human_robot_interaction",
        "instruction_parsing",
        "robot_control"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/aniskoubaa/rosgpt",
      "help_website": [],
      "license": null,
      "tags": [
        "ros",
        "robotics",
        "llm",
        "natural-language-control"
      ],
      "id": 854
    },
    {
      "name": "Marker",
      "one_line_profile": "High-accuracy PDF to Markdown/JSON converter for scientific documents",
      "detailed_description": "A powerful pipeline that converts PDF documents (including scientific papers) into Markdown and JSON formats. It handles equations, tables, and layout analysis with high accuracy, making it essential for ingesting scientific literature into LLMs or data workflows.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "parsing",
        "layout_analysis",
        "ocr"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/datalab-to/marker",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "pdf-parsing",
        "ocr",
        "scientific-literature",
        "markdown"
      ],
      "id": 855
    },
    {
      "name": "Surya",
      "one_line_profile": "Multilingual OCR and layout analysis toolkit",
      "detailed_description": "A comprehensive OCR and document layout analysis tool that supports over 90 languages. It performs text detection, reading order determination, and table recognition, serving as a foundational tool for digitizing and structuring scientific documents.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "ocr",
        "layout_analysis",
        "table_extraction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/datalab-to/surya",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "ocr",
        "layout-analysis",
        "document-understanding"
      ],
      "id": 856
    },
    {
      "name": "Probable People",
      "one_line_profile": "Parser for unstructured western names into structured components",
      "detailed_description": "A library using conditional random fields to parse unstructured name strings into structured components (e.g., given name, surname, title). It is widely used in bibliometrics and scientific data cleaning to normalize author names from citation data.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "data_cleaning",
        "normalization",
        "parsing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/datamade/probablepeople",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "name-parsing",
        "bibliometrics",
        "data-cleaning"
      ],
      "id": 857
    },
    {
      "name": "usaddress",
      "one_line_profile": "Parser for unstructured US address strings",
      "detailed_description": "A probabilistic parser that converts unstructured US address strings into structured components. It is valuable for geospatial scientific data processing, epidemiology, and social science research where location data normalization is required.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "data_cleaning",
        "normalization",
        "parsing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/datamade/usaddress",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "address-parsing",
        "geospatial",
        "data-cleaning"
      ],
      "id": 858
    },
    {
      "name": "deepdoctection",
      "one_line_profile": "Document AI package for analyzing and extracting data from documents",
      "detailed_description": "A comprehensive Document AI library that orchestrates OCR, layout analysis, and table extraction. It is specifically designed to handle complex document structures like scientific papers, enabling the extraction of structured data from PDFs.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "parsing",
        "layout_analysis",
        "table_extraction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/deepdoctection/deepdoctection",
      "help_website": [
        "https://deepdoctection.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "document-ai",
        "pdf-parsing",
        "ocr"
      ],
      "id": 859
    },
    {
      "name": "ExtractThinker",
      "one_line_profile": "Document Intelligence library for LLM-based extraction",
      "detailed_description": "A library that provides an ORM-style interaction for document workflows using LLMs. It facilitates the extraction of structured data from unstructured documents, enabling flexible and powerful document intelligence applications in scientific research.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "parsing",
        "information_extraction",
        "llm_workflow"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/enoch3712/ExtractThinker",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "document-intelligence",
        "extraction"
      ],
      "id": 860
    },
    {
      "name": "TaBERT",
      "one_line_profile": "Pre-trained language model for learning joint representations of natural language and structured tables",
      "detailed_description": "A pre-trained language model designed to learn joint representations of natural language utterances and semi-structured tables. It is trained on a massive corpus of web tables and can be used for semantic parsing tasks, enabling the conversion of natural language into structured queries or representations by understanding tabular data context.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "semantic_parsing",
        "table_understanding"
      ],
      "application_level": "model",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/TaBERT",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "nlp",
        "semantic-parsing",
        "table-understanding",
        "representation-learning"
      ],
      "id": 861
    },
    {
      "name": "Nougat",
      "one_line_profile": "Neural Optical Understanding for Academic Documents",
      "detailed_description": "A Visual Transformer model that understands academic documents and converts them into markup language (Markdown). It is specifically designed to handle the complex layout and formatting of scientific papers, enabling the extraction of structured text, equations, and tables from PDF documents for downstream scientific data processing.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "document_parsing",
        "ocr",
        "layout_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/nougat",
      "help_website": [
        "https://facebookresearch.github.io/nougat/"
      ],
      "license": "MIT",
      "tags": [
        "ocr",
        "pdf-parsing",
        "academic-documents",
        "transformer"
      ],
      "id": 862
    },
    {
      "name": "Gretel Synthetics",
      "one_line_profile": "Synthetic data generators for structured and unstructured text with differential privacy",
      "detailed_description": "A library for generating synthetic data for structured and unstructured text, incorporating differentially private learning techniques. It is used in data science and research to create privacy-preserving datasets for training models, simulating scientific data, or sharing sensitive information safely.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "data_generation",
        "synthetic_data",
        "privacy_preserving"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/gretelai/gretel-synthetics",
      "help_website": [
        "https://gretel-synthetics.readthedocs.io/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "synthetic-data",
        "differential-privacy",
        "data-generation",
        "nlp"
      ],
      "id": 863
    },
    {
      "name": "HanLP",
      "one_line_profile": "Multilingual Natural Language Processing library",
      "detailed_description": "A comprehensive Natural Language Processing (NLP) library supporting multiple languages. It provides functionalities such as tokenization, part-of-speech tagging, named entity recognition, and syntactic/semantic dependency parsing. It is widely used as a foundational tool for text mining and information extraction in scientific literature analysis.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "nlp",
        "text_mining",
        "parsing",
        "ner"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/hankcs/HanLP",
      "help_website": [
        "https://hanlp.hankcs.com/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "nlp",
        "tokenization",
        "ner",
        "dependency-parsing"
      ],
      "id": 864
    },
    {
      "name": "fAIr",
      "one_line_profile": "AI-assisted mapping tool for humanitarian and geographic data",
      "detailed_description": "An open-source AI-assisted mapping tool developed by the Humanitarian OpenStreetMap Team (HOT). It leverages AI models to detect features (like buildings and roads) from satellite imagery, assisting researchers and mappers in generating structured geographic data for humanitarian and scientific analysis.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "mapping",
        "feature_extraction",
        "geospatial_analysis"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/hotosm/fAIr",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "mapping",
        "gis",
        "ai-assisted",
        "satellite-imagery"
      ],
      "id": 865
    },
    {
      "name": "pdf-document-layout-analysis",
      "one_line_profile": "Docker-powered service for PDF document layout analysis and segmentation",
      "detailed_description": "A service designed to perform layout analysis on PDF documents. It segments and classifies different parts of PDF pages, identifying elements such as text blocks, titles, images, and tables. This tool is essential for preprocessing scientific literature and reports to extract structured data.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "document_analysis",
        "layout_segmentation",
        "pdf_parsing"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/huridocs/pdf-document-layout-analysis",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "pdf-analysis",
        "layout-analysis",
        "document-processing",
        "docker"
      ],
      "id": 866
    },
    {
      "name": "pdf-text-extraction",
      "one_line_profile": "Tool for extracting text from PDFs using layout analysis outputs",
      "detailed_description": "A utility that leverages the outputs of the pdf-document-layout-analysis service to extract text from PDF files. It uses the segmentation and classification information to accurately extract and structure text content from complex documents, facilitating downstream text mining and analysis.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "text_extraction",
        "pdf_processing"
      ],
      "application_level": "workflow",
      "primary_language": "Makefile",
      "repo_url": "https://github.com/huridocs/pdf-text-extraction",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "pdf",
        "text-extraction",
        "document-processing"
      ],
      "id": 867
    },
    {
      "name": "vision-parse",
      "one_line_profile": "PDF to Markdown converter using Vision LLMs for document parsing",
      "detailed_description": "A tool that leverages Vision Language Models (VLMs) to parse PDF documents (including scientific papers) into structured Markdown format, preserving layout and content structure.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "document_parsing",
        "ocr",
        "unstructured_data_extraction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/iamarunbrahma/vision-parse",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pdf-parsing",
        "vision-llm",
        "markdown",
        "document-intelligence"
      ],
      "id": 868
    },
    {
      "name": "schemer",
      "one_line_profile": "Schema registry and inference tool for CSV, JSON, Avro, and Parquet",
      "detailed_description": "A schema registry service that supports schema inference for common data formats (CSV, JSON, etc.), facilitating data management and validation in data workflows.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "schema_inference",
        "data_validation"
      ],
      "application_level": "service",
      "primary_language": "Scala",
      "repo_url": "https://github.com/indix/schemer",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "schema-inference",
        "csv",
        "parquet",
        "data-management"
      ],
      "id": 869
    },
    {
      "name": "layout_analysis",
      "one_line_profile": "Document layout analysis tool using YOLOv8",
      "detailed_description": "A tool for detecting and analyzing the layout of documents (specifically Chinese documents, but applicable to others) using YOLOv8, essential for extracting structured information from scientific literature.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "layout_analysis",
        "document_parsing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/jiangnanboy/layout_analysis",
      "help_website": [],
      "license": null,
      "tags": [
        "yolov8",
        "layout-analysis",
        "document-understanding"
      ],
      "id": 870
    },
    {
      "name": "layout_analysis4j",
      "one_line_profile": "Java implementation of document layout analysis using YOLOv8",
      "detailed_description": "The Java version of the layout_analysis tool, enabling document layout detection and analysis in Java-based scientific workflows.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "layout_analysis",
        "document_parsing"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/jiangnanboy/layout_analysis4j",
      "help_website": [],
      "license": null,
      "tags": [
        "java",
        "yolov8",
        "layout-analysis"
      ],
      "id": 871
    },
    {
      "name": "json-typedef-infer",
      "one_line_profile": "CLI tool for inferring JSON Typedef schemas from data",
      "detailed_description": "A command-line tool that generates JSON Typedef schemas from example data, aiding in schema inference and data structure discovery for JSON-based scientific datasets.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "schema_inference",
        "data_modeling"
      ],
      "application_level": "solver",
      "primary_language": "Rust",
      "repo_url": "https://github.com/jsontypedef/json-typedef-infer",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "schema-inference",
        "json-typedef",
        "cli"
      ],
      "id": 872
    },
    {
      "name": "genson-rs",
      "one_line_profile": "High-performance JSON Schema inference engine",
      "detailed_description": "A Rust-based engine for inferring JSON Schemas from data, providing fast schema discovery for large JSON datasets in scientific data pipelines.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "schema_inference",
        "data_profiling"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/junyu-w/genson-rs",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rust",
        "json-schema",
        "inference"
      ],
      "id": 873
    },
    {
      "name": "byte-vision",
      "one_line_profile": "Privacy-first document intelligence platform with RAG",
      "detailed_description": "A platform for transforming static documents into an interactive knowledge base using OCR, parsing, and RAG (Retrieval-Augmented Generation), suitable for scientific literature management.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "document_parsing",
        "ocr",
        "rag"
      ],
      "application_level": "platform",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/kbrisso/byte-vision",
      "help_website": [],
      "license": null,
      "tags": [
        "document-intelligence",
        "ocr",
        "rag",
        "elasticsearch"
      ],
      "id": 874
    },
    {
      "name": "GROBID",
      "one_line_profile": "Machine learning software for extracting information from scholarly documents",
      "detailed_description": "A leading library for extracting, parsing, and restructuring raw documents (PDF) such as scientific publications into structured XML/TEI encoded data, focusing on bibliographic data and full text.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "document_parsing",
        "information_extraction",
        "bibliographic_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Java",
      "repo_url": "https://github.com/kermitt2/grobid",
      "help_website": [
        "https://grobid.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "pdf-parsing",
        "scientific-literature",
        "tei",
        "crf",
        "deep-learning"
      ],
      "id": 875
    },
    {
      "name": "grobid-astro",
      "one_line_profile": "GROBID module for extracting astronomical entities",
      "detailed_description": "A specialized module for GROBID designed to recognize and extract astronomical entities (such as celestial objects) from scientific papers.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "entity_extraction",
        "document_parsing"
      ],
      "application_level": "solver",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/kermitt2/grobid-astro",
      "help_website": [],
      "license": null,
      "tags": [
        "astronomy",
        "ner",
        "grobid-module"
      ],
      "id": 876
    },
    {
      "name": "grobid-client-java",
      "one_line_profile": "Java client for GROBID REST services",
      "detailed_description": "The official Java client library for interacting with the GROBID service, facilitating the integration of scientific document parsing into Java applications.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "document_parsing"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/kermitt2/grobid-client-java",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "client",
        "java",
        "grobid"
      ],
      "id": 877
    },
    {
      "name": "grobid-client-node",
      "one_line_profile": "Node.js client for GROBID REST services",
      "detailed_description": "The official Node.js client library for interacting with the GROBID service, enabling scientific document parsing in JavaScript/Node.js environments.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "document_parsing"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/kermitt2/grobid-client-node",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "client",
        "node.js",
        "grobid"
      ],
      "id": 878
    },
    {
      "name": "grobid-client-python",
      "one_line_profile": "Python client for GROBID Web services",
      "detailed_description": "The official Python client library for interacting with the GROBID service, widely used in Python-based scientific data pipelines for literature mining.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "document_parsing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/kermitt2/grobid-client-python",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "client",
        "python",
        "grobid"
      ],
      "id": 879
    },
    {
      "name": "grobid-ner",
      "one_line_profile": "Named-Entity Recognition module based on Grobid for scientific text",
      "detailed_description": "A Named-Entity Recogniser (NER) built on top of Grobid, designed to extract entities from scientific documents. It leverages Grobid's cascading CRF models to identify and classify entities within the structure of academic papers.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "named_entity_recognition",
        "text_mining"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/kermitt2/grobid-ner",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ner",
        "grobid",
        "scientific-text-mining"
      ],
      "id": 880
    },
    {
      "name": "grobid-quantities",
      "one_line_profile": "Grobid extension for extracting physical quantities from scientific text",
      "detailed_description": "A module for Grobid that identifies, parses, and normalizes physical quantities and measurements in scientific and technical documents. It handles values, units, and their normalization.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "entity_extraction",
        "normalization"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/lfoppiano/grobid-quantities",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "grobid",
        "physical-quantities",
        "measurement-extraction"
      ],
      "id": 881
    },
    {
      "name": "grobid-superconductors",
      "one_line_profile": "Grobid module for superconductor material and property extraction",
      "detailed_description": "A specialized Grobid module designed to extract information about superconductor materials, their properties (like critical temperature), and related experimental conditions from scientific literature.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "material_extraction",
        "property_extraction"
      ],
      "application_level": "library",
      "primary_language": "HTML",
      "repo_url": "https://github.com/lfoppiano/grobid-superconductors",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "materials-science",
        "superconductors",
        "text-mining"
      ],
      "id": 882
    },
    {
      "name": "material-parsers",
      "one_line_profile": "Parsers and utilities for materials science data extraction",
      "detailed_description": "A collection of parsers and scripts developed to support the extraction of materials science data, particularly in the context of the Grobid Superconductors project. It includes utilities for handling material names and properties.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "parsing",
        "data_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/lfoppiano/material-parsers",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "materials-science",
        "parsing",
        "grobid-utils"
      ],
      "id": 883
    },
    {
      "name": "structure-vision",
      "one_line_profile": "Visualizer for Grobid-extracted document structure",
      "detailed_description": "A viewer tool designed to visualize the structured data extracted by Grobid from PDF documents. It helps researchers and developers inspect and validate the parsing results of scientific publications.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "visualization",
        "validation"
      ],
      "application_level": "tool",
      "primary_language": "Python",
      "repo_url": "https://github.com/lfoppiano/structure-vision",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "visualization",
        "grobid",
        "pdf-structure"
      ],
      "id": 884
    },
    {
      "name": "P2PaLA",
      "one_line_profile": "Layout analysis tool for historical documents (PAGE format)",
      "detailed_description": "Page to PAGE Layout Analysis Tool. A software for analyzing the layout of documents, particularly historical ones, and generating output in the PAGE XML format, which is a standard in document analysis research.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "layout_analysis",
        "document_processing"
      ],
      "application_level": "tool",
      "primary_language": "Python",
      "repo_url": "https://github.com/lquirosd/P2PaLA",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "layout-analysis",
        "page-xml",
        "historical-documents"
      ],
      "id": 885
    },
    {
      "name": "iFEM",
      "one_line_profile": "MATLAB package for adaptive finite element methods",
      "detailed_description": "A MATLAB software package containing robust and efficient codes for adaptive finite element methods (FEM) on unstructured simplicial grids. It is used for scientific simulation and modeling in 2D and 3D.",
      "domains": [
        "Scientific Computing",
        "Simulation"
      ],
      "subtask_category": [
        "simulation",
        "finite_element_method"
      ],
      "application_level": "library",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/lyc102/ifem",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "fem",
        "simulation",
        "matlab"
      ],
      "id": 886
    },
    {
      "name": "tesseract-recognize",
      "one_line_profile": "Layout analysis and OCR tool exporting to Page XML format",
      "detailed_description": "A C++ tool that performs layout analysis and text recognition using Tesseract, specifically designed to output results in Page XML format, which is a standard for document image analysis and digital archiving workflows.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "ocr",
        "layout_analysis",
        "document_parsing"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/mauvilsa/tesseract-recognize",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ocr",
        "page-xml",
        "layout-analysis",
        "tesseract"
      ],
      "id": 887
    },
    {
      "name": "llm-document-ocr",
      "one_line_profile": "LLM-based OCR and document parsing library",
      "detailed_description": "A TypeScript library that leverages Large Language Models (LLMs) to perform OCR and parse unstructured documents into structured formats, facilitating data extraction from scientific or technical documents.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "ocr",
        "document_parsing",
        "information_extraction"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/mercoa-finance/llm-document-ocr",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "ocr",
        "document-parsing",
        "typescript"
      ],
      "id": 888
    },
    {
      "name": "semantic-csv",
      "one_line_profile": "Higher-level semantic tools for CSV data manipulation",
      "detailed_description": "A Clojure library providing higher-level abstractions for working with CSV data, enabling semantic mapping and processing of tabular data commonly used in scientific research.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "data_processing",
        "csv_parsing",
        "data_normalization"
      ],
      "application_level": "library",
      "primary_language": "Clojure",
      "repo_url": "https://github.com/metasoarous/semantic-csv",
      "help_website": [],
      "license": "EPL-1.0",
      "tags": [
        "csv",
        "clojure",
        "data-processing"
      ],
      "id": 889
    },
    {
      "name": "AIDocIntelligence",
      "one_line_profile": "Client for AI Document Intelligence service",
      "detailed_description": "A Python tool/client for interacting with AI Document Intelligence services, enabling the extraction of text, key-value pairs, tables, and structures from unstructured scientific or technical documents.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "document_parsing",
        "information_extraction",
        "table_extraction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/AIDocIntelligence",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "document-intelligence",
        "extraction",
        "parsing"
      ],
      "id": 890
    },
    {
      "name": "Document-Knowledge-Mining-Solution-Accelerator",
      "one_line_profile": "Workflow for mining knowledge from unstructured documents",
      "detailed_description": "A solution accelerator that integrates Azure OpenAI and Document Intelligence to process, summarize, and extract entities/metadata from unstructured multi-modal documents, facilitating knowledge discovery in research.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "knowledge_mining",
        "document_summarization",
        "entity_extraction"
      ],
      "application_level": "workflow",
      "primary_language": "C#",
      "repo_url": "https://github.com/microsoft/Document-Knowledge-Mining-Solution-Accelerator",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "knowledge-mining",
        "rag",
        "document-processing"
      ],
      "id": 891
    },
    {
      "name": "OmniParser",
      "one_line_profile": "Screen parsing tool for vision-based GUI agents",
      "detailed_description": "A pure vision-based screen parsing tool designed to convert GUI screenshots into structured elements. While primarily for agents, its capability to parse visual information into structured data is relevant for AI-assisted data extraction and automation in research workflows.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "visual_parsing",
        "gui_automation",
        "screen_parsing"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/microsoft/OmniParser",
      "help_website": [],
      "license": "CC-BY-4.0",
      "tags": [
        "vision-parsing",
        "gui-agent",
        "screen-parsing"
      ],
      "id": 892
    },
    {
      "name": "data-formulator",
      "one_line_profile": "AI-powered data visualization creation tool",
      "detailed_description": "A tool that uses AI to transform data and create rich visualizations, assisting researchers in exploring and presenting scientific data effectively.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "data_visualization",
        "data_transformation",
        "exploratory_analysis"
      ],
      "application_level": "tool",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/microsoft/data-formulator",
      "help_website": [
        "https://microsoft.github.io/data-formulator"
      ],
      "license": "MIT",
      "tags": [
        "visualization",
        "ai-assisted",
        "data-analysis"
      ],
      "id": 893
    },
    {
      "name": "dstoolkit-text2sql-and-imageprocessing",
      "one_line_profile": "Toolkit for RAG, Text2SQL, and document processing",
      "detailed_description": "A development toolkit accelerating RAG applications by integrating SQL Warehouses and document analysis via Azure Document Intelligence, facilitating complex data querying and extraction for research.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "text2sql",
        "rag",
        "document_processing"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/dstoolkit-text2sql-and-imageprocessing",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "text2sql",
        "document-intelligence"
      ],
      "id": 894
    },
    {
      "name": "openscraping-lib-csharp",
      "one_line_profile": "Library for structured data extraction from HTML",
      "detailed_description": "A C# library designed to turn unstructured HTML pages into structured data using JSON configuration and XPath rules, capable of handling complex objects like tables, useful for scientific data collection from web sources.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "web_scraping",
        "data_extraction",
        "html_parsing"
      ],
      "application_level": "library",
      "primary_language": "C#",
      "repo_url": "https://github.com/microsoft/openscraping-lib-csharp",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "scraping",
        "structured-data",
        "xpath"
      ],
      "id": 895
    },
    {
      "name": "rat-sql",
      "one_line_profile": "Relation-aware semantic parsing model (Text-to-SQL)",
      "detailed_description": "A relation-aware semantic parsing model that translates natural language questions into SQL queries, facilitating natural language interfaces for scientific databases.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "semantic_parsing",
        "text2sql",
        "natural_language_interface"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/rat-sql",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "semantic-parsing",
        "text2sql",
        "nlp"
      ],
      "id": 896
    },
    {
      "name": "table-transformer",
      "one_line_profile": "Deep learning model for table extraction from documents",
      "detailed_description": "Table Transformer (TATR) is a deep learning model specifically designed to extract and structure tables from unstructured documents like PDFs and images, a critical task in scientific literature mining.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "table_extraction",
        "document_parsing",
        "structure_recognition"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/table-transformer",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "table-extraction",
        "deep-learning",
        "pdf-parsing"
      ],
      "id": 897
    },
    {
      "name": "YOLOv10-Document-Layout-Analysis",
      "one_line_profile": "YOLOv10 model for document layout analysis",
      "detailed_description": "A YOLOv10 model trained on the DocLayNet dataset for performing document layout analysis, enabling the segmentation and classification of document elements (text, tables, figures) in scientific papers.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "layout_analysis",
        "document_parsing",
        "computer_vision"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/moured/YOLOv10-Document-Layout-Analysis",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "yolo",
        "layout-analysis",
        "doclaynet"
      ],
      "id": 898
    },
    {
      "name": "YOLOv11-Document-Layout-Analysis",
      "one_line_profile": "YOLOv11 model for document layout analysis",
      "detailed_description": "A YOLOv11 model trained on the DocLayNet dataset for performing document layout analysis, providing updated performance for segmenting scientific document structures.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "layout_analysis",
        "document_parsing",
        "computer_vision"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/moured/YOLOv11-Document-Layout-Analysis",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "yolo",
        "layout-analysis",
        "doclaynet"
      ],
      "id": 899
    },
    {
      "name": "ccg2lambda",
      "one_line_profile": "Semantic parsing tool for natural language inference",
      "detailed_description": "A tool for semantic parsing that converts Combinatory Categorial Grammar (CCG) derivations into lambda calculus expressions, supporting natural language inference and logic-based semantics research.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "semantic_parsing",
        "natural_language_inference",
        "logic_representation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/mynlp/ccg2lambda",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "semantic-parsing",
        "nlp",
        "logic"
      ],
      "id": 900
    },
    {
      "name": "LLM Graph Builder",
      "one_line_profile": "Neo4j graph construction from unstructured data using LLMs",
      "detailed_description": "A tool that utilizes Large Language Models (LLMs) to extract nodes and relationships from unstructured text data and construct knowledge graphs in Neo4j. It supports schema inference and automated mapping, facilitating the conversion of scientific literature or reports into structured knowledge bases.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "knowledge_extraction",
        "graph_construction",
        "schema_inference"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/neo4j-labs/llm-graph-builder",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "knowledge-graph",
        "llm",
        "unstructured-data",
        "neo4j"
      ],
      "id": 901
    },
    {
      "name": "SPARQA",
      "one_line_profile": "Skeleton-based Semantic Parsing for Complex Questions over Knowledge Bases",
      "detailed_description": "A semantic parsing framework designed to handle complex questions over knowledge bases (KBQA). It uses a skeleton-based approach to map natural language queries into SPARQL or logical forms, facilitating the interrogation of structured scientific data.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "semantic_parsing",
        "kbqa",
        "query_mapping"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/nju-websoft/SPARQA",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "semantic-parsing",
        "knowledge-base",
        "nlp",
        "sparql"
      ],
      "id": 902
    },
    {
      "name": "arucogen",
      "one_line_profile": "Online ArUco markers generator for computer vision",
      "detailed_description": "A utility tool for generating ArUco markers, which are essential for camera pose estimation, robotics navigation, and augmented reality experiments in computer vision research.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "data_generation",
        "computer_vision_setup"
      ],
      "application_level": "solver",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/okalachev/arucogen",
      "help_website": [
        "http://chev.me/arucogen/"
      ],
      "license": "MIT",
      "tags": [
        "aruco",
        "computer-vision",
        "robotics",
        "marker-generation"
      ],
      "id": 903
    },
    {
      "name": "Open Politics HQ",
      "one_line_profile": "OSINT infrastructure for structured insights from unstructured data",
      "detailed_description": "A platform for Open Source Intelligence (OSINT) that ingests content, defines analytical frameworks, and visualizes patterns. It is designed for researchers in social and political sciences to turn domain expertise into structured insights from large volumes of documents.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "osint",
        "text_analysis",
        "pattern_recognition"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/open-politics/open-politics-hq",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "osint",
        "social-science",
        "data-analysis",
        "visualization"
      ],
      "id": 904
    },
    {
      "name": "DocLayout-YOLO",
      "one_line_profile": "Document Layout Analysis using YOLO",
      "detailed_description": "A deep learning model based on YOLO designed for document layout analysis. It detects and segments different components of a document (e.g., text blocks, tables, figures), which is a critical step in parsing scientific literature and technical documents.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "document_layout_analysis",
        "parsing",
        "segmentation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/opendatalab/DocLayout-YOLO",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "document-analysis",
        "yolo",
        "layout-analysis",
        "pdf-parsing"
      ],
      "id": 905
    },
    {
      "name": "PDF-Extract-Kit",
      "one_line_profile": "Comprehensive Toolkit for High-Quality PDF Content Extraction",
      "detailed_description": "A toolkit focused on extracting high-quality content from PDFs, including text, tables, and formulas. It serves as a preprocessing engine for scientific document analysis and knowledge base construction.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "pdf_extraction",
        "ocr",
        "layout_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/opendatalab/PDF-Extract-Kit",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "pdf-extraction",
        "ocr",
        "document-parsing"
      ],
      "id": 906
    },
    {
      "name": "semantic-bot",
      "one_line_profile": "Semi-Automatic Tool to generate RDF mappings",
      "detailed_description": "A tool designed to assist in generating RDF mappings for datasets. It helps in the semantic enrichment of data, facilitating the integration of datasets into the Semantic Web or knowledge graphs.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "rdf_mapping",
        "semantic_enrichment",
        "schema_mapping"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/opendatasoft/semantic-bot",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rdf",
        "semantic-web",
        "data-mapping"
      ],
      "id": 907
    },
    {
      "name": "GeoAI",
      "one_line_profile": "Artificial Intelligence for Geospatial Data",
      "detailed_description": "A Python library for applying artificial intelligence and machine learning techniques to geospatial data. It supports tasks such as image segmentation, object detection, and data analysis in the context of geography and earth sciences.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "geospatial_analysis",
        "image_segmentation",
        "remote_sensing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/opengeos/geoai",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "geospatial",
        "ai",
        "gis",
        "remote-sensing"
      ],
      "id": 908
    },
    {
      "name": "openalex-pdf-parser",
      "one_line_profile": "PDF parser powered by grobid for OpenAlex",
      "detailed_description": "A specialized PDF parsing tool used within the OpenAlex ecosystem, leveraging Grobid to extract structured metadata and full text from scientific publications.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "pdf_parsing",
        "metadata_extraction",
        "bibliometrics"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ourresearch/openalex-pdf-parser",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "grobid",
        "openalex",
        "pdf-parsing",
        "scientific-literature"
      ],
      "id": 909
    },
    {
      "name": "aruco_ros",
      "one_line_profile": "ROS wrappers for Aruco Augmented Reality marker detector",
      "detailed_description": "A ROS package that provides wrappers for the Aruco library, enabling the detection of augmented reality markers. This is widely used in robotics research for localization, calibration, and object tracking.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "marker_detection",
        "robotics_perception",
        "localization"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/pal-robotics/aruco_ros",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ros",
        "robotics",
        "computer-vision",
        "aruco"
      ],
      "id": 910
    },
    {
      "name": "papercast",
      "one_line_profile": "Pipeline tool for processing technical documents (arXiv, PDF)",
      "detailed_description": "A pipeline tool designed to process technical documents from sources like arXiv and SemanticScholar. It uses GROBID and LangChain to parse papers and can convert them into audio (podcast format) or other structured representations for analysis.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "literature_processing",
        "pdf_parsing",
        "audio_synthesis"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/papercast-dev/papercast",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "arxiv",
        "pdf-processing",
        "grobid",
        "langchain"
      ],
      "id": 911
    },
    {
      "name": "View-Parsing-Network",
      "one_line_profile": "Cross-view Semantic Segmentation for Sensing Surroundings",
      "detailed_description": "Implementation of the View Parsing Network (VPN) for cross-view semantic segmentation. This tool is used in robotics and autonomous driving research to transform first-person view images into top-down semantic maps.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "semantic_segmentation",
        "view_transformation",
        "robotics_perception"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/pbw-Berwin/View-Parsing-Network",
      "help_website": [],
      "license": null,
      "tags": [
        "computer-vision",
        "semantic-segmentation",
        "robotics",
        "view-parsing"
      ],
      "id": 912
    },
    {
      "name": "tranX",
      "one_line_profile": "Neural semantic parser for mapping natural language to code",
      "detailed_description": "A general-purpose neural semantic parser that maps natural language queries into machine-executable code or logical forms. It is widely used in NLP research for tasks like code generation and semantic parsing of complex queries.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "semantic_parsing",
        "code_generation",
        "nlp"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pcyin/tranX",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "semantic-parsing",
        "nlp",
        "code-generation"
      ],
      "id": 913
    },
    {
      "name": "pdfix-autotag-deepdoctection",
      "one_line_profile": "Autotag PDF documents using deepdoctection",
      "detailed_description": "A tool that uses the deepdoctection AI model to automatically analyze and tag the structure of PDF documents. This aids in making PDFs accessible and machine-readable for further data extraction.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "pdf_tagging",
        "document_structure_analysis",
        "accessibility"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/pdfix/pdfix-autotag-deepdoctection",
      "help_website": [],
      "license": null,
      "tags": [
        "pdf",
        "deepdoctection",
        "autotagging"
      ],
      "id": 914
    },
    {
      "name": "OCRIntegrator",
      "one_line_profile": "Integrated solution for OCR, layout analysis, and table parsing",
      "detailed_description": "A unified interface combining multiple open-source OCR models, layout analysis tools, and table parsers. It streamlines the process of extracting structured information from various document types.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "ocr",
        "layout_analysis",
        "table_extraction"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/peakhell/OCRIntegrator",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ocr",
        "document-processing",
        "layout-analysis"
      ],
      "id": 915
    },
    {
      "name": "SEMPRE",
      "one_line_profile": "Semantic Parser with Execution",
      "detailed_description": "A toolkit for training semantic parsers that map natural language to logical forms. It is a foundational tool in NLP research for building systems that can understand and execute natural language commands against knowledge bases.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "semantic_parsing",
        "nlp",
        "logical_form_generation"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/percyliang/sempre",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "semantic-parsing",
        "nlp",
        "stanford"
      ],
      "id": 916
    },
    {
      "name": "geodict",
      "one_line_profile": "Library for pulling location information from unstructured text",
      "detailed_description": "A simple Python library for geoparsing, allowing the extraction of location names and coordinates from unstructured text. Useful for geospatial analysis and social science research.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "geoparsing",
        "text_mining",
        "location_extraction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/petewarden/geodict",
      "help_website": [],
      "license": null,
      "tags": [
        "geoparsing",
        "nlp",
        "location-extraction"
      ],
      "id": 917
    },
    {
      "name": "yolo-doclaynet",
      "one_line_profile": "YOLO models trained on DocLayNet for document layout analysis",
      "detailed_description": "Provides YOLO models trained on the DocLayNet dataset specifically for document layout analysis. This tool enables the segmentation and classification of document elements (headers, paragraphs, tables) in scientific and technical documents.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "layout_analysis",
        "document_segmentation",
        "computer_vision"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ppaanngggg/yolo-doclaynet",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "yolo",
        "doclaynet",
        "document-analysis"
      ],
      "id": 918
    },
    {
      "name": "Advanced Deep Research",
      "one_line_profile": "Automated Deep Research agent with paper parsing",
      "detailed_description": "An AI agent designed to conduct deep research by performing web searches, parsing scientific papers, and generating didactic summaries. It automates the literature review and information synthesis process.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "literature_review",
        "paper_parsing",
        "automated_research"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/prodesk98/advanced-deep-research",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ai-agent",
        "research-automation",
        "paper-parsing"
      ],
      "id": 919
    },
    {
      "name": "gds2Para",
      "one_line_profile": "GDSII layout file parsing and parameter extraction tool",
      "detailed_description": "A tool for parsing GDSII files (standard format for IC layout data), performing layout analysis, and extracting parameters for semiconductor engineering and physics simulations.",
      "domains": [
        "D1",
        "D1-01"
      ],
      "subtask_category": [
        "layout_analysis",
        "parameter_extraction"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/purdue-onchip/gds2Para",
      "help_website": [],
      "license": "GPL-2.0",
      "tags": [
        "gdsii",
        "eda",
        "semiconductor",
        "layout-analysis"
      ],
      "id": 920
    },
    {
      "name": "QMiner",
      "one_line_profile": "Real-time large-scale data analytics platform",
      "detailed_description": "An analytics platform for processing real-time large-scale streams containing structured and unstructured data, suitable for data mining, sensor data analysis, and text mining tasks.",
      "domains": [
        "D1",
        "D1-04"
      ],
      "subtask_category": [
        "data_mining",
        "stream_analytics",
        "statistics"
      ],
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/qminer/qminer",
      "help_website": [
        "http://qminer.github.io/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "data-mining",
        "stream-processing",
        "analytics",
        "cpp"
      ],
      "id": 921
    },
    {
      "name": "eynollah",
      "one_line_profile": "Document layout analysis and segmentation tool",
      "detailed_description": "A tool for document layout analysis, capable of segmenting pages into regions (text, images, tables) and extracting structure, often used in OCR pipelines for scientific literature or historical documents.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "document_layout_analysis",
        "ocr_post_processing",
        "structure_extraction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/qurator-spk/eynollah",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "layout-analysis",
        "ocr",
        "document-processing",
        "deep-learning"
      ],
      "id": 922
    },
    {
      "name": "document-layout-analysis",
      "one_line_profile": "Simple document layout analysis using OpenCV",
      "detailed_description": "A Python-OpenCV based tool for analyzing document layouts, identifying blocks of text and images, useful for preprocessing scientific documents for data extraction.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "document_layout_analysis",
        "image_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/rbaguila/document-layout-analysis",
      "help_website": [],
      "license": null,
      "tags": [
        "opencv",
        "layout-analysis",
        "document-processing"
      ],
      "id": 923
    },
    {
      "name": "llm_data_parser",
      "one_line_profile": "LLM-based web data extraction tool",
      "detailed_description": "A proof-of-concept tool leveraging LLMs to identify and extract meaningful structured data from HTML content without heavy parsing, facilitating data collection for analysis.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "data_extraction",
        "html_parsing",
        "llm_inference"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/repollo/llm_data_parser",
      "help_website": [],
      "license": null,
      "tags": [
        "llm",
        "web-scraping",
        "data-extraction"
      ],
      "id": 924
    },
    {
      "name": "BotanicGarden",
      "one_line_profile": "Dataset for robot navigation in unstructured natural environments",
      "detailed_description": "A high-quality dataset designed for training and evaluating robot navigation algorithms in unstructured natural environments (botanic gardens), supporting robotics research.",
      "domains": [
        "D4",
        "D1"
      ],
      "subtask_category": [
        "robot_navigation",
        "dataset_access"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/robot-pesg/BotanicGarden",
      "help_website": [],
      "license": null,
      "tags": [
        "robotics",
        "dataset",
        "navigation",
        "unstructured-environment"
      ],
      "id": 925
    },
    {
      "name": "semtools",
      "one_line_profile": "Command line tools for semantic search and document parsing",
      "detailed_description": "A collection of CLI tools for semantic search, RAG (Retrieval-Augmented Generation), and document parsing, facilitating the processing of unstructured text for AI applications.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "semantic_search",
        "document_parsing",
        "rag"
      ],
      "application_level": "workflow",
      "primary_language": "Rust",
      "repo_url": "https://github.com/run-llama/semtools",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "semantic-search",
        "rag",
        "document-parsing",
        "rust"
      ],
      "id": 926
    },
    {
      "name": "TabularSemanticParsing",
      "one_line_profile": "Natural language to SQL semantic parsing model",
      "detailed_description": "Research code for translating natural language questions into structured query language (SQL), supporting semantic parsing tasks on tabular data.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "semantic_parsing",
        "text_to_sql",
        "nlp"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/salesforce/TabularSemanticParsing",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "nlp",
        "semantic-parsing",
        "sql",
        "deep-learning"
      ],
      "id": 927
    },
    {
      "name": "WikiSQL",
      "one_line_profile": "Large annotated semantic parsing corpus for Text-to-SQL",
      "detailed_description": "A large-scale dataset for developing natural language interfaces for relational databases, specifically for the task of semantic parsing (Text-to-SQL).",
      "domains": [
        "D4",
        "D1"
      ],
      "subtask_category": [
        "dataset_access",
        "semantic_parsing"
      ],
      "application_level": "dataset",
      "primary_language": "HTML",
      "repo_url": "https://github.com/salesforce/WikiSQL",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "dataset",
        "nlp",
        "text-to-sql",
        "semantic-parsing"
      ],
      "id": 928
    },
    {
      "name": "ingest",
      "one_line_profile": "CLI tool to parse files and websites for LLM ingestion",
      "detailed_description": "A command-line tool designed to parse code repositories and websites into a format suitable for ingestion by Large Language Models (LLMs), streamlining the data preparation workflow for AI.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "data_ingestion",
        "preprocessing",
        "llm_context_preparation"
      ],
      "application_level": "workflow",
      "primary_language": "Go",
      "repo_url": "https://github.com/sammcj/ingest",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "data-ingestion",
        "cli",
        "parsing"
      ],
      "id": 929
    },
    {
      "name": "HybridRAG",
      "one_line_profile": "Hybrid retrieval system combining vector and graph search",
      "detailed_description": "A Retrieval-Augmented Generation (RAG) system that integrates vector search with knowledge graph search to handle both structured and unstructured data for accurate LLM response generation.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "information_retrieval",
        "rag",
        "knowledge_graph"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/sarabesh/HybridRAG",
      "help_website": [],
      "license": null,
      "tags": [
        "rag",
        "knowledge-graph",
        "llm",
        "hybrid-search"
      ],
      "id": 930
    },
    {
      "name": "ACL-anthology-corpus",
      "one_line_profile": "ACL Anthology corpus data and extraction scripts",
      "detailed_description": "A repository providing the ACL Anthology corpus (computational linguistics research papers), including metadata, PDFs, and Grobid extractions, serving as a dataset for NLP research.",
      "domains": [
        "D4",
        "D1"
      ],
      "subtask_category": [
        "dataset_access",
        "nlp_corpus_processing"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/shauryr/ACL-anthology-corpus",
      "help_website": [],
      "license": null,
      "tags": [
        "nlp",
        "dataset",
        "acl-anthology",
        "corpus"
      ],
      "id": 931
    },
    {
      "name": "graph-parser",
      "one_line_profile": "Semantic parser for converting natural language to logical forms",
      "detailed_description": "A semantic parsing tool capable of converting natural language sentences into logical forms and graphs, supporting research in computational linguistics and knowledge representation.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "semantic_parsing",
        "knowledge_representation",
        "nlp"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/sivareddyg/graph-parser",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "semantic-parsing",
        "nlp",
        "logical-forms",
        "graphs"
      ],
      "id": 932
    },
    {
      "name": "SUQL",
      "one_line_profile": "Conversational search interface over hybrid structured and unstructured data",
      "detailed_description": "A formal language and framework for building conversational interfaces that can query both structured databases (SQL) and unstructured text corpora, useful for scientific knowledge retrieval.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "semantic_parsing",
        "text_to_sql",
        "information_retrieval"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/stanford-oval/suql",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "semantic-parsing",
        "nlp",
        "hybrid-retrieval"
      ],
      "id": 933
    },
    {
      "name": "TOUGH2Viewer",
      "one_line_profile": "3D visualization tool for TOUGH2 simulation grids",
      "detailed_description": "A Java-based visualization tool specifically designed for displaying structured and unstructured Voronoi grids used in TOUGH2 groundwater and heat flow simulations.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "scientific_visualization",
        "mesh_processing"
      ],
      "application_level": "solver",
      "primary_language": "Java",
      "repo_url": "https://github.com/stebond/TOUGH2Viewer",
      "help_website": [],
      "license": null,
      "tags": [
        "visualization",
        "geoscience",
        "tough2",
        "voronoi-grid"
      ],
      "id": 934
    },
    {
      "name": "Stencila",
      "one_line_profile": "Platform for executable and reproducible scientific documents",
      "detailed_description": "A platform and set of tools for creating, converting, and executing scientific documents, supporting conversion between formats like JATS XML, JSON-LD, Markdown, and Jupyter Notebooks.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "format_conversion",
        "reproducible_research",
        "document_parsing"
      ],
      "application_level": "platform",
      "primary_language": "Rust",
      "repo_url": "https://github.com/stencila/stencila",
      "help_website": [
        "https://stencila.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "reproducible-research",
        "document-conversion",
        "jats-xml",
        "scientific-publishing"
      ],
      "id": 935
    },
    {
      "name": "Galactic",
      "one_line_profile": "Data cleaning and curation tool for massive unstructured text datasets",
      "detailed_description": "A library designed for processing, cleaning, and curating large-scale unstructured text data, commonly used for preparing scientific literature datasets for LLM training.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "data_cleaning",
        "text_normalization",
        "dataset_curation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/taylorai/galactic",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "data-cleaning",
        "nlp",
        "llm-training",
        "unstructured-data"
      ],
      "id": 936
    },
    {
      "name": "UCNS3D",
      "one_line_profile": "Unstructured Compressible Navier-Stokes 3D CFD solver",
      "detailed_description": "A high-order accurate Computational Fluid Dynamics (CFD) code for solving unstructured compressible Navier-Stokes equations, used for scientific simulations in aerodynamics and fluid mechanics.",
      "domains": [
        "Physics",
        "Fluid Dynamics"
      ],
      "subtask_category": [
        "cfd_simulation",
        "fluid_dynamics_modeling"
      ],
      "application_level": "solver",
      "primary_language": "Fortran",
      "repo_url": "https://github.com/ucns3d-team/UCNS3D",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "cfd",
        "navier-stokes",
        "simulation"
      ],
      "id": 937
    },
    {
      "name": "GraphGPT",
      "one_line_profile": "Tool for extrapolating knowledge graphs from unstructured text using LLMs",
      "detailed_description": "A tool that leverages GPT models to convert unstructured text into structured knowledge graphs, facilitating the extraction of relationships and entities for scientific knowledge management.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "knowledge_graph_construction",
        "information_extraction"
      ],
      "application_level": "workflow",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/varunshenoy/GraphGPT",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "knowledge-graph",
        "llm",
        "unstructured-data"
      ],
      "id": 938
    },
    {
      "name": "ocrsegment",
      "one_line_profile": "Deep learning model for page layout analysis and segmentation",
      "detailed_description": "A deep learning based model specifically designed for document layout analysis and page segmentation, enabling the structural parsing of document images.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "layout_analysis",
        "document_segmentation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/watersink/ocrsegment",
      "help_website": [],
      "license": null,
      "tags": [
        "layout-analysis",
        "ocr",
        "segmentation"
      ],
      "id": 939
    },
    {
      "name": "knowledge-table",
      "one_line_profile": "Library for extracting structured data from unstructured documents",
      "detailed_description": "An open-source package designed to simplify the process of extracting structured data (tables, entities) from unstructured documents, facilitating data harmonization in scientific workflows.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "information_extraction",
        "structured_data_generation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/whyhow-ai/knowledge-table",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "extraction",
        "unstructured-data",
        "parsing"
      ],
      "id": 940
    },
    {
      "name": "e2m",
      "one_line_profile": "Universal document to Markdown converter for LLM processing",
      "detailed_description": "A comprehensive tool to convert various file formats (PDF, DOCX, EPUB, etc.) into Markdown, specifically optimized for feeding unstructured data into LLMs for analysis and parsing.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "document_conversion",
        "text_extraction"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/wisupai/e2m",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "markdown-converter",
        "document-parsing",
        "llm-preprocessing"
      ],
      "id": 941
    },
    {
      "name": "extractous",
      "one_line_profile": "High-performance unstructured data extraction library",
      "detailed_description": "A fast and efficient library written in Rust for extracting text and metadata from various unstructured data formats, serving as a foundational tool for scientific data ingestion pipelines.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "data_extraction",
        "etl"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/yobix-ai/extractous",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "data-extraction",
        "rust",
        "unstructured-data"
      ],
      "id": 942
    },
    {
      "name": "RoDLA",
      "one_line_profile": "Benchmark toolkit for Document Layout Analysis robustness",
      "detailed_description": "A benchmarking toolkit and dataset designed to evaluate the robustness of Document Layout Analysis (DLA) models, essential for validating tools used in scientific document parsing.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "benchmarking",
        "layout_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/yufanchen96/RoDLA",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "benchmark",
        "document-layout-analysis",
        "robustness"
      ],
      "id": 943
    },
    {
      "name": "faster-nougat",
      "one_line_profile": "Optimized local implementation of the Nougat model for parsing academic PDFs",
      "detailed_description": "An efficient, local implementation of the Nougat (Neural Optical Understanding for Academic Documents) model. It utilizes Transformer-based architecture to parse unstructured scientific documents (PDFs) and convert them into structured Markdown or LaTeX formats, specifically handling mathematical formulas and tables found in academic literature.",
      "domains": [
        "D1",
        "D1-05"
      ],
      "subtask_category": [
        "document_parsing",
        "optical_character_recognition",
        "formula_extraction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/zhuzilin/faster-nougat",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pdf-parsing",
        "scientific-literature",
        "ocr",
        "academic-documents",
        "nougat"
      ],
      "id": 944
    }
  ]
}
{
  "leaf_cluster_name": "科研文献-检索/解析/引用网络生态",
  "domain": "Sci Knowledge",
  "typical_objects": "PDFs/citations",
  "task_chain": "解析→抽取→索引→检索→评测",
  "tool_form": "解析器 + 检索 + 评测",
  "total_tools": 915,
  "tools": [
    {
      "name": "GNN-TableExtraction",
      "one_line_profile": "Graph Neural Networks based table extraction implementation for PDF documents",
      "detailed_description": "Implementation of the ICPR2022 paper 'Graph Neural Networks and Representation Embedding for table extraction in PDF Documents', providing a deep learning approach to identify and extract tabular structures from scientific PDFs.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "table_extraction",
        "structure_recognition"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/AILab-UniFI/GNN-TableExtraction",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "gnn",
        "table-extraction",
        "pdf-parsing",
        "deep-learning"
      ],
      "id": 1
    },
    {
      "name": "CTE-Dataset",
      "one_line_profile": "Contextualized Table Extraction Dataset for document analysis",
      "detailed_description": "A dataset designed for the task of contextualized table extraction, supporting the development and benchmarking of models that extract tables along with their surrounding context from documents.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "table_extraction",
        "dataset_creation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/AILab-UniFI/cte-dataset",
      "help_website": [],
      "license": null,
      "tags": [
        "dataset",
        "table-extraction",
        "document-analysis"
      ],
      "id": 2
    },
    {
      "name": "PDF_Form_OCR",
      "one_line_profile": "Table recognition and content extraction tool for PDF files",
      "detailed_description": "A Python-based tool for extracting content and recognizing tables within PDF documents, facilitating the digitization of structured data from scanned or native PDF forms.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "table_recognition",
        "pdf_extraction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/AlsoSprachZarathushtra/PDF_Form_OCR",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ocr",
        "table-recognition",
        "pdf-extraction"
      ],
      "id": 3
    },
    {
      "name": "TableNet",
      "one_line_profile": "Deep learning model for end-to-end table detection and extraction",
      "detailed_description": "An implementation of the TableNet architecture, an end-to-end deep learning model designed to detect tabular regions and recognize table structures (rows and columns) directly from scanned document images.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "table_detection",
        "structure_recognition"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/AmanSavaria1402/TableNet",
      "help_website": [],
      "license": null,
      "tags": [
        "deep-learning",
        "table-detection",
        "ocr",
        "document-analysis"
      ],
      "id": 4
    },
    {
      "name": "PTC-Mathcad-to-LaTeX-parser",
      "one_line_profile": "Converter for PTC Mathcad files to LaTeX format",
      "detailed_description": "A Python application that parses PTC Mathcad 15 files and converts the mathematical content into LaTeX, enabling the migration of engineering calculations to scientific publishing formats.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "format_conversion",
        "latex_generation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ArtificialTruth/PTC-Mathcad-to-LaTeX-parser",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "mathcad",
        "latex",
        "converter",
        "engineering-tools"
      ],
      "id": 5
    },
    {
      "name": "TableExtractor-Advanced-PDF-Table-Extraction",
      "one_line_profile": "Advanced PDF table extraction using OCR and image processing",
      "detailed_description": "A Python project leveraging OCR and image processing techniques to extract tabular data from scanned PDF documents, addressing challenges in digitizing unstructured scientific data.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "table_extraction",
        "ocr"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/Baskar-forever/TableExtractor-Advanced-PDF-Table-Extraction",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pdf",
        "table-extraction",
        "ocr",
        "image-processing"
      ],
      "id": 6
    },
    {
      "name": "GIANT-Dataset",
      "one_line_profile": "Large-scale synthetic bibliographic reference string dataset and generator",
      "detailed_description": "A massive dataset of 1 billion annotated synthetic bibliographic reference strings, accompanied by scripts to generate tagged XML citation strings, used for training and evaluating citation parsing models.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "citation_parsing",
        "dataset_creation"
      ],
      "application_level": "dataset",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/BeelGroup/GIANT-The-1-Billion-Annotated-Synthetic-Bibliographic-Reference-String-Dataset",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "citation-parsing",
        "bibliometrics",
        "dataset",
        "synthetic-data"
      ],
      "id": 7
    },
    {
      "name": "BADLAD",
      "one_line_profile": "Bengali Document Layout Analysis Dataset",
      "detailed_description": "A dataset specifically designed for document layout analysis of Bengali documents, supporting the development of OCR and layout recognition models for non-Latin scripts.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "layout_analysis",
        "dataset_creation"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/BengaliAI/BADLAD",
      "help_website": [],
      "license": null,
      "tags": [
        "document-layout-analysis",
        "dataset",
        "ocr",
        "bengali"
      ],
      "id": 8
    },
    {
      "name": "DocumentLayoutAnalysis",
      "one_line_profile": "C# resources and algorithms for document layout analysis",
      "detailed_description": "A collection of algorithms and resources for performing document layout analysis (DLA) using C# and PdfPig, including implementations of various page segmentation and reading order detection methods.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "layout_analysis",
        "page_segmentation"
      ],
      "application_level": "library",
      "primary_language": "C#",
      "repo_url": "https://github.com/BobLd/DocumentLayoutAnalysis",
      "help_website": [],
      "license": null,
      "tags": [
        "c-sharp",
        "pdfpig",
        "document-layout-analysis",
        "pdf-parsing"
      ],
      "id": 9
    },
    {
      "name": "camelot-sharp",
      "one_line_profile": "C# port of the Camelot PDF table extraction library",
      "detailed_description": "A C# library for extracting tabular data from PDFs, ported from the popular Python 'Camelot' library, utilizing PdfPig for underlying PDF operations.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "table_extraction",
        "pdf_parsing"
      ],
      "application_level": "library",
      "primary_language": "C#",
      "repo_url": "https://github.com/BobLd/camelot-sharp",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "c-sharp",
        "table-extraction",
        "pdf",
        "camelot"
      ],
      "id": 10
    },
    {
      "name": "tabula-sharp",
      "one_line_profile": "C# port of the Tabula PDF table extraction library",
      "detailed_description": "A C# library for extracting tables from PDF files, ported from 'tabula-java', enabling .NET developers to integrate table extraction capabilities into their applications.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "table_extraction",
        "pdf_parsing"
      ],
      "application_level": "library",
      "primary_language": "C#",
      "repo_url": "https://github.com/BobLd/tabula-sharp",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "c-sharp",
        "table-extraction",
        "pdf",
        "tabula"
      ],
      "id": 11
    },
    {
      "name": "any-parser",
      "one_line_profile": "Configurable document retrieval and parsing LLM tool",
      "detailed_description": "A tool designed to accurately parse and retrieve information from documents (PDFs, images) using Large Language Models, focusing on privacy and configurability for unstructured data extraction.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "document_parsing",
        "information_retrieval"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/CambioML/any-parser",
      "help_website": [],
      "license": null,
      "tags": [
        "llm",
        "document-parsing",
        "pdf",
        "ocr"
      ],
      "id": 12
    },
    {
      "name": "uniflow",
      "one_line_profile": "LLM-based workflow for PDF extraction, cleaning, and clustering",
      "detailed_description": "A unified interface to extract text from unstructured data (PDFs, Word, HTML) using LLMs, clean the data, and transform it into structured formats for downstream R&D tasks.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "text_extraction",
        "data_cleaning",
        "clustering"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/CambioML/uniflow-llm-based-pdf-extraction-text-cleaning-data-clustering",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "pdf-extraction",
        "etl",
        "unstructured-data"
      ],
      "id": 13
    },
    {
      "name": "publaynet-models",
      "one_line_profile": "Pre-trained Detectron2 models for document layout analysis",
      "detailed_description": "A collection of Detectron2 object detection models trained on the PubLayNet dataset, ready for use in document layout analysis tasks to identify text, figures, tables, and lists.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "layout_analysis",
        "object_detection"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/CaseDrive/publaynet-models",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "detectron2",
        "publaynet",
        "document-layout-analysis",
        "deep-learning"
      ],
      "id": 14
    },
    {
      "name": "text-extract-api",
      "one_line_profile": "API for extracting structured data from documents using OCR and LLMs",
      "detailed_description": "A comprehensive API solution for parsing and extracting text from various document formats (PDF, Word, PPTX) into structured JSON or Markdown, utilizing modern OCRs and Ollama-supported models.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "document_parsing",
        "ocr",
        "text_extraction"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/CatchTheTornado/text-extract-api",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ocr",
        "pdf-parsing",
        "llm",
        "api"
      ],
      "id": 15
    },
    {
      "name": "CERMINE",
      "one_line_profile": "Java library for extracting metadata and content from scientific PDFs",
      "detailed_description": "Content ExtRactor and MINEr (CERMINE) is a comprehensive Java library and web service for extracting metadata, bibliographic references, and structured full text from scientific articles in PDF format.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "metadata_extraction",
        "citation_parsing",
        "structure_recognition"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/CeON/CERMINE",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "pdf-extraction",
        "metadata-extraction",
        "bibliometrics",
        "java"
      ],
      "id": 16
    },
    {
      "name": "eparse",
      "one_line_profile": "Excel spreadsheet crawler and table parser",
      "detailed_description": "A Python library designed to crawl and parse tables from Excel spreadsheets, facilitating the extraction of structured scientific data often stored in spreadsheet formats.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "data_extraction",
        "spreadsheet_parsing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ChrisPappalardo/eparse",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "excel",
        "data-extraction",
        "parser",
        "python"
      ],
      "id": 17
    },
    {
      "name": "gptpdf",
      "one_line_profile": "PDF parsing tool utilizing GPT models",
      "detailed_description": "A tool that leverages GPT models (like GPT-4o) to parse PDF documents, converting them into structured formats like Markdown, suitable for ingesting scientific literature into RAG systems.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "pdf_parsing",
        "text_extraction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/CosmosShadow/gptpdf",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gpt",
        "pdf-parsing",
        "rag",
        "llm"
      ],
      "id": 18
    },
    {
      "name": "pdf_table",
      "one_line_profile": "Unified toolkit for deep learning-based table extraction",
      "detailed_description": "A toolkit that integrates various deep learning models and techniques for extracting tables from PDF documents, aiming to provide a unified interface for table detection and structure recognition.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "table_extraction",
        "deep_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/CycloneBoy/pdf_table",
      "help_website": [],
      "license": null,
      "tags": [
        "table-extraction",
        "pdf",
        "deep-learning",
        "toolkit"
      ],
      "id": 19
    },
    {
      "name": "dsRAG",
      "one_line_profile": "High-performance retrieval engine for unstructured data",
      "detailed_description": "A retrieval engine designed for unstructured data, facilitating the creation of RAG (Retrieval-Augmented Generation) pipelines for scientific literature and other document collections.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "information_retrieval",
        "rag"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/D-Star-AI/dsRAG",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "retrieval",
        "unstructured-data",
        "llm"
      ],
      "id": 20
    },
    {
      "name": "PySysML2",
      "one_line_profile": "Python parser for SysML 2.0 textual modeling language",
      "detailed_description": "A Python-based parser for the SysML 2.0 textual modeling language, enabling the transformation of system models into Python objects for analysis and data science applications in systems engineering.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "modeling_language_parsing",
        "systems_engineering"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/DAF-Digital-Transformation-Office/PySysML2",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "sysml",
        "parser",
        "systems-engineering",
        "modeling"
      ],
      "id": 21
    },
    {
      "name": "DawDreamer",
      "one_line_profile": "Python-based Digital Audio Workstation with JAX support",
      "detailed_description": "A library enabling audio generation and processing in Python, supporting VSTs and JAX for differentiable signal processing, applicable in audio research and machine learning.",
      "domains": [
        "G1"
      ],
      "subtask_category": [
        "signal_processing",
        "audio_synthesis"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/DBraun/DawDreamer",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "audio-processing",
        "jax",
        "signal-processing",
        "python"
      ],
      "id": 22
    },
    {
      "name": "refchaser",
      "one_line_profile": "Tool for checking reference lists in systematic reviews",
      "detailed_description": "A Python tool designed to assist in systematic reviews and literature reviews by checking reference lists, performing backward/forward searching, and ranking articles by relevance.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "literature_review",
        "citation_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/DQ-Zhang/refchaser",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "systematic-review",
        "citation-analysis",
        "literature-search"
      ],
      "id": 23
    },
    {
      "name": "DocLayNet",
      "one_line_profile": "Large human-annotated dataset for document layout analysis",
      "detailed_description": "A large-scale, human-annotated dataset for document layout analysis, covering diverse document types to support the training of robust layout recognition models.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "layout_analysis",
        "dataset_creation"
      ],
      "application_level": "dataset",
      "primary_language": null,
      "repo_url": "https://github.com/DS4SD/DocLayNet",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "dataset",
        "document-layout-analysis",
        "computer-vision"
      ],
      "id": 24
    },
    {
      "name": "SemTabNet",
      "one_line_profile": "Code for universal information extraction from tables using LLMs",
      "detailed_description": "Repository containing code for the ACL paper on universal information extraction from tables, leveraging Large Language Models to extract structured data like ESG KPIs.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "table_extraction",
        "information_extraction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/DS4SD/SemTabNet",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "table-extraction",
        "llm",
        "information-extraction",
        "esg"
      ],
      "id": 25
    },
    {
      "name": "MarkPDFdown",
      "one_line_profile": "High-quality PDF to Markdown conversion tool based on LLM visual recognition",
      "detailed_description": "A tool that leverages large language model visual recognition capabilities to convert PDF documents into high-quality Markdown format, preserving layout and structure.",
      "domains": [
        "Sci Knowledge",
        "G1-01"
      ],
      "subtask_category": [
        "document_parsing",
        "pdf_to_markdown"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/MarkPDFdown/markpdfdown",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "pdf-parser",
        "llm",
        "markdown",
        "ocr"
      ],
      "id": 26
    },
    {
      "name": "Arabic Nougat",
      "one_line_profile": "Fine-tuned Nougat model for Arabic document parsing",
      "detailed_description": "A specialized version of the Nougat model fine-tuned for parsing and extracting text from Arabic PDF documents.",
      "domains": [
        "Sci Knowledge",
        "G1-01"
      ],
      "subtask_category": [
        "document_parsing",
        "ocr"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/MohamedAliRashad/arabic-nougat",
      "help_website": [],
      "license": null,
      "tags": [
        "arabic",
        "nougat",
        "pdf-parser",
        "ocr"
      ],
      "id": 27
    },
    {
      "name": "nv-ingest",
      "one_line_profile": "Scalable microservice for document content and metadata extraction",
      "detailed_description": "A scalable, performance-oriented microservice by NVIDIA for extracting text, tables, charts, and images from documents, designed for generative AI and RAG workflows.",
      "domains": [
        "Sci Knowledge",
        "G1-01"
      ],
      "subtask_category": [
        "document_parsing",
        "data_ingestion",
        "rag"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/NVIDIA/nv-ingest",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "document-extraction",
        "rag",
        "nvidia-nim",
        "pdf-parsing"
      ],
      "id": 28
    },
    {
      "name": "docext",
      "one_line_profile": "OCR-free unstructured data extraction and markdown conversion toolkit",
      "detailed_description": "An on-premises toolkit for extracting unstructured data from documents and converting them to markdown without relying on traditional OCR, suitable for benchmarking.",
      "domains": [
        "Sci Knowledge",
        "G1-01"
      ],
      "subtask_category": [
        "document_parsing",
        "data_extraction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NanoNets/docext",
      "help_website": [
        "https://idp-leaderboard.org/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "document-extraction",
        "markdown",
        "pdf-parsing"
      ],
      "id": 29
    },
    {
      "name": "docstrange",
      "one_line_profile": "Intelligent structured data extraction from various document formats",
      "detailed_description": "A tool to extract and convert data from documents (PDFs, images, Word, etc.) into structured formats like Markdown, JSON, and CSV using advanced OCR and extraction techniques.",
      "domains": [
        "Sci Knowledge",
        "G1-01"
      ],
      "subtask_category": [
        "document_parsing",
        "structured_extraction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NanoNets/docstrange",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ocr",
        "data-extraction",
        "pdf-to-json",
        "document-processing"
      ],
      "id": 30
    },
    {
      "name": "nougat-latex-ocr",
      "one_line_profile": "Codebase for fine-tuning and evaluating Nougat-based image-to-LaTeX models",
      "detailed_description": "A repository providing tools and code for fine-tuning and evaluating Nougat-based models specifically for converting document images to LaTeX.",
      "domains": [
        "Sci Knowledge",
        "G1-01"
      ],
      "subtask_category": [
        "model_training",
        "document_parsing",
        "latex_generation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NormXU/nougat-latex-ocr",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "nougat",
        "latex-ocr",
        "fine-tuning",
        "document-processing"
      ],
      "id": 31
    },
    {
      "name": "General-Documents-Layout-parser",
      "one_line_profile": "Tool for general document layout analysis and parsing",
      "detailed_description": "A Python-based tool for analyzing document layouts and parsing content, with support for Chinese documents.",
      "domains": [
        "Sci Knowledge",
        "G1-01"
      ],
      "subtask_category": [
        "layout_analysis",
        "document_parsing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/OKC13/General-Documents-Layout-parser",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "layout-analysis",
        "document-parsing",
        "pdf-parser"
      ],
      "id": 32
    },
    {
      "name": "MegaParse",
      "one_line_profile": "File parser optimized for LLM ingestion without information loss",
      "detailed_description": "A powerful file parsing tool designed to convert PDFs, Docx, and PPTx into formats ideal for Large Language Model (LLM) ingestion, ensuring no data loss.",
      "domains": [
        "Sci Knowledge",
        "G1-01"
      ],
      "subtask_category": [
        "document_parsing",
        "data_ingestion",
        "llm_preprocessing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/QuivrHQ/MegaParse",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "pdf-parser",
        "llm",
        "data-ingestion",
        "document-processing"
      ],
      "id": 33
    },
    {
      "name": "RapidLaTeXOCR",
      "one_line_profile": "Efficient formula recognition tool based on LaTeX-OCR and ONNXRuntime",
      "detailed_description": "A tool for recognizing mathematical formulas from images and converting them to LaTeX, optimized for speed using ONNXRuntime.",
      "domains": [
        "Sci Knowledge",
        "G1-01"
      ],
      "subtask_category": [
        "formula_recognition",
        "ocr"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/RapidAI/RapidLaTeXOCR",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "latex-ocr",
        "formula-recognition",
        "onnx"
      ],
      "id": 34
    },
    {
      "name": "Spotlight",
      "one_line_profile": "Interactive exploration and quality control tool for unstructured datasets",
      "detailed_description": "A tool for interactively exploring, visualizing, and curating unstructured datasets (images, audio, text) directly from dataframes, useful for scientific data quality control.",
      "domains": [
        "Sci Knowledge",
        "G1-01"
      ],
      "subtask_category": [
        "data_visualization",
        "quality_control",
        "data_exploration"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/Renumics/spotlight",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "data-visualization",
        "unstructured-data",
        "quality-control",
        "eda"
      ],
      "id": 35
    },
    {
      "name": "MarkEverythingDown",
      "one_line_profile": "Multimodal LLM-based tool to convert various files to Markdown",
      "detailed_description": "A tool that uses multimodal Large Language Models to convert a wide range of file formats (PDF, images, Word, Excel, etc.) into Markdown.",
      "domains": [
        "Sci Knowledge",
        "G1-01"
      ],
      "subtask_category": [
        "document_parsing",
        "multimodal_conversion"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/RoffyS/MarkEverythingDown",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pdf-to-markdown",
        "multimodal-llm",
        "document-conversion"
      ],
      "id": 36
    },
    {
      "name": "SCOREC core",
      "one_line_profile": "Parallel finite element unstructured mesh management library",
      "detailed_description": "A library for managing parallel finite element unstructured meshes, supporting adaptive mesh refinement and load balancing for scientific simulations.",
      "domains": [
        "Physics",
        "Engineering"
      ],
      "subtask_category": [
        "mesh_generation",
        "finite_element_method"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/SCOREC/core",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "fem",
        "mesh-generation",
        "parallel-computing"
      ],
      "id": 37
    },
    {
      "name": "ThermoParser",
      "one_line_profile": "Data analysis and visualization tool for thermoelectrics",
      "detailed_description": "A tool designed to streamline data analysis and visualization for thermoelectrics and charge carrier transport in computational materials science.",
      "domains": [
        "Materials Science"
      ],
      "subtask_category": [
        "data_analysis",
        "visualization",
        "thermoelectrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/SMTG-Bham/ThermoParser",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "materials-science",
        "thermoelectrics",
        "data-analysis"
      ],
      "id": 38
    },
    {
      "name": "SPECFEM3D Cartesian",
      "one_line_profile": "Seismic wave propagation simulator",
      "detailed_description": "A software package that simulates acoustic, elastic, coupled acoustic/elastic, or poroelastic seismic wave propagation in any type of conforming mesh of hexahedra.",
      "domains": [
        "Geophysics",
        "Seismology"
      ],
      "subtask_category": [
        "simulation",
        "wave_propagation"
      ],
      "application_level": "solver",
      "primary_language": "Fortran",
      "repo_url": "https://github.com/SPECFEM/specfem3d",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "seismology",
        "wave-propagation",
        "simulation",
        "fem"
      ],
      "id": 39
    },
    {
      "name": "Table-Detection-using-Deep-learning",
      "one_line_profile": "Deep learning based table detection and extraction using Tensorflow and Luminoth",
      "detailed_description": "A deep learning implementation for detecting and extracting tabular data from documents, utilizing the Luminoth toolkit and Tensorflow.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "table_extraction",
        "document_parsing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Sargunan/Table-Detection-using-Deep-learning",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "table-detection",
        "deep-learning",
        "pdf-parsing"
      ],
      "id": 40
    },
    {
      "name": "VirtualMarker",
      "one_line_profile": "3D Human Mesh Estimation from Virtual Markers",
      "detailed_description": "Official PyTorch implementation of the CVPR 2023 paper for estimating 3D human meshes using virtual markers, serving as a scientific inference tool in computer vision.",
      "domains": [
        "Computer Vision",
        "Scientific Modeling"
      ],
      "subtask_category": [
        "human_mesh_estimation",
        "3d_reconstruction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ShirleyMaxx/VirtualMarker",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "cvpr",
        "3d-human-mesh",
        "pytorch"
      ],
      "id": 41
    },
    {
      "name": "Tabular-LLM",
      "one_line_profile": "Datasets and fine-tuning scripts for Table-related LLM tasks",
      "detailed_description": "A project collecting tabular intelligence datasets and providing instruction fine-tuning formats to enhance Large Language Models' understanding of tabular data.",
      "domains": [
        "G1",
        "Scientific Data Processing"
      ],
      "subtask_category": [
        "table_understanding",
        "llm_finetuning"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/SpursGoZmy/Tabular-LLM",
      "help_website": [],
      "license": null,
      "tags": [
        "tabular-data",
        "llm",
        "fine-tuning"
      ],
      "id": 42
    },
    {
      "name": "Merlin",
      "one_line_profile": "3D Vision Language Model for Computed Tomography",
      "detailed_description": "A 3D VLM leveraging structured EHR and unstructured radiology reports for pretraining on CT scans, serving medical imaging research.",
      "domains": [
        "Medical AI",
        "Scientific Modeling"
      ],
      "subtask_category": [
        "medical_imaging",
        "vlm"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/StanfordMIMI/Merlin",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ct-scan",
        "medical-ai",
        "vision-language-model"
      ],
      "id": 43
    },
    {
      "name": "table-transformer",
      "one_line_profile": "Table extraction tool combining OCR and Computer Vision",
      "detailed_description": "An open-source solution for extracting structured tabular data from images using OCR and computer vision techniques, suitable for LLM preprocessing.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "table_extraction",
        "ocr"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Sudhanshu1304/table-transformer",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "table-extraction",
        "ocr",
        "computer-vision"
      ],
      "id": 44
    },
    {
      "name": "Extractable",
      "one_line_profile": "Library for table extraction from documents",
      "detailed_description": "A library designed to extract tabular data from documents, facilitating data mining and analysis workflows.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "table_extraction"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/SuleyNL/Extractable",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "table-extraction",
        "pdf-mining"
      ],
      "id": 45
    },
    {
      "name": "TrialAndErrorOrg-parsers",
      "one_line_profile": "Suite of unified-compatible converters for scientific documents",
      "detailed_description": "A monorepo containing converters for transforming between formats like .docx, JATS XML, LaTeX, and PDF, specifically targeting scientific publishing workflows.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "document_conversion",
        "jats_parsing"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/TrialAndErrorOrg/parsers",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "jats-xml",
        "latex",
        "pdf-conversion"
      ],
      "id": 46
    },
    {
      "name": "PagePlus",
      "one_line_profile": "PAGE XML processing script for document layout analysis",
      "detailed_description": "A tool from UB Mannheim for processing PAGE XML files, performing validation, repair, and modification of text regions for OCR and layout analysis.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "layout_analysis",
        "ocr_postprocessing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/UB-Mannheim/PagePlus",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "page-xml",
        "ocr",
        "layout-analysis"
      ],
      "id": 47
    },
    {
      "name": "science_parse_py_api",
      "one_line_profile": "Python API wrapper for Science Parse",
      "detailed_description": "A Python interface for the Science Parse tool, developed by UCREL, facilitating the extraction of metadata and content from scientific PDFs.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "pdf_parsing",
        "metadata_extraction"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/UCREL/science_parse_py_api",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "science-parse",
        "pdf-extraction"
      ],
      "id": 48
    },
    {
      "name": "uxarray",
      "one_line_profile": "Xarray extension for unstructured climate data analysis",
      "detailed_description": "A Python package extending Xarray to support unstructured grid data, specifically designed for climate and global weather data analysis and visualization.",
      "domains": [
        "Climate Science",
        "Scientific Data Analysis"
      ],
      "subtask_category": [
        "data_analysis",
        "visualization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/UXARRAY/uxarray",
      "help_website": [
        "https://uxarray.readthedocs.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "climate-data",
        "unstructured-grids",
        "xarray"
      ],
      "id": 49
    },
    {
      "name": "fiducials",
      "one_line_profile": "Simultaneous localization and mapping using fiducial markers",
      "detailed_description": "A system for robot localization and mapping using fiducial markers, applicable in robotics research and scientific instrumentation tracking.",
      "domains": [
        "Robotics",
        "Scientific Inference"
      ],
      "subtask_category": [
        "slam",
        "localization"
      ],
      "application_level": "solver",
      "primary_language": "C",
      "repo_url": "https://github.com/UbiquityRobotics/fiducials",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "slam",
        "robotics",
        "fiducial-markers"
      ],
      "id": 50
    },
    {
      "name": "unstructured",
      "one_line_profile": "ETL solution for transforming unstructured documents into structured data",
      "detailed_description": "A comprehensive library for ingesting and processing unstructured documents (PDFs, HTML, etc.) to prepare them for Large Language Models and scientific data mining.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "document_parsing",
        "etl"
      ],
      "application_level": "library",
      "primary_language": "HTML",
      "repo_url": "https://github.com/Unstructured-IO/unstructured",
      "help_website": [
        "https://unstructured.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "etl",
        "pdf-parsing",
        "llm-preprocessing"
      ],
      "id": 51
    },
    {
      "name": "texify",
      "one_line_profile": "Math OCR model converting images to LaTeX and Markdown",
      "detailed_description": "A deep learning model designed to recognize mathematical equations and text in images and convert them into LaTeX or Markdown format.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "ocr",
        "latex_generation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/VikParuchuri/texify",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "math-ocr",
        "latex",
        "deep-learning"
      ],
      "id": 52
    },
    {
      "name": "pdftabextract",
      "one_line_profile": "Tools for extracting tables from PDF files",
      "detailed_description": "A set of tools developed by WZB Social Science Center for mining data from tables in scanned or OCR-processed PDF documents.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "table_extraction",
        "data_mining"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/WZBSocialScienceCenter/pdftabextract",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "pdf-mining",
        "table-extraction",
        "ocr-postprocessing"
      ],
      "id": 53
    },
    {
      "name": "HE2LaTeX",
      "one_line_profile": "Converting handwritten equations to LaTeX",
      "detailed_description": "A tool for recognizing handwritten mathematical equations and converting them into LaTeX code, facilitating digitization of scientific notes.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "ocr",
        "handwriting_recognition"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/Wikunia/HE2LaTeX",
      "help_website": [],
      "license": null,
      "tags": [
        "handwriting-recognition",
        "latex",
        "math-ocr"
      ],
      "id": 54
    },
    {
      "name": "Document-Layout-Analysis",
      "one_line_profile": "Tools for extracting figures, tables, and text from PDFs",
      "detailed_description": "A collection of tools for analyzing document layout and extracting structural elements like figures and tables from PDF documents.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "layout_analysis",
        "extraction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Wild-Rift/Document-Layout-Analysis",
      "help_website": [],
      "license": null,
      "tags": [
        "pdf-extraction",
        "layout-analysis"
      ],
      "id": 55
    },
    {
      "name": "GEM",
      "one_line_profile": "Online Globally consistent dense elevation mapping",
      "detailed_description": "A system for generating globally consistent dense elevation maps for unstructured terrain, used in robotics and terrain analysis.",
      "domains": [
        "Robotics",
        "Scientific Mapping"
      ],
      "subtask_category": [
        "mapping",
        "terrain_analysis"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/ZJU-Robotics-Lab/GEM",
      "help_website": [],
      "license": null,
      "tags": [
        "elevation-mapping",
        "robotics",
        "slam"
      ],
      "id": 56
    },
    {
      "name": "Citr",
      "one_line_profile": "Library for parsing citations between Markdown and CSL JSON",
      "detailed_description": "A library from the Zettlr ecosystem for parsing and converting citation data between Markdown and CSL JSON formats, supporting scientific writing workflows.",
      "domains": [
        "G1",
        "Citation Analysis"
      ],
      "subtask_category": [
        "citation_parsing",
        "reference_management"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/Zettlr/Citr",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "citation-parsing",
        "csl-json",
        "markdown"
      ],
      "id": 57
    },
    {
      "name": "unstract",
      "one_line_profile": "Platform to structure unstructured documents via LLM pipelines",
      "detailed_description": "A platform for building ETL pipelines that use Large Language Models to extract structured data from unstructured documents, suitable for scientific knowledge base construction.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "etl",
        "document_structuring"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/Zipstack/unstract",
      "help_website": [
        "https://unstract.com"
      ],
      "license": "AGPL-3.0",
      "tags": [
        "llm-etl",
        "unstructured-data",
        "pipeline"
      ],
      "id": 58
    },
    {
      "name": "TableExtraction",
      "one_line_profile": "Line-based framework for tabular data extraction",
      "detailed_description": "A framework using computer vision and Tesseract OCR to detect and extract tabular data from raster images into JSON format.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "table_extraction",
        "ocr"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/abdullahibneat/TableExtraction",
      "help_website": [],
      "license": null,
      "tags": [
        "table-detection",
        "ocr",
        "computer-vision"
      ],
      "id": 59
    },
    {
      "name": "marker-api",
      "one_line_profile": "API wrapper for the Marker PDF-to-Markdown conversion engine",
      "detailed_description": "A deployable API service that wraps the Marker library to convert PDF documents into markdown format with high accuracy, facilitating the integration of PDF parsing capabilities into other applications.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "pdf_parsing",
        "format_conversion"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/adithya-s-k/marker-api",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "pdf-to-markdown",
        "api",
        "document-parsing"
      ],
      "id": 60
    },
    {
      "name": "npm-pdfreader",
      "one_line_profile": "Node.js library for parsing text and tables from PDFs",
      "detailed_description": "A JavaScript library designed to parse text and tabular data from PDF files, enabling automated extraction of content for further processing in web-based or Node.js environments.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "pdf_parsing",
        "text_extraction"
      ],
      "application_level": "library",
      "primary_language": "HTML",
      "repo_url": "https://github.com/adrienjoly/npm-pdfreader",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pdf-parser",
        "nodejs",
        "table-extraction"
      ],
      "id": 61
    },
    {
      "name": "PDFs-TextExtract",
      "one_line_profile": "Tool for bulk text extraction from large PDF collections",
      "detailed_description": "A Python-based utility designed to handle the extraction of text from multiple and large PDF documents, suitable for building corpora from scientific literature or technical reports.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "pdf_parsing",
        "text_extraction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ahmedkhemiri95/PDFs-TextExtract",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "pdf",
        "text-mining",
        "bulk-processing"
      ],
      "id": 62
    },
    {
      "name": "CCTag",
      "one_line_profile": "Fiducial marker detection library for computer vision",
      "detailed_description": "A library for the detection of CCTag markers (concentric circles), widely used in photogrammetry, computer vision, and augmented reality for precise tracking and measurement.",
      "domains": [
        "Computer Vision",
        "Photogrammetry"
      ],
      "subtask_category": [
        "marker_detection",
        "image_processing"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/alicevision/CCTag",
      "help_website": [],
      "license": "MPL-2.0",
      "tags": [
        "computer-vision",
        "fiducial-markers",
        "photogrammetry"
      ],
      "id": 63
    },
    {
      "name": "science-parse",
      "one_line_profile": "Scientific paper parser converting PDFs to structured JSON",
      "detailed_description": "A Java-based tool developed by AllenAI to parse scientific papers in PDF format and extract structured information such as titles, authors, abstracts, and references into JSON format.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "pdf_parsing",
        "metadata_extraction"
      ],
      "application_level": "solver",
      "primary_language": "Java",
      "repo_url": "https://github.com/allenai/science-parse",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "pdf-parsing",
        "scientific-literature",
        "allenai"
      ],
      "id": 64
    },
    {
      "name": "spv2",
      "one_line_profile": "Version 2 of the Science Parse tool for PDF extraction",
      "detailed_description": "The second iteration of AllenAI's Science Parse, designed to improve the extraction of structured data from scientific PDF documents.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "pdf_parsing",
        "metadata_extraction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/allenai/spv2",
      "help_website": [],
      "license": null,
      "tags": [
        "pdf-parsing",
        "scientific-literature",
        "allenai"
      ],
      "id": 65
    },
    {
      "name": "TexSoup",
      "one_line_profile": "Fault-tolerant Python library for parsing and navigating LaTeX",
      "detailed_description": "A Python library that provides a Beautiful Soup-like interface for searching, navigating, and modifying LaTeX documents, enabling programmatic analysis of LaTeX source code.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "latex_parsing",
        "structure_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/alvinwan/TexSoup",
      "help_website": [],
      "license": "BSD-2-Clause",
      "tags": [
        "latex",
        "parser",
        "python"
      ],
      "id": 66
    },
    {
      "name": "tex2py",
      "one_line_profile": "Converter for LaTeX to Python parse trees",
      "detailed_description": "A utility that converts LaTeX documents into a Python parse tree, allowing for hierarchical navigation and analysis of the document structure.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "latex_parsing",
        "structure_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/alvinwan/tex2py",
      "help_website": [],
      "license": "BSD-2-Clause",
      "tags": [
        "latex",
        "parse-tree",
        "python"
      ],
      "id": 67
    },
    {
      "name": "rosgpt",
      "one_line_profile": "Interface connecting ChatGPT with ROS for robot control",
      "detailed_description": "A framework that integrates Large Language Models (like ChatGPT) with the Robot Operating System (ROS), enabling the translation of unstructured natural language commands into actionable robotic instructions.",
      "domains": [
        "Robotics",
        "AI4S"
      ],
      "subtask_category": [
        "robot_control",
        "human_robot_interaction"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/aniskoubaa/rosgpt",
      "help_website": [],
      "license": null,
      "tags": [
        "ros",
        "llm",
        "robotics"
      ],
      "id": 68
    },
    {
      "name": "ioc_parser",
      "one_line_profile": "Extractor for Indicators of Compromise from security PDFs",
      "detailed_description": "A specialized tool for extracting technical indicators of compromise (IOCs) from security reports in PDF format, useful for cybersecurity research and threat intelligence analysis.",
      "domains": [
        "Cybersecurity",
        "G1-01"
      ],
      "subtask_category": [
        "pdf_parsing",
        "information_extraction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/armbues/ioc_parser",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "security",
        "pdf-extraction",
        "ioc"
      ],
      "id": 69
    },
    {
      "name": "Table-Detection-Extraction",
      "one_line_profile": "Tool for detecting and extracting tables from forms",
      "detailed_description": "A Python-based tool designed to detect table structures within document forms and extract both the table boundaries and the cell contents.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "table_extraction",
        "layout_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/arnavdutta/Table-Detection-Extraction",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "table-detection",
        "ocr",
        "document-processing"
      ],
      "id": 70
    },
    {
      "name": "ocr-llm",
      "one_line_profile": "LLM-powered OCR for structured text extraction",
      "detailed_description": "A tool leveraging vision-language models to perform high-accuracy text extraction from images and PDFs, outputting structured markdown, suitable for handling complex document layouts.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "ocr",
        "pdf_parsing"
      ],
      "application_level": "solver",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/arshad-yaseen/ocr-llm",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "ocr",
        "markdown-extraction"
      ],
      "id": 71
    },
    {
      "name": "artoolkitx",
      "one_line_profile": "SDK for augmented reality tracking and video acquisition",
      "detailed_description": "A high-performance Software Development Kit (SDK) for augmented reality, providing capabilities for video acquisition, marker tracking, and texture tracking across multiple platforms.",
      "domains": [
        "Computer Vision",
        "Augmented Reality"
      ],
      "subtask_category": [
        "tracking",
        "image_processing"
      ],
      "application_level": "library",
      "primary_language": "C",
      "repo_url": "https://github.com/artoolkitx/artoolkitx",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "ar",
        "tracking",
        "sdk"
      ],
      "id": 72
    },
    {
      "name": "Sycamore",
      "one_line_profile": "LLM-powered search and analytics platform for unstructured data",
      "detailed_description": "A platform designed to process unstructured data (such as PDFs and documents) using Large Language Models, enabling search, analytics, and information extraction workflows for scientific and technical documentation.",
      "domains": [
        "G1",
        "G1-01",
        "Data Science"
      ],
      "subtask_category": [
        "document_processing",
        "information_retrieval"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/aryn-ai/sycamore",
      "help_website": [
        "https://sycamore.readthedocs.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "etl",
        "unstructured-data"
      ],
      "id": 73
    },
    {
      "name": "MERMaid",
      "one_line_profile": "Multimodal tool for mining chemical reactions from PDFs",
      "detailed_description": "A specialized tool developed by the Aspuru-Guzik group for extracting chemical reaction data from scientific PDF documents using multimodal analysis techniques.",
      "domains": [
        "Chemistry",
        "G1-01"
      ],
      "subtask_category": [
        "reaction_mining",
        "pdf_parsing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/aspuru-guzik-group/MERMaid",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "chemistry",
        "text-mining",
        "multimodal"
      ],
      "id": 74
    },
    {
      "name": "Camelot",
      "one_line_profile": "Python library for extracting tables from PDFs",
      "detailed_description": "A popular Python library specifically designed to extract tabular data from PDF files with high precision, offering fine-grained control over table detection parameters.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "table_extraction",
        "pdf_parsing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/atlanhq/camelot",
      "help_website": [
        "https://camelot-py.readthedocs.io/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "pdf",
        "table-extraction",
        "python"
      ],
      "id": 75
    },
    {
      "name": "latex2sympy",
      "one_line_profile": "Parser converting LaTeX math to SymPy expressions",
      "detailed_description": "A Python tool that parses LaTeX mathematical expressions and converts them into SymPy objects, facilitating symbolic mathematics and analysis from LaTeX sources.",
      "domains": [
        "Mathematics",
        "G1-01"
      ],
      "subtask_category": [
        "formula_parsing",
        "symbolic_math"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/augustt198/latex2sympy",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "latex",
        "sympy",
        "math-parsing"
      ],
      "id": 76
    },
    {
      "name": "Parsr",
      "one_line_profile": "Document processing tool for extracting structured data from PDFs",
      "detailed_description": "A toolchain that transforms PDF documents and images into enriched structured data (JSON, Markdown, Text), handling hierarchy, paragraphs, and tables.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "pdf_parsing",
        "layout_analysis"
      ],
      "application_level": "solver",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/axa-group/Parsr",
      "help_website": [
        "https://parsr.readthedocs.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "pdf-parsing",
        "ocr",
        "document-structure"
      ],
      "id": 77
    },
    {
      "name": "schema-to-json",
      "one_line_profile": "Schema-driven information extraction from heterogeneous tables",
      "detailed_description": "Implementation of the EMNLP 2024 Findings paper for extracting structured information from diverse table formats based on a defined schema.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "table_extraction",
        "information_extraction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/bflashcp3f/schema-to-json",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "nlp",
        "table-extraction",
        "emnlp"
      ],
      "id": 78
    },
    {
      "name": "geovista",
      "one_line_profile": "Cartographic rendering and mesh analytics library",
      "detailed_description": "A Python library powered by PyVista for cartographic rendering and mesh analytics, enabling the visualization and analysis of geospatial scientific data.",
      "domains": [
        "Earth Science",
        "Visualization"
      ],
      "subtask_category": [
        "visualization",
        "geospatial_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/bjlittle/geovista",
      "help_website": [
        "https://geovista.readthedocs.io"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "geospatial",
        "visualization",
        "mesh-analytics"
      ],
      "id": 79
    },
    {
      "name": "dependency2tree",
      "one_line_profile": "Converter for dependency parser output to tree visualizations",
      "detailed_description": "A tool to convert CoNLL output from dependency parsers into LaTeX or Graphviz tree visualizations, aiding in the analysis of linguistic structures.",
      "domains": [
        "NLP",
        "Linguistics"
      ],
      "subtask_category": [
        "visualization",
        "syntax_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/boberle/dependency2tree",
      "help_website": [],
      "license": "MPL-2.0",
      "tags": [
        "nlp",
        "dependency-parsing",
        "visualization"
      ],
      "id": 80
    },
    {
      "name": "Contextualise",
      "one_line_profile": "Knowledge management tool for organizing unstructured information",
      "detailed_description": "A tool for organizing information-heavy projects using topic maps, suitable for managing unstructured scientific data and building knowledge graphs.",
      "domains": [
        "Knowledge Management",
        "Sci Knowledge"
      ],
      "subtask_category": [
        "knowledge_graph_construction",
        "information_organization"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/brettkromkamp/contextualise",
      "help_website": [
        "https://contextualise.dev"
      ],
      "license": "MIT",
      "tags": [
        "topic-maps",
        "knowledge-graph",
        "unstructured-data"
      ],
      "id": 81
    },
    {
      "name": "table-parser-opencv",
      "one_line_profile": "Table extraction from images and PDFs using OpenCV",
      "detailed_description": "A shell/Python utility that uses OpenCV to detect and extract tables from images or PDF documents, converting them into Excel format.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "table_extraction",
        "image_processing"
      ],
      "application_level": "solver",
      "primary_language": "Shell",
      "repo_url": "https://github.com/brian-yang/table-parser-opencv",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "opencv",
        "table-extraction",
        "pdf"
      ],
      "id": 82
    },
    {
      "name": "CDLA",
      "one_line_profile": "Chinese Document Layout Analysis Dataset",
      "detailed_description": "A large-scale dataset specifically designed for Chinese document layout analysis, supporting the training and evaluation of document parsing models.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "layout_analysis",
        "dataset"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/buptlihang/CDLA",
      "help_website": [],
      "license": null,
      "tags": [
        "dataset",
        "layout-analysis",
        "document-parsing"
      ],
      "id": 83
    },
    {
      "name": "Dolphin",
      "one_line_profile": "Document image parsing model via heterogeneous anchor prompting",
      "detailed_description": "The official implementation of the Dolphin model (ACL 2025), designed for document image parsing using heterogeneous anchor prompting techniques.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "document_parsing",
        "layout_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/bytedance/Dolphin",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "document-parsing",
        "deep-learning",
        "acl-2025"
      ],
      "id": 84
    },
    {
      "name": "bibtex-js",
      "one_line_profile": "JavaScript library for parsing BibTeX files",
      "detailed_description": "A TypeScript library for parsing BibTeX formatted citation files, enabling the integration of bibliographic data into web-based scientific applications.",
      "domains": [
        "G1",
        "Sci Knowledge"
      ],
      "subtask_category": [
        "citation_parsing",
        "bibliography_management"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/cacfd3a/bibtex-js",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bibtex",
        "parser",
        "typescript"
      ],
      "id": 85
    },
    {
      "name": "Caradoc",
      "one_line_profile": "A PDF parser and validator written in OCaml",
      "detailed_description": "Caradoc is a PDF parser and validator that allows users to inspect the internal structure of PDF files, validate them against the PDF standard, and extract information. It is useful for ensuring the integrity of scientific documents and analyzing their structure.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "pdf_parsing",
        "document_validation"
      ],
      "application_level": "solver",
      "primary_language": "OCaml",
      "repo_url": "https://github.com/caradoc-org/caradoc",
      "help_website": [],
      "license": "GPL-2.0",
      "tags": [
        "pdf-parser",
        "validation",
        "ocaml"
      ],
      "id": 86
    },
    {
      "name": "pulldown-latex",
      "one_line_profile": "A pull parser for LaTeX parsing and MathML rendering",
      "detailed_description": "A Rust-based pull parser designed to process LaTeX syntax, specifically useful for parsing mathematical expressions and converting them to MathML. This supports the extraction and rendering of mathematical content from scientific LaTeX documents.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "latex_parsing",
        "mathml_rendering"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/carloskiki/pulldown-latex",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "latex",
        "parser",
        "mathml",
        "rust"
      ],
      "id": 87
    },
    {
      "name": "Docuglean OCR",
      "one_line_profile": "Intelligent document processing to extract structured data",
      "detailed_description": "An intelligent document processing tool that uses AI to extract structured data (JSON, Markdown, HTML) from documents. It integrates OCR capabilities to handle scanned scientific documents and convert them into machine-readable formats.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "ocr",
        "document_parsing",
        "structured_extraction"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/cernis-intelligence/docuglean-ocr",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ocr",
        "document-processing",
        "pdf-to-json"
      ],
      "id": 88
    },
    {
      "name": "nom-bibtex",
      "one_line_profile": "A feature-complete BibTeX parser using nom",
      "detailed_description": "A Rust library for parsing BibTeX files, which are standard for bibliography management in scientific writing. It enables the extraction and manipulation of citation data from BibTeX databases.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "citation_parsing",
        "bibliography_management"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/charlesvdv/nom-bibtex",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bibtex",
        "parser",
        "rust",
        "citation"
      ],
      "id": 89
    },
    {
      "name": "ParseStudio",
      "one_line_profile": "Python package to parse PDFs with different parsers",
      "detailed_description": "A Python package that provides a unified interface to parse PDF documents using various underlying parsers. It facilitates the extraction of text and metadata from scientific PDFs for downstream analysis.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "pdf_parsing",
        "text_extraction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/chatclimate-ai/ParseStudio",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pdf-parser",
        "python",
        "document-analysis"
      ],
      "id": 90
    },
    {
      "name": "OCRFlux",
      "one_line_profile": "Multimodal toolkit for PDF-to-Markdown conversion and layout analysis",
      "detailed_description": "A lightweight yet powerful multimodal toolkit designed for PDF-to-Markdown conversion. It excels in handling complex layouts, parsing complicated tables, and merging cross-page content, making it highly suitable for processing scientific papers.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "ocr",
        "layout_analysis",
        "table_extraction",
        "pdf_to_markdown"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/chatdoc-com/OCRFlux",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ocr",
        "pdf-parsing",
        "layout-analysis",
        "markdown"
      ],
      "id": 91
    },
    {
      "name": "tabula-py",
      "one_line_profile": "Simple wrapper of tabula-java to extract tables from PDF into pandas DataFrame",
      "detailed_description": "A Python wrapper for tabula-java that enables the extraction of tables from PDF files directly into pandas DataFrames. This is a critical tool for extracting structured data from scientific reports and papers.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "table_extraction",
        "pdf_parsing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/chezou/tabula-py",
      "help_website": [
        "https://tabula-py.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "pdf",
        "table-extraction",
        "pandas",
        "data-mining"
      ],
      "id": 92
    },
    {
      "name": "Blob Service",
      "one_line_profile": "Out-of-the-box file parsing service supporting Text/PDF/Docx/OCR",
      "detailed_description": "A comprehensive file parsing service that supports various formats including PDF, Docx, and images. It includes OCR capabilities and supports multiple storage backends, suitable for building pipelines to process scientific documents.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "document_parsing",
        "ocr",
        "file_processing"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/coaidev/blob-service",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "parsing",
        "ocr",
        "pdf",
        "service"
      ],
      "id": 93
    },
    {
      "name": "gmft",
      "one_line_profile": "Lightweight, performant, deep table extraction from PDFs",
      "detailed_description": "A specialized tool for extracting tables from PDFs using deep learning methods. It is designed to be lightweight and performant, making it suitable for extracting data from scientific literature.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "table_extraction",
        "pdf_parsing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/conjuncts/gmft",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "table-extraction",
        "pdf",
        "deep-learning"
      ],
      "id": 94
    },
    {
      "name": "ocr-table",
      "one_line_profile": "Extract tables from scanned image PDFs using OCR",
      "detailed_description": "A tool designed to extract tabular data from scanned PDF documents using Optical Character Recognition (OCR). This is particularly useful for digitizing legacy scientific data stored in scanned papers.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "table_extraction",
        "ocr",
        "pdf_parsing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/cseas/ocr-table",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ocr",
        "table-extraction",
        "scanned-pdf"
      ],
      "id": 95
    },
    {
      "name": "marker",
      "one_line_profile": "Convert PDF to markdown + JSON quickly with high accuracy",
      "detailed_description": "A high-performance tool for converting PDF documents into Markdown and JSON formats. It uses deep learning models to handle layout analysis, formula recognition, and table extraction, making it a state-of-the-art tool for parsing scientific papers.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "pdf_to_markdown",
        "layout_analysis",
        "formula_recognition"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/datalab-to/marker",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "pdf-parsing",
        "markdown",
        "deep-learning",
        "layout-analysis"
      ],
      "id": 96
    },
    {
      "name": "pdftext",
      "one_line_profile": "Extract structured text from PDFs quickly",
      "detailed_description": "A fast and efficient library for extracting structured text from PDF files. It focuses on performance and clean text extraction, suitable for processing large corpora of scientific documents.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "text_extraction",
        "pdf_parsing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/datalab-to/pdftext",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "pdf",
        "text-extraction",
        "python"
      ],
      "id": 97
    },
    {
      "name": "probablepeople",
      "one_line_profile": "Library for parsing unstructured western names into name components",
      "detailed_description": "A Python library using conditional random fields to parse unstructured name strings into structured components. In scientific literature analysis, it is essential for cleaning and normalizing author names in citation networks.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "entity_extraction",
        "name_parsing",
        "metadata_cleaning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/datamade/probablepeople",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nlp",
        "name-parsing",
        "citation-analysis"
      ],
      "id": 98
    },
    {
      "name": "usaddress",
      "one_line_profile": "Library for parsing unstructured US address strings",
      "detailed_description": "A Python library for parsing unstructured address strings into address components. It is widely used in bibliometrics and scientific knowledge graph construction to parse and normalize author affiliations and institutional addresses.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "entity_extraction",
        "address_parsing",
        "metadata_cleaning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/datamade/usaddress",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nlp",
        "address-parsing",
        "affiliation-normalization"
      ],
      "id": 99
    },
    {
      "name": "tex-math-parser",
      "one_line_profile": "Parser to evaluate TeX math and convert it into a MathJS expression tree",
      "detailed_description": "A TypeScript library that parses TeX mathematical expressions and converts them into a structured expression tree (MathJS). This is useful for semantic analysis of mathematical content in scientific documents.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "math_parsing",
        "latex_parsing",
        "semantic_analysis"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/davidtranhq/tex-math-parser",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "latex",
        "math-parser",
        "typescript"
      ],
      "id": 100
    },
    {
      "name": "deepdoctection",
      "one_line_profile": "A comprehensive Document AI package for layout analysis and extraction",
      "detailed_description": "A robust document AI package that orchestrates OCR, layout analysis, and table extraction pipelines. It is designed to extract structured information from complex documents like scientific papers and technical reports.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "layout_analysis",
        "table_extraction",
        "document_ai",
        "ocr"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/deepdoctection/deepdoctection",
      "help_website": [
        "https://deepdoctection.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "document-ai",
        "layout-analysis",
        "pdf-extraction",
        "deep-learning"
      ],
      "id": 101
    },
    {
      "name": "pdfsyntax",
      "one_line_profile": "Library to inspect and modify the internal structure of a PDF file",
      "detailed_description": "A lightweight Python library for inspecting and modifying the low-level internal structure of PDF files. It is useful for deep analysis of PDF composition and debugging extraction issues in scientific document processing.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "pdf_structure_analysis",
        "low_level_parsing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/desgeeko/pdfsyntax",
      "help_website": [
        "https://pdfsyntax.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "pdf",
        "structure-analysis",
        "python"
      ],
      "id": 102
    },
    {
      "name": "PyScholar",
      "one_line_profile": "A supervised parser for Google Scholar search results",
      "detailed_description": "A Python library designed to parse Google Scholar search results, enabling the programmatic retrieval of bibliographic data and citation metrics for scientific literature analysis.",
      "domains": [
        "G1",
        "Sci Knowledge"
      ],
      "subtask_category": [
        "literature_retrieval",
        "citation_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/dnlcrl/PyScholar",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "google-scholar",
        "parser",
        "bibliometrics"
      ],
      "id": 103
    },
    {
      "name": "DocBank",
      "one_line_profile": "Benchmark dataset for document layout analysis",
      "detailed_description": "A large-scale dataset constructed for document layout analysis, providing fine-grained token-level annotations for reading order and layout elements, essential for training AI models to parse scientific documents.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "layout_analysis",
        "model_training"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/doc-analysis/DocBank",
      "help_website": [
        "https://github.com/doc-analysis/DocBank"
      ],
      "license": "Apache-2.0",
      "tags": [
        "dataset",
        "layout-analysis",
        "document-understanding"
      ],
      "id": 104
    },
    {
      "name": "pdfminer-layout-scanner",
      "one_line_profile": "Advanced layout scanning tool based on PDFMiner",
      "detailed_description": "A tool that extends PDFMiner to provide enhanced layout analysis and text extraction capabilities, useful for parsing structured information from PDF documents.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "pdf_parsing",
        "layout_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/dpapathanasiou/pdfminer-layout-scanner",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pdfminer",
        "layout-analysis",
        "text-extraction"
      ],
      "id": 105
    },
    {
      "name": "seerai",
      "one_line_profile": "AI assistant plugin for Zotero with OCR and extraction capabilities",
      "detailed_description": "A Zotero plugin that integrates AI capabilities for OCR, table extraction, and semantic search, enhancing the management and analysis of scientific literature within the Zotero ecosystem.",
      "domains": [
        "G1",
        "Sci Knowledge"
      ],
      "subtask_category": [
        "literature_management",
        "ocr",
        "table_extraction"
      ],
      "application_level": "solver",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/dralkh/seerai",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "zotero-plugin",
        "ocr",
        "literature-review"
      ],
      "id": 106
    },
    {
      "name": "docling-api",
      "one_line_profile": "Backend service for converting documents to Markdown for AI processing",
      "detailed_description": "A scalable backend server designed to convert various document formats (PDF, DOCX, etc.) into Markdown, featuring OCR and table extraction, specifically optimized for feeding documents into Large Language Models (LLMs) for analysis.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "document_conversion",
        "text_extraction",
        "ocr"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/drmingler/docling-api",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "document-conversion",
        "llm-preprocessing",
        "ocr"
      ],
      "id": 107
    },
    {
      "name": "unit_parse",
      "one_line_profile": "Parser for physical units and numerical values",
      "detailed_description": "A Python tool for parsing and normalizing messy numerical strings and physical units, essential for cleaning and structuring scientific data extracted from text.",
      "domains": [
        "Sci Knowledge",
        "Data Processing"
      ],
      "subtask_category": [
        "data_cleaning",
        "normalization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/dylanwal/unit_parse",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "unit-parsing",
        "data-cleaning",
        "scientific-data"
      ],
      "id": 108
    },
    {
      "name": "pdfssa4met",
      "one_line_profile": "PDF structure analysis for metadata extraction",
      "detailed_description": "A tool for PDF Structure and Syntactic Analysis, specifically designed to extract metadata and tag document elements, aiding in the structured parsing of scientific publications.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "metadata_extraction",
        "structure_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/eliask/pdfssa4met",
      "help_website": [
        "https://code.google.com/p/pdfssa4met/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "pdf-parsing",
        "metadata-extraction",
        "structure-analysis"
      ],
      "id": 109
    },
    {
      "name": "sciencebeam-parser",
      "one_line_profile": "Pipeline for converting scientific PDFs to XML",
      "detailed_description": "A set of tools utilizing Apache Beam to convert scientific PDF documents into structured XML (JATS/TEI), facilitating large-scale literature mining and analysis.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "document_conversion",
        "xml_generation"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/elifesciences/sciencebeam-parser",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pdf-to-xml",
        "jats",
        "scientific-publishing"
      ],
      "id": 110
    },
    {
      "name": "paper_autotranslation",
      "one_line_profile": "Automatic translation tool for scientific papers",
      "detailed_description": "A tool designed to convert scientific papers from PDF to text and translate them (specifically English to Chinese), aiding in the consumption of foreign language scientific literature.",
      "domains": [
        "G1",
        "Sci Knowledge"
      ],
      "subtask_category": [
        "translation",
        "pdf_to_text"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/elliotxx/paper_autotranslation",
      "help_website": [],
      "license": "GPL-2.0",
      "tags": [
        "translation",
        "scientific-paper",
        "pdf-processing"
      ],
      "id": 111
    },
    {
      "name": "PDFScraper",
      "one_line_profile": "CLI for searching and extracting text/tables from PDFs",
      "detailed_description": "A command-line interface tool for searching text and extracting tables from PDF documents, useful for data mining within scientific reports.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "text_extraction",
        "table_extraction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/erikkastelec/PDFScraper",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pdf-scraping",
        "data-mining",
        "cli"
      ],
      "id": 112
    },
    {
      "name": "pdfminer",
      "one_line_profile": "Python PDF parser for extracting text and information",
      "detailed_description": "A foundational Python library for parsing PDF files, focusing on extracting text, layout information, and metadata, widely used as a building block for scientific document analysis tools.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "pdf_parsing",
        "text_extraction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/euske/pdfminer",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pdf-parser",
        "text-extraction",
        "python"
      ],
      "id": 113
    },
    {
      "name": "tabula-js",
      "one_line_profile": "JavaScript wrapper for Tabula PDF table extractor",
      "detailed_description": "A Node.js wrapper for Tabula-java, enabling the extraction of data tables from PDF files into CSV format, a critical task for recovering data from scientific publications.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "table_extraction",
        "data_recovery"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/ezodude/tabula-js",
      "help_website": [
        "https://github.com/tabulapdf/tabula-java"
      ],
      "license": "MIT",
      "tags": [
        "table-extraction",
        "pdf",
        "csv"
      ],
      "id": 114
    },
    {
      "name": "nougat",
      "one_line_profile": "Neural Optical Understanding for Academic Documents",
      "detailed_description": "A deep learning model and tool by Meta AI for converting scientific PDF documents into structured Markdown, specifically handling mathematical formulas and complex layouts.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "document_understanding",
        "formula_extraction",
        "ocr"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/nougat",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ocr",
        "academic-papers",
        "transformer"
      ],
      "id": 115
    },
    {
      "name": "mathparser",
      "one_line_profile": "Mathematical expression evaluator and parser",
      "detailed_description": "A C++ library for parsing and evaluating mathematical expressions, serving as a utility for scientific computing and modeling tasks.",
      "domains": [
        "Scientific Computing"
      ],
      "subtask_category": [
        "math_parsing",
        "calculation"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/fkkarakurt/mathparser",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "math",
        "parser",
        "cpp"
      ],
      "id": 116
    },
    {
      "name": "maltego_academia",
      "one_line_profile": "Maltego transforms for exploring academic literature",
      "detailed_description": "A set of transforms for the Maltego link analysis tool, enabling the exploration and visualization of academic literature networks, citations, and author relationships.",
      "domains": [
        "G1",
        "Sci Knowledge"
      ],
      "subtask_category": [
        "network_analysis",
        "literature_mining"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/flppgg/maltego_academia",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "maltego",
        "bibliometrics",
        "citation-network"
      ],
      "id": 117
    },
    {
      "name": "tabulator-py",
      "one_line_profile": "Stream-based tabular data reading and writing library",
      "detailed_description": "A Python library for reading and writing tabular data via streams, part of the Frictionless Data ecosystem, widely used for processing and normalizing scientific data tables.",
      "domains": [
        "Data Processing"
      ],
      "subtask_category": [
        "data_io",
        "tabular_data_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/frictionlessdata/tabulator-py",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "tabular-data",
        "frictionless-data",
        "etl"
      ],
      "id": 118
    },
    {
      "name": "kabeja",
      "one_line_profile": "Java library for parsing and processing DXF format",
      "detailed_description": "A library for parsing, processing, and converting Autodesk's DXF format, enabling the extraction and manipulation of engineering and CAD data for scientific modeling and analysis.",
      "domains": [
        "Engineering",
        "Data Processing"
      ],
      "subtask_category": [
        "cad_parsing",
        "geometry_extraction"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/fuzziness/kabeja",
      "help_website": [],
      "license": null,
      "tags": [
        "dxf",
        "cad",
        "parser"
      ],
      "id": 119
    },
    {
      "name": "COVID-Text-Extractor",
      "one_line_profile": "OCR tool for extracting COVID-19 info from documents",
      "detailed_description": "A specialized OCR tool designed to extract COVID-19 related information from images, PDFs, and text, supporting pandemic-related data collection and analysis.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "text_extraction",
        "ocr",
        "domain_specific_mining"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/gagangulyani/COVID-Text-Extractor",
      "help_website": [],
      "license": null,
      "tags": [
        "covid-19",
        "ocr",
        "text-mining"
      ],
      "id": 120
    },
    {
      "name": "pdf-text-extraction",
      "one_line_profile": "CLI for extracting text from PDF files",
      "detailed_description": "A C++ command-line tool specifically designed for extracting text content from PDF files, suitable for high-performance batch processing of scientific documents.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "text_extraction"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/galkahana/pdf-text-extraction",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "pdf",
        "text-extraction",
        "cli"
      ],
      "id": 121
    },
    {
      "name": "pdfshapeminer",
      "one_line_profile": "PDF text extraction using geometric shape analysis",
      "detailed_description": "A tool that combines PDFMiner with Shapely to perform layout analysis based on geometric shapes, improving text extraction accuracy for complex PDF layouts common in scientific papers.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "layout_analysis",
        "text_extraction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/garabik/pdfshapeminer",
      "help_website": [],
      "license": "LGPL-3.0",
      "tags": [
        "pdfminer",
        "shapely",
        "layout-analysis"
      ],
      "id": 122
    },
    {
      "name": "parsemypdf",
      "one_line_profile": "Unified interface for various PDF parsing libraries",
      "detailed_description": "A wrapper library that integrates multiple PDF parsing tools (like Docling, PDFMiner, PyMuPDF) and AI models to provide efficient extraction of text, tables, and metadata from documents.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "document_parsing",
        "extraction_pipeline"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/genieincodebottle/parsemypdf",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pdf-parsing",
        "wrapper",
        "ocr"
      ],
      "id": 123
    },
    {
      "name": "pdf_header_and_footer_detector",
      "one_line_profile": "Detector for PDF headers and footers",
      "detailed_description": "A Python tool using PdfMiner to detect and remove headers and footers from PDF pages, a crucial pre-processing step for cleaning scientific text before analysis.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "data_cleaning",
        "preprocessing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/gentrith78/pdf_header_and_footer_detector",
      "help_website": [],
      "license": null,
      "tags": [
        "pdf-cleaning",
        "header-detection",
        "preprocessing"
      ],
      "id": 124
    },
    {
      "name": "pdf-rag-assistant",
      "one_line_profile": "RAG system for Q&A on PDF documents",
      "detailed_description": "A Retrieval-Augmented Generation (RAG) system designed for performing Question & Answer tasks on PDF documents, featuring parsing, chunking, and citation support, useful for interacting with scientific literature.",
      "domains": [
        "G1",
        "Sci Knowledge"
      ],
      "subtask_category": [
        "rag",
        "question_answering",
        "literature_analysis"
      ],
      "application_level": "platform",
      "primary_language": "Java",
      "repo_url": "https://github.com/giasinguyen/pdf-rag-assistant",
      "help_website": [],
      "license": null,
      "tags": [
        "rag",
        "pdf-qa",
        "spring-ai"
      ],
      "id": 125
    },
    {
      "name": "langextract",
      "one_line_profile": "LLM-based structured information extraction tool",
      "detailed_description": "A library for extracting structured information from unstructured text using Large Language Models (LLMs), featuring precise source grounding, highly applicable to knowledge extraction from scientific literature.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "information_extraction",
        "llm_application"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/google/langextract",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "information-extraction",
        "structured-data"
      ],
      "id": 126
    },
    {
      "name": "Gretel Synthetics",
      "one_line_profile": "Synthetic data generators for structured and unstructured text with differential privacy",
      "detailed_description": "A library for generating synthetic data using recurrent neural networks (LSTMs) and differential privacy mechanisms. It is widely used in scientific research to create privacy-preserving datasets for healthcare, finance, and social sciences, enabling data sharing and model training without compromising sensitive information.",
      "domains": [
        "Scientific Data Generation",
        "Privacy Preserving ML"
      ],
      "subtask_category": [
        "synthetic_data_generation",
        "privacy_protection"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/gretelai/gretel-synthetics",
      "help_website": [
        "https://gretel.ai/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "synthetic-data",
        "differential-privacy",
        "lstm",
        "nlp"
      ],
      "id": 127
    },
    {
      "name": "pdfminer3",
      "one_line_profile": "Python library for extracting text, information, and layout from PDF documents",
      "detailed_description": "A Python 3 port of the pdfminer library, designed for parsing PDF files. It focuses on extracting textual content, analyzing document layout, and retrieving metadata, which is essential for mining scientific literature and converting unstructured PDF data into structured formats.",
      "domains": [
        "G1-01",
        "Document Parsing"
      ],
      "subtask_category": [
        "pdf_parsing",
        "text_extraction",
        "layout_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/gwk/pdfminer3",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pdf-parser",
        "text-extraction",
        "layout-analysis"
      ],
      "id": 128
    },
    {
      "name": "Rowfill",
      "one_line_profile": "AI-powered spreadsheet platform for deep research and document processing",
      "detailed_description": "An open-source spreadsheet interface designed for research tasks, integrating LLMs to process, fill, and analyze tabular data. It facilitates 'deep research' by automating data enrichment and document processing workflows within a familiar spreadsheet environment.",
      "domains": [
        "Scientific Data Analysis",
        "Research Workflow"
      ],
      "subtask_category": [
        "data_enrichment",
        "document_processing",
        "tabular_analysis"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/harishdeivanayagam/rowfill",
      "help_website": [
        "https://rowfill.ai"
      ],
      "license": "NOASSERTION",
      "tags": [
        "spreadsheet",
        "llm",
        "research-automation",
        "data-processing"
      ],
      "id": 129
    },
    {
      "name": "PyTorch Tabular",
      "one_line_profile": "Deep learning library for tabular data modeling and analysis",
      "detailed_description": "A library that makes state-of-the-art deep learning models for tabular data easy to use with PyTorch. It is applicable in scientific domains where data is structured in tables (e.g., bioinformatics, physical measurements) and requires advanced modeling beyond traditional tree-based methods.",
      "domains": [
        "Scientific Modeling",
        "Tabular Data Analysis"
      ],
      "subtask_category": [
        "tabular_modeling",
        "deep_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/hcarlens/pytorch-tabular",
      "help_website": [
        "https://pytorch-tabular.readthedocs.io/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "pytorch",
        "tabular-data",
        "deep-learning",
        "modeling"
      ],
      "id": 130
    },
    {
      "name": "PDF Document Layout Analysis",
      "one_line_profile": "Service for segmenting and classifying PDF document elements",
      "detailed_description": "A Docker-based service that performs layout analysis on PDF documents. It segments pages and classifies elements into text, titles, images, and tables, providing a structural foundation for extracting information from scientific literature and reports.",
      "domains": [
        "G1-01",
        "Document Analysis"
      ],
      "subtask_category": [
        "layout_analysis",
        "document_segmentation"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/huridocs/pdf-document-layout-analysis",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "pdf",
        "layout-analysis",
        "computer-vision",
        "document-structure"
      ],
      "id": 131
    },
    {
      "name": "PDF Text Extraction",
      "one_line_profile": "Tool for extracting text from PDFs based on layout analysis",
      "detailed_description": "A utility that leverages the output of layout analysis tools to extract clean text from PDF files. It is designed to handle complex document structures found in reports and scientific papers, ensuring text is extracted in a logical order.",
      "domains": [
        "G1-01",
        "Text Extraction"
      ],
      "subtask_category": [
        "text_extraction",
        "pdf_processing"
      ],
      "application_level": "workflow",
      "primary_language": "Makefile",
      "repo_url": "https://github.com/huridocs/pdf-text-extraction",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "pdf",
        "text-mining",
        "layout-aware"
      ],
      "id": 132
    },
    {
      "name": "Vision Parse",
      "one_line_profile": "Parse PDFs into markdown using Vision LLMs",
      "detailed_description": "A tool that utilizes Vision-Language Models (VLMs) to visually parse PDF documents and convert them into structured Markdown. This approach is particularly effective for scientific papers containing complex layouts, formulas, and tables that traditional OCR might miss.",
      "domains": [
        "G1-01",
        "Document Parsing"
      ],
      "subtask_category": [
        "pdf_to_markdown",
        "visual_document_understanding"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/iamarunbrahma/vision-parse",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pdf-parsing",
        "vision-llm",
        "markdown",
        "ocr"
      ],
      "id": 133
    },
    {
      "name": "AnyStyle",
      "one_line_profile": "Fast and accurate citation reference parser",
      "detailed_description": "A powerful tool for parsing bibliographic references from academic texts. It uses machine learning (Conditional Random Fields) to identify and extract structured metadata (author, title, year, etc.) from raw citation strings, essential for bibliometrics and citation network analysis.",
      "domains": [
        "G1",
        "Bibliometrics"
      ],
      "subtask_category": [
        "citation_parsing",
        "reference_extraction"
      ],
      "application_level": "solver",
      "primary_language": "Ruby",
      "repo_url": "https://github.com/inukshuk/anystyle",
      "help_website": [
        "https://anystyle.io"
      ],
      "license": "NOASSERTION",
      "tags": [
        "citation-parser",
        "bibliometrics",
        "crf",
        "ruby"
      ],
      "id": 134
    },
    {
      "name": "Parsing PDFs using YOLOv3",
      "one_line_profile": "Table detection and extraction from PDFs using YOLOv3",
      "detailed_description": "An implementation of table detection in PDF documents using the YOLOv3 object detection model. It addresses the specific challenge of extracting structured tabular data from scientific papers and reports for downstream analysis.",
      "domains": [
        "G1-01",
        "Table Extraction"
      ],
      "subtask_category": [
        "table_detection",
        "pdf_parsing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ismail-mebsout/Parsing-PDFs-using-YOLOV3",
      "help_website": [],
      "license": null,
      "tags": [
        "yolov3",
        "table-extraction",
        "pdf",
        "deep-learning"
      ],
      "id": 135
    },
    {
      "name": "Dedoc",
      "one_line_profile": "Universal document parsing and structure extraction system",
      "detailed_description": "A comprehensive library for automating the parsing of various document formats (PDF, DOCX, HTML, images) into a uniform structure. It extracts logical structure, tables, and metadata, making it a critical tool for ingesting scientific literature and technical documentation into analysis pipelines.",
      "domains": [
        "G1-01",
        "Document Ingestion"
      ],
      "subtask_category": [
        "document_parsing",
        "structure_extraction",
        "format_conversion"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ispras/dedoc",
      "help_website": [
        "https://dedoc.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "document-parsing",
        "ocr",
        "structure-extraction",
        "pdf"
      ],
      "id": 136
    },
    {
      "name": "TableNet Implementation",
      "one_line_profile": "Deep learning model for end-to-end table detection and extraction",
      "detailed_description": "An implementation of the TableNet architecture for detecting and extracting tabular data from scanned document images. This tool is valuable for digitizing historical scientific data and processing papers where tables are preserved only as images.",
      "domains": [
        "G1-01",
        "Table Extraction"
      ],
      "subtask_category": [
        "table_detection",
        "tabular_data_extraction"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/jainammm/TableNet",
      "help_website": [],
      "license": null,
      "tags": [
        "tablenet",
        "deep-learning",
        "table-extraction",
        "ocr"
      ],
      "id": 137
    },
    {
      "name": "Citation Map",
      "one_line_profile": "Tool to generate citation graphs from Zotero PDF collections",
      "detailed_description": "A Python tool that analyzes PDFs stored in Zotero to extract citations and generate a network graph file (for Gephi). It enables researchers to visualize and analyze the citation topology of their personal or project-specific literature collections.",
      "domains": [
        "G1",
        "Scientometrics"
      ],
      "subtask_category": [
        "citation_network_analysis",
        "visualization"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/jaks6/citation_map",
      "help_website": [],
      "license": null,
      "tags": [
        "zotero",
        "citation-graph",
        "gephi",
        "network-analysis"
      ],
      "id": 138
    },
    {
      "name": "Chinese Layout Analysis",
      "one_line_profile": "YOLOv8-based layout detection for Chinese documents",
      "detailed_description": "A tool utilizing YOLOv8 to perform layout analysis specifically on Chinese document images. It detects regions such as text, headers, and tables, facilitating the digitization and structural analysis of Chinese scientific literature and archives.",
      "domains": [
        "G1-01",
        "Document Analysis"
      ],
      "subtask_category": [
        "layout_analysis",
        "document_segmentation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/jiangnanboy/layout_analysis",
      "help_website": [],
      "license": null,
      "tags": [
        "yolov8",
        "layout-analysis",
        "chinese-nlp",
        "ocr"
      ],
      "id": 139
    },
    {
      "name": "Layout Analysis 4J",
      "one_line_profile": "Java implementation of YOLOv8-based layout analysis",
      "detailed_description": "The Java implementation of the Chinese layout analysis tool. It allows integration of document layout detection capabilities into Java-based scientific data processing pipelines.",
      "domains": [
        "G1-01",
        "Document Analysis"
      ],
      "subtask_category": [
        "layout_analysis",
        "document_segmentation"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/jiangnanboy/layout_analysis4j",
      "help_website": [],
      "license": null,
      "tags": [
        "java",
        "yolov8",
        "layout-analysis"
      ],
      "id": 140
    },
    {
      "name": "AutoTabular",
      "one_line_profile": "Automated Machine Learning (AutoML) for tabular data",
      "detailed_description": "An AutoML library designed to automate the processing and modeling of tabular data. It simplifies the pipeline of feature engineering, model selection, and hyperparameter tuning, which is useful for scientific data analysis where tabular datasets are common.",
      "domains": [
        "Scientific Data Analysis",
        "AutoML"
      ],
      "subtask_category": [
        "automl",
        "tabular_modeling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/jianzhnie/AutoTabular",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "automl",
        "tabular-data",
        "machine-learning"
      ],
      "id": 141
    },
    {
      "name": "PDF to Markdown",
      "one_line_profile": "Utility to convert PDF files into Markdown format",
      "detailed_description": "A Python tool designed to convert PDF documents into Markdown. This conversion is a critical step in modern scientific literature processing pipelines (e.g., for RAG or LLM ingestion), allowing unstructured PDF content to be treated as structured text.",
      "domains": [
        "G1-01",
        "Document Conversion"
      ],
      "subtask_category": [
        "pdf_parsing",
        "format_conversion"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/johnlinp/pdf-to-markdown",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "pdf",
        "markdown",
        "converter",
        "text-extraction"
      ],
      "id": 142
    },
    {
      "name": "GROBID",
      "one_line_profile": "Machine learning library for extracting, parsing, and restructuring raw documents such as PDF into structured XML/TEI.",
      "detailed_description": "GROBID (GeneRation Of BIbliographic Data) is a machine learning library for extracting, parsing and re-structuring raw documents such as PDF into structured XML/TEI encoded documents with a particular focus on technical and scientific publications.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "document_parsing",
        "structure_extraction",
        "citation_extraction"
      ],
      "application_level": "solver",
      "primary_language": "Java",
      "repo_url": "https://github.com/kermitt2/grobid",
      "help_website": [
        "https://grobid.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "pdf-parsing",
        "citation-extraction",
        "scholarly-documents",
        "crf",
        "machine-learning"
      ],
      "id": 143
    },
    {
      "name": "GROBID-astro",
      "one_line_profile": "GROBID extension for identifying and extracting astronomical entities from scientific documents.",
      "detailed_description": "A module for GROBID that specializes in recognizing and extracting astronomical objects, locations, and other domain-specific entities from scholarly articles in the field of astronomy.",
      "domains": [
        "G1",
        "G1-01",
        "Astronomy"
      ],
      "subtask_category": [
        "entity_recognition",
        "domain_specific_extraction"
      ],
      "application_level": "solver",
      "primary_language": "Java",
      "repo_url": "https://github.com/kermitt2/grobid-astro",
      "help_website": [],
      "license": null,
      "tags": [
        "astronomy",
        "ner",
        "grobid-extension"
      ],
      "id": 144
    },
    {
      "name": "GROBID-NER",
      "one_line_profile": "Named Entity Recognition module for GROBID to extract general and scientific entities.",
      "detailed_description": "A sequence labelling tool for Named Entity Recognition (NER) based on GROBID, designed to extract entities such as persons, locations, organizations, and other scientific entities from text.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "named_entity_recognition",
        "text_mining"
      ],
      "application_level": "solver",
      "primary_language": "Java",
      "repo_url": "https://github.com/kermitt2/grobid-ner",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ner",
        "nlp",
        "grobid-extension"
      ],
      "id": 145
    },
    {
      "name": "space_packet_parser",
      "one_line_profile": "Library for decoding CCSDS telemetry packets using XTCE packet definitions.",
      "detailed_description": "A Python library developed by LASP for parsing CCSDS telemetry packets based on the XTCE (XML Telemetric and Command Exchange) standard, widely used in space science missions.",
      "domains": [
        "Space Science",
        "Astrophysics"
      ],
      "subtask_category": [
        "telemetry_decoding",
        "data_parsing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/lasp/space_packet_parser",
      "help_website": [
        "https://space-packet-parser.readthedocs.io/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "ccsds",
        "telemetry",
        "space-science",
        "xtce"
      ],
      "id": 146
    },
    {
      "name": "llmdocparser",
      "one_line_profile": "Pipeline for parsing PDFs and analyzing content using Large Language Models (LLMs).",
      "detailed_description": "A Python package that leverages LLMs (like GPT-4 or local models) to parse PDF documents, extract structured content, and perform analysis, suitable for processing scientific literature.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "document_parsing",
        "content_analysis",
        "llm_extraction"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/lazyFrogLOL/llmdocparser",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pdf-parsing",
        "llm",
        "rag",
        "document-analysis"
      ],
      "id": 147
    },
    {
      "name": "GROBID-quantities",
      "one_line_profile": "GROBID extension for identifying, parsing, and normalizing physical quantities and measurements.",
      "detailed_description": "A machine learning module for GROBID that extracts physical quantities (values and units) from text and normalizes them, essential for scientific data mining.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "entity_extraction",
        "quantity_normalization"
      ],
      "application_level": "solver",
      "primary_language": "Java",
      "repo_url": "https://github.com/lfoppiano/grobid-quantities",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "quantities",
        "measurements",
        "grobid-extension",
        "normalization"
      ],
      "id": 148
    },
    {
      "name": "GROBID-superconductors",
      "one_line_profile": "GROBID module for extracting superconductor materials and their properties from scientific literature.",
      "detailed_description": "A specialized GROBID module designed to mine superconductor material names, critical temperatures (Tc), and other related properties from scientific papers.",
      "domains": [
        "G1",
        "G1-01",
        "Materials Science"
      ],
      "subtask_category": [
        "material_extraction",
        "property_extraction"
      ],
      "application_level": "solver",
      "primary_language": "Java",
      "repo_url": "https://github.com/lfoppiano/grobid-superconductors",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "superconductors",
        "materials-science",
        "text-mining",
        "grobid-extension"
      ],
      "id": 149
    },
    {
      "name": "material-parsers",
      "one_line_profile": "Collection of parsers and utility scripts for processing material science data.",
      "detailed_description": "A set of Python tools and parsers initially developed for the Grobid Superconductor project, used for processing and structuring material science data extracted from literature.",
      "domains": [
        "Materials Science",
        "G1-01"
      ],
      "subtask_category": [
        "data_parsing",
        "text_mining"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/lfoppiano/material-parsers",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "materials-science",
        "parsing",
        "utility"
      ],
      "id": 150
    },
    {
      "name": "structure-vision",
      "one_line_profile": "Visualization tool for inspecting the structured data extracted by GROBID from PDF documents.",
      "detailed_description": "A viewer application designed to visualize the XML/TEI structure extracted by GROBID overlaid on the original PDF, aiding in the validation and debugging of scientific document parsing.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "visualization",
        "quality_control"
      ],
      "application_level": "application",
      "primary_language": "Python",
      "repo_url": "https://github.com/lfoppiano/structure-vision",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "visualization",
        "grobid",
        "pdf-structure",
        "quality-control"
      ],
      "id": 151
    },
    {
      "name": "LaTeX-OCR",
      "one_line_profile": "Deep learning model (ViT) to convert images of mathematical equations into LaTeX code.",
      "detailed_description": "A tool using a Vision Transformer (ViT) architecture to recognize mathematical formulas in images and convert them into their corresponding LaTeX representation, facilitating the digitization of scientific documents.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "optical_character_recognition",
        "formula_extraction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/lukas-blecher/LaTeX-OCR",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ocr",
        "latex",
        "mathematical-formulas",
        "deep-learning"
      ],
      "id": 152
    },
    {
      "name": "iFEM",
      "one_line_profile": "MATLAB toolbox for adaptive finite element methods (FEM) on unstructured grids.",
      "detailed_description": "A MATLAB software package containing robust and efficient codes for adaptive finite element methods on unstructured simplicial grids in 2D and 3D, used for numerical simulation in physics and engineering.",
      "domains": [
        "Mathematics",
        "Physics"
      ],
      "subtask_category": [
        "finite_element_analysis",
        "simulation"
      ],
      "application_level": "library",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/lyc102/ifem",
      "help_website": [
        "https://github.com/lyc102/ifem/wiki"
      ],
      "license": "GPL-3.0",
      "tags": [
        "fem",
        "finite-element-method",
        "simulation",
        "matlab"
      ],
      "id": 153
    },
    {
      "name": "mhchemParser",
      "one_line_profile": "Parser for converting mhchem chemical equation syntax to standard LaTeX",
      "detailed_description": "A JavaScript library designed to parse the 'mhchem' LaTeX extension syntax, enabling the rendering of chemical equations and formulas in web environments or downstream LaTeX processors.",
      "domains": [
        "G1",
        "G1-01",
        "Chemistry"
      ],
      "subtask_category": [
        "formula_parsing",
        "latex_rendering"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/mhchem/mhchemParser",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "chemistry",
        "latex",
        "parser",
        "mhchem"
      ],
      "id": 154
    },
    {
      "name": "LaTeX.js",
      "one_line_profile": "JavaScript LaTeX to HTML5 translator",
      "detailed_description": "A JavaScript library that parses LaTeX source code and renders it to HTML5. It enables the structured extraction and display of LaTeX-based scientific documents in web browsers.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "document_parsing",
        "latex_rendering"
      ],
      "application_level": "library",
      "primary_language": "LiveScript",
      "repo_url": "https://github.com/michael-brade/LaTeX.js",
      "help_website": [
        "https://latex.js.org"
      ],
      "license": "MIT",
      "tags": [
        "latex",
        "parser",
        "html5",
        "rendering"
      ],
      "id": 155
    },
    {
      "name": "Document Knowledge Mining Solution Accelerator",
      "one_line_profile": "Azure-based solution for extracting knowledge from unstructured documents",
      "detailed_description": "A solution accelerator that leverages Azure OpenAI and Document Intelligence to ingest, parse, and extract structured metadata, entities, and summaries from unstructured multi-modal documents (PDFs, images).",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "information_extraction",
        "document_summarization",
        "knowledge_mining"
      ],
      "application_level": "workflow",
      "primary_language": "C#",
      "repo_url": "https://github.com/microsoft/Document-Knowledge-Mining-Solution-Accelerator",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "azure",
        "openai",
        "document-intelligence",
        "knowledge-mining"
      ],
      "id": 156
    },
    {
      "name": "MarkItDown",
      "one_line_profile": "Converter for transforming office documents and PDFs to Markdown",
      "detailed_description": "A Python tool developed by Microsoft for converting various file formats (PDF, Word, Excel, PowerPoint) into Markdown. It facilitates the ingestion of scientific documents into LLM pipelines or text analysis workflows.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "document_conversion",
        "layout_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/markitdown",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pdf-to-markdown",
        "document-conversion",
        "nlp",
        "llm-preprocessing"
      ],
      "id": 157
    },
    {
      "name": "Table Transformer (TATR)",
      "one_line_profile": "Deep learning model for table extraction from unstructured documents",
      "detailed_description": "A deep learning model based on the DETR architecture, specifically trained for detecting and extracting tables from PDFs and images. It includes the PubTables-1M dataset and GriTS evaluation metric.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "table_extraction",
        "layout_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/table-transformer",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "table-extraction",
        "deep-learning",
        "pdf-parsing",
        "transformer"
      ],
      "id": 158
    },
    {
      "name": "YOLOv10-Document-Layout-Analysis",
      "one_line_profile": "YOLOv10 model fine-tuned for document layout analysis",
      "detailed_description": "A YOLOv10-based object detection model trained on the DocLayNet dataset to segment and classify document layout elements (headers, footers, tables, figures) in scientific and technical documents.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "layout_analysis",
        "document_segmentation"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/moured/YOLOv10-Document-Layout-Analysis",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "yolov10",
        "document-layout-analysis",
        "doclaynet",
        "computer-vision"
      ],
      "id": 159
    },
    {
      "name": "YOLOv11-Document-Layout-Analysis",
      "one_line_profile": "YOLOv11 model fine-tuned for document layout analysis",
      "detailed_description": "An updated YOLOv11-based model trained on the DocLayNet dataset for high-performance document layout analysis, identifying structure in PDFs and images.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "layout_analysis",
        "document_segmentation"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/moured/YOLOv11-Document-Layout-Analysis",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "yolov11",
        "document-layout-analysis",
        "doclaynet",
        "computer-vision"
      ],
      "id": 160
    },
    {
      "name": "pdf2md",
      "one_line_profile": "Browser-based PDF to Markdown converter",
      "detailed_description": "A tool to convert PDF documents into Markdown format, facilitating the extraction of text and structure from scientific papers for downstream processing or LLM ingestion.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "document_conversion",
        "text_extraction"
      ],
      "application_level": "tool",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/mrmps/pdf2md",
      "help_website": [],
      "license": null,
      "tags": [
        "pdf",
        "markdown",
        "converter",
        "text-extraction"
      ],
      "id": 161
    },
    {
      "name": "pdf3md",
      "one_line_profile": "Web application for converting PDFs to formatted Markdown",
      "detailed_description": "A modern web tool designed to convert PDF documents into clean, formatted Markdown text, useful for digitizing and restructuring scientific literature.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "document_conversion",
        "text_extraction"
      ],
      "application_level": "tool",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/murtaza-nasir/pdf3md",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "pdf",
        "markdown",
        "converter",
        "ocr"
      ],
      "id": 162
    },
    {
      "name": "pretty-formula",
      "one_line_profile": "Java library to parse and render mathematical formulas",
      "detailed_description": "A library that parses mathematical formulas into LaTeX and renders them as images, supporting the visualization of mathematical content in scientific applications.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "formula_parsing",
        "visualization"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/mzur/pretty-formula",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "math",
        "latex",
        "formula-rendering",
        "java"
      ],
      "id": 163
    },
    {
      "name": "image_tabular",
      "one_line_profile": "Integration of image and tabular data for deep learning",
      "detailed_description": "A library extending fastai to handle multimodal datasets containing both images and tabular data, facilitating complex scientific data analysis tasks.",
      "domains": [
        "Scientific Data Analysis"
      ],
      "subtask_category": [
        "multimodal_learning",
        "data_integration"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/naity/image_tabular",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "deep-learning",
        "multimodal",
        "fastai",
        "tabular-data"
      ],
      "id": 164
    },
    {
      "name": "pyDownlinkParser",
      "one_line_profile": "CCSDS binary downlink parser for space science data",
      "detailed_description": "A NASA-developed Python tool for parsing binary downlink files following CCSDS standards, specifically designed for the Europa-Clipper Science Data System.",
      "domains": [
        "Space Science",
        "Data Processing"
      ],
      "subtask_category": [
        "binary_parsing",
        "telemetry_processing"
      ],
      "application_level": "tool",
      "primary_language": "Python",
      "repo_url": "https://github.com/nasa/pyDownlinkParser",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "nasa",
        "ccsds",
        "space-science",
        "telemetry"
      ],
      "id": 165
    },
    {
      "name": "llm-graph-builder",
      "one_line_profile": "Knowledge graph construction from unstructured data using LLMs",
      "detailed_description": "A tool by Neo4j Labs that uses Large Language Models to extract entities and relationships from unstructured text (documents, PDFs) and construct a knowledge graph.",
      "domains": [
        "G1",
        "Knowledge Graph"
      ],
      "subtask_category": [
        "knowledge_extraction",
        "graph_construction"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/neo4j-labs/llm-graph-builder",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "neo4j",
        "llm",
        "knowledge-graph",
        "unstructured-data"
      ],
      "id": 166
    },
    {
      "name": "TabInOut",
      "one_line_profile": "Framework for information extraction from tables",
      "detailed_description": "A framework designed to extract information from tables in documents, supporting the structured analysis of scientific data presented in tabular format.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "table_extraction",
        "information_extraction"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/nikolamilosevic86/TabInOut",
      "help_website": [],
      "license": null,
      "tags": [
        "table-extraction",
        "information-extraction",
        "document-analysis"
      ],
      "id": 167
    },
    {
      "name": "arucogen",
      "one_line_profile": "Online generator for ArUco markers",
      "detailed_description": "A tool to generate ArUco markers, which are widely used in computer vision and robotics research for pose estimation and camera calibration.",
      "domains": [
        "Computer Vision",
        "Robotics"
      ],
      "subtask_category": [
        "marker_generation",
        "calibration"
      ],
      "application_level": "tool",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/okalachev/arucogen",
      "help_website": [
        "http://chev.me/arucogen/"
      ],
      "license": "MIT",
      "tags": [
        "aruco",
        "computer-vision",
        "robotics",
        "marker-generator"
      ],
      "id": 168
    },
    {
      "name": "DocLayout-YOLO",
      "one_line_profile": "Advanced document layout analysis model using YOLO architecture",
      "detailed_description": "A specialized computer vision model for document layout analysis, capable of detecting and segmenting diverse document elements (text, tables, figures) to support downstream scientific literature parsing.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "layout_analysis",
        "document_parsing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/opendatalab/DocLayout-YOLO",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "layout-analysis",
        "yolo",
        "document-understanding"
      ],
      "id": 169
    },
    {
      "name": "MinerU",
      "one_line_profile": "High-quality PDF to Markdown/JSON converter for scientific documents",
      "detailed_description": "A comprehensive data extraction tool designed to transform complex scientific PDFs into LLM-ready structured formats (Markdown/JSON), handling formulas, tables, and layout preservation.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "pdf_parsing",
        "text_extraction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/opendatalab/MinerU",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "pdf-to-markdown",
        "llm-ready",
        "data-extraction"
      ],
      "id": 170
    },
    {
      "name": "PDF-Extract-Kit",
      "one_line_profile": "Comprehensive toolkit for high-quality PDF content extraction",
      "detailed_description": "A toolkit integrating layout analysis, formula detection, table recognition, and OCR to extract high-quality structured content from PDF documents for scientific research and QA systems.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "pdf_extraction",
        "ocr",
        "layout_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/opendatalab/PDF-Extract-Kit",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "pdf-extraction",
        "ocr",
        "table-recognition"
      ],
      "id": 171
    },
    {
      "name": "Open Semantic Search",
      "one_line_profile": "Integrated search engine and text analytics platform for document collections",
      "detailed_description": "An open-source research tool combining ETL, OCR, Named Entity Recognition (NER), and full-text search to analyze and explore large collections of unstructured documents.",
      "domains": [
        "G1"
      ],
      "subtask_category": [
        "semantic_search",
        "text_mining",
        "document_analysis"
      ],
      "application_level": "platform",
      "primary_language": "Shell",
      "repo_url": "https://github.com/opensemanticsearch/open-semantic-search",
      "help_website": [
        "https://www.opensemanticsearch.org/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "semantic-search",
        "etl",
        "text-analytics"
      ],
      "id": 172
    },
    {
      "name": "CRRF-Det",
      "one_line_profile": "Web application for PDF content and table extraction for climate data",
      "detailed_description": "A tool developed under OS-Climate for extracting structured data from PDF reports, featuring visual layout analysis and table parsing, specifically targeted at climate risk and financial reporting.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "table_extraction",
        "pdf_parsing"
      ],
      "application_level": "application",
      "primary_language": "C++",
      "repo_url": "https://github.com/os-climate/crrf-det",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "climate-data",
        "pdf-extraction",
        "table-parsing"
      ],
      "id": 173
    },
    {
      "name": "OpenAlex PDF Parser",
      "one_line_profile": "GROBID-based PDF parser for OpenAlex pipeline",
      "detailed_description": "A Python wrapper and utility set used by OpenAlex to parse scientific PDFs using GROBID, facilitating the extraction of metadata and full text for the OpenAlex knowledge graph.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "citation_parsing",
        "metadata_extraction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ourresearch/openalex-pdf-parser",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "openalex",
        "grobid",
        "pdf-parsing"
      ],
      "id": 174
    },
    {
      "name": "Papercast",
      "one_line_profile": "Pipeline tool for processing technical documents from arXiv/PDF",
      "detailed_description": "A modular pipeline tool for processing scientific papers (from arXiv or local PDFs) using GROBID and LangChain, capable of converting technical documents into various formats including audio summaries.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "document_processing",
        "summarization"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/papercast-dev/papercast",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "arxiv",
        "grobid",
        "pipeline"
      ],
      "id": 175
    },
    {
      "name": "Parsee Core",
      "one_line_profile": "Structured data extraction framework for PDFs and HTML",
      "detailed_description": "A framework for extracting fully structured data from PDFs and HTML files, supporting LLMs and custom models, with a focus on tabular data and multimodal queries for scientific and financial documents.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "data_extraction",
        "table_extraction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/parsee-ai/parsee-core",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "structured-extraction",
        "pdf-parsing",
        "llm"
      ],
      "id": 176
    },
    {
      "name": "Parsee PDF Reader",
      "one_line_profile": "Specialized PDF reader for table and text extraction",
      "detailed_description": "A tool specialized in extracting tables with numeric values and preserving text paragraphs from PDFs, including support for scanned documents and images.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "table_extraction",
        "ocr"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/parsee-ai/parsee-pdf-reader",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pdf-reader",
        "table-extraction",
        "ocr"
      ],
      "id": 177
    },
    {
      "name": "Parsing Science Toolkit",
      "one_line_profile": "Toolkit for parsing and analyzing scientific literature",
      "detailed_description": "A collection of utilities designed for parsing scientific documents and analyzing their structure and content, supporting bibliometric and meta-scientific research.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "document_parsing",
        "bibliometrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/parsing-science/ps-toolkit",
      "help_website": [],
      "license": null,
      "tags": [
        "science-parsing",
        "bibliometrics"
      ],
      "id": 178
    },
    {
      "name": "pdfminer.six",
      "one_line_profile": "Community maintained PDF parsing library",
      "detailed_description": "A robust Python library for extracting text, layout, and metadata from PDF files, serving as a foundational component for many scientific document analysis pipelines.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "pdf_parsing",
        "text_extraction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pdfminer/pdfminer.six",
      "help_website": [
        "https://pdfminersix.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "pdf-parsing",
        "text-extraction",
        "layout-analysis"
      ],
      "id": 179
    },
    {
      "name": "geodict",
      "one_line_profile": "Library for extracting location information from unstructured text",
      "detailed_description": "A tool for identifying and extracting geographic location names from unstructured text, useful for geoparsing scientific literature and reports.",
      "domains": [
        "G1"
      ],
      "subtask_category": [
        "named_entity_recognition",
        "geoparsing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/petewarden/geodict",
      "help_website": [],
      "license": null,
      "tags": [
        "geoparsing",
        "ner",
        "text-mining"
      ],
      "id": 180
    },
    {
      "name": "pylatexenc",
      "one_line_profile": "Simple LaTeX parser and encoder/decoder",
      "detailed_description": "A Python library providing a LaTeX parser and converters for LaTeX-to-Unicode and Unicode-to-LaTeX, essential for processing mathematical formulas and special characters in scientific texts.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "latex_parsing",
        "text_normalization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/phfaist/pylatexenc",
      "help_website": [
        "https://pylatexenc.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "latex",
        "parser",
        "unicode-conversion"
      ],
      "id": 181
    },
    {
      "name": "YOLO-DocLayNet",
      "one_line_profile": "YOLO models trained on DocLayNet for document layout analysis",
      "detailed_description": "Provides YOLO-based models trained on the DocLayNet dataset to perform document layout analysis, identifying headers, footers, paragraphs, and tables in scientific documents.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "layout_analysis",
        "document_segmentation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ppaanngggg/yolo-doclaynet",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "yolo",
        "doclaynet",
        "layout-analysis"
      ],
      "id": 182
    },
    {
      "name": "QMiner",
      "one_line_profile": "Analytic platform for real-time large-scale data streams",
      "detailed_description": "A high-performance analytics platform for processing structured and unstructured data (including text) streams, suitable for large-scale text mining and citation network analysis.",
      "domains": [
        "G1"
      ],
      "subtask_category": [
        "text_mining",
        "data_analytics"
      ],
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/qminer/qminer",
      "help_website": [
        "http://qminer.github.io/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "data-mining",
        "stream-processing",
        "text-analytics"
      ],
      "id": 183
    },
    {
      "name": "Eynollah",
      "one_line_profile": "Document Layout Analysis tool for historical and scientific documents",
      "detailed_description": "A tool developed by the Berlin State Library for document layout analysis, capable of segmenting pages into regions (text, image, table) and extracting reading order, useful for digitizing scientific archives.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "layout_analysis",
        "document_segmentation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/qurator-spk/eynollah",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "layout-analysis",
        "ocr",
        "document-processing"
      ],
      "id": 184
    },
    {
      "name": "Pdf2Dom",
      "one_line_profile": "PDF to HTML DOM parser based on PDFBox",
      "detailed_description": "A Java library that parses PDF documents and converts them into an HTML DOM representation, facilitating the structural analysis and content extraction of PDF files.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "pdf_parsing",
        "format_conversion"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/radkovo/Pdf2Dom",
      "help_website": [
        "http://cssbox.sourceforge.net/pdf2dom/"
      ],
      "license": "LGPL-3.0",
      "tags": [
        "pdf-to-html",
        "dom-parsing",
        "pdfbox"
      ],
      "id": 185
    },
    {
      "name": "PyBibTeX",
      "one_line_profile": "Utility functions for parsing BibTeX files",
      "detailed_description": "A Python utility library for parsing BibTeX files and managing citation reference lists, supporting the construction and analysis of citation networks.",
      "domains": [
        "G1"
      ],
      "subtask_category": [
        "citation_parsing",
        "bibliography_management"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/rasbt/pybibtex",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bibtex",
        "citation-parsing",
        "reference-management"
      ],
      "id": 186
    },
    {
      "name": "document-layout-analysis",
      "one_line_profile": "Document layout analysis tool using Python-OpenCV",
      "detailed_description": "A Python-based tool leveraging OpenCV to perform layout analysis on documents, identifying and segmenting different regions such as text blocks and images, useful for preprocessing scientific literature.",
      "domains": [
        "G1-01",
        "Computer Vision"
      ],
      "subtask_category": [
        "layout_analysis",
        "document_segmentation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/rbaguila/document-layout-analysis",
      "help_website": [],
      "license": null,
      "tags": [
        "opencv",
        "layout-analysis",
        "document-processing"
      ],
      "id": 187
    },
    {
      "name": "smilesDrawer",
      "one_line_profile": "High performance SMILES parser and drawer",
      "detailed_description": "A robust JavaScript component for parsing SMILES strings (chemical structure notation) and rendering them as 2D chemical structure diagrams, essential for cheminformatics and chemical data visualization.",
      "domains": [
        "Chemistry",
        "Cheminformatics"
      ],
      "subtask_category": [
        "molecular_visualization",
        "structure_parsing"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/reymond-group/smilesDrawer",
      "help_website": [
        "https://doc.gdb.tools/smilesDrawer/"
      ],
      "license": "MIT",
      "tags": [
        "smiles",
        "chemistry",
        "visualization"
      ],
      "id": 188
    },
    {
      "name": "TaG",
      "one_line_profile": "Table-to-Graph generation for entity and relation extraction",
      "detailed_description": "Implementation of the ACL 2023 paper 'A Novel Table-to-Graph Generation Approach', providing a method for joint entity and relation extraction from document tables, supporting structured data extraction from scientific literature.",
      "domains": [
        "G1-01",
        "NLP"
      ],
      "subtask_category": [
        "information_extraction",
        "table_processing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ridiculouz/TaG",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "table-extraction",
        "relation-extraction",
        "acl-2023"
      ],
      "id": 189
    },
    {
      "name": "reftagger",
      "one_line_profile": "Parser for unstructured academic citations",
      "detailed_description": "A Python tool designed to parse and tag unstructured academic citations, facilitating the extraction of metadata from scientific bibliographies.",
      "domains": [
        "G1",
        "Bibliometrics"
      ],
      "subtask_category": [
        "citation_parsing",
        "metadata_extraction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/rmcgibbo/reftagger",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "citation",
        "parsing",
        "academic-ref"
      ],
      "id": 190
    },
    {
      "name": "BotanicGarden",
      "one_line_profile": "Dataset for robot navigation in unstructured environments",
      "detailed_description": "A high-quality dataset designed for training and evaluating robot navigation algorithms in unstructured natural environments, supporting robotics research.",
      "domains": [
        "Robotics",
        "Computer Vision"
      ],
      "subtask_category": [
        "dataset",
        "navigation_simulation"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/robot-pesg/BotanicGarden",
      "help_website": [],
      "license": null,
      "tags": [
        "robotics",
        "dataset",
        "navigation"
      ],
      "id": 191
    },
    {
      "name": "tabulapdf",
      "one_line_profile": "R bindings for Tabula PDF Table Extractor",
      "detailed_description": "An R package providing bindings to the Tabula Java library, enabling the extraction of data tables from PDF documents into R dataframes for scientific data analysis.",
      "domains": [
        "G1-01",
        "Data Science"
      ],
      "subtask_category": [
        "table_extraction",
        "pdf_parsing"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/ropensci/tabulapdf",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "pdf",
        "table-extraction",
        "r-package"
      ],
      "id": 192
    },
    {
      "name": "TabularSemanticParsing",
      "one_line_profile": "Translating natural language questions to SQL",
      "detailed_description": "A research codebase from Salesforce Research for semantic parsing, specifically translating natural language questions into structured query language (SQL), supporting interaction with structured scientific databases.",
      "domains": [
        "NLP",
        "Database"
      ],
      "subtask_category": [
        "semantic_parsing",
        "nl2sql"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/salesforce/TabularSemanticParsing",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "nlp",
        "sql",
        "semantic-parsing"
      ],
      "id": 193
    },
    {
      "name": "python-bibtexparser",
      "one_line_profile": "BibTeX parsing library for Python",
      "detailed_description": "A robust Python library for parsing, modifying, and writing BibTeX files, essential for managing and processing bibliographic data in scientific workflows.",
      "domains": [
        "G1",
        "Bibliometrics"
      ],
      "subtask_category": [
        "bibliography_parsing",
        "metadata_management"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/sciunto-org/python-bibtexparser",
      "help_website": [
        "https://bibtexparser.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "bibtex",
        "parser",
        "bibliography"
      ],
      "id": 194
    },
    {
      "name": "ACL-anthology-corpus",
      "one_line_profile": "ACL Anthology corpus with PDFs and GROBID extractions",
      "detailed_description": "A dataset repository providing the ACL Anthology corpus, including PDFs, BibTeX metadata, and structured extractions generated by GROBID, serving as a resource for NLP and scientific literature mining research.",
      "domains": [
        "G1",
        "NLP"
      ],
      "subtask_category": [
        "dataset",
        "corpus_construction"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/shauryr/ACL-anthology-corpus",
      "help_website": [],
      "license": null,
      "tags": [
        "acl",
        "corpus",
        "grobid"
      ],
      "id": 195
    },
    {
      "name": "markdrop",
      "one_line_profile": "PDF to Markdown converter with LLM-based description generation",
      "detailed_description": "A Python package for converting PDF documents to Markdown, featuring extraction of images and tables, and utilizing LLMs to generate descriptive text for extracted elements, facilitating semantic analysis of scientific papers.",
      "domains": [
        "G1-01",
        "NLP"
      ],
      "subtask_category": [
        "pdf_parsing",
        "content_extraction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/shoryasethia/markdrop",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "pdf-to-markdown",
        "llm",
        "extraction"
      ],
      "id": 196
    },
    {
      "name": "latex-parser",
      "one_line_profile": "LaTeX parser and AST generator",
      "detailed_description": "A tool to parse LaTeX code into an Abstract Syntax Tree (AST) and pretty-print it, enabling programmatic analysis and manipulation of LaTeX-based scientific documents.",
      "domains": [
        "G1-01",
        "Document Processing"
      ],
      "subtask_category": [
        "latex_parsing",
        "ast_generation"
      ],
      "application_level": "library",
      "primary_language": "TeX",
      "repo_url": "https://github.com/siefkenj/latex-parser",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "latex",
        "parser",
        "ast"
      ],
      "id": 197
    },
    {
      "name": "unified-latex",
      "one_line_profile": "Unified.js processor for LaTeX",
      "detailed_description": "A comprehensive ecosystem for parsing, inspecting, transforming, and printing LaTeX, built on the Unified.js interface. It allows for structured manipulation of scientific manuscripts written in LaTeX.",
      "domains": [
        "G1-01",
        "Document Processing"
      ],
      "subtask_category": [
        "latex_processing",
        "ast_transformation"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/siefkenj/unified-latex",
      "help_website": [
        "https://unified-latex.siefkenj.com/"
      ],
      "license": "MIT",
      "tags": [
        "latex",
        "unifiedjs",
        "parsing"
      ],
      "id": 198
    },
    {
      "name": "suql",
      "one_line_profile": "Conversational search over structured and unstructured data",
      "detailed_description": "A framework for conversational search that integrates structured (SQL) and unstructured data retrieval using LLMs, applicable to querying complex scientific databases and literature collections.",
      "domains": [
        "NLP",
        "Information Retrieval"
      ],
      "subtask_category": [
        "conversational_search",
        "hybrid_retrieval"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/stanford-oval/suql",
      "help_website": [
        "https://suql.stanford.edu/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "search",
        "sql"
      ],
      "id": 199
    },
    {
      "name": "laypa",
      "one_line_profile": "Document layout analysis tool",
      "detailed_description": "A tool for layout analysis to identify and classify elements in documents, similar to P2PaLA, useful for structural analysis of scientific papers.",
      "domains": [
        "G1-01",
        "Computer Vision"
      ],
      "subtask_category": [
        "layout_analysis",
        "document_understanding"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/stefanklut/laypa",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "layout-analysis",
        "document-processing"
      ],
      "id": 200
    },
    {
      "name": "pdf2text",
      "one_line_profile": "A simplified wrapper for PDFMiner to facilitate text extraction from PDFs",
      "detailed_description": "A Python library that wraps PDFMiner to provide a simpler interface for extracting text content from PDF files, aiming to reduce the complexity of using the raw PDFMiner API.",
      "domains": [
        "Sci Knowledge",
        "G1-01"
      ],
      "subtask_category": [
        "text_extraction",
        "pdf_parsing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/syllabs/pdf2text",
      "help_website": [],
      "license": null,
      "tags": [
        "pdf",
        "text-extraction",
        "pdfminer-wrapper"
      ],
      "id": 201
    },
    {
      "name": "mdast-util-math",
      "one_line_profile": "Extension for mdast to parse and serialize mathematical equations",
      "detailed_description": "A utility library for the mdast (Markdown Abstract Syntax Tree) ecosystem that enables parsing and serialization of math syntax (LaTeX equations) within Markdown documents.",
      "domains": [
        "Sci Knowledge",
        "G1-01"
      ],
      "subtask_category": [
        "math_parsing",
        "syntax_tree_processing"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/syntax-tree/mdast-util-math",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "markdown",
        "latex",
        "math",
        "ast"
      ],
      "id": 202
    },
    {
      "name": "Tabula",
      "one_line_profile": "A tool for liberating data tables trapped inside PDF files",
      "detailed_description": "A popular tool designed to extract tabular data from PDF files into CSV or Excel formats. It provides a user-friendly interface and a robust extraction engine to handle various PDF table layouts.",
      "domains": [
        "Sci Knowledge",
        "G1-01"
      ],
      "subtask_category": [
        "table_extraction",
        "data_liberation"
      ],
      "application_level": "solver",
      "primary_language": "CSS",
      "repo_url": "https://github.com/tabulapdf/tabula",
      "help_website": [
        "https://tabula.technology/"
      ],
      "license": "MIT",
      "tags": [
        "pdf",
        "table-extraction",
        "ocr"
      ],
      "id": 203
    },
    {
      "name": "tabula-extractor",
      "one_line_profile": "Ruby-based engine for extracting tables from PDF files",
      "detailed_description": "The core extraction engine for Tabula written in Ruby, providing the functionality to detect and extract tables from PDF documents programmatically.",
      "domains": [
        "Sci Knowledge",
        "G1-01"
      ],
      "subtask_category": [
        "table_extraction"
      ],
      "application_level": "library",
      "primary_language": "Ruby",
      "repo_url": "https://github.com/tabulapdf/tabula-extractor",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ruby",
        "pdf",
        "table-extraction"
      ],
      "id": 204
    },
    {
      "name": "tabula-java",
      "one_line_profile": "Java library for extracting tables from PDF files",
      "detailed_description": "The Java implementation of the Tabula extraction engine, allowing developers to embed PDF table extraction capabilities into Java applications.",
      "domains": [
        "Sci Knowledge",
        "G1-01"
      ],
      "subtask_category": [
        "table_extraction"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/tabulapdf/tabula-java",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "java",
        "pdf",
        "table-extraction"
      ],
      "id": 205
    },
    {
      "name": "latex-utensils",
      "one_line_profile": "A LaTeX and BibTeX parser and utility library",
      "detailed_description": "A TypeScript library providing parsers for LaTeX and BibTeX formats, enabling the analysis and manipulation of bibliographic data and scientific document structures.",
      "domains": [
        "Sci Knowledge",
        "G1-01"
      ],
      "subtask_category": [
        "latex_parsing",
        "bibtex_parsing"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/tamuratak/latex-utensils",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "latex",
        "bibtex",
        "parser",
        "typescript"
      ],
      "id": 206
    },
    {
      "name": "parse-latex",
      "one_line_profile": "Pandoc filter to parse raw LaTeX snippets",
      "detailed_description": "A Lua filter for Pandoc that parses raw LaTeX snippets within documents and includes the structured results in the output, facilitating mixed-format document processing.",
      "domains": [
        "Sci Knowledge",
        "G1-01"
      ],
      "subtask_category": [
        "document_conversion",
        "latex_parsing"
      ],
      "application_level": "library",
      "primary_language": "Lua",
      "repo_url": "https://github.com/tarleb/parse-latex",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pandoc",
        "latex",
        "filter",
        "lua"
      ],
      "id": 207
    },
    {
      "name": "galactic",
      "one_line_profile": "Data cleaning and curation library for unstructured text",
      "detailed_description": "A Python library designed for massive-scale data cleaning and curation of unstructured text, particularly useful for preparing scientific text datasets for LLM training.",
      "domains": [
        "Sci Knowledge",
        "G1"
      ],
      "subtask_category": [
        "data_cleaning",
        "text_curation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/taylorai/galactic",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "nlp",
        "data-cleaning",
        "unstructured-data"
      ],
      "id": 208
    },
    {
      "name": "Tabulo",
      "one_line_profile": "Deep learning based table detection and extraction tool",
      "detailed_description": "A Python tool leveraging deep learning models (Luminoth, TensorFlow) to detect and extract tables from document images, providing labeled coordinates and confidence scores.",
      "domains": [
        "Sci Knowledge",
        "G1-01"
      ],
      "subtask_category": [
        "table_detection",
        "layout_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/the-black-knight-01/Tabulo",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "deep-learning",
        "table-detection",
        "ocr"
      ],
      "id": 209
    },
    {
      "name": "markdown2pdf",
      "one_line_profile": "Markdown to PDF transpiler written in Rust",
      "detailed_description": "A high-performance command-line tool and library written in Rust for converting Markdown documents into PDF format, useful for generating scientific reports from structured text.",
      "domains": [
        "Sci Knowledge",
        "G1-01"
      ],
      "subtask_category": [
        "document_generation",
        "format_conversion"
      ],
      "application_level": "solver",
      "primary_language": "Rust",
      "repo_url": "https://github.com/theiskaa/markdown2pdf",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "markdown",
        "pdf",
        "rust",
        "converter"
      ],
      "id": 210
    },
    {
      "name": "wdoc",
      "one_line_profile": "Tool for summarizing and querying heterogeneous documents using LLMs",
      "detailed_description": "A Python tool that enables advanced RAG (Retrieval-Augmented Generation) and summarization over large collections of heterogeneous documents, supporting scientific literature review and knowledge extraction.",
      "domains": [
        "Sci Knowledge",
        "G1"
      ],
      "subtask_category": [
        "literature_review",
        "summarization",
        "rag"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/thiswillbeyourgithub/wdoc",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "llm",
        "rag",
        "summarization",
        "document-analysis"
      ],
      "id": 211
    },
    {
      "name": "traprange",
      "one_line_profile": "Java method to extract tabular content from PDF files",
      "detailed_description": "A Java library implementing the TrapRange method for extracting complex tables from PDF documents, focusing on handling row and column detection in challenging layouts.",
      "domains": [
        "Sci Knowledge",
        "G1-01"
      ],
      "subtask_category": [
        "table_extraction"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/thoqbk/traprange",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pdf",
        "table-extraction",
        "java"
      ],
      "id": 212
    },
    {
      "name": "synonym_detection",
      "one_line_profile": "Tool for mining synonyms from unstructured and semi-structured data",
      "detailed_description": "A Python toolkit for detecting and mining synonyms from large-scale unstructured text data, useful for constructing scientific knowledge graphs and normalizing terminology.",
      "domains": [
        "Sci Knowledge",
        "G1"
      ],
      "subtask_category": [
        "synonym_mining",
        "terminology_extraction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/tigerchen52/synonym_detection",
      "help_website": [],
      "license": null,
      "tags": [
        "nlp",
        "synonym-detection",
        "text-mining"
      ],
      "id": 213
    },
    {
      "name": "scipdf_parser",
      "one_line_profile": "Python parser for scientific PDF publications",
      "detailed_description": "A Python library specifically designed to parse scientific PDF publications, extracting structured content including title, abstract, sections, and figures using GROBID and other backend tools.",
      "domains": [
        "Sci Knowledge",
        "G1-01"
      ],
      "subtask_category": [
        "pdf_parsing",
        "structure_extraction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/titipata/scipdf_parser",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pdf",
        "scientific-literature",
        "grobid",
        "parsing"
      ],
      "id": 214
    },
    {
      "name": "wos_parser",
      "one_line_profile": "Parser for Web of Science XML data",
      "detailed_description": "A Python library for parsing XML data exports from Web of Science, enabling bibliometric analysis and metadata extraction from scientific citation databases.",
      "domains": [
        "Sci Knowledge",
        "G1"
      ],
      "subtask_category": [
        "metadata_extraction",
        "bibliometrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/titipata/wos_parser",
      "help_website": [],
      "license": null,
      "tags": [
        "web-of-science",
        "xml-parser",
        "bibliometrics"
      ],
      "id": 215
    },
    {
      "name": "ColiVara",
      "one_line_profile": "Visual retrieval service for documents using vision models",
      "detailed_description": "A document retrieval system that uses vision models to generate visual embeddings for pages, allowing for search and retrieval without relying on OCR or text extraction, particularly useful for complex scientific layouts.",
      "domains": [
        "Sci Knowledge",
        "G1"
      ],
      "subtask_category": [
        "document_retrieval",
        "visual_search"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/tjmlabs/ColiVara",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "rag",
        "vision-model",
        "document-retrieval"
      ],
      "id": 216
    },
    {
      "name": "SpanMarkerNER",
      "one_line_profile": "Named Entity Recognition using SpanMarker",
      "detailed_description": "A library for Named Entity Recognition (NER) that uses the SpanMarker architecture to effectively identify and classify entities in text, applicable to scientific entity extraction.",
      "domains": [
        "Sci Knowledge",
        "G1"
      ],
      "subtask_category": [
        "named_entity_recognition",
        "information_extraction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/tomaarsen/SpanMarkerNER",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ner",
        "nlp",
        "transformers"
      ],
      "id": 217
    },
    {
      "name": "YaLafi",
      "one_line_profile": "Yet another LaTeX filter for text processing",
      "detailed_description": "A Python tool designed to filter and process LaTeX documents, often used to prepare LaTeX text for grammar checking or other NLP tasks by stripping commands while preserving content.",
      "domains": [
        "Sci Knowledge",
        "G1-01"
      ],
      "subtask_category": [
        "latex_processing",
        "text_filtering"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/torik42/YaLafi",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "latex",
        "filter",
        "nlp"
      ],
      "id": 218
    },
    {
      "name": "citationberg",
      "one_line_profile": "Library for parsing CSL (Citation Style Language) styles",
      "detailed_description": "A Rust library for parsing and processing Citation Style Language (CSL) files, used for formatting citations and bibliographies in scientific publishing workflows (e.g., within Typst).",
      "domains": [
        "Sci Knowledge",
        "G1"
      ],
      "subtask_category": [
        "citation_parsing",
        "bibliography_formatting"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/typst/citationberg",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "csl",
        "citation",
        "rust",
        "typst"
      ],
      "id": 219
    },
    {
      "name": "UCNS3D",
      "one_line_profile": "Unstructured Compressible Navier Stokes 3D solver",
      "detailed_description": "A high-order computational fluid dynamics (CFD) code for solving the Unstructured Compressible Navier-Stokes equations in 3D, used for aerodynamic and fluid flow simulations.",
      "domains": [
        "Physics",
        "Fluid Dynamics"
      ],
      "subtask_category": [
        "simulation",
        "cfd_solver"
      ],
      "application_level": "solver",
      "primary_language": "Fortran",
      "repo_url": "https://github.com/ucns3d-team/UCNS3D",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "cfd",
        "fluid-dynamics",
        "simulation",
        "fortran"
      ],
      "id": 220
    },
    {
      "name": "nanonispy",
      "one_line_profile": "Library to parse Nanonis binary and ASCII files",
      "detailed_description": "A Python library for parsing binary (.sxm) and ASCII (.dat) files generated by Nanonis control systems, widely used in scanning tunneling microscopy (STM) and atomic force microscopy (AFM).",
      "domains": [
        "Materials Science",
        "Physics"
      ],
      "subtask_category": [
        "data_parsing",
        "microscopy_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/underchemist/nanonispy",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nanonis",
        "stm",
        "afm",
        "file-parsing"
      ],
      "id": 221
    },
    {
      "name": "GraphGPT",
      "one_line_profile": "Tool for extrapolating knowledge graphs from unstructured text using LLMs",
      "detailed_description": "A tool that leverages GPT models to extract entities and relationships from unstructured text and visualize them as a knowledge graph, aiding in scientific knowledge discovery.",
      "domains": [
        "Sci Knowledge",
        "G1"
      ],
      "subtask_category": [
        "knowledge_graph_extraction",
        "relation_extraction"
      ],
      "application_level": "solver",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/varunshenoy/GraphGPT",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "knowledge-graph",
        "llm",
        "visualization"
      ],
      "id": 222
    },
    {
      "name": "tabular-feature-selection",
      "one_line_profile": "Feature selection library for tabular data models",
      "detailed_description": "A Python repository providing methods and tools for feature selection specifically designed for tabular data, supporting machine learning workflows in data science.",
      "domains": [
        "Data Science",
        "Machine Learning"
      ],
      "subtask_category": [
        "feature_selection",
        "data_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/vcherepanova/tabular-feature-selection",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "feature-selection",
        "tabular-data",
        "machine-learning"
      ],
      "id": 223
    },
    {
      "name": "algebra-latex",
      "one_line_profile": "Library to parse and calculate LaTeX formatted math",
      "detailed_description": "A JavaScript library that parses LaTeX-formatted mathematical expressions and performs calculations or algebraic manipulations, useful for educational tools or scientific web applications.",
      "domains": [
        "Sci Knowledge",
        "G1-01"
      ],
      "subtask_category": [
        "math_parsing",
        "calculation"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/viktorstrate/algebra-latex",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "latex",
        "math",
        "parser",
        "calculator"
      ],
      "id": 224
    },
    {
      "name": "Knowledge Table",
      "one_line_profile": "Tool for extracting structured data from unstructured documents for RAG/KG",
      "detailed_description": "A package designed to simplify the extraction and exploration of structured data (tables, entities) from unstructured documents like PDFs, facilitating knowledge base construction.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "information_extraction",
        "structured_data_extraction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/whyhow-ai/knowledge-table",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "unstructured-data",
        "knowledge-graph"
      ],
      "id": 225
    },
    {
      "name": "E2M",
      "one_line_profile": "Universal document to Markdown converter for LLM pipelines",
      "detailed_description": "A tool that converts various file types (doc, docx, pdf, epub, html) into Markdown, specifically designed to normalize scientific and technical documents for LLM processing and RAG systems.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "document_conversion",
        "format_normalization"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/wisupai/e2m",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "pdf-to-markdown",
        "rag",
        "document-parsing"
      ],
      "id": 226
    },
    {
      "name": "img2table",
      "one_line_profile": "Table identification and extraction from PDFs and images",
      "detailed_description": "A library for identifying and extracting tables from PDF documents and images, utilizing OpenCV for image processing to recover structural data from scientific literature.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "table_extraction",
        "ocr"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/xavctn/img2table",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "table-extraction",
        "pdf-mining",
        "opencv"
      ],
      "id": 227
    },
    {
      "name": "freki",
      "one_line_profile": "Analyzer for XML extracted from scientific PDFs",
      "detailed_description": "A tool to analyze and process XML data extracted from PDFs (e.g., via PDFMiner or TET), facilitating the structural analysis of scientific documents.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "xml_parsing",
        "document_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/xigt/freki",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pdf-analysis",
        "xml-processing",
        "scientific-literature"
      ],
      "id": 228
    },
    {
      "name": "llm-based-ocr",
      "one_line_profile": "LLM-powered PDF to Markdown OCR tool",
      "detailed_description": "An API and tool that leverages Large Language Models with vision capabilities to perform high-accuracy OCR and convert PDF documents into Markdown, suitable for scientific paper digitization.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "ocr",
        "document_conversion"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/yigitkonur/llm-based-ocr",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "llm",
        "ocr",
        "pdf-to-markdown"
      ],
      "id": 229
    },
    {
      "name": "extractous",
      "one_line_profile": "High-performance unstructured data extraction library",
      "detailed_description": "A fast data extraction library written in Rust that converts unstructured files (PDF, DOCX, etc.) into structured text, suitable for building scientific knowledge bases and ETL pipelines.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "data_extraction",
        "etl"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/yobix-ai/extractous",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rust",
        "pdf-extraction",
        "unstructured-data"
      ],
      "id": 230
    },
    {
      "name": "RoDLA",
      "one_line_profile": "Benchmark for Document Layout Analysis robustness",
      "detailed_description": "A benchmark suite and dataset designed to evaluate the robustness of Document Layout Analysis (DLA) models, critical for assessing tools used in scientific literature parsing.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "benchmarking",
        "layout_analysis"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/yufanchen96/RoDLA",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "benchmark",
        "document-layout-analysis",
        "robustness"
      ],
      "id": 231
    },
    {
      "name": "faster-nougat",
      "one_line_profile": "Optimized local implementation of Nougat PDF parser",
      "detailed_description": "An efficient implementation of the Nougat model designed for local processing of scientific PDFs, converting them into structured Markdown with mathematical formulas.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "pdf_parsing",
        "ocr"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/zhuzilin/faster-nougat",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nougat",
        "pdf-to-markdown",
        "scientific-parsing"
      ],
      "id": 232
    },
    {
      "name": "tetsuo-dox-agent",
      "one_line_profile": "AI-powered research assistant for drafting answers with citations",
      "detailed_description": "A research agent leveraging Large Language Models to provide technically-focused answers with robust citations. It employs a graph-based architecture to draft answers, perform self-reflection, conduct research, and iteratively improve responses.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "literature_review",
        "citation_retrieval"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/7etsuo/tetsuo-dox-agent",
      "help_website": [],
      "license": null,
      "tags": [
        "llm",
        "research-assistant",
        "citation-generation"
      ],
      "id": 233
    },
    {
      "name": "GIANT-Citation-String-Generator",
      "one_line_profile": "Script to generate synthetic annotated bibliographic reference strings",
      "detailed_description": "A tool designed to generate tagged XML citation strings for training and evaluating citation parsing algorithms. It supports the creation of large-scale synthetic datasets for bibliometric analysis.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "data_generation",
        "citation_parsing"
      ],
      "application_level": "solver",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/BeelGroup/GIANT-The-1-Billion-Annotated-Synthetic-Bibliographic-Reference-String-Dataset",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "synthetic-data",
        "citation-parsing",
        "bibliometrics"
      ],
      "id": 234
    },
    {
      "name": "citation-network-explorer",
      "one_line_profile": "Interactive tool for exploring and visualizing local citation networks",
      "detailed_description": "A web-based tool that allows users to visualize and explore citation networks. It helps researchers understand the connections between papers and discover relevant literature through network visualization.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "visualization",
        "network_analysis"
      ],
      "application_level": "solver",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/CitationGecko/citation-network-explorer",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "visualization",
        "citation-network",
        "literature-review"
      ],
      "id": 235
    },
    {
      "name": "CiteGraph",
      "one_line_profile": "Web-based visualizer for citation graphs",
      "detailed_description": "A web application for visualizing citation graphs, enabling users to interactively explore the relationships between scientific publications.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "visualization",
        "network_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Java",
      "repo_url": "https://github.com/Citegraph/citegraph",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "visualization",
        "citation-graph",
        "bibliometrics"
      ],
      "id": 236
    },
    {
      "name": "citation-graph-builder",
      "one_line_profile": "Tool for creating and visualizing citation networks from PDFs and APIs",
      "detailed_description": "A tool that combines citation data obtained from parsing paper PDF files and querying bibliographic APIs to create and visualize citation networks. Developed by Forschungszentrum Jülich (FZJ).",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "visualization",
        "network_construction",
        "pdf_parsing"
      ],
      "application_level": "solver",
      "primary_language": "TeX",
      "repo_url": "https://github.com/FZJ-IEK3-VSA/citation-graph-builder",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "citation-network",
        "visualization",
        "bibliometrics"
      ],
      "id": 237
    },
    {
      "name": "BibliometricVisualization",
      "one_line_profile": "Integrated platform for bibliometric visualization and analysis",
      "detailed_description": "A bibliometric visualization platform that integrates Gestalt design principles, keyword extraction, temporal algorithms, machine learning, and large language models to analyze and visualize scientific literature data.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "visualization",
        "bibliometrics",
        "keyword_extraction"
      ],
      "application_level": "platform",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/FengDushuo/BibliometricVisualization",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bibliometrics",
        "visualization",
        "llm"
      ],
      "id": 238
    },
    {
      "name": "OneCite",
      "one_line_profile": "Intelligent toolkit for parsing and formatting academic references",
      "detailed_description": "A toolkit designed to automatically parse, complete, and format academic references. It supports the Model Context Protocol (MCP) and aids in managing bibliographic data.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "reference_parsing",
        "formatting",
        "metadata_completion"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/HzaCode/OneCite",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "reference-management",
        "parsing",
        "formatting"
      ],
      "id": 239
    },
    {
      "name": "PubTrends",
      "one_line_profile": "Scientific literature explorer for visualizing and analyzing research trends and citation networks",
      "detailed_description": "A tool that runs searches on Pubmed or Semantic Scholar and allows users to explore the high-level structure of result papers through visualization and topological analysis.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "literature_exploration",
        "citation_network_analysis",
        "visualization"
      ],
      "application_level": "application",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/JetBrains-Research/pubtrends",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "scientometrics",
        "visualization",
        "literature-review"
      ],
      "id": 240
    },
    {
      "name": "MeDuSA",
      "one_line_profile": "Fine-resolution cellular deconvolution method for RNA-seq data",
      "detailed_description": "A method that leverages scRNA-seq data as a reference to estimate cell-state abundance in bulk RNA-seq data, useful for biological data analysis.",
      "domains": [
        "Bioinformatics",
        "Genomics"
      ],
      "subtask_category": [
        "deconvolution",
        "abundance_estimation"
      ],
      "application_level": "solver",
      "primary_language": "R",
      "repo_url": "https://github.com/JianYang-Lab/MeDuSA",
      "help_website": [],
      "license": null,
      "tags": [
        "rna-seq",
        "deconvolution",
        "single-cell"
      ],
      "id": 241
    },
    {
      "name": "novelpy",
      "one_line_profile": "Python package for computing bibliometric novelty indicators",
      "detailed_description": "A library designed to compute various novelty indicators for scientific publications based on co-occurrence matrices of references, keywords, or other metadata.",
      "domains": [
        "G1",
        "G1-02",
        "Scientometrics"
      ],
      "subtask_category": [
        "bibliometrics",
        "novelty_detection",
        "indicator_calculation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Kwirtz/novelpy",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bibliometrics",
        "novelty",
        "indicators"
      ],
      "id": 242
    },
    {
      "name": "Local Citation Network",
      "one_line_profile": "Web application for literature review using local citation networks",
      "detailed_description": "A tool that helps scientists perform literature reviews by visualizing and analyzing local citation networks using metadata from OpenAlex, Semantic Scholar, and Crossref.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "literature_review",
        "citation_network_visualization",
        "metadata_aggregation"
      ],
      "application_level": "platform",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/LocalCitationNetwork/LocalCitationNetwork.github.io",
      "help_website": [
        "https://localcitationnetwork.github.io"
      ],
      "license": "GPL-3.0",
      "tags": [
        "citation-network",
        "literature-review",
        "openalex"
      ],
      "id": 243
    },
    {
      "name": "scimeetr",
      "one_line_profile": "R package and Shiny app for bibliometric analysis and mapping",
      "detailed_description": "A suite of functions and a web interface to load bibliometric data, create networks, identify research communities, and generate reading lists.",
      "domains": [
        "G1",
        "G1-02",
        "Scientometrics"
      ],
      "subtask_category": [
        "bibliometrics",
        "network_mapping",
        "community_detection"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/MaximeRivest/scimeetr",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bibliometrics",
        "shiny",
        "r-package"
      ],
      "id": 244
    },
    {
      "name": "LitStudy",
      "one_line_profile": "Python library for automated scientific literature analysis",
      "detailed_description": "A Python package that enables researchers to perform literature analysis, including searching, refining, and visualizing bibliometric data within Jupyter notebooks.",
      "domains": [
        "G1",
        "G1-02",
        "Scientometrics"
      ],
      "subtask_category": [
        "literature_analysis",
        "bibliometrics",
        "visualization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NLeSC/litstudy",
      "help_website": [
        "https://nlesc.github.io/litstudy/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "literature-analysis",
        "bibliometrics",
        "python"
      ],
      "id": 245
    },
    {
      "name": "bibtexParseJs",
      "one_line_profile": "JavaScript library for parsing BibTeX files",
      "detailed_description": "A library maintained by ORCID for parsing BibTeX formatted citations into JSON, facilitating the handling of bibliographic data in web applications.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "parsing",
        "citation_processing"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/ORCID/bibtexParseJs",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bibtex",
        "parser",
        "javascript"
      ],
      "id": 246
    },
    {
      "name": "talaria_bibtex",
      "one_line_profile": "OCaml library for parsing BibTeX files",
      "detailed_description": "A parser for BibTeX files written in OCaml, useful for processing bibliographic databases in functional programming environments.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "parsing",
        "bibliography_management"
      ],
      "application_level": "library",
      "primary_language": "OCaml",
      "repo_url": "https://github.com/Octachron/talaria_bibtex",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "bibtex",
        "parser",
        "ocaml"
      ],
      "id": 247
    },
    {
      "name": "Equi7Grid",
      "one_line_profile": "Spatial reference system optimized for global high-resolution raster data",
      "detailed_description": "A Python package defining the Equi7Grid, a spatial reference system designed to handle global high-resolution raster data efficiently with minimal distortion.",
      "domains": [
        "Earth Science",
        "GIS"
      ],
      "subtask_category": [
        "spatial_referencing",
        "raster_processing",
        "grid_definition"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/TUW-GEO/Equi7Grid",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gis",
        "raster",
        "spatial-reference"
      ],
      "id": 248
    },
    {
      "name": "metaknowledge",
      "one_line_profile": "Python library for bibliometric and network analysis",
      "detailed_description": "A comprehensive Python library designed for doing bibliometric and network analysis in science and health policy research, supporting parsing, analysis, and visualization of citation data.",
      "domains": [
        "G1",
        "G1-02",
        "Scientometrics"
      ],
      "subtask_category": [
        "bibliometrics",
        "network_analysis",
        "citation_parsing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/UWNETLAB/metaknowledge",
      "help_website": [
        "https://metaknowledge.readthedocs.io/"
      ],
      "license": "GPL-2.0",
      "tags": [
        "bibliometrics",
        "network-analysis",
        "python"
      ],
      "id": 249
    },
    {
      "name": "pybibx",
      "one_line_profile": "AI-powered bibliometric and scientometric Python library",
      "detailed_description": "A Python library that integrates artificial intelligence tools for performing bibliometric and scientometric analyses, including data processing and visualization.",
      "domains": [
        "G1",
        "G1-02",
        "Scientometrics"
      ],
      "subtask_category": [
        "bibliometrics",
        "scientometrics",
        "ai_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Valdecy/pybibx",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "bibliometrics",
        "ai",
        "python"
      ],
      "id": 250
    },
    {
      "name": "Scholia",
      "one_line_profile": "Wikidata-based scholarly profile visualization tool",
      "detailed_description": "A web service and tool that generates scholarly profiles for researchers, organizations, and topics using data from Wikidata, visualizing citation networks and research outputs.",
      "domains": [
        "G1",
        "G1-02",
        "Scientometrics"
      ],
      "subtask_category": [
        "scholarly_profiling",
        "visualization",
        "wikidata_integration"
      ],
      "application_level": "service",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/WDscholia/scholia",
      "help_website": [
        "https://scholia.toolforge.org/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "wikidata",
        "scientometrics",
        "visualization"
      ],
      "id": 251
    },
    {
      "name": "biblicit",
      "one_line_profile": "Citation extraction tool for PDF documents",
      "detailed_description": "A Ruby-based utility developed by Academia.edu to extract and parse citations directly from PDF files of scientific papers, facilitating metadata retrieval and citation network construction.",
      "domains": [
        "Sci Knowledge",
        "G1-02"
      ],
      "subtask_category": [
        "citation_extraction",
        "pdf_parsing"
      ],
      "application_level": "solver",
      "primary_language": "Ruby",
      "repo_url": "https://github.com/academia-edu/biblicit",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "citation-extraction",
        "pdf-parsing",
        "bibliometrics"
      ],
      "id": 252
    },
    {
      "name": "biblib",
      "one_line_profile": "Robust BibTeX parser and processing library",
      "detailed_description": "A simple yet faithful BibTeX parser for Python 3 that handles common BibTeX idiosyncrasies, allowing for the manipulation, normalization, and analysis of bibliographic databases.",
      "domains": [
        "Sci Knowledge",
        "G1-02"
      ],
      "subtask_category": [
        "metadata_parsing",
        "bibliography_management"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/aclements/biblib",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bibtex",
        "parser",
        "bibliography"
      ],
      "id": 253
    },
    {
      "name": "Academic Review Tool (ART)",
      "one_line_profile": "Automated pipeline for academic literature review and bibliometrics",
      "detailed_description": "A Python package developed by the Alan Turing Institute to streamline the process of academic reviews. It provides capabilities for discovering, retrieving, and analyzing academic literature at scale, supporting systematic reviews and bibliometric studies.",
      "domains": [
        "Sci Knowledge",
        "G1-02"
      ],
      "subtask_category": [
        "literature_review",
        "bibliometrics",
        "systematic_review"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/alan-turing-institute/academic_review_tool",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "systematic-review",
        "bibliometrics",
        "literature-search"
      ],
      "id": 254
    },
    {
      "name": "S2AND",
      "one_line_profile": "Semantic Scholar's Author Name Disambiguation algorithm",
      "detailed_description": "The official implementation of the Author Name Disambiguation (AND) algorithm used by Semantic Scholar. It provides a unified framework and evaluation suite for clustering author signatures into distinct author entities within bibliographic databases.",
      "domains": [
        "Sci Knowledge",
        "G1-02"
      ],
      "subtask_category": [
        "author_disambiguation",
        "entity_resolution"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/allenai/S2AND",
      "help_website": [
        "https://www.semanticscholar.org/product/api"
      ],
      "license": "NOASSERTION",
      "tags": [
        "author-disambiguation",
        "clustering",
        "bibliometrics"
      ],
      "id": 255
    },
    {
      "name": "Citeomatic",
      "one_line_profile": "AI-powered citation recommendation system",
      "detailed_description": "A tool developed by AllenAI for citation recommendation, allowing users to find relevant citations for paper drafts based on text context. It leverages the Semantic Scholar OpenCorpus dataset to suggest missing references.",
      "domains": [
        "Sci Knowledge",
        "G1-02"
      ],
      "subtask_category": [
        "citation_recommendation",
        "literature_discovery"
      ],
      "application_level": "service",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/allenai/citeomatic",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "citation-recommendation",
        "nlp",
        "scientific-writing"
      ],
      "id": 256
    },
    {
      "name": "peS2o",
      "one_line_profile": "Pretraining Efficiently on S2ORC dataset",
      "detailed_description": "A toolkit and dataset derived from S2ORC for pretraining language models on scientific text. It provides cleaned and filtered scientific papers to facilitate the development of Sci-BERT/Sci-LLM style models.",
      "domains": [
        "Sci Knowledge",
        "NLP"
      ],
      "subtask_category": [
        "model_pretraining",
        "corpus_processing"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/allenai/peS2o",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-pretraining",
        "scientific-corpus",
        "nlp"
      ],
      "id": 257
    },
    {
      "name": "S2ORC",
      "one_line_profile": "The Semantic Scholar Open Research Corpus",
      "detailed_description": "A large-scale corpus of English-language academic papers spanning many disciplines. While primarily a dataset, the repository serves as the entry point for accessing structured full-text and metadata for millions of scientific papers.",
      "domains": [
        "Sci Knowledge",
        "G1"
      ],
      "subtask_category": [
        "corpus_access",
        "text_mining"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/allenai/s2orc",
      "help_website": [
        "https://github.com/allenai/s2orc"
      ],
      "license": null,
      "tags": [
        "corpus",
        "scientific-text",
        "dataset"
      ],
      "id": 258
    },
    {
      "name": "s2orc-doc2json",
      "one_line_profile": "Scientific paper parsers (PDF/TeX/JATS to JSON)",
      "detailed_description": "A suite of parsers developed by AllenAI to convert scientific documents from various formats (PDF, LaTeX, JATS XML) into a unified, structured JSON format (S2ORC format), enabling large-scale text mining and analysis.",
      "domains": [
        "Sci Knowledge",
        "G1-02"
      ],
      "subtask_category": [
        "document_parsing",
        "pdf_to_json",
        "format_conversion"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/allenai/s2orc-doc2json",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "pdf-parsing",
        "latex-parsing",
        "scientific-text-processing"
      ],
      "id": 259
    },
    {
      "name": "openeditors",
      "one_line_profile": "Data and scraper for scientific journal editors",
      "detailed_description": "An R package and dataset for collecting and analyzing data about editors of scientific journals. It facilitates bibliometric research into editorial board composition, diversity, and network structures.",
      "domains": [
        "Sci Knowledge",
        "Bibliometrics"
      ],
      "subtask_category": [
        "data_collection",
        "editorial_analysis"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/andreaspacher/openeditors",
      "help_website": [],
      "license": "CC0-1.0",
      "tags": [
        "bibliometrics",
        "journal-editors",
        "web-scraping"
      ],
      "id": 260
    },
    {
      "name": "Chinese-Elite",
      "one_line_profile": "Social network mapping tool for elite relationships",
      "detailed_description": "An experimental project that uses LLMs to parse public data and cross-reference official sources to map relationship networks of Chinese elites. While focused on social science/political science, it represents a tool for network inference from unstructured text.",
      "domains": [
        "Social Science",
        "Network Analysis"
      ],
      "subtask_category": [
        "relationship_extraction",
        "network_mapping"
      ],
      "application_level": "workflow",
      "primary_language": "Roff",
      "repo_url": "https://github.com/anonym-g/Chinese-Elite",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "social-network-analysis",
        "llm",
        "relationship-mapping"
      ],
      "id": 261
    },
    {
      "name": "obsidian-reference-map",
      "one_line_profile": "Citation network visualization for literature review",
      "detailed_description": "An Obsidian plugin that creates interactive citation maps for literature reviews. It helps researchers visualize connections between papers, discover related works, and manage bibliographic references within their knowledge base.",
      "domains": [
        "Sci Knowledge",
        "G1-02"
      ],
      "subtask_category": [
        "visualization",
        "literature_review",
        "knowledge_management"
      ],
      "application_level": "workflow",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/anoopkcn/obsidian-reference-map",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "obsidian",
        "citation-network",
        "visualization"
      ],
      "id": 262
    },
    {
      "name": "pythonbible",
      "one_line_profile": "Library for parsing and normalizing scripture references",
      "detailed_description": "A Python library designed for Digital Humanities and Theological studies to parse, normalize, and validate references to scripture texts. It supports extracting structured citation data from unstructured text.",
      "domains": [
        "Digital Humanities",
        "Text Analysis"
      ],
      "subtask_category": [
        "text_parsing",
        "reference_extraction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/avendesora/pythonbible",
      "help_website": [
        "https://pythonbible.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "digital-humanities",
        "text-parsing",
        "reference-normalization"
      ],
      "id": 263
    },
    {
      "name": "geomodelgrids",
      "one_line_profile": "Geographic referenced grid-based modeling library",
      "detailed_description": "A C++ library developed by USGS for handling geographic referenced grid-based models. It supports querying and manipulating multi-resolution grid data for earth science applications like seismology and geology.",
      "domains": [
        "Earth Science",
        "Geology"
      ],
      "subtask_category": [
        "spatial_modeling",
        "grid_processing"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/baagaard-usgs/geomodelgrids",
      "help_website": [
        "https://geomodelgrids.readthedocs.io"
      ],
      "license": "NOASSERTION",
      "tags": [
        "geology",
        "spatial-data",
        "usgs"
      ],
      "id": 264
    },
    {
      "name": "arxivcheck",
      "one_line_profile": "Utility to check arXiv publication status and update BibTeX metadata",
      "detailed_description": "A Python tool that checks if an arXiv paper has been published in a peer-reviewed venue and updates the corresponding BibTeX entry with the official publication metadata.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "metadata_alignment",
        "citation_management"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/bibcure/arxivcheck",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "arxiv",
        "bibtex",
        "metadata-correction"
      ],
      "id": 265
    },
    {
      "name": "bibcure",
      "one_line_profile": "CLI tool for managing and normalizing BibTeX files",
      "detailed_description": "A command-line utility to keep BibTeX files up to date, normalize entries, and download papers associated with the bibliography.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "citation_management",
        "data_retrieval"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/bibcure/bibcure",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "bibtex",
        "cli",
        "bibliography-management"
      ],
      "id": 266
    },
    {
      "name": "doi2bib",
      "one_line_profile": "Tool to retrieve BibTeX metadata from DOIs",
      "detailed_description": "A lightweight utility that fetches the BibTeX citation string for a given Digital Object Identifier (DOI).",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "metadata_retrieval",
        "citation_extraction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/bibcure/doi2bib",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "doi",
        "bibtex",
        "metadata"
      ],
      "id": 267
    },
    {
      "name": "scihub2pdf",
      "one_line_profile": "Bulk PDF downloader using DOIs or BibTeX files",
      "detailed_description": "A tool to download research paper PDFs in bulk via DOI numbers, article titles, or BibTeX files, utilizing various repositories.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "data_retrieval",
        "document_acquisition"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/bibcure/scihub2pdf",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "pdf-download",
        "scihub",
        "automation"
      ],
      "id": 268
    },
    {
      "name": "title2bib",
      "one_line_profile": "Tool to retrieve BibTeX metadata from paper titles",
      "detailed_description": "A utility that searches for and returns the BibTeX citation format given a scientific paper's title.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "metadata_retrieval",
        "citation_extraction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/bibcure/title2bib",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bibtex",
        "metadata",
        "search"
      ],
      "id": 269
    },
    {
      "name": "sidewall",
      "one_line_profile": "Library for interacting with the Dimensions search API",
      "detailed_description": "A Python library developed by Caltech Library to facilitate interaction with the Dimensions API for retrieving and analyzing scientometric data.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "data_retrieval",
        "scientometrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/caltechlibrary/sidewall",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "dimensions-api",
        "scientometrics",
        "library"
      ],
      "id": 270
    },
    {
      "name": "python-bibx",
      "one_line_profile": "Bibliometric analysis tools for Python",
      "detailed_description": "A Python library developed by Core of Science for performing bibliometric analyses, including processing and analyzing bibliographic data.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "bibliometrics",
        "data_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/coreofscience/python-bibx",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bibliometrics",
        "scientometrics",
        "python"
      ],
      "id": 271
    },
    {
      "name": "GAKG",
      "one_line_profile": "Multimodal Geoscience Academic Knowledge Graph framework",
      "detailed_description": "A framework for constructing a multimodal Geoscience Academic Knowledge Graph by fusing information from paper illustrations, text, and bibliometric data.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "knowledge_graph_construction",
        "data_fusion"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/davendw49/gakg",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "knowledge-graph",
        "geoscience",
        "multimodal"
      ],
      "id": 272
    },
    {
      "name": "papergraph",
      "one_line_profile": "AI/ML citation graph exploration platform",
      "detailed_description": "A project providing a citation graph implementation for AI/ML papers using Postgres and GraphQL, enabling analysis and visualization of citation networks.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "citation_network_analysis",
        "visualization"
      ],
      "application_level": "platform",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/dennybritz/papergraph",
      "help_website": [],
      "license": null,
      "tags": [
        "citation-graph",
        "network-analysis",
        "machine-learning"
      ],
      "id": 273
    },
    {
      "name": "tethne",
      "one_line_profile": "Bibliographic network analysis library",
      "detailed_description": "A Python module designed for parsing bibliographic data and generating networks for analysis, supporting various citation formats and network types.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "network_analysis",
        "bibliometrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/diging/tethne",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "bibliometrics",
        "network-analysis",
        "citation-graph"
      ],
      "id": 274
    },
    {
      "name": "dimcli",
      "one_line_profile": "CLI and Python client for Dimensions Analytics API",
      "detailed_description": "The official Python client and command-line interface for interacting with the Dimensions Analytics API, facilitating scientometric research and data extraction.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "data_retrieval",
        "scientometrics"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/digital-science/dimcli",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "dimensions-api",
        "scientometrics",
        "cli"
      ],
      "id": 275
    },
    {
      "name": "Docling",
      "one_line_profile": "Advanced document parsing library for converting PDFs to structured formats for GenAI",
      "detailed_description": "A comprehensive library designed to parse and convert documents (including complex scientific PDFs with tables, equations, and layouts) into structured formats like JSON and Markdown, enabling downstream Generative AI and analysis tasks.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "document_parsing",
        "layout_analysis",
        "pdf_to_text"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/docling-project/docling",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pdf-parsing",
        "ocr",
        "layout-analysis",
        "scientific-literature"
      ],
      "id": 276
    },
    {
      "name": "Alexandria3k",
      "one_line_profile": "High-performance relational interface for open publication datasets",
      "detailed_description": "A library and command-line tool that provides fast, relational SQL-like access to massive open bibliographic datasets such as OpenAlex and Crossref, facilitating large-scale bibliometric analysis and metadata retrieval.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "metadata_retrieval",
        "bibliometrics",
        "database_querying"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/dspinellis/alexandria3k",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "openalex",
        "bibliometrics",
        "citation-analysis",
        "relational-data"
      ],
      "id": 277
    },
    {
      "name": "Etudier",
      "one_line_profile": "Command-line tool for extracting citation networks from Google Scholar",
      "detailed_description": "A utility to scrape and extract citation network data from Google Scholar profiles and publications, enabling the construction of citation graphs for bibliometric research.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "citation_extraction",
        "network_construction"
      ],
      "application_level": "solver",
      "primary_language": "HTML",
      "repo_url": "https://github.com/edsu/etudier",
      "help_website": [],
      "license": null,
      "tags": [
        "google-scholar",
        "citation-network",
        "scraping"
      ],
      "id": 278
    },
    {
      "name": "ScienceBeam Parser",
      "one_line_profile": "Scalable PDF to XML conversion tool for scientific publications",
      "detailed_description": "A set of tools utilizing Apache Beam and computer vision models to convert scientific PDF documents into structured XML (JATS/TEI) formats at scale, developed by eLife Sciences.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "pdf_parsing",
        "format_conversion",
        "xml_generation"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/elifesciences/sciencebeam-parser",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pdf-to-xml",
        "jats",
        "scientific-publishing",
        "apache-beam"
      ],
      "id": 279
    },
    {
      "name": "OpenAlex Networks",
      "one_line_profile": "Library for generating citation and co-authorship networks from OpenAlex",
      "detailed_description": "A helper library designed to interface with the OpenAlex API, facilitating the retrieval of bibliographic data and the generation of citation and co-authorship networks for scientometric analysis.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "network_generation",
        "api_client",
        "bibliometrics"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/filipinascimento/openalexnet",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "openalex",
        "citation-graph",
        "co-authorship",
        "network-analysis"
      ],
      "id": 280
    },
    {
      "name": "RefExtract",
      "one_line_profile": "Bibliographic reference extraction tool for scientific articles",
      "detailed_description": "A tool developed by INSPIRE-HEP to extract, parse, and structure bibliographic references from scientific article PDFs, supporting the construction of citation networks in high-energy physics and beyond.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "reference_extraction",
        "citation_parsing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/inspirehep/refextract",
      "help_website": [],
      "license": "GPL-2.0",
      "tags": [
        "reference-extraction",
        "pdf-parsing",
        "inspire-hep",
        "bibliometrics"
      ],
      "id": 281
    },
    {
      "name": "AnyStyle CLI",
      "one_line_profile": "Command-line interface for the AnyStyle citation parser",
      "detailed_description": "The command-line interface wrapper for the AnyStyle library, allowing users to parse bibliographic references from the terminal and convert them into structured formats like BibTeX or JSON.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "citation_parsing",
        "metadata_extraction"
      ],
      "application_level": "solver",
      "primary_language": "Ruby",
      "repo_url": "https://github.com/inukshuk/anystyle-cli",
      "help_website": [
        "https://anystyle.io"
      ],
      "license": "BSD-2-Clause",
      "tags": [
        "cli",
        "citation-parser",
        "bibliography"
      ],
      "id": 282
    },
    {
      "name": "bibtex-ruby",
      "one_line_profile": "A strict BibTeX library, parser, and converter for Ruby",
      "detailed_description": "A Ruby library for parsing, converting, and manipulating BibTeX files. It supports lexical analysis and parsing of BibTeX data, providing a robust foundation for bibliographic data management.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "bibliography_management",
        "format_conversion"
      ],
      "application_level": "library",
      "primary_language": "Ruby",
      "repo_url": "https://github.com/inukshuk/bibtex-ruby",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "bibtex",
        "parser",
        "bibliography"
      ],
      "id": 283
    },
    {
      "name": "citation_map",
      "one_line_profile": "Tool to generate Gephi citation graphs from Zotero PDF collections",
      "detailed_description": "A Python tool that analyzes text from PDFs stored in Zotero to create citation graphs compatible with Gephi. It helps in visualizing the connection between papers in a personal library.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "citation_network_generation",
        "visualization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/jaks6/citation_map",
      "help_website": [],
      "license": null,
      "tags": [
        "citation-graph",
        "gephi",
        "zotero",
        "bibliometrics"
      ],
      "id": 284
    },
    {
      "name": "ScientoPy",
      "one_line_profile": "Open-source scientometric analysis tool",
      "detailed_description": "ScientoPy is a Python-based tool for performing scientometric analysis. It allows researchers to analyze bibliographic data (from Scopus, WoS) to identify trends, top authors, and research topics.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "scientometrics",
        "bibliometric_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/jpruiz84/ScientoPy",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "scientometrics",
        "bibliometrics",
        "data-analysis"
      ],
      "id": 285
    },
    {
      "name": "reffix",
      "one_line_profile": "Tool for fixing BibTeX reference lists using the DBLP API",
      "detailed_description": "Reffix is a utility that cleans and corrects BibTeX reference lists by cross-referencing them with the DBLP computer science bibliography database, ensuring citation accuracy.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "bibliography_management",
        "metadata_correction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/kasnerz/reffix",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bibtex",
        "dblp",
        "citation-cleaning"
      ],
      "id": 286
    },
    {
      "name": "grobid-astro",
      "one_line_profile": "GROBID module for extracting astronomical entities from scholarly documents",
      "detailed_description": "A specialized module for GROBID designed to recognize and extract astronomical entities (such as celestial objects, coordinates, and instrument names) from scientific texts.",
      "domains": [
        "G1",
        "G1-02",
        "Astronomy"
      ],
      "subtask_category": [
        "entity_extraction",
        "ner"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/kermitt2/grobid-astro",
      "help_website": [],
      "license": null,
      "tags": [
        "astronomy",
        "ner",
        "grobid-module"
      ],
      "id": 287
    },
    {
      "name": "grobid-client-java",
      "one_line_profile": "Java client library for GROBID REST services",
      "detailed_description": "A Java client library to interact with the GROBID REST API, facilitating the integration of GROBID's document extraction capabilities into Java applications.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "api_client",
        "pdf_extraction"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/kermitt2/grobid-client-java",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "java",
        "client",
        "grobid"
      ],
      "id": 288
    },
    {
      "name": "grobid-client-node",
      "one_line_profile": "Node.js client library for GROBID REST services",
      "detailed_description": "A Node.js client library for consuming GROBID services, enabling JavaScript/TypeScript applications to perform scientific document parsing.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "api_client",
        "pdf_extraction"
      ],
      "application_level": "library",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/kermitt2/grobid-client-node",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "nodejs",
        "client",
        "grobid"
      ],
      "id": 289
    },
    {
      "name": "grobid-client-python",
      "one_line_profile": "Python client library for GROBID Web services",
      "detailed_description": "A Python client for the GROBID web service, allowing Python users to easily extract metadata and citations from scientific PDFs in bulk.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "api_client",
        "pdf_extraction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/kermitt2/grobid-client-python",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "python",
        "client",
        "grobid"
      ],
      "id": 290
    },
    {
      "name": "grobid-ner",
      "one_line_profile": "Named-Entity Recogniser based on GROBID",
      "detailed_description": "A module for GROBID that performs Named Entity Recognition (NER) on scientific texts, leveraging the structural parsing capabilities of the main GROBID engine.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "ner",
        "entity_extraction"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/kermitt2/grobid-ner",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ner",
        "nlp",
        "grobid"
      ],
      "id": 291
    },
    {
      "name": "ParsCit",
      "one_line_profile": "CRF-based reference string parsing package",
      "detailed_description": "ParsCit is an open-source package for parsing reference strings and extracting citations from scholarly documents using Conditional Random Fields (CRF). It also includes modules for logical structure parsing of scientific articles.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "citation_parsing",
        "structure_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Perl",
      "repo_url": "https://github.com/knmnyn/ParsCit",
      "help_website": [
        "http://parscit.sng-lls.com/"
      ],
      "license": "LGPL-3.0",
      "tags": [
        "citation-parsing",
        "crf",
        "perl",
        "bibliometrics"
      ],
      "id": 292
    },
    {
      "name": "Global Canopy Height Model",
      "one_line_profile": "High-resolution global canopy height estimation model using Sentinel-2 and GEDI data",
      "detailed_description": "A deep learning model for estimating canopy top height globally at high resolution (10m). It fuses Sentinel-2 optical imagery with sparse GEDI LIDAR data to generate continuous canopy height maps.",
      "domains": [
        "Earth Science",
        "Ecology"
      ],
      "subtask_category": [
        "canopy_height_estimation",
        "remote_sensing",
        "environmental_modeling"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/langnico/global-canopy-height-model",
      "help_website": [
        "https://langnico.github.io/global-canopy-height-model/"
      ],
      "license": "MIT",
      "tags": [
        "remote-sensing",
        "deep-learning",
        "ecology",
        "sentinel-2",
        "gedi"
      ],
      "id": 293
    },
    {
      "name": "Citation-Graph-Python",
      "one_line_profile": "Tool for auto-generation of citation graphs from references",
      "detailed_description": "A Python tool to generate and visualize citation graphs from a set of papers. It helps researchers understand the citation network and relationships between publications.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "citation_network_generation",
        "visualization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/lanstonchu/Citation-Graph-Python",
      "help_website": [],
      "license": null,
      "tags": [
        "citation-graph",
        "visualization",
        "bibliometrics"
      ],
      "id": 294
    },
    {
      "name": "grobid-quantities",
      "one_line_profile": "GROBID extension for identifying and normalizing physical quantities",
      "detailed_description": "A machine learning module for GROBID that extracts, parses, and normalizes physical quantities (values and units) from scientific text.",
      "domains": [
        "G1",
        "G1-02",
        "Physics",
        "Materials Science"
      ],
      "subtask_category": [
        "quantity_extraction",
        "normalization",
        "ner"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/lfoppiano/grobid-quantities",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "quantities",
        "ner",
        "grobid",
        "unit-normalization"
      ],
      "id": 295
    },
    {
      "name": "grobid-superconductors",
      "one_line_profile": "GROBID module for superconductor material and properties extraction",
      "detailed_description": "A specialized GROBID module designed to extract information about superconductor materials, their properties (like critical temperature), and related experimental conditions from scientific literature.",
      "domains": [
        "Materials Science",
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "material_extraction",
        "property_extraction",
        "ner"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/lfoppiano/grobid-superconductors",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "superconductors",
        "materials-science",
        "text-mining",
        "grobid"
      ],
      "id": 296
    },
    {
      "name": "StrainScan",
      "one_line_profile": "High-resolution strain-level microbiome composition analysis tool",
      "detailed_description": "StrainScan is a bioinformatics tool for strain-level microbiome composition analysis. It uses a reference-based approach with k-mers to identify and quantify bacterial strains from metagenomic data.",
      "domains": [
        "Bioinformatics",
        "Genomics"
      ],
      "subtask_category": [
        "microbiome_analysis",
        "strain_identification",
        "metagenomics"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/liaoherui/StrainScan",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "microbiome",
        "metagenomics",
        "strain-identification",
        "bioinformatics"
      ],
      "id": 297
    },
    {
      "name": "bibliometrix",
      "one_line_profile": "Comprehensive science mapping analysis tool for R",
      "detailed_description": "An R-tool for quantitative research in scientometrics and bibliometrics, providing various routines for importing bibliographic data, performing bibliometric analysis, and building data matrices for co-citation, coupling, scientific collaboration analysis, and co-word analysis.",
      "domains": [
        "Sci Knowledge",
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "bibliometrics",
        "network_analysis",
        "science_mapping"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/massimoaria/bibliometrix",
      "help_website": [
        "https://www.bibliometrix.org"
      ],
      "license": "NOASSERTION",
      "tags": [
        "bibliometrics",
        "scientometrics",
        "r-package",
        "citation-analysis"
      ],
      "id": 298
    },
    {
      "name": "dimensionsR",
      "one_line_profile": "R interface for Digital Science Dimensions API",
      "detailed_description": "An R package designed to gather metadata about publications, patents, grants, clinical trials, and policy documents from the Digital Science Dimensions database using its DSL API, facilitating bibliometric data collection.",
      "domains": [
        "Sci Knowledge",
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "data_retrieval",
        "metadata_extraction"
      ],
      "application_level": "library",
      "primary_language": "HTML",
      "repo_url": "https://github.com/massimoaria/dimensionsR",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "dimensions-api",
        "bibliometrics",
        "data-collection"
      ],
      "id": 299
    },
    {
      "name": "draw-citation-graph",
      "one_line_profile": "Citation graph generator from BibTeX and PDFs",
      "detailed_description": "A Python script that attempts to generate a visual citation graph by analyzing a BibTeX file and a corresponding directory of PDF files, linking papers based on their references.",
      "domains": [
        "Sci Knowledge",
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "visualization",
        "graph_generation",
        "citation_network"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/mhl/draw-citation-graph",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "citation-graph",
        "bibtex",
        "visualization"
      ],
      "id": 300
    },
    {
      "name": "CitationExtractor",
      "one_line_profile": "Tool to extract canonical references from text",
      "detailed_description": "A tool designed to extract canonical references from textual documents, aiding in the creation of citation networks and metadata alignment.",
      "domains": [
        "Sci Knowledge",
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "citation_extraction",
        "information_extraction"
      ],
      "application_level": "solver",
      "primary_language": "HTML",
      "repo_url": "https://github.com/mromanello/CitationExtractor",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "citation-extraction",
        "reference-parsing"
      ],
      "id": 301
    },
    {
      "name": "GetLattesData",
      "one_line_profile": "R package for reading Lattes curriculum data",
      "detailed_description": "An R package for downloading and reading data from the Lattes platform (Brazilian academic CV database), enabling scientometric analysis of researcher data.",
      "domains": [
        "Sci Knowledge",
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "data_retrieval",
        "scientometrics"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/msperlin/GetLattesData",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "lattes",
        "scientometrics",
        "data-import"
      ],
      "id": 302
    },
    {
      "name": "rscopus",
      "one_line_profile": "Scopus Database API Interface to R",
      "detailed_description": "An R package that provides an interface to the Elsevier Scopus API, allowing users to retrieve bibliographic information, citation counts, and other metadata for bibliometric analysis.",
      "domains": [
        "Sci Knowledge",
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "data_retrieval",
        "bibliometrics"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/muschellij2/rscopus",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "scopus",
        "api-wrapper",
        "bibliometrics"
      ],
      "id": 303
    },
    {
      "name": "bibfix",
      "one_line_profile": "Tool to repair bibliographic data",
      "detailed_description": "An R package designed to repair and standardize bibliographic data, useful for cleaning datasets prior to bibliometric analysis or evidence synthesis.",
      "domains": [
        "Sci Knowledge",
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "data_cleaning",
        "normalization"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/nealhaddaway/bibfix",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "bibliographic-data",
        "data-cleaning"
      ],
      "id": 304
    },
    {
      "name": "citationchaser",
      "one_line_profile": "Forward and backward citation chasing tool",
      "detailed_description": "An R package and Shiny app for performing forward and backward citation chasing, automating the process of finding related articles for systematic reviews and evidence synthesis.",
      "domains": [
        "Sci Knowledge",
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "literature_review",
        "citation_analysis",
        "network_traversal"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/nealhaddaway/citationchaser",
      "help_website": [
        "https://estech.shinyapps.io/citationchaser/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "systematic-review",
        "citation-chasing",
        "evidence-synthesis"
      ],
      "id": 305
    },
    {
      "name": "VOSviewer-Online",
      "one_line_profile": "Web-based network visualization tool for bibliometrics",
      "detailed_description": "The web-based version of VOSviewer, a popular tool for constructing and visualizing bibliometric networks such as co-citation, co-authorship, and term co-occurrence networks.",
      "domains": [
        "Sci Knowledge",
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "visualization",
        "network_analysis",
        "bibliometrics"
      ],
      "application_level": "platform",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/neesjanvaneck/VOSviewer-Online",
      "help_website": [
        "https://app.vosviewer.com"
      ],
      "license": "MIT",
      "tags": [
        "visualization",
        "bibliometrics",
        "network-analysis"
      ],
      "id": 306
    },
    {
      "name": "citegraph",
      "one_line_profile": "Citation network explorer",
      "detailed_description": "A Python tool for exploring and visualizing citation networks, allowing users to navigate the connections between scientific papers.",
      "domains": [
        "Sci Knowledge",
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "visualization",
        "network_exploration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/oowekyala/citegraph",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "citation-network",
        "visualization"
      ],
      "id": 307
    },
    {
      "name": "openalex-pdf-parser",
      "one_line_profile": "PDF parser powered by Grobid for OpenAlex",
      "detailed_description": "A Python-based PDF parsing pipeline used by OpenAlex, leveraging Grobid to extract structured metadata and full text from scientific articles.",
      "domains": [
        "Sci Knowledge",
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "pdf_parsing",
        "metadata_extraction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ourresearch/openalex-pdf-parser",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "openalex",
        "grobid",
        "pdf-parsing"
      ],
      "id": 308
    },
    {
      "name": "pybliometrics",
      "one_line_profile": "Python-based API-Wrapper to access Scopus",
      "detailed_description": "A Python library that provides a wrapper for the Scopus API, enabling researchers to access and analyze citation data, author profiles, and publication metadata for bibliometric studies.",
      "domains": [
        "Sci Knowledge",
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "data_retrieval",
        "bibliometrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/pybliometrics-dev/pybliometrics",
      "help_website": [
        "https://pybliometrics.readthedocs.io"
      ],
      "license": "NOASSERTION",
      "tags": [
        "scopus",
        "bibliometrics",
        "python-api"
      ],
      "id": 309
    },
    {
      "name": "bibtex-parser (PHP)",
      "one_line_profile": "PHP library for parsing BibTeX files programmatically",
      "detailed_description": "A PHP library designed to parse BibTeX files into structured data, enabling the integration of bibliographic metadata processing into PHP-based applications.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "metadata_parsing",
        "bibliographic_data_handling"
      ],
      "application_level": "library",
      "primary_language": "PHP",
      "repo_url": "https://github.com/renanbr/bibtex-parser",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bibtex",
        "php",
        "parser",
        "bibliography"
      ],
      "id": 310
    },
    {
      "name": "bibtex-parser (JS)",
      "one_line_profile": "BibTeX parser driving Better BibTeX for Zotero",
      "detailed_description": "A robust JavaScript/TypeScript parser for BibTeX files, primarily developed for and used by the Better BibTeX extension for Zotero to handle complex citation data.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "metadata_parsing",
        "citation_management"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/retorquere/bibtex-parser",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bibtex",
        "zotero",
        "parser",
        "javascript"
      ],
      "id": 311
    },
    {
      "name": "bib2df",
      "one_line_profile": "Parse BibTeX files into R data frames",
      "detailed_description": "An R package that parses BibTeX files and converts them into tibbles (data frames), facilitating bibliometric analysis and data manipulation within the R ecosystem.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "metadata_parsing",
        "data_conversion"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/ropensci/bib2df",
      "help_website": [
        "https://docs.ropensci.org/bib2df/"
      ],
      "license": "MIT",
      "tags": [
        "r",
        "bibtex",
        "bibliometrics",
        "data-frame"
      ],
      "id": 312
    },
    {
      "name": "bibtex",
      "one_line_profile": "BibTeX parser for R",
      "detailed_description": "An R package providing utilities to parse and manipulate BibTeX files, serving as a foundational library for bibliographic data processing in R.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "metadata_parsing"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/ropensci/bibtex",
      "help_website": [],
      "license": "GPL-2.0",
      "tags": [
        "r",
        "bibtex",
        "parser"
      ],
      "id": 313
    },
    {
      "name": "europepmc",
      "one_line_profile": "R Interface to Europe PMC RESTful Web Service",
      "detailed_description": "An R client for the Europe PMC RESTful API, allowing users to search and retrieve open access biomedical literature and metadata programmatically.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "literature_retrieval",
        "metadata_extraction"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/ropensci/europepmc",
      "help_website": [
        "https://docs.ropensci.org/europepmc/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "r",
        "europe-pmc",
        "literature-search",
        "biomedical"
      ],
      "id": 314
    },
    {
      "name": "openalexR",
      "one_line_profile": "R client for retrieving bibliographic records from OpenAlex",
      "detailed_description": "An R package designed to interface with the OpenAlex API, enabling researchers to query and analyze the massive graph of scholarly citations, authors, and institutions.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "literature_retrieval",
        "citation_graph_analysis"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/ropensci/openalexR",
      "help_website": [
        "https://docs.ropensci.org/openalexR/"
      ],
      "license": "MIT",
      "tags": [
        "r",
        "openalex",
        "bibliometrics",
        "citation-graph"
      ],
      "id": 315
    },
    {
      "name": "rAltmetric",
      "one_line_profile": "Query and visualize metrics from Altmetric.com",
      "detailed_description": "An R package to retrieve and visualize alternative impact metrics (Altmetrics) for scholarly articles from the Altmetric.com API.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "impact_analysis",
        "scientometrics"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/ropensci/rAltmetric",
      "help_website": [
        "https://docs.ropensci.org/rAltmetric/"
      ],
      "license": "CC0-1.0",
      "tags": [
        "r",
        "altmetrics",
        "scientometrics",
        "api-client"
      ],
      "id": 316
    },
    {
      "name": "habanero",
      "one_line_profile": "Python client for Crossref search API",
      "detailed_description": "A Python library that provides an interface to the Crossref API, allowing users to search for metadata of scientific publications, retrieve DOIs, and access citation data.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "literature_retrieval",
        "metadata_extraction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/sckott/habanero",
      "help_website": [
        "https://habanero.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "python",
        "crossref",
        "doi",
        "bibliometrics"
      ],
      "id": 317
    },
    {
      "name": "GCMex",
      "one_line_profile": "Matlab wrapper for Graph Cut algorithm",
      "detailed_description": "A Matlab wrapper for the Graph Cut algorithm (max-flow/min-cut), commonly used in scientific image segmentation and computer vision research.",
      "domains": [
        "Scientific Imaging"
      ],
      "subtask_category": [
        "image_segmentation",
        "graph_cut"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/shaibagon/GCMex",
      "help_website": [],
      "license": "GPL-2.0",
      "tags": [
        "matlab",
        "graph-cut",
        "image-segmentation",
        "computer-vision"
      ],
      "id": 318
    },
    {
      "name": "metacheck",
      "one_line_profile": "Metadata compliance checker for hybrid open access",
      "detailed_description": "A tool developed by SUB Göttingen to automatically check metadata compliance for hybrid open access publications, supporting library science and scholarly communication workflows.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "metadata_validation",
        "open_access_compliance"
      ],
      "application_level": "tool",
      "primary_language": "R",
      "repo_url": "https://github.com/subugoe/metacheck",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "open-access",
        "metadata",
        "library-science",
        "compliance"
      ],
      "id": 319
    },
    {
      "name": "typst-biblatex",
      "one_line_profile": "Rust library for parsing BibTeX and BibLaTeX files for the Typst typesetting system",
      "detailed_description": "A high-performance Rust crate designed to parse and write BibTeX and BibLaTeX bibliography files. It serves as a core component for bibliography management in the Typst scientific typesetting ecosystem.",
      "domains": [
        "Sci Knowledge",
        "G1-02"
      ],
      "subtask_category": [
        "bibliography_management",
        "metadata_parsing"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/typst/biblatex",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "bibtex",
        "biblatex",
        "parser",
        "typst"
      ],
      "id": 320
    },
    {
      "name": "ALIGNN",
      "one_line_profile": "Atomistic Line Graph Neural Network for material property prediction",
      "detailed_description": "A Python package implementing Atomistic Line Graph Neural Networks (ALIGNN) for predicting material properties from atomic structures. It captures both bond lengths and bond angles to model atomic interactions effectively.",
      "domains": [
        "Materials Science",
        "G1"
      ],
      "subtask_category": [
        "structure_property_prediction",
        "molecular_modeling"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/usnistgov/alignn",
      "help_website": [
        "https://jarvis.nist.gov/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "gnn",
        "materials-science",
        "atomistic-modeling"
      ],
      "id": 321
    },
    {
      "name": "ReCiter",
      "one_line_profile": "Enterprise author disambiguation system for academic institutions",
      "detailed_description": "An open-source system developed by Weill Cornell Medicine for accurate author disambiguation in academic publications. It uses machine learning and institutional data to maintain up-to-date publication lists for researchers.",
      "domains": [
        "Sci Knowledge",
        "G1-02"
      ],
      "subtask_category": [
        "author_disambiguation",
        "metadata_alignment"
      ],
      "application_level": "platform",
      "primary_language": "Java",
      "repo_url": "https://github.com/wcmc-its/ReCiter",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "author-disambiguation",
        "bibliometrics",
        "academic-profiling"
      ],
      "id": 322
    },
    {
      "name": "ReCiter-PubMed-Retrieval-Tool",
      "one_line_profile": "Tool for retrieving and synchronizing articles from PubMed",
      "detailed_description": "A standalone utility designed to fetch article metadata from PubMed. It is part of the ReCiter ecosystem, facilitating the ingestion of publication data for author disambiguation and institutional reporting.",
      "domains": [
        "Sci Knowledge",
        "G1-02"
      ],
      "subtask_category": [
        "literature_retrieval",
        "data_ingestion"
      ],
      "application_level": "solver",
      "primary_language": "Java",
      "repo_url": "https://github.com/wcmc-its/ReCiter-PubMed-Retrieval-Tool",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "pubmed",
        "data-retrieval",
        "reciter"
      ],
      "id": 323
    },
    {
      "name": "deep_reference_parser",
      "one_line_profile": "Deep learning model for extracting references from text",
      "detailed_description": "A tool developed by Wellcome Trust that uses deep learning models to identify, extract, and parse bibliographic references from unstructured text documents.",
      "domains": [
        "Sci Knowledge",
        "G1-02"
      ],
      "subtask_category": [
        "reference_extraction",
        "citation_parsing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/wellcometrust/deep_reference_parser",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "deep-learning",
        "reference-parsing",
        "nlp"
      ],
      "id": 324
    },
    {
      "name": "reach",
      "one_line_profile": "Tool to parse references from policy documents",
      "detailed_description": "A machine learning-based tool created by Wellcome Trust to scrape and parse references found in policy documents, enabling the tracking of research impact in policy making.",
      "domains": [
        "Sci Knowledge",
        "G1-02"
      ],
      "subtask_category": [
        "impact_analysis",
        "reference_parsing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/wellcometrust/reach",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "policy-analysis",
        "citation-tracking",
        "scraping"
      ],
      "id": 325
    },
    {
      "name": "semantic-scholar-fastmcp",
      "one_line_profile": "MCP server for accessing Semantic Scholar API via LLM agents",
      "detailed_description": "A FastMCP server implementation that provides a standardized interface for Large Language Model (LLM) agents to query the Semantic Scholar API. It enables AI-driven workflows to retrieve academic papers, author details, and citation networks.",
      "domains": [
        "Sci Knowledge",
        "G1-02"
      ],
      "subtask_category": [
        "literature_retrieval",
        "agent_interface"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/zongmin-yu/semantic-scholar-fastmcp-mcp-server",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mcp",
        "semantic-scholar",
        "llm-agent"
      ],
      "id": 326
    },
    {
      "name": "Docs2KG",
      "one_line_profile": "Unified Knowledge Graph Construction from Heterogeneous Documents",
      "detailed_description": "A framework for constructing unified knowledge graphs from heterogeneous documents using a Human-LLM collaborative approach, facilitating information extraction from scientific or technical documentation.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "knowledge_graph_construction",
        "information_extraction"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/AI4WA/Docs2KG",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "knowledge-graph",
        "llm",
        "document-parsing",
        "rag"
      ],
      "id": 327
    },
    {
      "name": "semantic-zotero",
      "one_line_profile": "Zotero plugin for Semantic Scholar integration",
      "detailed_description": "A plugin for Zotero that retrieves reference data and metadata from the Semantic Scholar API, enhancing bibliography management for researchers.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "literature_management",
        "metadata_retrieval"
      ],
      "application_level": "solver",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/AgiNetz/semantic-zotero",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "zotero-plugin",
        "semantic-scholar",
        "bibliography"
      ],
      "id": 328
    },
    {
      "name": "OpenScholar",
      "one_line_profile": "Retrieval-augmented LM for scientific literature synthesis",
      "detailed_description": "A retrieval-augmented language model system designed to synthesize scientific literature and answer scientific questions by retrieving relevant papers.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "literature_synthesis",
        "question_answering"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/AkariAsai/OpenScholar",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "scientific-literature",
        "llm",
        "synthesis"
      ],
      "id": 329
    },
    {
      "name": "Scholarpy",
      "one_line_profile": "Python wrapper for Semantic Scholar API",
      "detailed_description": "A Python library that wraps Semantic Scholar APIs to streamline academic research, data retrieval, and literature analysis.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "literature_retrieval",
        "api_wrapper"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/AndreaBasile97/Scholarpy",
      "help_website": [],
      "license": null,
      "tags": [
        "semantic-scholar",
        "api-wrapper",
        "literature-search"
      ],
      "id": 330
    },
    {
      "name": "trove",
      "one_line_profile": "Flexible Toolkit for Dense Retrieval",
      "detailed_description": "A toolkit for dense retrieval research and implementation, developed by BatsResearch, facilitating information retrieval tasks which are foundational to scientific literature search.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "information_retrieval",
        "dense_retrieval"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/BatsResearch/trove",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "dense-retrieval",
        "information-retrieval",
        "research-toolkit"
      ],
      "id": 331
    },
    {
      "name": "CWTS-Leiden-Ranking-Open-Edition",
      "one_line_profile": "Source code for generating CWTS Leiden Ranking indicators",
      "detailed_description": "Source code and SQL scripts for calculating the CWTS Leiden Ranking indicators for universities based on bibliometric data, enabling reproducible scientometric analysis.",
      "domains": [
        "G1"
      ],
      "subtask_category": [
        "scientometrics",
        "bibliometric_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "TSQL",
      "repo_url": "https://github.com/CWTSLeiden/CWTS-Leiden-Ranking-Open-Edition",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "scientometrics",
        "ranking",
        "bibliometrics",
        "reproducibility"
      ],
      "id": 332
    },
    {
      "name": "CWTS-OpenAlex-databases",
      "one_line_profile": "ETL pipeline for processing OpenAlex bibliographic data for scientometrics",
      "detailed_description": "A set of SQL scripts and documentation for creating a local version of the OpenAlex database, optimized for bibliometric analysis and science of science research. Developed by the Centre for Science and Technology Studies (CWTS).",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "data_ingestion",
        "bibliometrics"
      ],
      "application_level": "workflow",
      "primary_language": "TSQL",
      "repo_url": "https://github.com/CWTSLeiden/CWTS-OpenAlex-databases",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "openalex",
        "bibliometrics",
        "etl",
        "scientometrics"
      ],
      "id": 333
    },
    {
      "name": "CGSum",
      "one_line_profile": "Scientific paper summarization model using citation graphs",
      "detailed_description": "Implementation of a citation graph-based summarization framework for scientific papers. It leverages the citation network to enhance the quality of generated summaries, capturing the broader context of research impact.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "summarization",
        "citation_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ChenxinAn-fdu/CGSum",
      "help_website": [],
      "license": null,
      "tags": [
        "nlp",
        "summarization",
        "citation-graph",
        "scientific-papers"
      ],
      "id": 334
    },
    {
      "name": "wosis",
      "one_line_profile": "Python package for Web of Science bibliometric analysis",
      "detailed_description": "A library designed to support bibliometric analysis of Web of Science data, covering the workflow from querying the API to visualizing the results.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "bibliometrics",
        "data_retrieval"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ConnectedSystems/wosis",
      "help_website": [],
      "license": null,
      "tags": [
        "web-of-science",
        "bibliometrics",
        "wos-api"
      ],
      "id": 335
    },
    {
      "name": "emvb",
      "one_line_profile": "Efficient Multi-vector Dense Retrieval with Bit Vectors",
      "detailed_description": "Implementation of a dense retrieval method using bit vectors for efficient multi-vector representation, applicable to information retrieval tasks including document ranking.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "information_retrieval",
        "indexing"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/CosimoRulli/emvb",
      "help_website": [],
      "license": null,
      "tags": [
        "dense-retrieval",
        "vector-search",
        "ir"
      ],
      "id": 336
    },
    {
      "name": "SciKG",
      "one_line_profile": "Scientific Knowledge Graph Construction Framework",
      "detailed_description": "A framework for constructing knowledge graphs from scientific literature, facilitating the organization and retrieval of scientific knowledge.",
      "domains": [
        "G1",
        "G1-02"
      ],
      "subtask_category": [
        "knowledge_graph_construction",
        "information_extraction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/DM2-ND/SciKG",
      "help_website": [],
      "license": null,
      "tags": [
        "knowledge-graph",
        "scientific-literature",
        "kdd"
      ],
      "id": 337
    },
    {
      "name": "COCA",
      "one_line_profile": "Context-Aware Document Ranking with Contrastive Learning",
      "detailed_description": "Implementation of a contrastive learning framework for user behavior sequence modeling in context-aware document ranking, applicable to literature search and recommendation.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "document_ranking",
        "recommendation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/DaoD/COCA",
      "help_website": [],
      "license": null,
      "tags": [
        "document-ranking",
        "contrastive-learning",
        "ir"
      ],
      "id": 338
    },
    {
      "name": "KRAGEN",
      "one_line_profile": "Knowledge Retrieval Augmented Generation Engine for biomedical research",
      "detailed_description": "Software to implement retrieval augmented generation (RAG) workflows using vectorized databases, specifically designed for biomedical knowledge discovery and integration.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "knowledge_retrieval",
        "rag"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/EpistasisLab/KRAGEN",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "biomedical",
        "knowledge-retrieval",
        "vector-database"
      ],
      "id": 339
    },
    {
      "name": "SeleniumSemanticScraper",
      "one_line_profile": "Scraper for retrieving paper metadata from Semantic Scholar",
      "detailed_description": "A tool to automatically crawl and extract metadata from scientific papers on Semantic Scholar based on key phrases, facilitating literature data collection.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "data_acquisition",
        "scraping"
      ],
      "application_level": "tool",
      "primary_language": "Python",
      "repo_url": "https://github.com/EvertonCa/SeleniumSemanticScraper",
      "help_website": [],
      "license": null,
      "tags": [
        "semantic-scholar",
        "scraping",
        "literature-review"
      ],
      "id": 340
    },
    {
      "name": "DPTDR",
      "one_line_profile": "Deep Prompt Tuning for Dense Passage Retrieval",
      "detailed_description": "Implementation of a deep prompt tuning method for dense passage retrieval, improving the efficiency and effectiveness of retrieving relevant scientific passages.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "information_retrieval",
        "dense_retrieval"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/FreedomIntelligence/DPTDR",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "dense-retrieval",
        "prompt-tuning",
        "ir"
      ],
      "id": 341
    },
    {
      "name": "GR-as-MVDR",
      "one_line_profile": "Generative Retrieval as Multi-Vector Dense Retrieval",
      "detailed_description": "A retrieval framework that unifies generative retrieval and multi-vector dense retrieval, offering a novel approach to document ranking and retrieval.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "generative_retrieval",
        "dense_retrieval"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Furyton/GR-as-MVDR",
      "help_website": [],
      "license": null,
      "tags": [
        "generative-retrieval",
        "ir",
        "sigir"
      ],
      "id": 342
    },
    {
      "name": "paper-qa",
      "one_line_profile": "High-accuracy RAG for scientific literature QA",
      "detailed_description": "A library for performing high-accuracy Retrieval Augmented Generation (RAG) on scientific papers. It retrieves relevant context and generates answers with precise citations to the source documents.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "question_answering",
        "rag",
        "literature_review"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Future-House/paper-qa",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "scientific-qa",
        "citations",
        "llm"
      ],
      "id": 343
    },
    {
      "name": "oh-my-papers",
      "one_line_profile": "Hybrid Context-aware Paper Recommendation System",
      "detailed_description": "A recommendation system designed for scientific papers that utilizes hybrid context-aware techniques to suggest relevant literature to researchers.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "recommendation",
        "literature_discovery"
      ],
      "application_level": "application",
      "primary_language": "Python",
      "repo_url": "https://github.com/Galaxies99/oh-my-papers",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "recommender-system",
        "scientific-papers",
        "context-aware"
      ],
      "id": 344
    },
    {
      "name": "cedr",
      "one_line_profile": "Contextualized Embeddings for Document Ranking",
      "detailed_description": "Implementation of the CEDR model, which utilizes contextualized embeddings (like BERT) for effective document ranking in information retrieval tasks.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "document_ranking",
        "neural_ir"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Georgetown-IR-Lab/cedr",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "document-ranking",
        "bert",
        "ir",
        "sigir"
      ],
      "id": 345
    },
    {
      "name": "AutoSchemaKG",
      "one_line_profile": "Framework for automatic knowledge graph construction with schema generation",
      "detailed_description": "A framework designed for automatic knowledge graph construction that combines schema generation via conceptualization, facilitating the structuring of unstructured text into knowledge graphs.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "knowledge_graph_construction",
        "schema_generation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/HKUST-KnowComp/AutoSchemaKG",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "knowledge-graph",
        "schema-induction",
        "nlp"
      ],
      "id": 346
    },
    {
      "name": "pyalex",
      "one_line_profile": "Python library for accessing OpenAlex scientific database",
      "detailed_description": "A Python library that provides a convenient interface to the OpenAlex API, enabling researchers to retrieve and analyze scientific literature, authors, and institutions data.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "literature_retrieval",
        "bibliometrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/J535D165/pyalex",
      "help_website": [
        "https://github.com/J535D165/pyalex"
      ],
      "license": "MIT",
      "tags": [
        "openalex",
        "bibliometrics",
        "api-wrapper"
      ],
      "id": 347
    },
    {
      "name": "semanticscholar-MCP-Server",
      "one_line_profile": "MCP server for interacting with Semantic Scholar API",
      "detailed_description": "Implements a Model Context Protocol (MCP) server to allow Large Language Models to search papers, retrieve details, and fetch citations from the Semantic Scholar database.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "literature_retrieval",
        "citation_analysis"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/JackKuo666/semanticscholar-MCP-Server",
      "help_website": [],
      "license": null,
      "tags": [
        "semantic-scholar",
        "mcp",
        "llm-tool"
      ],
      "id": 348
    },
    {
      "name": "RAKG",
      "one_line_profile": "Retrieval Augmented Knowledge Graph Construction framework",
      "detailed_description": "A framework for document-level Retrieval Augmented Knowledge Graph Construction, designed to enhance KGC tasks using retrieval mechanisms.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "knowledge_graph_construction",
        "information_extraction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/KnowledgeXLab/RAKG",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "knowledge-graph",
        "nlp"
      ],
      "id": 349
    },
    {
      "name": "alex-paper-search-mcp",
      "one_line_profile": "MCP server for searching scientific papers via OpenAlex",
      "detailed_description": "Allows Large Language Models to search and retrieve scientific papers from the OpenAlex database using the Model Context Protocol (MCP).",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "literature_retrieval",
        "paper_search"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/LeoGitGuy/alex-paper-search-mcp",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "openalex",
        "mcp",
        "llm-tool"
      ],
      "id": 350
    },
    {
      "name": "LocalCitationNetwork",
      "one_line_profile": "Web tool for visualizing and analyzing local citation networks",
      "detailed_description": "A web application that helps scientists perform literature reviews by visualizing local citation networks using metadata from OpenAlex, Semantic Scholar, and Crossref.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "citation_analysis",
        "literature_review"
      ],
      "application_level": "platform",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/LocalCitationNetwork/LocalCitationNetwork.github.io",
      "help_website": [
        "https://localcitationnetwork.github.io/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "citation-network",
        "visualization",
        "literature-review"
      ],
      "id": 351
    },
    {
      "name": "vicinity",
      "one_line_profile": "Lightweight nearest neighbor search library with flexible backends",
      "detailed_description": "A Python library that provides a unified interface for nearest neighbor search using various backends like PyTorch, Scikit-Learn, and Faiss, facilitating vector retrieval tasks in scientific data analysis.",
      "domains": [
        "G1-03",
        "Machine Learning"
      ],
      "subtask_category": [
        "vector_search",
        "nearest_neighbor_search"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/MinishLab/vicinity",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nearest-neighbors",
        "vector-search",
        "pytorch"
      ],
      "id": 352
    },
    {
      "name": "AMG-RAG",
      "one_line_profile": "Agentic framework for Medical Knowledge Graph construction and QA",
      "detailed_description": "A comprehensive framework that automates the construction and continuous updating of Medical Knowledge Graphs (MKGs) and integrates reasoning for medical Question Answering (QA).",
      "domains": [
        "G1",
        "Medical AI"
      ],
      "subtask_category": [
        "knowledge_graph_construction",
        "question_answering"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/MrRezaeiUofT/AMG-RAG",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "medical-qa",
        "knowledge-graph",
        "rag"
      ],
      "id": 353
    },
    {
      "name": "SGPT",
      "one_line_profile": "GPT-based sentence embeddings for semantic search",
      "detailed_description": "A library for generating sentence embeddings using GPT models, specifically optimized for semantic search tasks and asymmetric search scenarios.",
      "domains": [
        "G1-03",
        "NLP"
      ],
      "subtask_category": [
        "semantic_embedding",
        "dense_retrieval"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/Muennighoff/sgpt",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "sentence-embeddings",
        "semantic-search",
        "gpt"
      ],
      "id": 354
    },
    {
      "name": "vespa-seismic",
      "one_line_profile": "Seismic data analysis library for VESPA and FK analysis",
      "detailed_description": "Python code for performing VESPA (Velocity Spectral Analysis) and FK (Frequency-Wavenumber) analysis on seismic data, serving geophysics research.",
      "domains": [
        "Geophysics"
      ],
      "subtask_category": [
        "seismic_analysis",
        "signal_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NeilWilkins/vespa",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "seismology",
        "geophysics",
        "spectral-analysis"
      ],
      "id": 355
    },
    {
      "name": "NeumAI",
      "one_line_profile": "Framework for large-scale vector embedding management",
      "detailed_description": "A data engineering framework to manage the creation, synchronization, and governance of vector embeddings at scale, supporting RAG pipelines for scientific and general applications.",
      "domains": [
        "G1-03",
        "Data Engineering"
      ],
      "subtask_category": [
        "embedding_management",
        "data_pipeline"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/NeumTry/NeumAI",
      "help_website": [
        "https://docs.neum.ai/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "vector-database",
        "embeddings",
        "rag"
      ],
      "id": 356
    },
    {
      "name": "MM-NIAH",
      "one_line_profile": "Benchmark for evaluating multimodal LLMs on long documents",
      "detailed_description": "A comprehensive benchmark suite designed to systematically evaluate the capability of Multimodal Large Language Models (MLLMs) to comprehend long multimodal documents (Needle In A Multimodal Haystack).",
      "domains": [
        "G1",
        "AI Evaluation"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenGVLab/MM-NIAH",
      "help_website": [],
      "license": null,
      "tags": [
        "benchmark",
        "multimodal",
        "long-context"
      ],
      "id": 357
    },
    {
      "name": "RocketQA",
      "one_line_profile": "Dense retrieval toolkit for QA and IR",
      "detailed_description": "An optimized toolkit for dense retrieval in information retrieval and question answering scenarios, providing state-of-the-art models for English and Chinese.",
      "domains": [
        "G1-03",
        "NLP"
      ],
      "subtask_category": [
        "dense_retrieval",
        "question_answering"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/PaddlePaddle/RocketQA",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "dense-retrieval",
        "qa",
        "paddlepaddle"
      ],
      "id": 358
    },
    {
      "name": "PathwayCommons-semantic-search",
      "one_line_profile": "Semantic search engine for scientific papers",
      "detailed_description": "A simple semantic search engine specifically designed for indexing and retrieving scientific papers, likely leveraging biological pathway data context.",
      "domains": [
        "G1-03",
        "Bioinformatics"
      ],
      "subtask_category": [
        "literature_search",
        "semantic_retrieval"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/PathwayCommons/semantic-search",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "semantic-search",
        "scientific-literature",
        "bioinformatics"
      ],
      "id": 359
    },
    {
      "name": "Piazza-Updater",
      "one_line_profile": "Automation tool for Weaviate-based RAG updates",
      "detailed_description": "A tool that automates the updating of Weaviate vector databases with real-time data to enhance Retrieval-Augmented Generation (RAG) capabilities for knowledge applications.",
      "domains": [
        "G1-03",
        "Knowledge Management"
      ],
      "subtask_category": [
        "rag_pipeline",
        "database_update"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/Piazza-tech/Piazza-Updater",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "weaviate",
        "automation"
      ],
      "id": 360
    },
    {
      "name": "Raphtory",
      "one_line_profile": "Scalable temporal graph analytics engine",
      "detailed_description": "A scalable graph analytics database and engine powered by Rust, designed for analyzing temporal networks and dynamic graph data.",
      "domains": [
        "G1",
        "Network Science"
      ],
      "subtask_category": [
        "graph_analytics",
        "temporal_network_analysis"
      ],
      "application_level": "platform",
      "primary_language": "Rust",
      "repo_url": "https://github.com/Pometry/Raphtory",
      "help_website": [
        "https://raphtory.readthedocs.io/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "graph-analytics",
        "temporal-graphs",
        "rust"
      ],
      "id": 361
    },
    {
      "name": "VESPA-SAV",
      "one_line_profile": "Single Amino Acid Variant effect predictor",
      "detailed_description": "A predictor for Single Amino Acid Variant (SAV) effects based on embeddings from the Protein Language Model ProtT5.",
      "domains": [
        "Biology",
        "Bioinformatics"
      ],
      "subtask_category": [
        "variant_effect_prediction",
        "protein_modeling"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Rostlab/VESPA",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "protein-language-model",
        "variant-prediction",
        "prott5"
      ],
      "id": 362
    },
    {
      "name": "SDM-RDFizer",
      "one_line_profile": "RML-compliant engine for Knowledge Graph construction",
      "detailed_description": "An efficient engine for constructing Knowledge Graphs from structured data using RML (RDF Mapping Language) mappings, facilitating semantic data integration.",
      "domains": [
        "G1",
        "Semantic Web"
      ],
      "subtask_category": [
        "knowledge_graph_construction",
        "data_integration"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/SDM-TIB/SDM-RDFizer",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "knowledge-graph",
        "rml",
        "rdf"
      ],
      "id": 363
    },
    {
      "name": "RaiseWikibase",
      "one_line_profile": "Fast bulk data insertion tool for Wikibase knowledge graphs",
      "detailed_description": "A Python tool designed to facilitate the construction of knowledge graphs by performing fast, batched inserts into Wikibase instances. It is particularly useful for library science and scientific knowledge graph management.",
      "domains": [
        "G1"
      ],
      "subtask_category": [
        "knowledge_graph_construction",
        "data_ingestion"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/UB-Mannheim/RaiseWikibase",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "wikibase",
        "knowledge-graph",
        "semantic-web"
      ],
      "id": 364
    },
    {
      "name": "GPL",
      "one_line_profile": "Generative Pseudo Labeling for unsupervised domain adaptation of dense retrieval",
      "detailed_description": "A library for training dense retrieval models on specific domains (e.g., scientific literature) using only unlabeled text. It uses a generative approach to create pseudo-labels, significantly improving retrieval performance in specialized scientific fields.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "retrieval_adaptation",
        "model_training"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/UKPLab/gpl",
      "help_website": [
        "https://arxiv.org/abs/2112.07577"
      ],
      "license": "Apache-2.0",
      "tags": [
        "information-retrieval",
        "domain-adaptation",
        "dense-retrieval"
      ],
      "id": 365
    },
    {
      "name": "Deep Lake",
      "one_line_profile": "Database for AI optimized for deep learning and scientific data",
      "detailed_description": "A data lake for deep learning that stores complex data (images, videos, tensors) and connects them to LLMs and PyTorch/TensorFlow. It serves as critical infrastructure for AI4S applications requiring efficient storage and retrieval of multimodal scientific data.",
      "domains": [
        "G1-03"
      ],
      "subtask_category": [
        "data_storage",
        "vector_retrieval",
        "dataset_management"
      ],
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/activeloopai/deeplake",
      "help_website": [
        "https://docs.deeplake.ai"
      ],
      "license": "Apache-2.0",
      "tags": [
        "vector-database",
        "datalake",
        "multimodal"
      ],
      "id": 366
    },
    {
      "name": "Academic Search MCP Server",
      "one_line_profile": "Model Context Protocol server for scientific literature search",
      "detailed_description": "A tool that integrates Semantic Scholar and Crossref search capabilities into LLM environments (like Claude Desktop) via the Model Context Protocol, enabling AI agents to perform accurate scientific literature discovery.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "literature_search",
        "paper_discovery"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/afrise/academic-search-mcp-server",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "semantic-scholar",
        "mcp",
        "literature-search"
      ],
      "id": 367
    },
    {
      "name": "DocumentGPT",
      "one_line_profile": "RAG-based web application for semantic search and chat with research documents",
      "detailed_description": "A web application that enables researchers to interact with their documents using a Retrieval-Augmented Generation (RAG) approach. It utilizes OpenAI's API and vector databases to perform semantic search and generate responses based on the content of uploaded research papers.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "document_qa",
        "semantic_search"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/aju22/DocumentGPT",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "llm",
        "semantic-search",
        "research-assistant"
      ],
      "id": 368
    },
    {
      "name": "singleCellHaystack",
      "one_line_profile": "Method for finding differentially active genes in single-cell transcriptome data",
      "detailed_description": "A bioinformatics tool designed to identify genes with non-random spatial distributions or differential activity in single-cell transcriptome data without relying on prior clustering of cells. It uses a Kullback-Leibler divergence-based approach.",
      "domains": [
        "Bioinformatics",
        "Genomics"
      ],
      "subtask_category": [
        "differential_expression_analysis",
        "gene_selection"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/alexisvdb/singleCellHaystack",
      "help_website": [
        "https://github.com/alexisvdb/singleCellHaystack"
      ],
      "license": "NOASSERTION",
      "tags": [
        "single-cell",
        "transcriptomics",
        "gene-expression",
        "bioinformatics"
      ],
      "id": 369
    },
    {
      "name": "citeomatic",
      "one_line_profile": "Citation recommendation system for academic paper drafts",
      "detailed_description": "A tool that predicts relevant citations for a given scientific manuscript. It uses a neural network model trained on the Semantic Scholar OpenCorpus to suggest missing references based on the text of the paper.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "citation_recommendation"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/allenai/citeomatic",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "citation-prediction",
        "recommender-system",
        "academic-writing"
      ],
      "id": 370
    },
    {
      "name": "s2search",
      "one_line_profile": "Search reranking library for academic literature",
      "detailed_description": "A Python library implementing the search reranking algorithms used by Semantic Scholar. It allows researchers to improve the relevance of search results in academic retrieval tasks using linear and neural ranking models.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "search_reranking",
        "information_retrieval"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/allenai/s2search",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "reranking",
        "search",
        "semantic-scholar",
        "ir"
      ],
      "id": 371
    },
    {
      "name": "Scientific-Papers-MCP",
      "one_line_profile": "MCP server for LLM access to arXiv and OpenAlex papers",
      "detailed_description": "A Model Context Protocol (MCP) server implementation that enables Large Language Models to search, retrieve, and read scientific papers directly from arXiv and OpenAlex APIs, facilitating AI-assisted literature review.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "literature_retrieval",
        "llm_integration"
      ],
      "application_level": "service",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/benedict2310/Scientific-Papers-MCP",
      "help_website": [],
      "license": null,
      "tags": [
        "mcp",
        "arxiv",
        "openalex",
        "llm-agent"
      ],
      "id": 372
    },
    {
      "name": "Paperion",
      "one_line_profile": "Academic search engine interface",
      "detailed_description": "An open-source academic search engine platform designed to provide comprehensive access to scholarly articles. It serves as a tool for discovering and filtering scientific literature.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "literature_search"
      ],
      "application_level": "platform",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/blankresearch/Paperion",
      "help_website": [],
      "license": null,
      "tags": [
        "search-engine",
        "academic-search",
        "literature-discovery"
      ],
      "id": 373
    },
    {
      "name": "BABILong",
      "one_line_profile": "Benchmark for evaluating long-context LLMs on needle-in-a-haystack tasks",
      "detailed_description": "A benchmark suite designed to evaluate the performance of Large Language Models in processing extremely long contexts. It uses a 'needle-in-a-haystack' approach to test retrieval and reasoning capabilities, which is critical for analyzing long scientific texts.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmark"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/booydar/babilong",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "benchmark",
        "llm",
        "long-context",
        "evaluation"
      ],
      "id": 374
    },
    {
      "name": "Anserini",
      "one_line_profile": "Reproducible information retrieval toolkit based on Lucene",
      "detailed_description": "A comprehensive toolkit for information retrieval research that supports both sparse (BM25) and dense (vector) retrieval. It is widely used for building academic search engines and reproducing IR baselines in scientific literature tasks.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "information_retrieval",
        "indexing"
      ],
      "application_level": "platform",
      "primary_language": "Java",
      "repo_url": "https://github.com/castorini/anserini",
      "help_website": [
        "http://anserini.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "information-retrieval",
        "lucene",
        "search-engine",
        "bm25"
      ],
      "id": 375
    },
    {
      "name": "anserini-tools",
      "one_line_profile": "Evaluation and utility scripts for the Anserini ecosystem",
      "detailed_description": "A collection of shared utilities and evaluation scripts used across the Anserini, Pyserini, and PyGaggle projects. It facilitates the standardized evaluation of information retrieval models.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "evaluation",
        "utility"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/castorini/anserini-tools",
      "help_website": [],
      "license": null,
      "tags": [
        "evaluation",
        "ir",
        "metrics"
      ],
      "id": 376
    },
    {
      "name": "Birch",
      "one_line_profile": "BERT-based document ranking model",
      "detailed_description": "A document ranking tool that utilizes BERT for sentence-level modeling to improve retrieval effectiveness. It aggregates sentence scores to rank documents, often used in high-precision academic search tasks.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "document_ranking",
        "information_retrieval"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/castorini/birch",
      "help_website": [],
      "license": null,
      "tags": [
        "bert",
        "ranking",
        "ir",
        "document-retrieval"
      ],
      "id": 377
    },
    {
      "name": "DHR",
      "one_line_profile": "Dense Hybrid Representations for text retrieval",
      "detailed_description": "A retrieval model that combines dense and sparse representations to achieve efficient and effective text retrieval. It is designed to handle the trade-off between retrieval speed and accuracy in large-scale document collections.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "text_retrieval",
        "embedding"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/castorini/dhr",
      "help_website": [],
      "license": null,
      "tags": [
        "dense-retrieval",
        "hybrid-search",
        "ir"
      ],
      "id": 378
    },
    {
      "name": "hf-spacerini",
      "one_line_profile": "Search interface builder for Pyserini and Hugging Face models",
      "detailed_description": "A toolkit designed to facilitate reproducible information retrieval research by providing plug-and-play search interfaces that integrate Pyserini's retrieval capabilities with Hugging Face's models, enabling researchers to visualize and interact with search results.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "information_retrieval",
        "visualization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/castorini/hf-spacerini",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "search-interface",
        "pyserini",
        "huggingface",
        "ir-research"
      ],
      "id": 379
    },
    {
      "name": "PyGaggle",
      "one_line_profile": "Neural ranking library for text ranking and question answering",
      "detailed_description": "A library containing various deep neural architectures for text ranking and question answering, specifically designed to work in conjunction with Pyserini for information retrieval tasks.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "ranking",
        "question_answering"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/castorini/pygaggle",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "neural-ranking",
        "text-ranking",
        "qa",
        "pyserini"
      ],
      "id": 380
    },
    {
      "name": "Pyserini",
      "one_line_profile": "Toolkit for reproducible information retrieval research",
      "detailed_description": "A Python toolkit that supports reproducible information retrieval research, offering sparse and dense retrieval representations, and integrating with the Anserini IR toolkit.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "information_retrieval",
        "indexing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/castorini/pyserini",
      "help_website": [
        "https://pyserini.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "information-retrieval",
        "bm25",
        "dense-retrieval",
        "lucene"
      ],
      "id": 381
    },
    {
      "name": "dspy-neo4j-knowledge-graph",
      "one_line_profile": "Automated knowledge graph construction tool using LLMs",
      "detailed_description": "A tool leveraging DSPy and Neo4j to automate the construction of knowledge graphs from unstructured text, facilitating the structuring of scientific knowledge.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "knowledge_graph_construction",
        "information_extraction"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/chrisammon3000/dspy-neo4j-knowledge-graph",
      "help_website": [],
      "license": null,
      "tags": [
        "knowledge-graph",
        "neo4j",
        "dspy",
        "llm"
      ],
      "id": 382
    },
    {
      "name": "Rektor",
      "one_line_profile": "Lightweight vector database",
      "detailed_description": "A vector database designed for storing and retrieving high-dimensional vectors, serving as infrastructure for semantic search and RAG applications in scientific domains.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "vector_storage",
        "similarity_search"
      ],
      "application_level": "solver",
      "primary_language": null,
      "repo_url": "https://github.com/codediodeio/rektor-db",
      "help_website": [],
      "license": null,
      "tags": [
        "vector-database",
        "search",
        "embeddings"
      ],
      "id": 383
    },
    {
      "name": "Adaptive Classifier",
      "one_line_profile": "Flexible system for dynamic text classification",
      "detailed_description": "A classification system designed to adaptively classify text data, suitable for organizing and filtering scientific literature or textual data.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "text_classification",
        "filtering"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/codelion/adaptive-classifier",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "classification",
        "nlp",
        "text-mining"
      ],
      "id": 384
    },
    {
      "name": "HNSW (Go)",
      "one_line_profile": "In-memory vector index library for Go",
      "detailed_description": "A Go implementation of the Hierarchical Navigable Small World (HNSW) algorithm for efficient approximate nearest neighbor search, a core component for vector retrieval systems.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "vector_indexing",
        "similarity_search"
      ],
      "application_level": "library",
      "primary_language": "Go",
      "repo_url": "https://github.com/coder/hnsw",
      "help_website": [],
      "license": "CC0-1.0",
      "tags": [
        "hnsw",
        "vector-search",
        "ann"
      ],
      "id": 385
    },
    {
      "name": "Abstracts Search",
      "one_line_profile": "Semantic search engine for academic publications",
      "detailed_description": "A semantic search engine implementation capable of indexing and retrieving from a large corpus of academic publications (110 million abstracts), facilitating literature discovery.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "semantic_search",
        "literature_retrieval"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/colonelwatch/abstracts-search",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "semantic-search",
        "academic-search",
        "nlp"
      ],
      "id": 386
    },
    {
      "name": "Cosdata",
      "one_line_profile": "AI data platform for search pipelines",
      "detailed_description": "A data platform designed for next-generation search pipelines, featuring semantic search, hybrid capabilities, and ML integration, suitable for managing scientific data retrieval.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "semantic_search",
        "data_management"
      ],
      "application_level": "platform",
      "primary_language": "Rust",
      "repo_url": "https://github.com/cosdata/cosdata",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "vector-search",
        "data-platform",
        "rust"
      ],
      "id": 387
    },
    {
      "name": "CozoDB",
      "one_line_profile": "Transactional relational-graph-vector database",
      "detailed_description": "A hybrid database system combining relational, graph, and vector capabilities with Datalog query support, enabling complex knowledge representation and retrieval for AI applications.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "graph_storage",
        "vector_storage",
        "knowledge_representation"
      ],
      "application_level": "solver",
      "primary_language": "Rust",
      "repo_url": "https://github.com/cozodb/cozo",
      "help_website": [
        "https://docs.cozodb.org/"
      ],
      "license": "MPL-2.0",
      "tags": [
        "graph-database",
        "vector-database",
        "datalog"
      ],
      "id": 388
    },
    {
      "name": "CrateDB",
      "one_line_profile": "Distributed SQL database for real-time analytics",
      "detailed_description": "A distributed SQL database based on Lucene, optimized for storing and analyzing massive amounts of data, including vector search capabilities relevant for scientific data retrieval.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "data_storage",
        "analytics",
        "vector_search"
      ],
      "application_level": "solver",
      "primary_language": "Java",
      "repo_url": "https://github.com/crate/crate",
      "help_website": [
        "https://crate.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "sql",
        "distributed-database",
        "lucene",
        "vector-search"
      ],
      "id": 389
    },
    {
      "name": "Autofaiss",
      "one_line_profile": "Tool for automatic Faiss index creation",
      "detailed_description": "A tool that automatically creates and optimizes Faiss k-nearest neighbor indices, simplifying the deployment of efficient vector search systems for large datasets.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "vector_indexing",
        "optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/criteo/autofaiss",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "faiss",
        "vector-search",
        "knn"
      ],
      "id": 390
    },
    {
      "name": "Semantic Scholar Client",
      "one_line_profile": "Unofficial Python client for Semantic Scholar API",
      "detailed_description": "A Python library that provides an interface to the Semantic Scholar API, enabling researchers to programmatically retrieve and analyze scientific literature data.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "literature_retrieval",
        "data_access"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/danielnsilva/semanticscholar",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "semantic-scholar",
        "api-client",
        "bibliometrics"
      ],
      "id": 391
    },
    {
      "name": "Databend",
      "one_line_profile": "AI-native cloud data warehouse",
      "detailed_description": "A modern cloud data warehouse that supports vector search and AI capabilities, providing infrastructure for storing and analyzing large-scale scientific and multimodal data.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "data_warehouse",
        "analytics",
        "vector_search"
      ],
      "application_level": "platform",
      "primary_language": "Rust",
      "repo_url": "https://github.com/databendlabs/databend",
      "help_website": [
        "https://databend.com"
      ],
      "license": "NOASSERTION",
      "tags": [
        "data-warehouse",
        "olap",
        "vector-search",
        "rust"
      ],
      "id": 392
    },
    {
      "name": "Bolt",
      "one_line_profile": "High-performance matrix and vector operations library",
      "detailed_description": "A C++ library optimized for extremely fast matrix and vector operations, serving as a foundational building block for efficient vector retrieval and scientific computing tasks.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "linear_algebra",
        "optimization"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/dblalock/bolt",
      "help_website": [],
      "license": "MPL-2.0",
      "tags": [
        "matrix-operations",
        "simd",
        "performance",
        "cpp"
      ],
      "id": 393
    },
    {
      "name": "Haystack",
      "one_line_profile": "Orchestration framework for LLM and search applications",
      "detailed_description": "An open-source framework for building production-ready LLM applications, retrieval-augmented generation (RAG), and semantic search systems, widely used for processing and querying scientific knowledge bases.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "semantic_search",
        "rag",
        "question_answering"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/deepset-ai/haystack",
      "help_website": [
        "https://haystack.deepset.ai/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "nlp",
        "rag",
        "llm",
        "search-framework"
      ],
      "id": 394
    },
    {
      "name": "OpenAlexAPI",
      "one_line_profile": "Python wrapper library for the OpenAlex scientific database API",
      "detailed_description": "A Python library designed to interact with the OpenAlex API, enabling researchers to programmatically retrieve and analyze scientific metadata, citations, and author networks.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "metadata_retrieval",
        "citation_analysis",
        "scientific_data_access"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/dpriskorn/OpenAlexAPI",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "openalex",
        "bibliometrics",
        "citation-analysis",
        "api-wrapper"
      ],
      "id": 395
    },
    {
      "name": "alex-mcp",
      "one_line_profile": "Model Context Protocol (MCP) server for OpenAlex integration",
      "detailed_description": "An MCP server implementation that connects Large Language Models (LLMs) with the OpenAlex database, allowing AI agents to query scientific literature and citation data directly.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "literature_retrieval",
        "llm_integration",
        "scientific_knowledge_access"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/drAbreu/alex-mcp",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mcp",
        "openalex",
        "llm-agent",
        "literature-search"
      ],
      "id": 396
    },
    {
      "name": "Anserini Solr Plugin",
      "one_line_profile": "Solr plugin for Anserini-style query expansion and reranking",
      "detailed_description": "A Solr plugin developed by Elsevier Labs that enables Anserini-style query expansion and reranking techniques directly within Solr indexes, facilitating advanced information retrieval research and applications.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "information_retrieval",
        "query_expansion"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/elsevierlabs-os/anserini-solr-plugin",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "solr",
        "anserini",
        "information-retrieval",
        "reranking"
      ],
      "id": 397
    },
    {
      "name": "JSTOR Workset Browser",
      "one_line_profile": "Tool suite for indexing and analyzing JSTOR citation datasets",
      "detailed_description": "A suite of software tools designed to cache, index, analyze, and visualize content from JSTOR Data For Research citations.xml files, supporting 'distant reading' and bibliometric analysis of scholarly journal articles.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "text_mining",
        "bibliometrics",
        "visualization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ericleasemorgan/JSTOR-Workset-Browser",
      "help_website": [],
      "license": "GPL-2.0",
      "tags": [
        "jstor",
        "digital-humanities",
        "text-analysis",
        "bibliometrics"
      ],
      "id": 398
    },
    {
      "name": "Contriever",
      "one_line_profile": "Unsupervised dense information retrieval model with contrastive learning",
      "detailed_description": "A dense information retrieval model and library developed by Facebook Research that uses contrastive learning for unsupervised training, widely used in scientific document retrieval and RAG pipelines.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "information_retrieval",
        "embedding"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/contriever",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "dense-retrieval",
        "contrastive-learning",
        "information-retrieval"
      ],
      "id": 399
    },
    {
      "name": "Distributed Faiss",
      "one_line_profile": "Library for building and serving multi-node distributed Faiss indices",
      "detailed_description": "An extension of the Faiss library enabling the construction and serving of vector similarity search indices across multiple nodes, essential for handling large-scale scientific embedding datasets.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "vector_search",
        "indexing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/distributed-faiss",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "faiss",
        "distributed-computing",
        "vector-search"
      ],
      "id": 400
    },
    {
      "name": "DPR Scale",
      "one_line_profile": "Scalable training framework for dense retrieval models",
      "detailed_description": "A library for scalable training of Dense Passage Retrieval (DPR) models, facilitating the development of high-performance neural search systems for scientific literature and open-domain QA.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "model_training",
        "information_retrieval"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/dpr-scale",
      "help_website": [],
      "license": null,
      "tags": [
        "dpr",
        "dense-retrieval",
        "training-framework"
      ],
      "id": 401
    },
    {
      "name": "Faiss",
      "one_line_profile": "Library for efficient similarity search and clustering of dense vectors",
      "detailed_description": "A standard library for efficient similarity search and clustering of dense vectors, serving as the computational backbone for modern scientific information retrieval, molecule similarity search, and genomic sequence embedding analysis.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "vector_search",
        "clustering",
        "similarity_search"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/facebookresearch/faiss",
      "help_website": [
        "https://github.com/facebookresearch/faiss/wiki"
      ],
      "license": "MIT",
      "tags": [
        "vector-search",
        "similarity-search",
        "clustering",
        "gpu-accelerated"
      ],
      "id": 402
    },
    {
      "name": "Multi-hop Dense Retrieval",
      "one_line_profile": "Multi-hop dense retrieval system for complex question answering",
      "detailed_description": "A library implementing multi-hop dense retrieval techniques, enabling the answering of complex questions that require reasoning over multiple documents, applicable to scientific question answering tasks.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "question_answering",
        "information_retrieval"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/multihop_dense_retrieval",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "multi-hop-qa",
        "dense-retrieval",
        "question-answering"
      ],
      "id": 403
    },
    {
      "name": "OpenAlex Raw Tools",
      "one_line_profile": "Tools to process OpenAlex raw snapshot files",
      "detailed_description": "A set of utilities for parsing and processing the raw data snapshots from OpenAlex, a massive open catalog of the global research system, enabling large-scale bibliometric analysis.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "data_parsing",
        "bibliometrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/filipinascimento/openalex-raw",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "openalex",
        "bibliometrics",
        "data-processing"
      ],
      "id": 404
    },
    {
      "name": "OpenAlexNet",
      "one_line_profile": "Library for generating citation and co-authorship networks from OpenAlex",
      "detailed_description": "A helper library to interface with the OpenAlex API, specifically designed to generate and analyze citation and co-authorship networks for scientometric research.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "network_analysis",
        "bibliometrics",
        "data_retrieval"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/filipinascimento/openalexnet",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "openalex",
        "citation-network",
        "co-authorship",
        "scientometrics"
      ],
      "id": 405
    },
    {
      "name": "Zotero Citation Counts Agent",
      "one_line_profile": "Zotero plugin to fetch citation counts from academic APIs",
      "detailed_description": "A plugin for the Zotero reference manager that automatically fetches and displays citation counts from sources like Crossref, Semantic Scholar, INSPIRE-HEP, and NASA ADS.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "bibliometrics",
        "reference_management"
      ],
      "application_level": "solver",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/flychen50/ZoteroCitationCountsAgent",
      "help_website": [],
      "license": "MPL-2.0",
      "tags": [
        "zotero-plugin",
        "citation-count",
        "bibliometrics"
      ],
      "id": 406
    },
    {
      "name": "Semantra",
      "one_line_profile": "Local semantic search tool for documents",
      "detailed_description": "A multi-tool for performing semantic search on local document collections (PDFs, text files), useful for researchers conducting literature reviews or exploring large text corpora.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "semantic_search",
        "literature_review"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/freedmand/semantra",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "semantic-search",
        "local-search",
        "embeddings"
      ],
      "id": 407
    },
    {
      "name": "ScholarLensViz",
      "one_line_profile": "Visualization tool for semantic user profiles in academia",
      "detailed_description": "A visualization tool developed by the University of Jena for displaying and exploring semantic user profiles, supporting research in expert finding and academic profiling.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "visualization",
        "profiling"
      ],
      "application_level": "solver",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/fusion-jena/ScholarLensViz",
      "help_website": [],
      "license": "LGPL-3.0",
      "tags": [
        "visualization",
        "semantic-web",
        "academic-profiling"
      ],
      "id": 408
    },
    {
      "name": "Automatic KG Creation with LLM",
      "one_line_profile": "Framework for constructing knowledge graphs using LLMs",
      "detailed_description": "A Python framework for the automatic construction of ontologies and knowledge graphs from text using Large Language Models, applicable to scientific knowledge extraction and organization.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "knowledge_graph_construction",
        "information_extraction"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/fusion-jena/automatic-KG-creation-with-LLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "knowledge-graph",
        "llm",
        "ontology-construction"
      ],
      "id": 409
    },
    {
      "name": "CODEC Dataset",
      "one_line_profile": "Document and entity ranking dataset for complex topics",
      "detailed_description": "A dataset for document and entity ranking focusing on complex essay-style topics, serving as a benchmark for information retrieval and ranking algorithms in the social sciences and humanities.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "benchmarking",
        "information_retrieval"
      ],
      "application_level": "dataset",
      "primary_language": "Shell",
      "repo_url": "https://github.com/grill-lab/CODEC",
      "help_website": [],
      "license": null,
      "tags": [
        "dataset",
        "information-retrieval",
        "ranking",
        "complex-topics"
      ],
      "id": 410
    },
    {
      "name": "PromptReps",
      "one_line_profile": "Prompting LLMs to generate dense/sparse representations for document retrieval",
      "detailed_description": "A tool that leverages Large Language Models to generate high-quality dense and sparse vector representations for zero-shot document retrieval tasks, enhancing retrieval performance without extensive training data.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "dense_retrieval",
        "representation_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ielab/PromptReps",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "dense-retrieval",
        "zero-shot",
        "information-retrieval"
      ],
      "id": 411
    },
    {
      "name": "llm-rankers",
      "one_line_profile": "Document ranking library using Large Language Models",
      "detailed_description": "A library for implementing and evaluating various document ranking and re-ranking strategies using Large Language Models (LLMs), supporting point-wise, pair-wise, and list-wise ranking approaches.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "ranking",
        "reranking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ielab/llm-rankers",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ranking",
        "llm",
        "information-retrieval",
        "reranking"
      ],
      "id": 412
    },
    {
      "name": "Infinity",
      "one_line_profile": "AI-native database for hybrid search and LLM applications",
      "detailed_description": "A high-performance database designed for LLM applications, offering hybrid search capabilities that combine dense vector, sparse vector, tensor, and full-text search for efficient information retrieval.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "vector_search",
        "retrieval",
        "hybrid_search"
      ],
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/infiniflow/infinity",
      "help_website": [
        "https://infiniflow.org"
      ],
      "license": "Apache-2.0",
      "tags": [
        "vector-database",
        "hybrid-search",
        "llm",
        "retrieval"
      ],
      "id": 413
    },
    {
      "name": "analysis-ik",
      "one_line_profile": "IK Analysis plugin for Elasticsearch/OpenSearch",
      "detailed_description": "A widely used Chinese text analysis plugin for Elasticsearch and OpenSearch, providing customized dictionary support and segmentation for accurate text indexing and retrieval.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "text_processing",
        "tokenization"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/infinilabs/analysis-ik",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "elasticsearch",
        "text-analysis",
        "tokenization",
        "search"
      ],
      "id": 414
    },
    {
      "name": "hyperDB",
      "one_line_profile": "Hyper-fast local vector database for LLM Agents",
      "detailed_description": "A lightweight, local vector database designed for use with LLM agents, enabling fast semantic search and memory storage without the need for external server infrastructure.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "vector_search",
        "retrieval"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/jdagdelen/hyperDB",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "vector-database",
        "local-search",
        "llm-agents"
      ],
      "id": 415
    },
    {
      "name": "vectordb",
      "one_line_profile": "Minimalist Python vector database",
      "detailed_description": "A simple and efficient Python vector database library for managing and searching vector embeddings, suitable for lightweight semantic search applications.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "vector_search",
        "retrieval"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/jina-ai/vectordb",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "vector-database",
        "semantic-search",
        "embeddings"
      ],
      "id": 416
    },
    {
      "name": "kelindar/search",
      "one_line_profile": "High-performance embedded vector search library for Go",
      "detailed_description": "A lightweight Go library designed for embedded vector search and semantic embeddings, enabling efficient similarity search and retrieval capabilities within applications without external dependencies.",
      "domains": [
        "G1-03"
      ],
      "subtask_category": [
        "vector_search",
        "semantic_retrieval"
      ],
      "application_level": "library",
      "primary_language": "Go",
      "repo_url": "https://github.com/kelindar/search",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "vector-search",
        "embedding",
        "go",
        "semantic-search"
      ],
      "id": 417
    },
    {
      "name": "Resin",
      "one_line_profile": "Embedded NoSQL search engine with vector support",
      "detailed_description": "A language model search engine built on a vector database and key/value store, designed to facilitate semantic search and retrieval operations in .NET applications.",
      "domains": [
        "G1-03"
      ],
      "subtask_category": [
        "vector_search",
        "information_retrieval"
      ],
      "application_level": "library",
      "primary_language": "C#",
      "repo_url": "https://github.com/kreeben/resin",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "search-engine",
        "vector-database",
        "dotnet"
      ],
      "id": 418
    },
    {
      "name": "Kùzu",
      "one_line_profile": "Embedded property graph database with vector search capabilities",
      "detailed_description": "An embedded property graph database management system built for speed, featuring built-in vector search and full-text search capabilities, optimized for handling complex relationships in scientific and knowledge graph data.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "graph_database",
        "vector_search",
        "knowledge_graph"
      ],
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/kuzudb/kuzu",
      "help_website": [
        "https://kuzudb.com"
      ],
      "license": "MIT",
      "tags": [
        "graph-database",
        "vector-search",
        "cypher",
        "embedded-db"
      ],
      "id": 419
    },
    {
      "name": "LangChain4j",
      "one_line_profile": "Java library for integrating LLMs and vector databases",
      "detailed_description": "A Java library that simplifies the integration of Large Language Models (LLMs) into applications, providing unified APIs for accessing vector databases and implementing Retrieval-Augmented Generation (RAG) workflows for scientific text analysis.",
      "domains": [
        "G1-03"
      ],
      "subtask_category": [
        "rag",
        "llm_integration",
        "vector_store_connector"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/langchain4j/langchain4j",
      "help_website": [
        "https://docs.langchain4j.dev"
      ],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "llm",
        "java",
        "vector-database"
      ],
      "id": 420
    },
    {
      "name": "Lantern",
      "one_line_profile": "PostgreSQL extension for vector similarity search",
      "detailed_description": "A PostgreSQL extension that adds vector database capabilities, enabling efficient vector similarity search and embedding management directly within the relational database environment, suitable for scientific data indexing.",
      "domains": [
        "G1-03"
      ],
      "subtask_category": [
        "vector_search",
        "database_extension"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/lanterndata/lantern",
      "help_website": [
        "https://lantern.dev"
      ],
      "license": "AGPL-3.0",
      "tags": [
        "postgresql",
        "vector-search",
        "embedding",
        "database"
      ],
      "id": 421
    },
    {
      "name": "Manticore Search",
      "one_line_profile": "High-performance database for full-text and vector search",
      "detailed_description": "An open-source database designed for fast full-text search and vector search, serving as a drop-in replacement for Elasticsearch in scientific data retrieval and indexing pipelines.",
      "domains": [
        "G1-03"
      ],
      "subtask_category": [
        "fulltext_search",
        "vector_search",
        "indexing"
      ],
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/manticoresoftware/manticoresearch",
      "help_website": [
        "https://manticoresearch.com"
      ],
      "license": "GPL-3.0",
      "tags": [
        "search-engine",
        "vector-search",
        "database",
        "elasticsearch-alternative"
      ],
      "id": 422
    },
    {
      "name": "MatrixOne",
      "one_line_profile": "Hyper-converged cloud-edge native database with vector search",
      "detailed_description": "A MySQL-compatible HTAP database that integrates vector search and full-text search capabilities, supporting AI-driven data applications and scientific data management.",
      "domains": [
        "G1-03"
      ],
      "subtask_category": [
        "vector_search",
        "htap_database",
        "data_management"
      ],
      "application_level": "platform",
      "primary_language": "Go",
      "repo_url": "https://github.com/matrixorigin/matrixone",
      "help_website": [
        "https://matrixorigin.cn"
      ],
      "license": "Apache-2.0",
      "tags": [
        "database",
        "vector-search",
        "htap",
        "mysql-compatible"
      ],
      "id": 423
    },
    {
      "name": "OpenKP",
      "one_line_profile": "Keyphrase extraction toolkit and dataset for scientific documents",
      "detailed_description": "A toolkit and dataset for extracting salient keyphrases from documents, specifically optimized for scientific papers and diverse document structures using neural seq2seq models.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "keyphrase_extraction",
        "document_parsing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/OpenKP",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "keyphrase-extraction",
        "nlp",
        "scientific-documents"
      ],
      "id": 424
    },
    {
      "name": "SDR",
      "one_line_profile": "Self-Supervised Document-to-Document Similarity Ranking",
      "detailed_description": "A library implementing self-supervised methods for document-to-document similarity ranking using contextualized language models and hierarchical inference, applicable to scientific literature retrieval.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "document_ranking",
        "similarity_search"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/SDR",
      "help_website": [],
      "license": null,
      "tags": [
        "ranking",
        "similarity",
        "nlp"
      ],
      "id": 425
    },
    {
      "name": "Knowhere",
      "one_line_profile": "Core vector search engine library for Milvus",
      "detailed_description": "The core vector search engine library that powers Milvus, integrating various vector indexing algorithms like FAISS and HNSW for high-performance similarity search.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "vector_search",
        "indexing"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/milvus-io/knowhere",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "vector-search",
        "ann",
        "indexing"
      ],
      "id": 426
    },
    {
      "name": "Milvus",
      "one_line_profile": "High-performance vector database for scalable similarity search",
      "detailed_description": "A cloud-native vector database designed for scalable vector ANN search, serving as critical infrastructure for scientific literature retrieval and embedding management.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "vector_database",
        "retrieval"
      ],
      "application_level": "platform",
      "primary_language": "Go",
      "repo_url": "https://github.com/milvus-io/milvus",
      "help_website": [
        "https://milvus.io/docs"
      ],
      "license": "Apache-2.0",
      "tags": [
        "vector-database",
        "embedding-store",
        "search-engine"
      ],
      "id": 427
    },
    {
      "name": "Milvus Lite",
      "one_line_profile": "Lightweight embedded version of Milvus vector database",
      "detailed_description": "A lightweight, embedded version of the Milvus vector database that can be integrated directly into Python applications for local vector search and retrieval tasks.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "vector_search",
        "local_retrieval"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/milvus-io/milvus-lite",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "vector-search",
        "embedded-db",
        "python"
      ],
      "id": 428
    },
    {
      "name": "Milvus Model",
      "one_line_profile": "Embedding and reranking model integration library",
      "detailed_description": "A library for integrating embedding generation and reranking models, facilitating the processing of scientific text into vectors for semantic search.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "embedding_generation",
        "reranking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/milvus-io/milvus-model",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "embeddings",
        "reranking",
        "semantic-search"
      ],
      "id": 429
    },
    {
      "name": "PyS2",
      "one_line_profile": "Python library for Semantic Scholar API",
      "detailed_description": "A Python library providing typed objects and functionalities to interact with the Semantic Scholar API, facilitating the retrieval of scientific literature data.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "literature_retrieval",
        "api_client"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/mirandrom/PyS2",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "semantic-scholar",
        "literature-search",
        "api-wrapper"
      ],
      "id": 430
    },
    {
      "name": "Baguetter",
      "one_line_profile": "Flexible search engine library for sparse, dense, and hybrid retrieval",
      "detailed_description": "A search engine library designed for benchmarking and implementing sparse, dense, and hybrid retrieval methods, applicable to scientific document search systems.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "retrieval",
        "search_engine"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/mixedbread-ai/baguetter",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "search-engine",
        "hybrid-retrieval",
        "dense-retrieval"
      ],
      "id": 431
    },
    {
      "name": "Dense-Homolog-Retrieval",
      "one_line_profile": "Deep dense retrieval for protein remote homolog detection",
      "detailed_description": "A tool implementing deep dense retrieval methods for the ultra-fast and sensitive detection of protein remote homologs, aiding in bioinformatics analysis.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "homolog_detection",
        "protein_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ml4bio/Dense-Homolog-Retrieval",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "bioinformatics",
        "protein-homology",
        "dense-retrieval"
      ],
      "id": 432
    },
    {
      "name": "SS-self-hosting",
      "one_line_profile": "Tools for self-hosting Semantic Scholar data",
      "detailed_description": "A repository providing scripts and tools to enable researchers to self-host and manage data from the Semantic Scholar dataset for local analysis.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "data_management",
        "literature_hosting"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/moaraio/SS-self-hosting",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "semantic-scholar",
        "data-hosting",
        "scientific-literature"
      ],
      "id": 433
    },
    {
      "name": "MULTICOM",
      "one_line_profile": "Protein structure prediction system",
      "detailed_description": "A comprehensive protein structure prediction system including template-based and template-free modeling, 1D feature prediction, and model ranking.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "structure_prediction",
        "protein_modeling"
      ],
      "application_level": "solver",
      "primary_language": "Perl",
      "repo_url": "https://github.com/multicom-toolbox/multicom",
      "help_website": [],
      "license": null,
      "tags": [
        "protein-structure",
        "bioinformatics",
        "modeling"
      ],
      "id": 434
    },
    {
      "name": "QuickGraph",
      "one_line_profile": "Collaborative annotation tool for knowledge graph information extraction",
      "detailed_description": "A web-based annotation tool designed for rapid, multi-task collaborative information extraction, specifically facilitating the construction of knowledge graphs from text data.",
      "domains": [
        "G1",
        "Sci Knowledge"
      ],
      "subtask_category": [
        "annotation",
        "information_extraction",
        "knowledge_graph_construction"
      ],
      "application_level": "tool",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/nlp-tlp/quickgraph",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "annotation-tool",
        "knowledge-graph",
        "nlp"
      ],
      "id": 435
    },
    {
      "name": "Open Semantic ETL",
      "one_line_profile": "ETL pipeline for processing and enriching document collections for semantic search",
      "detailed_description": "A Python-based ETL toolset for file crawling, document processing (OCR, text extraction), and content analysis (NER), designed to ingest data into Solr/Elasticsearch or Knowledge Graphs.",
      "domains": [
        "G1",
        "Sci Knowledge"
      ],
      "subtask_category": [
        "data_ingestion",
        "text_extraction",
        "named_entity_recognition"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/opensemanticsearch/open-semantic-etl",
      "help_website": [
        "https://www.opensemanticsearch.org/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "etl",
        "ocr",
        "ner",
        "semantic-search"
      ],
      "id": 436
    },
    {
      "name": "pandoc-scholar",
      "one_line_profile": "Pandoc extension for authoring semantic scientific publications",
      "detailed_description": "A tool that enables the creation of semantically meaningful scientific articles using Pandoc, supporting metadata enrichment and conversion to various scientific formats.",
      "domains": [
        "G1",
        "Sci Knowledge"
      ],
      "subtask_category": [
        "scientific_writing",
        "metadata_enrichment"
      ],
      "application_level": "tool",
      "primary_language": "Lua",
      "repo_url": "https://github.com/pandoc-scholar/pandoc-scholar",
      "help_website": [
        "https://github.com/pandoc-scholar/pandoc-scholar"
      ],
      "license": "GPL-2.0",
      "tags": [
        "pandoc",
        "publishing",
        "semantic-publishing"
      ],
      "id": 437
    },
    {
      "name": "LitSearch",
      "one_line_profile": "A retrieval benchmark and evaluation suite for scientific literature search",
      "detailed_description": "A comprehensive benchmark dataset and evaluation framework designed to test and improve retrieval systems specifically for scientific literature, addressing the unique challenges of complex queries and domain-specific terminology in science.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "benchmarking",
        "retrieval_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/princeton-nlp/LitSearch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "scientific-retrieval",
        "benchmark",
        "evaluation",
        "nlp"
      ],
      "id": 438
    },
    {
      "name": "HypEx",
      "one_line_profile": "Framework for automatic causal inference and hypothesis testing",
      "detailed_description": "A fast and customizable Python framework designed for causal inference, enabling researchers to estimate treatment effects and validate hypotheses using statistical methods.",
      "domains": [
        "Sci Knowledge",
        "Data Analysis"
      ],
      "subtask_category": [
        "causal_inference",
        "statistical_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/sb-ai-lab/HypEx",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "causal-inference",
        "statistics",
        "hypothesis-testing"
      ],
      "id": 439
    },
    {
      "name": "vlite",
      "one_line_profile": "Lightweight vector database built on NumPy",
      "detailed_description": "A simple and fast vector database implementation using NumPy, suitable for lightweight semantic search and embedding retrieval tasks in scientific workflows.",
      "domains": [
        "G1-03",
        "Sci Knowledge"
      ],
      "subtask_category": [
        "vector_search",
        "embedding_retrieval"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/sdan/vlite",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "vector-database",
        "numpy",
        "semantic-search"
      ],
      "id": 440
    },
    {
      "name": "matchmaker",
      "one_line_profile": "Library for training and evaluating neural re-ranking models",
      "detailed_description": "A PyTorch-based library designed for the training, evaluation, and analysis of dense retrieval and neural re-ranking models in information retrieval research.",
      "domains": [
        "G1-03",
        "Sci Knowledge"
      ],
      "subtask_category": [
        "information_retrieval",
        "model_training",
        "re-ranking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/sebastian-hofstaetter/matchmaker",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "information-retrieval",
        "neural-ranking",
        "pytorch"
      ],
      "id": 441
    },
    {
      "name": "similarities",
      "one_line_profile": "Toolkit for similarity calculation and semantic search",
      "detailed_description": "A comprehensive toolkit for calculating text and image similarities, supporting semantic search, duplicate detection, and retrieval tasks often used in scientific text mining.",
      "domains": [
        "G1-03",
        "Sci Knowledge"
      ],
      "subtask_category": [
        "semantic_search",
        "similarity_calculation",
        "text_mining"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/shibing624/similarities",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "semantic-search",
        "similarity",
        "nlp"
      ],
      "id": 442
    },
    {
      "name": "diophila",
      "one_line_profile": "Python API wrapper for OpenAlex scientific database",
      "detailed_description": "A Python library for querying the OpenAlex database, facilitating the retrieval and analysis of scientific metadata, authors, and citation networks.",
      "domains": [
        "G1",
        "Sci Knowledge"
      ],
      "subtask_category": [
        "literature_retrieval",
        "bibliometrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/smierz/diophila",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "openalex",
        "bibliometrics",
        "api-wrapper"
      ],
      "id": 443
    },
    {
      "name": "sqlite-vector",
      "one_line_profile": "Vector search extension for SQLite",
      "detailed_description": "A lightweight SQLite extension that adds vector search capabilities, enabling local embedding storage and retrieval for scientific applications and edge computing.",
      "domains": [
        "G1-03",
        "Sci Knowledge"
      ],
      "subtask_category": [
        "vector_search",
        "database_extension"
      ],
      "application_level": "library",
      "primary_language": "C",
      "repo_url": "https://github.com/sqliteai/sqlite-vector",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "sqlite",
        "vector-search",
        "embeddings"
      ],
      "id": 444
    },
    {
      "name": "NOUS",
      "one_line_profile": "Platform for Knowledge Graph construction and reasoning",
      "detailed_description": "A system for the construction, querying, and reasoning of knowledge graphs, supporting streaming data and complex relationship modeling in scientific domains.",
      "domains": [
        "Sci Knowledge",
        "G1"
      ],
      "subtask_category": [
        "knowledge_graph_construction",
        "reasoning"
      ],
      "application_level": "platform",
      "primary_language": "Scala",
      "repo_url": "https://github.com/streaming-graphs/NOUS",
      "help_website": [],
      "license": null,
      "tags": [
        "knowledge-graph",
        "streaming",
        "reasoning"
      ],
      "id": 445
    },
    {
      "name": "t-rex",
      "one_line_profile": "Vector tile server for geospatial data",
      "detailed_description": "A high-performance vector tile server specialized in publishing MVT tiles from geospatial data, supporting scientific mapping and geographic information system (GIS) visualization.",
      "domains": [
        "Earth Science",
        "Visualization"
      ],
      "subtask_category": [
        "geospatial_visualization",
        "map_serving"
      ],
      "application_level": "service",
      "primary_language": "Rust",
      "repo_url": "https://github.com/t-rex-tileserver/t-rex",
      "help_website": [
        "https://t-rex.tileserver.ch/"
      ],
      "license": "MIT",
      "tags": [
        "gis",
        "vector-tiles",
        "mapping"
      ],
      "id": 446
    },
    {
      "name": "semantic_research_engine",
      "one_line_profile": "Semantic search engine for scientific papers",
      "detailed_description": "A research tool that leverages semantic search and large language models to retrieve and synthesize relevant scientific papers based on user queries.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "literature_search",
        "paper_retrieval"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/tahreemrasul/semantic_research_engine",
      "help_website": [],
      "license": null,
      "tags": [
        "semantic-search",
        "rag",
        "literature-review"
      ],
      "id": 447
    },
    {
      "name": "raggo",
      "one_line_profile": "Lightweight RAG library for Go",
      "detailed_description": "A production-ready library for Retrieval Augmented Generation (RAG) in Go, facilitating the creation of semantic search and question-answering systems for knowledge bases.",
      "domains": [
        "G1-03",
        "Sci Knowledge"
      ],
      "subtask_category": [
        "rag",
        "retrieval",
        "question_answering"
      ],
      "application_level": "library",
      "primary_language": "Go",
      "repo_url": "https://github.com/teilomillet/raggo",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "go",
        "retrieval"
      ],
      "id": 448
    },
    {
      "name": "pgvecto.rs",
      "one_line_profile": "Scalable vector search extension for PostgreSQL",
      "detailed_description": "A PostgreSQL extension that adds vector similarity search capabilities, enabling the database to be used for AI applications like semantic search and RAG (Retrieval-Augmented Generation) within the scientific literature ecosystem.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "vector_search",
        "indexing"
      ],
      "application_level": "solver",
      "primary_language": "Rust",
      "repo_url": "https://github.com/tensorchord/pgvecto.rs",
      "help_website": [
        "https://docs.pgvecto.rs"
      ],
      "license": "Apache-2.0",
      "tags": [
        "postgres",
        "vector-search",
        "rag",
        "embeddings"
      ],
      "id": 449
    },
    {
      "name": "HyDE",
      "one_line_profile": "Zero-shot dense retrieval method using hypothetical document embeddings",
      "detailed_description": "An implementation of Hypothetical Document Embeddings (HyDE), a method for precise zero-shot dense retrieval that generates hypothetical documents from queries to improve retrieval accuracy without relevance labels.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "information_retrieval",
        "query_expansion"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/texttron/hyde",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "dense-retrieval",
        "zero-shot",
        "nlp",
        "search"
      ],
      "id": 450
    },
    {
      "name": "pgai",
      "one_line_profile": "PostgreSQL extension suite for AI and RAG application development",
      "detailed_description": "A suite of tools designed to facilitate the development of RAG (Retrieval-Augmented Generation) and semantic search applications directly within PostgreSQL, integrating vector search and embedding workflows.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "rag",
        "semantic_search"
      ],
      "application_level": "solver",
      "primary_language": "PLpgSQL",
      "repo_url": "https://github.com/timescale/pgai",
      "help_website": [
        "https://github.com/timescale/pgai"
      ],
      "license": "PostgreSQL",
      "tags": [
        "postgresql",
        "rag",
        "ai-engineering",
        "vector-search"
      ],
      "id": 451
    },
    {
      "name": "VESPA (Exoplanet)",
      "one_line_profile": "Probabilistic algorithm for validating exoplanet transit signals",
      "detailed_description": "A tool for calculating false positive probabilities for transiting planet candidates, used in astronomy to validate exoplanet signals from photometric data.",
      "domains": [
        "Astronomy"
      ],
      "subtask_category": [
        "signal_validation",
        "statistical_inference"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/timothydmorton/VESPA",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "exoplanets",
        "astronomy",
        "probability",
        "validation"
      ],
      "id": 452
    },
    {
      "name": "Towhee",
      "one_line_profile": "Framework for building neural data processing pipelines",
      "detailed_description": "A framework dedicated to making neural data processing pipelines simple and fast, specifically for generating embeddings and processing unstructured data like text and images for retrieval tasks.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "embedding_generation",
        "pipeline_orchestration"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/towhee-io/towhee",
      "help_website": [
        "https://towhee.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "embeddings",
        "neural-search",
        "pipeline",
        "etl"
      ],
      "id": 453
    },
    {
      "name": "USearch",
      "one_line_profile": "High-performance vector search and clustering engine",
      "detailed_description": "A fast, open-source search and clustering engine for vectors and arbitrary objects, supporting multiple languages. It is used for similarity search in large-scale scientific datasets and embeddings.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "vector_search",
        "clustering"
      ],
      "application_level": "solver",
      "primary_language": "C++",
      "repo_url": "https://github.com/unum-cloud/USearch",
      "help_website": [
        "https://unum-cloud.github.io/usearch/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "vector-search",
        "clustering",
        "similarity-search",
        "cpp"
      ],
      "id": 454
    },
    {
      "name": "UrbanKGent",
      "one_line_profile": "Agent for urban knowledge graph construction",
      "detailed_description": "An agent-based tool designed for the construction of urban knowledge graphs, facilitating the organization and analysis of urban science data.",
      "domains": [
        "Urban Science"
      ],
      "subtask_category": [
        "knowledge_graph_construction",
        "data_modeling"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/usail-hkust/UrbanKGent",
      "help_website": [],
      "license": null,
      "tags": [
        "knowledge-graph",
        "urban-computing",
        "agent"
      ],
      "id": 455
    },
    {
      "name": "Vespa",
      "one_line_profile": "Big data serving engine for AI and vector search",
      "detailed_description": "A platform for low-latency computation over large datasets, specializing in vector search, recommendation, and real-time AI inference. It is widely used for building large-scale scientific literature retrieval and ranking systems.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "search_platform",
        "vector_search"
      ],
      "application_level": "platform",
      "primary_language": "Java",
      "repo_url": "https://github.com/vespa-engine/vespa",
      "help_website": [
        "https://vespa.ai"
      ],
      "license": "Apache-2.0",
      "tags": [
        "search-engine",
        "vector-database",
        "big-data",
        "ai-serving"
      ],
      "id": 456
    },
    {
      "name": "Vespa-MRS",
      "one_line_profile": "Python tools for Magnetic Resonance Spectroscopy (MRS) simulation and analysis",
      "detailed_description": "A suite of Python tools for Magnetic Resonance Spectroscopy (MRS), including pulse simulation, spectral simulation, and data analysis. It supports processing and visualization of MRS data.",
      "domains": [
        "Physics",
        "Spectroscopy"
      ],
      "subtask_category": [
        "simulation",
        "data_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/vespa-mrs/vespa",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "mrs",
        "spectroscopy",
        "simulation",
        "magnetic-resonance"
      ],
      "id": 457
    },
    {
      "name": "GeneticFlow",
      "one_line_profile": "Graph signatures for scholar impact evaluation using self-citations",
      "detailed_description": "A tool for evaluating scholar impact by analyzing citation graphs, specifically focusing on self-citation patterns to generate graph signatures for scientometric analysis.",
      "domains": [
        "G1",
        "Scientometrics"
      ],
      "subtask_category": [
        "network_analysis",
        "impact_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/visdata/GeneticFlow",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "scientometrics",
        "citation-graph",
        "impact-evaluation"
      ],
      "id": 458
    },
    {
      "name": "open-hummingbird-eval",
      "one_line_profile": "Dense NN retrieval evaluation for vision encoders",
      "detailed_description": "Implements the Dense NN Retrieval Evaluation framework used for assessing the In-Context Learning capabilities of Vision Encoders, facilitating benchmarking of retrieval models.",
      "domains": [
        "G1-03",
        "Computer Vision"
      ],
      "subtask_category": [
        "evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/vpariza/open-hummingbird-eval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "retrieval",
        "evaluation",
        "vision-encoder",
        "neural-network"
      ],
      "id": 459
    },
    {
      "name": "context_attentive_ir",
      "one_line_profile": "Implementation of Context-aware Neural Information Retrieval models",
      "detailed_description": "Provides implementations for Context-aware Neural Information Retrieval models, supporting research in neural IR and document ranking.",
      "domains": [
        "G1-03",
        "Information Retrieval"
      ],
      "subtask_category": [
        "modeling",
        "information_retrieval"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/wasiahmad/context_attentive_ir",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "neural-ir",
        "information-retrieval",
        "context-aware"
      ],
      "id": 460
    },
    {
      "name": "Weaviate",
      "one_line_profile": "Open-source vector database for AI-native search",
      "detailed_description": "A cloud-native vector database that stores both objects and vectors, enabling semantic search, vector search, and combination with structured filtering. It is a foundational tool for scientific literature retrieval and knowledge graph applications.",
      "domains": [
        "G1-03",
        "Database"
      ],
      "subtask_category": [
        "vector_search",
        "indexing",
        "storage"
      ],
      "application_level": "platform",
      "primary_language": "Go",
      "repo_url": "https://github.com/weaviate/weaviate",
      "help_website": [
        "https://weaviate.io/developers/weaviate"
      ],
      "license": "BSD-3-Clause",
      "tags": [
        "vector-database",
        "semantic-search",
        "knn",
        "ann"
      ],
      "id": 461
    },
    {
      "name": "ir_axioms",
      "one_line_profile": "Framework for axiomatic retrieval experimentation",
      "detailed_description": "A framework for conducting axiomatic retrieval experimentation, allowing researchers to define and test retrieval axioms and diagnostics for information retrieval models.",
      "domains": [
        "G1-03",
        "Information Retrieval"
      ],
      "subtask_category": [
        "experimentation",
        "retrieval_analysis"
      ],
      "application_level": "library",
      "primary_language": "Julia",
      "repo_url": "https://github.com/webis-de/ir_axioms",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "information-retrieval",
        "axiomatic-retrieval",
        "experimentation"
      ],
      "id": 462
    },
    {
      "name": "weggli",
      "one_line_profile": "Semantic search tool for C and C++ codebases",
      "detailed_description": "A fast and robust semantic search tool designed for security researchers to identify interesting functionality and vulnerabilities in large C and C++ codebases using structural patterns.",
      "domains": [
        "Computer Science",
        "Security"
      ],
      "subtask_category": [
        "static_analysis",
        "code_retrieval"
      ],
      "application_level": "solver",
      "primary_language": "Rust",
      "repo_url": "https://github.com/weggli-rs/weggli",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "static-analysis",
        "security",
        "semantic-search",
        "code-analysis"
      ],
      "id": 463
    },
    {
      "name": "rule-based-retrieval",
      "one_line_profile": "Rule-based retrieval package for RAG applications",
      "detailed_description": "A Python package that enables the creation and management of Retrieval Augmented Generation (RAG) applications with advanced filtering capabilities, integrating with vector databases.",
      "domains": [
        "G1-03",
        "Information Retrieval"
      ],
      "subtask_category": [
        "retrieval",
        "filtering",
        "rag"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/whyhow-ai/rule-based-retrieval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "retrieval",
        "filtering",
        "vector-database"
      ],
      "id": 464
    },
    {
      "name": "tinkerbird",
      "one_line_profile": "Client-side vector database",
      "detailed_description": "A lightweight client-side vector database designed for efficient vector storage and similarity search directly in the browser or client environment.",
      "domains": [
        "G1-03",
        "Database"
      ],
      "subtask_category": [
        "vector_search",
        "storage"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/wizenheimer/tinkerbird",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "vector-database",
        "client-side",
        "similarity-search"
      ],
      "id": 465
    },
    {
      "name": "ProQA",
      "one_line_profile": "Progressively Pretrained Dense Corpus Index for Open-Domain QA",
      "detailed_description": "Implements a progressively pretrained dense corpus index for Open-Domain Question Answering and Information Retrieval, enhancing retrieval accuracy for QA tasks.",
      "domains": [
        "G1-03",
        "Information Retrieval"
      ],
      "subtask_category": [
        "question_answering",
        "indexing",
        "retrieval"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/xwhan/ProQA",
      "help_website": [],
      "license": null,
      "tags": [
        "qa",
        "information-retrieval",
        "dense-retrieval"
      ],
      "id": 466
    },
    {
      "name": "STAN-POI-Recommendation",
      "one_line_profile": "Spatial-Temporal Attention Network for POI Recommendation",
      "detailed_description": "Implementation of a Spatial-Temporal Attention Network for Point-of-Interest (POI) recommendation, used for location and trajectory prediction in recommender systems research.",
      "domains": [
        "G1",
        "Recommender Systems"
      ],
      "subtask_category": [
        "recommendation",
        "modeling",
        "trajectory_prediction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/yingtaoluo/Spatial-Temporal-Attention-Network-for-POI-Recommendation",
      "help_website": [],
      "license": null,
      "tags": [
        "recommender-system",
        "poi",
        "spatial-temporal",
        "attention-network"
      ],
      "id": 467
    },
    {
      "name": "Medical-RAG",
      "one_line_profile": "A medical domain-specific RAG system for retrieving and answering medical queries",
      "detailed_description": "A Retrieval-Augmented Generation (RAG) project specifically designed for the medical domain, utilizing LangChain and Milvus. It supports rapid deployment and domain migration, enabling efficient retrieval and question answering from medical literature or knowledge bases.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "medical_question_answering",
        "information_retrieval"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/yolo-hyl/medical-rag",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "medical-nlp",
        "langchain",
        "milvus"
      ],
      "id": 468
    },
    {
      "name": "ubib",
      "one_line_profile": "CLI tool for managing BibTeX citations and researching academic literature",
      "detailed_description": "A command-line interface tool designed to assist researchers in managing BibTeX citations and conducting research. It provides functionalities to handle bibliographic data, making it easier to organize and retrieve scientific references.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "citation_management",
        "literature_management"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/zeitlings/ubib",
      "help_website": [],
      "license": null,
      "tags": [
        "bibtex",
        "citation",
        "research-tool",
        "cli"
      ],
      "id": 469
    },
    {
      "name": "DeepSearcher",
      "one_line_profile": "Agentic framework for deep research and information retrieval from private data",
      "detailed_description": "An open-source deep research agent that combines Large Language Models (LLMs) with vector databases to perform deep information retrieval and reasoning. It is designed to automate the process of gathering and synthesizing information from private data or the web, suitable for scientific literature review and data gathering.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "literature_review",
        "information_retrieval",
        "knowledge_discovery"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/zilliztech/deep-searcher",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "agent",
        "rag",
        "deep-research",
        "retrieval"
      ],
      "id": 470
    },
    {
      "name": "Phantoscope",
      "one_line_profile": "Cloud-native search engine platform powered by neural networks",
      "detailed_description": "A platform for building search engines using neural networks and vector databases. It enables the creation of applications for retrieving unstructured data (images, text) which can be applied to scientific data retrieval tasks.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "image_search",
        "vector_retrieval"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/zilliztech/phantoscope",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "search-engine",
        "vector-search",
        "neural-network"
      ],
      "id": 471
    },
    {
      "name": "DeepKE",
      "one_line_profile": "Open toolkit for knowledge graph extraction and construction",
      "detailed_description": "A deep learning-based knowledge extraction toolkit designed for knowledge graph construction. It supports various tasks including named entity recognition, relation extraction, and attribute extraction from unstructured text, facilitating the organization of scientific knowledge.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "knowledge_graph_construction",
        "information_extraction",
        "relation_extraction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/zjunlp/DeepKE",
      "help_website": [
        "https://zjunlp.github.io/DeepKE/"
      ],
      "license": "MIT",
      "tags": [
        "knowledge-graph",
        "ie",
        "nlp",
        "relation-extraction"
      ],
      "id": 472
    },
    {
      "name": "semantic-scholar-mcp-server",
      "one_line_profile": "MCP server interface for accessing Semantic Scholar academic data",
      "detailed_description": "A FastMCP server implementation that provides an interface to the Semantic Scholar API. It allows AI agents and tools to programmatically access academic paper data, author information, and citation networks, directly supporting scientific literature retrieval tasks.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "literature_retrieval",
        "citation_analysis"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/zongmin-yu/semantic-scholar-fastmcp-mcp-server",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "semantic-scholar",
        "mcp",
        "literature-search",
        "api-wrapper"
      ],
      "id": 473
    },
    {
      "name": "GraphRAG Agent",
      "one_line_profile": "Integrated RAG framework combining GraphRAG, LightRAG, and Neo4j for knowledge graph construction and search",
      "detailed_description": "A comprehensive RAG solution that integrates multiple advanced RAG techniques (GraphRAG, LightRAG) and graph databases (Neo4j) to construct knowledge graphs from documents and perform deep search reasoning. It includes a custom evaluation framework for GraphRAG performance.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "knowledge_graph_construction",
        "retrieval_augmented_generation",
        "deep_search"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/1517005260/graph-rag-agent",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "graphrag",
        "knowledge-graph",
        "neo4j",
        "rag"
      ],
      "id": 474
    },
    {
      "name": "Zotero MCP",
      "one_line_profile": "Model Context Protocol server connecting Zotero libraries to AI agents",
      "detailed_description": "A bridge tool that enables AI agents (like Claude) to directly access and interact with Zotero research libraries via the Model Context Protocol (MCP). It allows for analyzing citations, retrieving paper summaries, and discussing research papers stored in Zotero.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "literature_management",
        "citation_analysis",
        "evidence_retrieval"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/54yyyu/zotero-mcp",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "zotero",
        "mcp",
        "literature-review",
        "citation"
      ],
      "id": 475
    },
    {
      "name": "Tetsuo Dox Agent",
      "one_line_profile": "Graph-based research assistant for drafting answers with robust citations",
      "detailed_description": "A research assistant agent designed to provide technically focused answers with strict citation backing. It utilizes a graph-based architecture to perform iterative research, self-reflection, and drafts responses based on discovered evidence, ensuring grounding in source materials.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "literature_review",
        "evidence_grounding",
        "citation_generation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/7etsuo/tetsuo-dox-agent",
      "help_website": [],
      "license": null,
      "tags": [
        "research-assistant",
        "citations",
        "agent",
        "grounding"
      ],
      "id": 476
    },
    {
      "name": "VerbaAurea",
      "one_line_profile": "Document preprocessing tool for high-quality knowledge base construction",
      "detailed_description": "A specialized tool for preprocessing documents to build high-quality text data for knowledge bases. It focuses on cleaning, formatting, and structuring text data to improve the performance of downstream RAG and retrieval tasks.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "data_preprocessing",
        "text_cleaning",
        "knowledge_base_construction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/AEPAX/VerbaAurea",
      "help_website": [],
      "license": "CC-BY-4.0",
      "tags": [
        "preprocessing",
        "rag",
        "knowledge-base",
        "text-mining"
      ],
      "id": 477
    },
    {
      "name": "Deep Research Web UI",
      "one_line_profile": "AI-powered assistant for iterative deep research and synthesis",
      "detailed_description": "A web interface and agent system designed for deep research tasks. It supports iterative searching, web scraping, and synthesis of information using Large Language Models (including DeepSeek R1), enabling users to conduct comprehensive literature reviews and topic research.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "literature_review",
        "information_synthesis",
        "web_research"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/AnotiaWang/deep-research-web-ui",
      "help_website": [],
      "license": null,
      "tags": [
        "deep-research",
        "literature-review",
        "agent",
        "web-scraping"
      ],
      "id": 478
    },
    {
      "name": "DeepParseX",
      "one_line_profile": "Multimodal document parsing and knowledge graph construction platform",
      "detailed_description": "A platform for parsing multimodal documents (PDF, Word, PPT, Images) to extract key information and construct Knowledge Graphs (KG) and RAG systems. It focuses on structured data extraction and reasoning for intelligent retrieval.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "document_parsing",
        "knowledge_graph_construction",
        "multimodal_extraction"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/Arterning/DeepParseX",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "document-parsing",
        "knowledge-graph",
        "rag",
        "multimodal"
      ],
      "id": 479
    },
    {
      "name": "PapersChat",
      "one_line_profile": "Agentic AI for chatting with papers and retrieving from ArXiv/PubMed",
      "detailed_description": "An AI agent application that enables users to interact with their local papers and retrieve information directly from scientific repositories like ArXiv and PubMed. It facilitates literature discovery and reading comprehension.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "literature_retrieval",
        "paper_reading",
        "arxiv_integration"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/AstraBert/PapersChat",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "arxiv",
        "pubmed",
        "literature-chat",
        "agent"
      ],
      "id": 480
    },
    {
      "name": "AutoRA",
      "one_line_profile": "Framework for automating the closed-loop scientific research process",
      "detailed_description": "AutoRA (Automated Research Assistant) is a framework designed to automate the empirical research process. It supports closed-loop discovery by integrating experimental design, data collection, and model discovery/refinement.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "automated_discovery",
        "experimental_design",
        "scientific_modeling"
      ],
      "application_level": "framework",
      "primary_language": "TeX",
      "repo_url": "https://github.com/AutoResearch/autora",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "automated-science",
        "closed-loop",
        "experiment-design"
      ],
      "id": 481
    },
    {
      "name": "GraphRAG Accelerator",
      "one_line_profile": "Accelerator for deploying Knowledge Graph powered RAG on Azure",
      "detailed_description": "A deployment tool and accelerator for setting up GraphRAG (Retrieval-Augmented Generation with Knowledge Graphs) infrastructure. It facilitates the creation of knowledge-driven retrieval systems essential for complex scientific query answering.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "rag_deployment",
        "knowledge_graph_infrastructure"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/Azure-Samples/graphrag-accelerator",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "graphrag",
        "azure",
        "knowledge-graph",
        "deployment"
      ],
      "id": 482
    },
    {
      "name": "KG-RAG",
      "one_line_profile": "Knowledge Graph based RAG framework for biomedical knowledge tasks",
      "detailed_description": "A framework developed by Baranzini Lab that leverages Knowledge Graphs to empower Large Language Models for knowledge-intensive tasks, specifically tailored for biomedical research and discovery.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "biomedical_rag",
        "knowledge_graph_reasoning",
        "literature_mining"
      ],
      "application_level": "framework",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/BaranziniLab/KG_RAG",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "biomedical",
        "knowledge-graph",
        "rag",
        "llm"
      ],
      "id": 483
    },
    {
      "name": "ClaimeAI",
      "one_line_profile": "AI-powered fact-checking system using LangGraph for claim verification",
      "detailed_description": "A system that dissects text into verifiable claims and cross-references them with real-world evidence via web searches to generate accuracy reports. It is designed to combat misinformation in LLM outputs and other text sources.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "fact_checking",
        "evidence_verification"
      ],
      "application_level": "workflow",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/BharathxD/ClaimeAI",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fact-checking",
        "langgraph",
        "misinformation-detection"
      ],
      "id": 484
    },
    {
      "name": "Verbalized Sampling",
      "one_line_profile": "Training-free prompting strategy for diverse LLM sampling",
      "detailed_description": "A framework implementing a prompting strategy to mitigate mode collapse in LLMs by requesting responses with probabilities. It serves as a tool for synthetic data generation and creative writing without model fine-tuning.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "synthetic_data_generation",
        "sampling_strategy"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/CHATS-lab/verbalized-sampling",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "llm",
        "prompting",
        "synthetic-data"
      ],
      "id": 485
    },
    {
      "name": "pyhaystack",
      "one_line_profile": "Python client for Project Haystack IoT data retrieval",
      "detailed_description": "A module allowing Python programs to connect to Haystack servers (Niagara, Skyspark, etc.) to retrieve and process building automation and IoT sensor data, facilitating engineering and scientific analysis of building performance.",
      "domains": [
        "G1"
      ],
      "subtask_category": [
        "data_retrieval",
        "iot_data_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ChristianTremblay/pyhaystack",
      "help_website": [
        "https://pyhaystack.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "iot",
        "building-automation",
        "haystack-protocol"
      ],
      "id": 486
    },
    {
      "name": "Doctor Dok",
      "one_line_profile": "Medical data parsing and analysis framework",
      "detailed_description": "A framework to parse health-related PDFs or images into JSON format and utilize LLMs for analysis. It serves as a tool for digitizing and processing medical records for research or personal health data management.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "medical_data_parsing",
        "ocr",
        "clinical_data_extraction"
      ],
      "application_level": "workflow",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/Doctor-One/doctor-dok",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "medical-data",
        "ocr",
        "pdf-parsing"
      ],
      "id": 487
    },
    {
      "name": "RAGMeUp",
      "one_line_profile": "Generic RAG framework for dataset interaction",
      "detailed_description": "A generic Retrieval-Augmented Generation (RAG) framework designed to apply LLM capabilities to arbitrary datasets, facilitating knowledge retrieval and analysis.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "rag_framework",
        "knowledge_retrieval"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/ErikTromp/RAGMeUp",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "llm",
        "data-analysis"
      ],
      "id": 488
    },
    {
      "name": "PaperQA",
      "one_line_profile": "High-accuracy RAG for scientific document QA with citations",
      "detailed_description": "A specialized RAG tool for answering questions from scientific documents (PDFs/text) with high accuracy. It provides grounded answers with specific citations, designed for literature review and evidence extraction.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "scientific_qa",
        "citation_generation",
        "literature_review"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Future-House/paper-qa",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "scientific-literature",
        "citations"
      ],
      "id": 489
    },
    {
      "name": "OpenResearcher",
      "one_line_profile": "AI assistant for scientific research workflows",
      "detailed_description": "An advanced scientific research assistant tool designed to aid researchers in literature review, information synthesis, and research workflow automation.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "literature_review",
        "research_assistant"
      ],
      "application_level": "platform",
      "primary_language": "HTML",
      "repo_url": "https://github.com/GAIR-NLP/OpenResearcher",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "research-assistant",
        "literature-review",
        "ai4s"
      ],
      "id": 490
    },
    {
      "name": "Auto-Deep-Research",
      "one_line_profile": "Automated agent for deep scientific research",
      "detailed_description": "A fully automated AI assistant designed to conduct deep research, likely including literature search, reading, and summarization, streamlining the scientific discovery process.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "literature_review",
        "research_automation"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/HKUDS/Auto-Deep-Research",
      "help_website": [],
      "license": null,
      "tags": [
        "agent",
        "research-automation",
        "literature-mining"
      ],
      "id": 491
    },
    {
      "name": "RAG-Anything",
      "one_line_profile": "Comprehensive RAG framework for diverse data types",
      "detailed_description": "An all-in-one Retrieval-Augmented Generation framework capable of handling various data modalities, suitable for building complex scientific knowledge retrieval systems.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "rag_framework",
        "multimodal_retrieval"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/HKUDS/RAG-Anything",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "multimodal",
        "knowledge-base"
      ],
      "id": 492
    },
    {
      "name": "Infosys Responsible AI Toolkit",
      "one_line_profile": "Toolkit for ensuring AI safety, security, and explainability",
      "detailed_description": "A comprehensive toolkit incorporating features for safety, security, explainability, fairness, bias, and hallucination detection to ensure AI solutions are trustworthy. Crucial for validating evidence chains in scientific RAG systems.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "hallucination_detection",
        "ai_safety",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Infosys/Infosys-Responsible-AI-Toolkit",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "responsible-ai",
        "hallucination-detection",
        "explainability"
      ],
      "id": 493
    },
    {
      "name": "RAG-FiT",
      "one_line_profile": "Framework for enhancing LLMs for RAG tasks using fine-tuning",
      "detailed_description": "A framework developed by Intel Labs designed to enhance Large Language Models (LLMs) specifically for Retrieval-Augmented Generation (RAG) tasks through fine-tuning techniques.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "rag_optimization",
        "model_finetuning"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/IntelLabs/RAG-FiT",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "fine-tuning",
        "llm"
      ],
      "id": 494
    },
    {
      "name": "fastRAG",
      "one_line_profile": "Efficient Retrieval Augmentation and Generation Framework",
      "detailed_description": "An efficient framework by Intel Labs for building Retrieval-Augmented Generation (RAG) systems, optimized for performance and integration with various retrieval and generation components.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "rag_framework",
        "information_retrieval"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/IntelLabs/fastRAG",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "efficient-retrieval",
        "intel"
      ],
      "id": 495
    },
    {
      "name": "Research Agents 3.0",
      "one_line_profile": "Multi-agent framework for autonomous research workflows",
      "detailed_description": "A framework combining Autogen and GPTs to build swarms of AI researchers, enabling autonomous information gathering and synthesis for research tasks.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "autonomous_research",
        "agent_simulation",
        "literature_mining"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/JayZeeDesign/research-agents-3.0",
      "help_website": [],
      "license": null,
      "tags": [
        "autogen",
        "multi-agent",
        "research-automation"
      ],
      "id": 496
    },
    {
      "name": "LettuceDetect",
      "one_line_profile": "Hallucination detection framework for RAG applications",
      "detailed_description": "A specialized framework designed to detect hallucinations in Retrieval-Augmented Generation (RAG) applications, ensuring the factual accuracy of generated scientific content.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "hallucination_detection",
        "fact_checking",
        "quality_control"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/KRLabsOrg/LettuceDetect",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "hallucination-detection",
        "rag",
        "reliability"
      ],
      "id": 497
    },
    {
      "name": "OWL Verbalizer",
      "one_line_profile": "Tool for converting OWL ontologies into human-readable text",
      "detailed_description": "A Prolog-based tool that converts machine-readable knowledge (OWL ontologies) into human-readable text, useful for validating scientific knowledge graphs and ontologies.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "ontology_parsing",
        "knowledge_representation",
        "text_generation"
      ],
      "application_level": "solver",
      "primary_language": "Prolog",
      "repo_url": "https://github.com/Kaljurand/owl-verbalizer",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "owl",
        "ontology",
        "prolog",
        "verbalization"
      ],
      "id": 498
    },
    {
      "name": "QualiGPT",
      "one_line_profile": "Tool for qualitative research analysis using LLMs",
      "detailed_description": "An easy-to-use tool designed to assist in qualitative research, likely automating the coding and analysis of textual data using GPT models.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "qualitative_analysis",
        "text_mining",
        "social_science_research"
      ],
      "application_level": "tool",
      "primary_language": "Python",
      "repo_url": "https://github.com/KindOPSTAR/QualiGPT",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "qualitative-research",
        "gpt",
        "text-analysis"
      ],
      "id": 499
    },
    {
      "name": "LeanRAG",
      "one_line_profile": "Knowledge-Graph-Based RAG with Semantic Aggregation",
      "detailed_description": "A RAG framework that utilizes Knowledge Graphs and hierarchical retrieval with semantic aggregation to improve generation quality, particularly for complex knowledge domains.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "rag_framework",
        "knowledge_graph_rag",
        "information_retrieval"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/KnowledgeXLab/LeanRAG",
      "help_website": [],
      "license": null,
      "tags": [
        "knowledge-graph",
        "rag",
        "semantic-aggregation"
      ],
      "id": 500
    },
    {
      "name": "HyperGraphRAG",
      "one_line_profile": "RAG via Hypergraph-Structured Knowledge Representation",
      "detailed_description": "Implementation of HyperGraphRAG (NeurIPS 2025), a method for Retrieval-Augmented Generation that uses hypergraph structures to represent complex relationships in knowledge bases.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "rag_algorithm",
        "knowledge_representation",
        "hypergraph_learning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/LHRLAB/HyperGraphRAG",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "hypergraph",
        "rag",
        "neurips-2025"
      ],
      "id": 501
    },
    {
      "name": "LazyLLM",
      "one_line_profile": "Low-code framework for building multi-agent LLM applications",
      "detailed_description": "A framework designed to simplify the construction of multi-agent Large Language Model applications, facilitating the development of complex AI workflows including research agents.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "agent_framework",
        "workflow_automation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/LazyAGI/LazyLLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "multi-agent",
        "llm",
        "low-code"
      ],
      "id": 502
    },
    {
      "name": "Local Deep Research",
      "one_line_profile": "Local AI agent for deep scientific research across multiple sources",
      "detailed_description": "A local, privacy-focused AI research tool that searches and synthesizes information from over 10 sources including arXiv, PubMed, and the web, achieving high performance on QA benchmarks.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "literature_search",
        "scientific_qa",
        "evidence_synthesis"
      ],
      "application_level": "application",
      "primary_language": "Python",
      "repo_url": "https://github.com/LearningCircuit/local-deep-research",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "deep-research",
        "arxiv",
        "pubmed",
        "local-llm"
      ],
      "id": 503
    },
    {
      "name": "docGPT-langchain",
      "one_line_profile": "Tool for chatting with documents using GPT and LangChain",
      "detailed_description": "A tool leveraging LangChain and GPT models to enable natural language queries over various document formats (PDF, WORD, CSV, TXT), facilitating literature review and data extraction.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "document_qa",
        "literature_mining"
      ],
      "application_level": "tool",
      "primary_language": "Python",
      "repo_url": "https://github.com/Lin-jun-xiang/docGPT-langchain",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "document-qa",
        "langchain",
        "pdf-parsing"
      ],
      "id": 504
    },
    {
      "name": "MiniCheck",
      "one_line_profile": "Efficient fact-checking tool for LLM grounding",
      "detailed_description": "A tool for efficient fact-checking of Large Language Model outputs against grounding documents, implementing methods from EMNLP 2024 to ensure scientific accuracy.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "fact_checking",
        "hallucination_detection",
        "grounding"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Liyan06/MiniCheck",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "fact-checking",
        "emnlp-2024",
        "grounding"
      ],
      "id": 505
    },
    {
      "name": "SurfSense",
      "one_line_profile": "Open source knowledge aggregation and RAG platform",
      "detailed_description": "An open-source alternative to NotebookLM and Perplexity that connects to various data sources (search engines, GitHub, etc.) for comprehensive information retrieval and synthesis.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "knowledge_aggregation",
        "rag_platform",
        "literature_search"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/MODSetter/SurfSense",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "notebooklm-alternative",
        "rag",
        "search-aggregation"
      ],
      "id": 506
    },
    {
      "name": "AutoRAG",
      "one_line_profile": "Automated framework for RAG evaluation and optimization",
      "detailed_description": "An AutoML-style framework for evaluating and optimizing Retrieval-Augmented Generation (RAG) pipelines, helping researchers select the best RAG strategies for their data.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "rag_evaluation",
        "automl",
        "pipeline_optimization"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/Marker-Inc-Korea/AutoRAG",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rag-evaluation",
        "automl",
        "optimization"
      ],
      "id": 507
    },
    {
      "name": "ChatPaper2Xmind",
      "one_line_profile": "Tool to convert academic papers into XMind mind maps",
      "detailed_description": "A tool that uses ChatGPT to summarize academic PDF papers and convert them into XMind mind maps containing images and formulas, facilitating rapid literature review.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "literature_visualization",
        "summarization",
        "pdf_parsing"
      ],
      "application_level": "tool",
      "primary_language": "Python",
      "repo_url": "https://github.com/MasterYip/ChatPaper2Xmind",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "xmind",
        "paper-summarization",
        "visualization"
      ],
      "id": 508
    },
    {
      "name": "go-light-rag",
      "one_line_profile": "Go implementation of LightRAG for graph-based retrieval",
      "detailed_description": "A Go library implementation of the LightRAG system, which combines vector databases with graph database relationships to enhance knowledge retrieval capabilities.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "rag_framework",
        "graph_retrieval",
        "knowledge_graph"
      ],
      "application_level": "library",
      "primary_language": "Go",
      "repo_url": "https://github.com/MegaGrindStone/go-light-rag",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "lightrag",
        "go",
        "graph-rag"
      ],
      "id": 509
    },
    {
      "name": "MemOS",
      "one_line_profile": "Memory management framework for long-term AI agent memory",
      "detailed_description": "An open-source framework for building memory-native AI agents, providing systems for long-term memory, retrieval, and adaptive learning, essential for complex research agents.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "agent_memory",
        "context_management",
        "long_term_retrieval"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/MemTensor/MemOS",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "memory-management",
        "ai-agents",
        "long-context"
      ],
      "id": 510
    },
    {
      "name": "ResearchGPT",
      "one_line_profile": "LLM-based research assistant for conversing with research papers",
      "detailed_description": "An open-source research assistant that leverages Large Language Models (LLMs) to enable interactive conversations with research papers, facilitating literature review and information extraction.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "literature_review",
        "qa"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/MrPeterJin/researchgpt",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "research-assistant",
        "rag",
        "literature-review"
      ],
      "id": 511
    },
    {
      "name": "summarization-eval",
      "one_line_profile": "Reference-free automatic summarization evaluation toolkit",
      "detailed_description": "A toolkit for evaluating automatic summarization systems without reference summaries, featuring potential hallucination detection capabilities, useful for NLP research and quality control of generated text.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "evaluation",
        "quality_control"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Muhtasham/summarization-eval",
      "help_website": [],
      "license": null,
      "tags": [
        "summarization",
        "evaluation",
        "hallucination-detection",
        "nlp"
      ],
      "id": 512
    },
    {
      "name": "Context-Aware RAG",
      "one_line_profile": "Context-aware RAG library for Knowledge Graph ingestion",
      "detailed_description": "A library by NVIDIA for building context-aware Retrieval-Augmented Generation (RAG) systems, specifically focusing on Knowledge Graph ingestion and retrieval functions to enhance evidence grounding.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "rag",
        "knowledge_graph_construction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NVIDIA/context-aware-rag",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "knowledge-graph",
        "nvidia",
        "context-aware"
      ],
      "id": 513
    },
    {
      "name": "GaussianSTORM",
      "one_line_profile": "Spatiotemporal reconstruction model for large-scale outdoor scenes",
      "detailed_description": "A PyTorch implementation of STORM (Spatiotemporal Reconstruction Model) for reconstructing large-scale outdoor scenes using Gaussian Splatting, applicable in computer vision and spatial modeling research.",
      "domains": [
        "Sci Knowledge"
      ],
      "subtask_category": [
        "reconstruction",
        "modeling"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/NVlabs/GaussianSTORM",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "3d-reconstruction",
        "gaussian-splatting",
        "computer-vision",
        "spatiotemporal"
      ],
      "id": 514
    },
    {
      "name": "HippoRAG",
      "one_line_profile": "Neurobiologically inspired RAG framework for long-term memory integration",
      "detailed_description": "A novel RAG framework inspired by human long-term memory (hippocampal indexing theory) that enables LLMs to continuously integrate knowledge across external documents using Knowledge Graphs and Personalized PageRank.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "rag",
        "knowledge_integration"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/OSU-NLP-Group/HippoRAG",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "knowledge-graph",
        "long-term-memory",
        "nlp"
      ],
      "id": 515
    },
    {
      "name": "UltraRAG",
      "one_line_profile": "Low-code framework for building complex RAG pipelines",
      "detailed_description": "A framework for building advanced Retrieval-Augmented Generation (RAG) pipelines, supporting complex information retrieval and generation tasks, suitable for scientific knowledge management systems.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "rag",
        "pipeline_construction"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenBMB/UltraRAG",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "pipeline",
        "llm",
        "retrieval"
      ],
      "id": 516
    },
    {
      "name": "Ask-Anything",
      "one_line_profile": "Multimodal ChatGPT for video understanding and analysis",
      "detailed_description": "A multimodal AI tool (VideoChatGPT) that extends ChatGPT capabilities to video understanding, allowing users to query and analyze video content, applicable in multimodal scientific data analysis.",
      "domains": [
        "Sci Knowledge"
      ],
      "subtask_category": [
        "multimodal_analysis",
        "video_understanding"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenGVLab/Ask-Anything",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "multimodal",
        "video-understanding",
        "chatgpt",
        "visual-qa"
      ],
      "id": 517
    },
    {
      "name": "InternGPT",
      "one_line_profile": "Interactive multimodal AI platform for model showcasing and editing",
      "detailed_description": "An open-source demo platform (iGPT) supporting various multimodal AI models like DragGAN, ChatGPT, and ImageBind, facilitating interactive image editing and multimodal chat for research demonstration and experimentation.",
      "domains": [
        "Sci Knowledge"
      ],
      "subtask_category": [
        "multimodal_interaction",
        "visualization"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenGVLab/InternGPT",
      "help_website": [
        "http://igpt.opengvlab.com"
      ],
      "license": "Apache-2.0",
      "tags": [
        "multimodal",
        "interactive-demo",
        "image-editing",
        "llm"
      ],
      "id": 518
    },
    {
      "name": "EasyDetect",
      "one_line_profile": "Hallucination detection framework for Large Language Models",
      "detailed_description": "An easy-to-use framework designed to detect hallucinations in Large Language Models (LLMs), providing tools for evaluating and improving the reliability of generative AI in scientific contexts.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "hallucination_detection",
        "evaluation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenKG-ORG/EasyDetect",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "hallucination-detection",
        "llm",
        "evaluation",
        "reliability"
      ],
      "id": 519
    },
    {
      "name": "KAG",
      "one_line_profile": "Logical form-guided reasoning and retrieval framework",
      "detailed_description": "A framework combining OpenSPG engine and LLMs for logical reasoning and factual Q&A over professional domain knowledge bases, addressing limitations of vector-based RAG in scientific reasoning.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "reasoning",
        "retrieval",
        "qa"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/OpenSPG/KAG",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "knowledge-graph",
        "reasoning",
        "rag",
        "logical-form"
      ],
      "id": 520
    },
    {
      "name": "local-rag-llamaindex",
      "one_line_profile": "Local RAG assistant for navigating research papers",
      "detailed_description": "A local RAG implementation using LlamaIndex designed to assist researchers in quickly navigating and querying research papers, ensuring data privacy and efficient literature review.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "literature_review",
        "navigation"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/Otman404/local-rag-llamaindex",
      "help_website": [],
      "license": null,
      "tags": [
        "rag",
        "llamaindex",
        "research-assistant",
        "local-deployment"
      ],
      "id": 521
    },
    {
      "name": "ollama-deep-researcher-ts",
      "one_line_profile": "Local web research and report writing assistant",
      "detailed_description": "A fully local web research and report writing assistant powered by Ollama, designed to automate the process of gathering information and synthesizing research reports.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "literature_search",
        "report_generation"
      ],
      "application_level": "workflow",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/PacoVK/ollama-deep-researcher-ts",
      "help_website": [],
      "license": "Unlicense",
      "tags": [
        "research-assistant",
        "ollama",
        "automation",
        "report-writing"
      ],
      "id": 522
    },
    {
      "name": "ITFormer",
      "one_line_profile": "Framework for temporal-textual multimodal question answering",
      "detailed_description": "Implementation of ITFormer, a framework for temporal-textual multimodal question answering (QA), enabling complex reasoning over multimodal data with temporal context.",
      "domains": [
        "Sci Knowledge"
      ],
      "subtask_category": [
        "multimodal_qa",
        "reasoning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Pandalin98/ITFormer-ICML25",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "multimodal",
        "qa",
        "temporal-reasoning",
        "transformer"
      ],
      "id": 523
    },
    {
      "name": "LatteReview",
      "one_line_profile": "Automated systematic literature review agent",
      "detailed_description": "A low-code Python package designed to automate systematic literature review processes through AI-powered agents, facilitating the screening and synthesis of research papers.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "literature_review",
        "automation"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/PouriaRouzrokh/LatteReview",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "literature-review",
        "agent",
        "automation",
        "systematic-review"
      ],
      "id": 524
    },
    {
      "name": "ScholiumAI",
      "one_line_profile": "AI research assistant for scientific literature",
      "detailed_description": "An AI-powered research assistant designed to help researchers interact with and extract information from scientific literature.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "literature_review",
        "research_assistant"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/QDScholium/ScholiumAI",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "research-assistant",
        "ai",
        "literature"
      ],
      "id": 525
    },
    {
      "name": "quantcoder-cli",
      "one_line_profile": "CLI tool to transform research papers into trading algorithms",
      "detailed_description": "An AI-powered command-line interface that utilizes GPT-4 to parse trading research papers and convert them into executable QuantConnect algorithms, automating the literature-to-code workflow.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "literature_parsing",
        "code_generation"
      ],
      "application_level": "tool",
      "primary_language": "Python",
      "repo_url": "https://github.com/SL-Mar/quantcoder-cli",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "cli",
        "research-to-code",
        "quant",
        "llm"
      ],
      "id": 526
    },
    {
      "name": "ragflow-upload",
      "one_line_profile": "Automated document uploader for RagFlow knowledge bases",
      "detailed_description": "A utility tool designed to batch upload and parse documents into the RagFlow knowledge base, streamlining the data ingestion process for RAG systems.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "data_ingestion",
        "knowledge_base_management"
      ],
      "application_level": "tool",
      "primary_language": "Python",
      "repo_url": "https://github.com/Samge0/ragflow-upload",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ragflow",
        "document-upload",
        "automation"
      ],
      "id": 527
    },
    {
      "name": "EmbedAI",
      "one_line_profile": "Private document interaction application using GPT",
      "detailed_description": "A desktop application enabling private interaction with local documents via GPT models, ensuring data privacy while performing retrieval-augmented generation tasks.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "document_qa",
        "rag"
      ],
      "application_level": "application",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/SamurAIGPT/EmbedAI",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "privacy",
        "document-chat",
        "gpt",
        "desktop-app"
      ],
      "id": 528
    },
    {
      "name": "GPT-Agent",
      "one_line_profile": "CAMEL role-playing agent framework",
      "detailed_description": "An implementation of the CAMEL (Communicative Agents for \"Mind\" Exploration of Large Scale Society) framework, enabling autonomous multi-agent collaboration for complex task solving.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "agent_simulation",
        "multi_agent_collaboration"
      ],
      "application_level": "framework",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/SamurAIGPT/GPT-Agent",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "agent",
        "camel",
        "multi-agent",
        "llm"
      ],
      "id": 529
    },
    {
      "name": "RAG-Performance",
      "one_line_profile": "Benchmarking tool for RAG solution throughput and latency",
      "detailed_description": "A toolkit for measuring and evaluating the performance metrics (throughput and latency) of various Retrieval-Augmented Generation (RAG) solutions.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "benchmarking",
        "performance_evaluation"
      ],
      "application_level": "tool",
      "primary_language": "Python",
      "repo_url": "https://github.com/SciPhi-AI/RAG-Performance",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "benchmark",
        "latency",
        "throughput"
      ],
      "id": 530
    },
    {
      "name": "SciPhi Synthesizer",
      "one_line_profile": "LLM framework for RAG and synthetic data creation",
      "detailed_description": "A multi-purpose Large Language Model framework designed to facilitate Retrieval-Augmented Generation (RAG) workflows and the generation of synthetic datasets for training and evaluation.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "rag_framework",
        "synthetic_data_generation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/SciPhi-AI/synthesizer",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "rag",
        "data-generation"
      ],
      "id": 531
    },
    {
      "name": "DeepLiterature",
      "one_line_profile": "Open-source intelligent research assistant",
      "detailed_description": "A comprehensive research assistant tool integrating search, code execution, link resolution, and information expansion to support scientific research workflows.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "literature_search",
        "research_assistant"
      ],
      "application_level": "application",
      "primary_language": "Python",
      "repo_url": "https://github.com/ScienceOne-AI/DeepLiterature",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "research-assistant",
        "literature-review",
        "automation"
      ],
      "id": 532
    },
    {
      "name": "SeekStorm",
      "one_line_profile": "Sub-millisecond full-text search library and server",
      "detailed_description": "A high-performance full-text search library and multi-tenancy server written in Rust, suitable for indexing and retrieving large-scale scientific literature and text data.",
      "domains": [
        "G1"
      ],
      "subtask_category": [
        "full_text_search",
        "indexing"
      ],
      "application_level": "library",
      "primary_language": "Rust",
      "repo_url": "https://github.com/SeekStorm/SeekStorm",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "search-engine",
        "rust",
        "information-retrieval"
      ],
      "id": 533
    },
    {
      "name": "chatWeb",
      "one_line_profile": "Web crawler and document parser for QA",
      "detailed_description": "A tool capable of crawling web pages and parsing PDF/DOCX/TXT files to extract content and perform question answering or summarization.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "web_crawling",
        "document_parsing",
        "summarization"
      ],
      "application_level": "tool",
      "primary_language": "Python",
      "repo_url": "https://github.com/SkywalkerDarren/chatWeb",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "crawler",
        "parser",
        "qa",
        "summarization"
      ],
      "id": 534
    },
    {
      "name": "solace-agent-mesh",
      "one_line_profile": "Event-driven multi-agent orchestration framework",
      "detailed_description": "A framework for building and orchestrating multi-agent AI systems using an event-driven architecture, enabling integration with real-world data sources for complex workflows.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "agent_orchestration",
        "workflow_automation"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/SolaceLabs/solace-agent-mesh",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "multi-agent",
        "event-driven",
        "orchestration"
      ],
      "id": 535
    },
    {
      "name": "pdfchat",
      "one_line_profile": "Local PDF chat application with Mistral 7B",
      "detailed_description": "A local application leveraging Mistral 7B, Langchain, and Ollama to enable chat-based interaction with PDF documents.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "document_qa",
        "local_inference"
      ],
      "application_level": "application",
      "primary_language": "Python",
      "repo_url": "https://github.com/SonicWarrior1/pdfchat",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "pdf",
        "chat",
        "local-llm",
        "mistral"
      ],
      "id": 536
    },
    {
      "name": "AdalFlow",
      "one_line_profile": "Library to build and auto-optimize LLM applications",
      "detailed_description": "A library designed to assist developers in building and automatically optimizing Large Language Model (LLM) applications, suitable for constructing scientific RAG pipelines.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "llm_optimization",
        "application_building"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/SylphAI-Inc/AdalFlow",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "optimization",
        "pipeline"
      ],
      "id": 537
    },
    {
      "name": "TaskingAI",
      "one_line_profile": "Open source platform for AI-native application development",
      "detailed_description": "A platform providing infrastructure for building AI-native applications, including tools for model inference, retrieval, and agent management.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "ai_development",
        "agent_management",
        "retrieval"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/TaskingAI/TaskingAI",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ai-platform",
        "agent",
        "retrieval"
      ],
      "id": 538
    },
    {
      "name": "WeKnora",
      "one_line_profile": "LLM-powered framework for deep document understanding and RAG",
      "detailed_description": "A framework leveraging LLMs for deep document understanding, semantic retrieval, and context-aware question answering using the RAG paradigm.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "document_understanding",
        "semantic_retrieval",
        "rag"
      ],
      "application_level": "framework",
      "primary_language": "Go",
      "repo_url": "https://github.com/Tencent/WeKnora",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "rag",
        "document-understanding",
        "llm"
      ],
      "id": 539
    },
    {
      "name": "chipper",
      "one_line_profile": "AI interface for RAG using Ollama and Haystack",
      "detailed_description": "An AI interface designed for experimentation with local LLMs (Ollama) and RAG pipelines (Haystack), facilitating custom AI workflows.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "rag_interface",
        "local_llm"
      ],
      "application_level": "application",
      "primary_language": "Python",
      "repo_url": "https://github.com/TilmanGriesel/chipper",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ollama",
        "haystack",
        "rag",
        "ui"
      ],
      "id": 540
    },
    {
      "name": "Stormwater Management Model (SWMM)",
      "one_line_profile": "Dynamic hydrology-hydraulic water quality simulation model",
      "detailed_description": "A dynamic rainfall-runoff simulation model used for single event or long-term (continuous) simulation of runoff quantity and quality from primarily urban areas.",
      "domains": [
        "G1"
      ],
      "subtask_category": [
        "hydrology_simulation",
        "water_quality_modeling"
      ],
      "application_level": "solver",
      "primary_language": "C",
      "repo_url": "https://github.com/USEPA/Stormwater-Management-Model",
      "help_website": [
        "https://www.epa.gov/water-research/storm-water-management-model-swmm"
      ],
      "license": null,
      "tags": [
        "hydrology",
        "simulation",
        "epa",
        "water-quality"
      ],
      "id": 541
    },
    {
      "name": "Researcher",
      "one_line_profile": "Automated research assistant for query answering with citations",
      "detailed_description": "A tool that leverages Google Search and GPT-3 to provide concise answers to research queries, explicitly including citations to ensure evidence-based responses.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "literature_search",
        "citation_retrieval"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/VikParuchuri/researcher",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "literature-search",
        "citations",
        "rag",
        "research-assistant"
      ],
      "id": 542
    },
    {
      "name": "Cyber Doctor (Cyber Huatuo)",
      "one_line_profile": "Medical agent framework based on LLM and Knowledge Graph",
      "detailed_description": "A framework for building personal doctor agents using multimodal LLMs and medical knowledge graphs. It supports disease preliminary diagnosis, medical record analysis, and professional Q&A.",
      "domains": [
        "Sci Knowledge",
        "Medical AI"
      ],
      "subtask_category": [
        "medical_diagnosis",
        "clinical_decision_support"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Warma10032/cyber-doctor",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "medical-ai",
        "knowledge-graph",
        "diagnosis",
        "llm-agent"
      ],
      "id": 543
    },
    {
      "name": "SAG",
      "one_line_profile": "SQL-driven RAG engine with automatic knowledge graph construction",
      "detailed_description": "A RAG engine that uses SQL to drive retrieval and automatically builds knowledge graphs during querying, facilitating structured knowledge management and retrieval for complex queries.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "knowledge_graph_construction",
        "rag"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Zleap-AI/SAG",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "knowledge-graph",
        "sql",
        "retrieval"
      ],
      "id": 544
    },
    {
      "name": "AutoSurveyGPT",
      "one_line_profile": "Automated literature survey and review assistant",
      "detailed_description": "An intelligent research assistant that leverages GPT-3.5/4 to automatically find, analyze, and rank relevant academic papers from Google Scholar based on user queries, generating literature reviews.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "literature_review",
        "paper_ranking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/a554b554/AutoSurveyGPT",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "literature-survey",
        "google-scholar",
        "automated-review",
        "gpt"
      ],
      "id": 545
    },
    {
      "name": "LARS",
      "one_line_profile": "Local LLM application for document QA with detailed citations",
      "detailed_description": "A local RAG application designed to run LLMs on personal devices with user documents, specifically focusing on generating detailed citations in responses to ensure research integrity.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "local_rag",
        "citation_generation"
      ],
      "application_level": "application",
      "primary_language": "Python",
      "repo_url": "https://github.com/abgulati/LARS",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "local-llm",
        "citations",
        "rag",
        "privacy"
      ],
      "id": 546
    },
    {
      "name": "DeepLake",
      "one_line_profile": "Database for AI data (vectors, images, videos) management",
      "detailed_description": "A data lake for deep learning that allows storing, querying, versioning, and visualizing complex AI data (vectors, images, text). It streamlines data streaming to PyTorch/TensorFlow for scientific modeling.",
      "domains": [
        "Sci Data",
        "AI Infrastructure"
      ],
      "subtask_category": [
        "data_management",
        "vector_storage"
      ],
      "application_level": "platform",
      "primary_language": "C++",
      "repo_url": "https://github.com/activeloopai/deeplake",
      "help_website": [
        "https://activeloop.ai"
      ],
      "license": "Apache-2.0",
      "tags": [
        "database",
        "deep-learning",
        "vector-store",
        "data-lake"
      ],
      "id": 547
    },
    {
      "name": "OpenResearchAssistant",
      "one_line_profile": "Tool for discovering insights from research paper corpora",
      "detailed_description": "An automated tool designed to analyze corpora of research papers to discover insights, aiding researchers in understanding large collections of scientific literature.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "insight_discovery",
        "literature_analysis"
      ],
      "application_level": "solver",
      "primary_language": "HTML",
      "repo_url": "https://github.com/ai8hyf/OpenResearchAssistant",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "literature-analysis",
        "insight-discovery",
        "research-assistant"
      ],
      "id": 548
    },
    {
      "name": "Zotero Review Assistant",
      "one_line_profile": "Zotero plugin for streamlining systematic literature reviews",
      "detailed_description": "A plugin for Zotero that helps researchers organize and manage articles specifically for review research, streamlining the evidence synthesis process by providing tools for tagging, sorting, and tracking review progress.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "literature_review",
        "evidence_synthesis",
        "reference_management"
      ],
      "application_level": "workflow",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/alima-webdev/zotero-review-assistant",
      "help_website": [],
      "license": null,
      "tags": [
        "zotero",
        "systematic-review",
        "literature-management",
        "plugin"
      ],
      "id": 549
    },
    {
      "name": "RAGChecker",
      "one_line_profile": "Diagnostic framework for analyzing Retrieval-Augmented Generation systems",
      "detailed_description": "A fine-grained framework for diagnosing RAG systems, providing metrics and tools to evaluate the quality of retrieval and generation. It is essential for validating the reliability of literature-based RAG systems in scientific workflows.",
      "domains": [
        "G1-04"
      ],
      "subtask_category": [
        "rag_evaluation",
        "quality_control",
        "model_diagnostics"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/amazon-science/RAGChecker",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "evaluation",
        "metrics",
        "hallucination-detection"
      ],
      "id": 550
    },
    {
      "name": "GPT Researcher",
      "one_line_profile": "Autonomous agent for comprehensive online research and report generation",
      "detailed_description": "An LLM-based autonomous agent that conducts deep research on a given topic by scraping web sources, aggregating information, and generating detailed reports with citations. It automates the literature review and information synthesis process.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "literature_review",
        "information_retrieval",
        "report_generation"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/assafelovic/gpt-researcher",
      "help_website": [
        "https://gptr.dev"
      ],
      "license": "Apache-2.0",
      "tags": [
        "agent",
        "autonomous-research",
        "web-scraping",
        "citations"
      ],
      "id": 551
    },
    {
      "name": "DeerFlow",
      "one_line_profile": "Community-driven framework for deep research and agentic workflows",
      "detailed_description": "A framework combining language models with tools like web search and crawling to perform deep research tasks, automating the collection and synthesis of information for scientific or general inquiry.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "deep_research",
        "information_synthesis",
        "agentic_workflow"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/bytedance/deer-flow",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "agent",
        "deep-research",
        "workflow-automation"
      ],
      "id": 552
    },
    {
      "name": "Langchain-Chatchat",
      "one_line_profile": "Local knowledge-based LLM RAG and Agent application framework",
      "detailed_description": "A RAG and Agent application based on Langchain and local LLMs (like ChatGLM, Qwen, Llama), enabling offline knowledge base construction and question answering.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "rag_framework",
        "knowledge_base_qa"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/chatchat-space/Langchain-Chatchat",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "llm",
        "knowledge-base",
        "agent"
      ],
      "id": 553
    },
    {
      "name": "langchain-chat-with-documents",
      "one_line_profile": "Application to chat with documents using ChatGPT and LangChain",
      "detailed_description": "A tool enabling users to interact with documents (PDF, DOCX, TXT) via a chat interface powered by ChatGPT and LangChain, facilitating document analysis and information retrieval.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "document_qa",
        "rag_application"
      ],
      "application_level": "application",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/ciocan/langchain-chat-with-documents",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "chat-with-pdf",
        "langchain"
      ],
      "id": 554
    },
    {
      "name": "Eino",
      "one_line_profile": "Ultimate LLM/AI application development framework in Golang",
      "detailed_description": "A framework for building LLM and AI applications in Golang, providing components for orchestration, retrieval, and agent construction, suitable for high-performance RAG systems.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "rag_framework",
        "agent_orchestration"
      ],
      "application_level": "workflow",
      "primary_language": "Go",
      "repo_url": "https://github.com/cloudwego/eino",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "golang",
        "llm-framework",
        "rag"
      ],
      "id": 555
    },
    {
      "name": "CocoIndex",
      "one_line_profile": "High-performance data transformation framework for AI",
      "detailed_description": "A data transformation framework designed for AI workflows, supporting incremental processing and high-performance ETL for RAG and vector search pipelines.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "data_processing",
        "etl_for_rag"
      ],
      "application_level": "workflow",
      "primary_language": "Rust",
      "repo_url": "https://github.com/cocoindex-io/cocoindex",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "etl",
        "rag",
        "data-pipeline"
      ],
      "id": 556
    },
    {
      "name": "CodeFuse-muAgent",
      "one_line_profile": "Multi-agent framework driven by Knowledge Graph engine",
      "detailed_description": "An agent framework that integrates Knowledge Graph engines to drive multi-agent collaboration, enhancing reasoning and information retrieval capabilities for complex tasks.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "agent_framework",
        "knowledge_graph_reasoning"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/codefuse-ai/CodeFuse-muAgent",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "multi-agent",
        "knowledge-graph",
        "llm"
      ],
      "id": 557
    },
    {
      "name": "CodeFuse-ChatBot",
      "one_line_profile": "Intelligent assistant for SDLC with Multi-Agent and RAG support",
      "detailed_description": "A chatbot framework designed for the software development lifecycle, incorporating Multi-Agent collaboration and RAG for code and document repositories.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "rag_chatbot",
        "agent_framework"
      ],
      "application_level": "application",
      "primary_language": "Python",
      "repo_url": "https://github.com/codefuse-ai/codefuse-chatbot",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "chatbot",
        "rag",
        "devops-agent"
      ],
      "id": 558
    },
    {
      "name": "Ollama Ebook Summary",
      "one_line_profile": "LLM-based tool for long text and ebook summarization",
      "detailed_description": "A tool utilizing Ollama and LLMs to generate comprehensive bulleted summaries for long texts and ebooks, aiding in literature review and knowledge extraction.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "summarization",
        "literature_analysis"
      ],
      "application_level": "application",
      "primary_language": "Python",
      "repo_url": "https://github.com/cognitivetech/ollama-ebook-summary",
      "help_website": [],
      "license": null,
      "tags": [
        "summarization",
        "ollama",
        "ebook"
      ],
      "id": 559
    },
    {
      "name": "Opik",
      "one_line_profile": "Platform for debugging, evaluating, and monitoring LLM/RAG applications",
      "detailed_description": "A comprehensive platform for tracing, automated evaluation, and monitoring of LLM applications, RAG systems, and agentic workflows, ensuring reliability and performance.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "evaluation",
        "monitoring",
        "rag_optimization"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/comet-ml/opik",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-evaluation",
        "rag-monitoring",
        "observability"
      ],
      "id": 560
    },
    {
      "name": "Open Co-Scientist Agents",
      "one_line_profile": "Implementation of AI co-scientist agents for research automation",
      "detailed_description": "An open-source implementation of AI co-scientist agents using LangGraph and GPT Researcher, designed to assist in scientific research tasks such as hypothesis generation and literature review.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "scientific_agent",
        "literature_review"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/conradry/open-coscientist-agents",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ai-scientist",
        "agent",
        "research-automation"
      ],
      "id": 561
    },
    {
      "name": "Coze-Loop",
      "one_line_profile": "Next-generation AI Agent optimization platform",
      "detailed_description": "A platform for the full-lifecycle management of AI agents, including development, debugging, evaluation, and monitoring, aimed at optimizing agent performance.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "agent_optimization",
        "agent_management"
      ],
      "application_level": "platform",
      "primary_language": "Go",
      "repo_url": "https://github.com/coze-dev/coze-loop",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "agent-platform",
        "optimization",
        "llm"
      ],
      "id": 562
    },
    {
      "name": "UQLM",
      "one_line_profile": "Uncertainty Quantification for Language Models",
      "detailed_description": "A Python package for uncertainty quantification-based hallucination detection in Large Language Models, enhancing the reliability of LLM outputs in scientific and other critical contexts.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "hallucination_detection",
        "uncertainty_quantification"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/cvs-health/uqlm",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "uncertainty-quantification",
        "hallucination",
        "llm-reliability"
      ],
      "id": 563
    },
    {
      "name": "LibreChat",
      "one_line_profile": "Advanced LLM interface with RAG and Agent capabilities",
      "detailed_description": "A comprehensive chat platform supporting multiple LLM providers, Agents, and RAG, offering features like code interpretation and plugin integration for advanced data interaction.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "rag_platform",
        "llm_interface"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/danny-avila/LibreChat",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "chat-ui",
        "rag",
        "multi-model"
      ],
      "id": 564
    },
    {
      "name": "RAG Chatbot",
      "one_line_profile": "Local RAG chatbot for multiple PDFs",
      "detailed_description": "A chatbot application that enables users to chat with multiple PDF documents locally using RAG technology, facilitating private document analysis.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "document_qa",
        "rag_application"
      ],
      "application_level": "application",
      "primary_language": "Python",
      "repo_url": "https://github.com/datvodinh/rag-chatbot",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "pdf-chat",
        "local-llm"
      ],
      "id": 565
    },
    {
      "name": "Neural RDF Verbalizer",
      "one_line_profile": "Multilingual RDF to text generation tool",
      "detailed_description": "A tool for verbalizing RDF triples into natural language text, supporting multilingual output, useful for interpreting Knowledge Graph data.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "knowledge_graph_verbalization",
        "nlg"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/dbpedia/neural-rdf-verbalizer",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rdf",
        "knowledge-graph",
        "nlg"
      ],
      "id": 566
    },
    {
      "name": "Hayhooks",
      "one_line_profile": "Deployment tool for Haystack pipelines as REST APIs",
      "detailed_description": "A utility to easily deploy Haystack RAG pipelines as REST APIs and MCP Tools, facilitating the integration of RAG workflows into other applications.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "pipeline_deployment",
        "rag_service"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/deepset-ai/hayhooks",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "haystack",
        "deployment",
        "rest-api"
      ],
      "id": 567
    },
    {
      "name": "Haystack Core Integrations",
      "one_line_profile": "Core integration packages for Haystack framework",
      "detailed_description": "A collection of additional components and document stores that extend the core capabilities of the Haystack framework, enabling integration with various vector databases and model providers.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "rag_components",
        "integration_library"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/deepset-ai/haystack-core-integrations",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "haystack",
        "integrations",
        "rag"
      ],
      "id": 568
    },
    {
      "name": "Haystack Experimental",
      "one_line_profile": "Experimental features and components for Haystack",
      "detailed_description": "A library containing experimental components and features for the Haystack framework, allowing users to test cutting-edge RAG and LLM functionalities.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "rag_components",
        "experimental_features"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/deepset-ai/haystack-experimental",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "haystack",
        "experimental",
        "rag"
      ],
      "id": 569
    },
    {
      "name": "Denser Chat",
      "one_line_profile": "Chat with PDF files with source highlights",
      "detailed_description": "An application enabling users to chat with PDF documents, featuring source highlighting to ground answers in the original text, enhancing verification and evidence tracking.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "document_qa",
        "evidence_highlighting"
      ],
      "application_level": "application",
      "primary_language": "Python",
      "repo_url": "https://github.com/denser-org/denser-chat",
      "help_website": [],
      "license": null,
      "tags": [
        "rag",
        "pdf-chat",
        "source-highlighting"
      ],
      "id": 570
    },
    {
      "name": "Deep Research",
      "one_line_profile": "AI-powered research assistant for iterative deep research and synthesis",
      "detailed_description": "An autonomous agentic workflow that performs deep research on any topic by combining search engines, web scraping, and LLMs. It iteratively refines research directions, gathers evidence, and synthesizes findings, directly supporting scientific literature review and knowledge discovery.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "literature_review",
        "knowledge_synthesis",
        "evidence_gathering"
      ],
      "application_level": "workflow",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/dzhng/deep-research",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "research-agent",
        "literature-review",
        "autonomous-agent",
        "rag"
      ],
      "id": 571
    },
    {
      "name": "GPT-4o Research Assistant",
      "one_line_profile": "Automated academic paper research assistant using ArXiv and GPT-4o",
      "detailed_description": "A specialized tool designed to assist with academic research by searching ArXiv, identifying promising papers, downloading and extracting content, and generating summaries. It automates the evidence gathering and literature screening process.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "literature_search",
        "paper_summarization",
        "arxiv_mining"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/echohive42/GPT-4o-Research-assistant",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "arxiv",
        "research-assistant",
        "literature-mining",
        "summarization"
      ],
      "id": 572
    },
    {
      "name": "Open Researcher",
      "one_line_profile": "Visual AI research assistant with real-time analysis and automatic citations",
      "detailed_description": "A visual research assistant that leverages LLMs (Claude) and web crawling (Firecrawl) to perform research tasks. It features split-view analysis and automatic citation generation, directly supporting the evidence chain and grounding in scientific research.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "literature_review",
        "citation_generation",
        "evidence_grounding"
      ],
      "application_level": "application",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/firecrawl/open-researcher",
      "help_website": [],
      "license": null,
      "tags": [
        "research-assistant",
        "citations",
        "grounding",
        "web-research"
      ],
      "id": 573
    },
    {
      "name": "Deep Research Agent",
      "one_line_profile": "Agentic deep research assistant for automated literature review and synthesis",
      "detailed_description": "An AI agent designed to conduct deep research by searching, reading, and synthesizing information from multiple sources, automating the literature review process similar to OpenAI's Deep Research.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "literature_review",
        "evidence_synthesis",
        "automated_research"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/grapeot/deep_research_agent",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "agent",
        "literature-review",
        "research-assistant",
        "rag"
      ],
      "id": 574
    },
    {
      "name": "RAGFlow",
      "one_line_profile": "Retrieval-Augmented Generation engine with deep document understanding",
      "detailed_description": "A RAG engine that combines retrieval-augmented generation with agent capabilities, featuring a specialized 'DeepDoc' module for parsing complex scientific documents (PDFs, tables, layouts) to build high-quality evidence chains.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "document_parsing",
        "knowledge_base_management",
        "rag_pipeline",
        "evidence_retrieval"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/infiniflow/ragflow",
      "help_website": [
        "https://ragflow.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "pdf-parsing",
        "knowledge-graph",
        "document-understanding"
      ],
      "id": 575
    },
    {
      "name": "OntologyRAG",
      "one_line_profile": "Biomedical code mapping with RAG and ontology knowledge graphs",
      "detailed_description": "A specialized tool for biomedical code mapping that leverages Retrieval-Augmented Generation (RAG) and Ontology Knowledge Graphs to improve the accuracy of mapping medical text to standardized codes.",
      "domains": [
        "G1-04",
        "Sci Knowledge"
      ],
      "subtask_category": [
        "entity_normalization",
        "biomedical_mapping",
        "ontology_alignment"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/iqvianlp/ontologyRAG",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "biomedical",
        "ontology",
        "rag",
        "normalization"
      ],
      "id": 576
    },
    {
      "name": "Chat With Your Docs",
      "one_line_profile": "RAG-based tool to converse with documents (PDFs, web pages) using various LLMs",
      "detailed_description": "A Python-based application that enables users to interact with documents such as PDFs and web pages using advanced LLMs like Mistral, Llama2, and GPT. It facilitates information extraction and insight generation from scientific literature.",
      "domains": [
        "G1-04"
      ],
      "subtask_category": [
        "document_qa",
        "information_extraction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/jorge-armando-navarro-flores/chat_with_your_docs",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "pdf-chat",
        "llm"
      ],
      "id": 577
    },
    {
      "name": "IncarnaMind",
      "one_line_profile": "Multi-document chat tool supporting PDF/TXT and multiple LLM backends",
      "detailed_description": "A tool designed to connect and chat with multiple documents simultaneously using GPT-4, Claude, or local open-source LLMs. It supports RAG workflows for processing research documents.",
      "domains": [
        "G1-04"
      ],
      "subtask_category": [
        "document_qa",
        "literature_review"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/junruxiong/IncarnaMind",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "multi-doc-qa",
        "llm"
      ],
      "id": 578
    },
    {
      "name": "Gemini PDF Chatbot",
      "one_line_profile": "Streamlit application for chatting with multiple PDFs using Gemini AI",
      "detailed_description": "A user-friendly chatbot interface built with Streamlit that allows users to upload multiple PDF files and engage in natural language conversations to extract context-aware responses, suitable for literature review.",
      "domains": [
        "G1-04"
      ],
      "subtask_category": [
        "document_qa",
        "literature_review"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/kaifcoder/gemini_multipdf_chat",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "streamlit",
        "gemini",
        "pdf-chat"
      ],
      "id": 579
    },
    {
      "name": "Khoj",
      "one_line_profile": "Self-hostable AI second brain for searching and chatting with local docs and web",
      "detailed_description": "An open-source, self-hostable AI assistant that acts as a second brain. It indexes local documents (PDFs, markdown) and web content, allowing researchers to perform deep semantic search and RAG-based Q&A.",
      "domains": [
        "G1-04"
      ],
      "subtask_category": [
        "literature_search",
        "knowledge_management",
        "document_qa"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/khoj-ai/khoj",
      "help_website": [
        "https://khoj.dev"
      ],
      "license": "AGPL-3.0",
      "tags": [
        "second-brain",
        "rag",
        "search"
      ],
      "id": 580
    },
    {
      "name": "Open Paper",
      "one_line_profile": "Workbench for managing research library and conducting AI-assisted literature reviews",
      "detailed_description": "A dedicated tool for managing research papers, offering features to read, annotate, and understand papers. It includes an AI assistant to help conduct literature reviews and synthesize information from the library.",
      "domains": [
        "G1-04"
      ],
      "subtask_category": [
        "literature_review",
        "reference_management"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/khoj-ai/openpaper",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "literature-review",
        "paper-management",
        "ai-assistant"
      ],
      "id": 581
    },
    {
      "name": "Kreuzberg",
      "one_line_profile": "Polyglot document intelligence framework for extracting text from PDFs and images",
      "detailed_description": "A document intelligence library that provides text, metadata, and structured information extraction from PDFs, Office documents, and images. It serves as a critical preprocessing tool for scientific literature parsing and RAG pipelines.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "document_parsing",
        "text_extraction"
      ],
      "application_level": "library",
      "primary_language": "HTML",
      "repo_url": "https://github.com/kreuzberg-dev/kreuzberg",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ocr",
        "pdf-parsing",
        "document-intelligence"
      ],
      "id": 582
    },
    {
      "name": "Swarms",
      "one_line_profile": "Enterprise-grade multi-agent orchestration framework",
      "detailed_description": "A framework for building and orchestrating multi-agent systems. In the context of scientific research, it enables the creation of autonomous research agents that can perform complex tasks like literature search, synthesis, and reasoning.",
      "domains": [
        "G1-04"
      ],
      "subtask_category": [
        "agent_orchestration",
        "autonomous_research"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/kyegomez/swarms",
      "help_website": [
        "https://swarms.ai"
      ],
      "license": "Apache-2.0",
      "tags": [
        "multi-agent",
        "agents",
        "framework"
      ],
      "id": 583
    },
    {
      "name": "DeepAgents",
      "one_line_profile": "Agent harness for complex agentic tasks built on LangChain",
      "detailed_description": "A framework equipped with planning tools and filesystem backends to spawn subagents, designed to handle complex tasks which can include deep research and evidence gathering.",
      "domains": [
        "G1-04"
      ],
      "subtask_category": [
        "agent_orchestration",
        "task_planning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/langchain-ai/deepagents",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "agents",
        "langchain",
        "planning"
      ],
      "id": 584
    },
    {
      "name": "LangChain",
      "one_line_profile": "Framework for developing applications powered by language models",
      "detailed_description": "The foundational framework for building RAG applications and agents. It provides the necessary abstractions for document loading, text splitting, embedding, and vector storage, enabling scientific literature analysis and QA systems.",
      "domains": [
        "G1-04"
      ],
      "subtask_category": [
        "rag_framework",
        "agent_development"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/langchain-ai/langchain",
      "help_website": [
        "https://python.langchain.com"
      ],
      "license": "MIT",
      "tags": [
        "rag",
        "llm",
        "framework"
      ],
      "id": 585
    },
    {
      "name": "LangChain.js",
      "one_line_profile": "JavaScript framework for building context-aware reasoning applications",
      "detailed_description": "The JavaScript/TypeScript implementation of the LangChain framework, enabling the development of RAG and agentic applications in Node.js and web environments.",
      "domains": [
        "G1-04"
      ],
      "subtask_category": [
        "rag_framework",
        "agent_development"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/langchain-ai/langchainjs",
      "help_website": [
        "https://js.langchain.com"
      ],
      "license": "MIT",
      "tags": [
        "rag",
        "javascript",
        "framework"
      ],
      "id": 586
    },
    {
      "name": "Local Deep Researcher",
      "one_line_profile": "Fully local web research and report writing assistant",
      "detailed_description": "An AI agent designed to perform deep web research and generate reports locally. It automates the process of gathering evidence and synthesizing information from online sources.",
      "domains": [
        "G1-04"
      ],
      "subtask_category": [
        "web_research",
        "report_generation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/langchain-ai/local-deep-researcher",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "research-agent",
        "local-llm",
        "automation"
      ],
      "id": 587
    },
    {
      "name": "Bricky",
      "one_line_profile": "Haystack/OpenAI based chatbot for curating custom knowledge bases",
      "detailed_description": "A chatbot tool that uses Haystack and OpenAI to curate and interact with a custom knowledge base, facilitating information retrieval from specific document sets.",
      "domains": [
        "G1-04"
      ],
      "subtask_category": [
        "knowledge_base_curation",
        "document_qa"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/larsbaunwall/bricky",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "haystack",
        "chatbot",
        "knowledge-base"
      ],
      "id": 588
    },
    {
      "name": "LaVague",
      "one_line_profile": "Large Action Model framework to develop AI Web Agents",
      "detailed_description": "A framework for building AI web agents that can automate browser interactions. In a scientific context, it can be used to automate data collection, literature search, and interaction with web-based scientific databases.",
      "domains": [
        "G1-04"
      ],
      "subtask_category": [
        "web_automation",
        "data_collection"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/lavague-ai/LaVague",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "web-agent",
        "automation",
        "lam"
      ],
      "id": 589
    },
    {
      "name": "PaperQA Zotero",
      "one_line_profile": "LLM tool to answer questions from Zotero documents",
      "detailed_description": "A tool that integrates with Zotero to allow users to perform Question Answering (QA) on their reference library using LLMs, directly supporting literature review workflows.",
      "domains": [
        "G1-04"
      ],
      "subtask_category": [
        "literature_review",
        "reference_management"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/lejacobroy/paperqa-zotero",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "zotero",
        "paper-qa",
        "rag"
      ],
      "id": 590
    },
    {
      "name": "ChatGPT Doc Translator",
      "one_line_profile": "Service to translate PDF, Word, and PPTX documents using GPT",
      "detailed_description": "A FastAPI-based service for translating various document formats. It aids in the preprocessing and accessibility of scientific literature across different languages.",
      "domains": [
        "G1-04"
      ],
      "subtask_category": [
        "document_translation",
        "preprocessing"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/leogogog/chatgpt-doc-translator",
      "help_website": [],
      "license": null,
      "tags": [
        "translation",
        "pdf",
        "gpt"
      ],
      "id": 591
    },
    {
      "name": "Contextual Chunking Graph RAG",
      "one_line_profile": "Advanced retrieval system combining semantic search with knowledge graphs",
      "detailed_description": "A RAG implementation that utilizes contextual chunking and knowledge graphs to improve retrieval accuracy. It includes LLM-based answer checking and graph visualization for evidence verification.",
      "domains": [
        "G1-04"
      ],
      "subtask_category": [
        "rag_implementation",
        "knowledge_graph_construction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/lesteroliver911/contextual-chunking-graphpowered-rag",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "graph-rag",
        "contextual-chunking",
        "retrieval"
      ],
      "id": 592
    },
    {
      "name": "Aria AI Research Assistant",
      "one_line_profile": "AI Research Assistant powered by GPT models",
      "detailed_description": "A research assistant tool named Aria that leverages Large Language Models to assist with scientific research tasks, likely including literature query and summarization.",
      "domains": [
        "G1-04"
      ],
      "subtask_category": [
        "research_assistant",
        "literature_review"
      ],
      "application_level": "solver",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/lifan0127/ai-research-assistant",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "research-assistant",
        "gpt",
        "automation"
      ],
      "id": 593
    },
    {
      "name": "Chat With Your Doc",
      "one_line_profile": "Tool to chat with PDF/PPTX/DOCX using LangChain and Azure OpenAI",
      "detailed_description": "A document QA tool that supports multiple formats (PDF, PPTX, DOCX) and integrates with Azure OpenAI and LangChain to provide RAG-based interaction with user documents.",
      "domains": [
        "G1-04"
      ],
      "subtask_category": [
        "document_qa",
        "rag_implementation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/linjungz/chat-with-your-doc",
      "help_website": [],
      "license": null,
      "tags": [
        "azure-openai",
        "langchain",
        "document-chat"
      ],
      "id": 594
    },
    {
      "name": "RAGAT",
      "one_line_profile": "Relation Aware Graph Attention Network for Knowledge Graph Completion",
      "detailed_description": "An implementation of the RAGAT model for Knowledge Graph Completion. It serves as a scientific modeling tool for inferring missing links in knowledge graphs, which is relevant to citation networks and evidence chains.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "knowledge_graph_completion",
        "scientific_modeling"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/liuxiyang641/RAGAT",
      "help_website": [],
      "license": null,
      "tags": [
        "knowledge-graph",
        "gnn",
        "link-prediction"
      ],
      "id": 595
    },
    {
      "name": "EmbedJs",
      "one_line_profile": "NodeJS RAG framework for LLMs and embeddings",
      "detailed_description": "A framework designed to simplify the creation of RAG applications in Node.js. It provides tools for working with LLMs and embeddings, facilitating the development of scientific text analysis tools in JS environments.",
      "domains": [
        "G1-04"
      ],
      "subtask_category": [
        "rag_framework",
        "embedding_management"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/llm-tools/embedJs",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "nodejs",
        "rag",
        "embeddings"
      ],
      "id": 596
    },
    {
      "name": "LLMWare",
      "one_line_profile": "Unified framework for building enterprise RAG pipelines with specialized models",
      "detailed_description": "A comprehensive framework for building RAG pipelines, focusing on small, specialized models. It provides an integrated stack for parsing, indexing, and querying documents, suitable for secure and efficient scientific data processing.",
      "domains": [
        "G1-04"
      ],
      "subtask_category": [
        "rag_framework",
        "pipeline_orchestration"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/llmware-ai/llmware",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "enterprise",
        "small-models"
      ],
      "id": 597
    },
    {
      "name": "ScienceQA",
      "one_line_profile": "Benchmark dataset and baseline models for multimodal science question answering",
      "detailed_description": "A large-scale multimodal science question answering dataset that annotates answers with lectures and explanations, designed to evaluate reasoning capabilities of AI models in scientific domains.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "question_answering",
        "multimodal_reasoning"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/lupantech/ScienceQA",
      "help_website": [
        "https://scienceqa.github.io/"
      ],
      "license": "MIT",
      "tags": [
        "benchmark",
        "multimodal",
        "science-qa"
      ],
      "id": 598
    },
    {
      "name": "FIRE",
      "one_line_profile": "Agent-style framework for fact-checking atomic claims",
      "detailed_description": "A lightweight framework for fact-checking atomic claims using iterative retrieval and verification, designed to reduce LLM and search costs while maintaining strong factuality performance.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "fact_checking",
        "evidence_retrieval"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/mbzuai-nlp/fire",
      "help_website": [],
      "license": null,
      "tags": [
        "fact-checking",
        "retrieval",
        "verification"
      ],
      "id": 599
    },
    {
      "name": "fid-med-eval",
      "one_line_profile": "Feature extraction metrics for generative medical imaging evaluation",
      "detailed_description": "A tool providing feature extraction methods and metrics (like FID) specifically calibrated for evaluating generative models in medical imaging.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "quality_control",
        "evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/mckellwoodland/fid-med-eval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "medical-imaging",
        "evaluation",
        "generative-models"
      ],
      "id": 600
    },
    {
      "name": "CoNLI",
      "one_line_profile": "Framework for ungrounded hallucination detection and reduction",
      "detailed_description": "A plug-and-play framework designed to detect and reduce ungrounded hallucinations in large language models, applicable to ensuring factual consistency in scientific text generation.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "hallucination_detection",
        "factuality"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/CoNLI_hallucination",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "hallucination-detection",
        "nli",
        "factuality"
      ],
      "id": 601
    },
    {
      "name": "HaDes",
      "one_line_profile": "Token-level reference-free hallucination detection",
      "detailed_description": "A framework and benchmark for detecting hallucinations at the token level without references, useful for validating the accuracy of generated scientific content.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "hallucination_detection",
        "quality_control"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/HaDes",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "hallucination",
        "detection",
        "benchmark"
      ],
      "id": 602
    },
    {
      "name": "A2rchi",
      "one_line_profile": "RAG framework specialized for scientific and academic applications",
      "detailed_description": "A general Retrieval-Augmented Generation (RAG) framework specifically designed to handle the requirements of scientific and academic data, developed with institutional backing.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "rag",
        "knowledge_retrieval"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/mit-submit/A2rchi",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "science",
        "academic"
      ],
      "id": 603
    },
    {
      "name": "LLM Graph Builder",
      "one_line_profile": "Tool to construct Knowledge Graphs from unstructured data using LLMs for GraphRAG",
      "detailed_description": "A Neo4j-based tool that leverages Large Language Models to extract nodes and relationships from unstructured text documents (PDFs, YouTube transcripts, Wikipedia) to build knowledge graphs, enabling GraphRAG applications.",
      "domains": [
        "Sci Knowledge",
        "G1-04"
      ],
      "subtask_category": [
        "knowledge_graph_construction",
        "rag_pipeline"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/neo4j-labs/llm-graph-builder",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "knowledge-graph",
        "rag",
        "neo4j",
        "llm",
        "unstructured-data"
      ],
      "id": 604
    },
    {
      "name": "txtai",
      "one_line_profile": "All-in-one embeddings database and RAG framework for semantic search and LLM workflows",
      "detailed_description": "An open-source platform for semantic search, LLM orchestration, and language model workflows. It integrates vector indexes, graph networks, and relational databases to support advanced RAG and retrieval tasks.",
      "domains": [
        "Sci Knowledge",
        "G1-04"
      ],
      "subtask_category": [
        "semantic_search",
        "rag_pipeline",
        "embeddings"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/neuml/txtai",
      "help_website": [
        "https://neuml.github.io/txtai"
      ],
      "license": "Apache-2.0",
      "tags": [
        "semantic-search",
        "rag",
        "llm-orchestration",
        "embeddings",
        "vector-search"
      ],
      "id": 605
    },
    {
      "name": "KG2RAG",
      "one_line_profile": "Knowledge Graph-Guided Retrieval Augmented Generation framework",
      "detailed_description": "Implementation of a Knowledge Graph-Guided Retrieval Augmented Generation approach (NAACL 2025), designed to enhance LLM generation by retrieving structured knowledge from KGs.",
      "domains": [
        "Sci Knowledge",
        "G1-04"
      ],
      "subtask_category": [
        "rag_pipeline",
        "knowledge_graph_retrieval"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/nju-websoft/KG2RAG",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "knowledge-graph",
        "rag",
        "retrieval-augmented-generation",
        "nlp"
      ],
      "id": 606
    },
    {
      "name": "Pautobot",
      "one_line_profile": "Private RAG assistant for document Q&A",
      "detailed_description": "A private task assistant that enables users to ask questions about their documents using GPT models, functioning as a local RAG application for personal knowledge management.",
      "domains": [
        "Sci Knowledge",
        "G1-04"
      ],
      "subtask_category": [
        "document_qa",
        "rag_application"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/nrl-ai/pautobot",
      "help_website": [],
      "license": null,
      "tags": [
        "rag",
        "document-qa",
        "gpt",
        "private-ai"
      ],
      "id": 607
    },
    {
      "name": "Hallucination Probes",
      "one_line_profile": "Real-time detection of hallucinated entities in long-form generation",
      "detailed_description": "A tool for detecting hallucinations in Large Language Model generations, specifically focusing on entity consistency and grounding in long-form text.",
      "domains": [
        "Sci Knowledge",
        "G1-04"
      ],
      "subtask_category": [
        "hallucination_detection",
        "model_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/obalcells/hallucination_probes",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "hallucination-detection",
        "llm-evaluation",
        "grounding"
      ],
      "id": 608
    },
    {
      "name": "Knowledge MCP",
      "one_line_profile": "Local knowledge base with hybrid vector and graph RAG engine",
      "detailed_description": "A Model Context Protocol (MCP) server that runs a local knowledge base using LightRAG, combining vector and graph retrieval for AI agents.",
      "domains": [
        "Sci Knowledge",
        "G1-04"
      ],
      "subtask_category": [
        "rag_engine",
        "knowledge_base"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/olafgeibig/knowledge-mcp",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mcp",
        "rag",
        "graph-rag",
        "lightrag",
        "knowledge-base"
      ],
      "id": 609
    },
    {
      "name": "LLM Proteomics Hallucination",
      "one_line_profile": "Framework for evaluating hallucination risks in LLMs for clinical proteomics",
      "detailed_description": "A systematic evaluation and detection framework for assessing hallucination risks in Large Language Models when applied to clinical proteomics and mass spectrometry interpretation.",
      "domains": [
        "Sci Knowledge",
        "G1-04"
      ],
      "subtask_category": [
        "hallucination_detection",
        "domain_specific_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/olaflaitinen/llm-proteomics-hallucination",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "proteomics",
        "hallucination",
        "llm-evaluation",
        "clinical-ai"
      ],
      "id": 610
    },
    {
      "name": "INSIGHT",
      "one_line_profile": "Autonomous AI agent for medical research",
      "detailed_description": "An autonomous AI system designed to conduct medical research, capable of processing medical literature and data to generate insights.",
      "domains": [
        "Sci Knowledge",
        "G1-04"
      ],
      "subtask_category": [
        "literature_analysis",
        "autonomous_research"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/oneil512/INSIGHT",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "medical-research",
        "autonomous-agent",
        "ai-researcher"
      ],
      "id": 611
    },
    {
      "name": "Palico AI",
      "one_line_profile": "Framework for building and productionizing LLM RAG applications",
      "detailed_description": "An integrated framework to build, improve performance, and productionize LLM applications, specifically focusing on RAG workflows.",
      "domains": [
        "Sci Knowledge",
        "G1-04"
      ],
      "subtask_category": [
        "rag_framework",
        "llm_ops"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/palico-ai/palico-ai",
      "help_website": [
        "https://palico.ai"
      ],
      "license": "MIT",
      "tags": [
        "rag",
        "llm-framework",
        "typescript"
      ],
      "id": 612
    },
    {
      "name": "PapersGPT for Zotero",
      "one_line_profile": "Zotero plugin for AI-assisted literature analysis",
      "detailed_description": "A Zotero plugin that integrates various LLMs (ChatGPT, Claude, DeepSeek, etc.) to assist with reading, summarizing, and analyzing scientific papers directly within the reference manager.",
      "domains": [
        "Sci Knowledge",
        "G1-04"
      ],
      "subtask_category": [
        "literature_analysis",
        "reference_management"
      ],
      "application_level": "platform",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/papersgpt/papersgpt-for-zotero",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "zotero",
        "literature-review",
        "llm-plugin",
        "academic-writing"
      ],
      "id": 613
    },
    {
      "name": "RAPTOR",
      "one_line_profile": "Recursive Abstractive Processing for Tree-Organized Retrieval",
      "detailed_description": "The official implementation of RAPTOR, a retrieval technique that constructs a tree of document summaries to enable retrieval across different levels of abstraction, improving RAG performance for long documents.",
      "domains": [
        "Sci Knowledge",
        "G1-04"
      ],
      "subtask_category": [
        "retrieval",
        "summarization",
        "rag_optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/parthsarthi03/raptor",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "retrieval",
        "summarization",
        "tree-structure"
      ],
      "id": 614
    },
    {
      "name": "Pathway",
      "one_line_profile": "High-performance data processing framework for RAG and LLM pipelines",
      "detailed_description": "A Python ETL framework for stream processing and real-time analytics, optimized for building live RAG pipelines and handling data updates for LLM applications.",
      "domains": [
        "Sci Knowledge",
        "G1-04"
      ],
      "subtask_category": [
        "data_pipeline",
        "rag_infrastructure",
        "stream_processing"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/pathwaycom/pathway",
      "help_website": [
        "https://pathway.com"
      ],
      "license": "NOASSERTION",
      "tags": [
        "etl",
        "rag",
        "stream-processing",
        "real-time-analytics"
      ],
      "id": 615
    },
    {
      "name": "RasaGPT",
      "one_line_profile": "Headless LLM chatbot platform for RAG applications",
      "detailed_description": "A chatbot platform built on Rasa and LangChain that integrates LlamaIndex and vector databases to create RAG-based agents for querying documents and data.",
      "domains": [
        "Sci Knowledge",
        "G1-04"
      ],
      "subtask_category": [
        "rag_platform",
        "chatbot_agent"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/paulpierre/RasaGPT",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rasa",
        "langchain",
        "rag",
        "chatbot"
      ],
      "id": 616
    },
    {
      "name": "GPT4 Internet Research Agent",
      "one_line_profile": "Agent for automated internet research and report generation",
      "detailed_description": "A Streamlit web application that uses GPT-4 and LangChain to conduct internet research, retrieve information, and generate comprehensive research reports with citations.",
      "domains": [
        "Sci Knowledge",
        "G1-04"
      ],
      "subtask_category": [
        "automated_research",
        "report_generation"
      ],
      "application_level": "application",
      "primary_language": "Python",
      "repo_url": "https://github.com/petermartens98/GPT4-LangChain-Internet-Research-Agent-App",
      "help_website": [],
      "license": null,
      "tags": [
        "research-agent",
        "gpt-4",
        "langchain",
        "automated-reporting"
      ],
      "id": 617
    },
    {
      "name": "arXivRAG",
      "one_line_profile": "RAG tool for arXiv academic content",
      "detailed_description": "A tool designed to enhance the retrieval and generation of academic content specifically from the arXiv database using RAG techniques.",
      "domains": [
        "Sci Knowledge",
        "G1-04"
      ],
      "subtask_category": [
        "literature_retrieval",
        "rag_pipeline"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/phitrann/arXivRAG",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "arxiv",
        "rag",
        "academic-retrieval"
      ],
      "id": 618
    },
    {
      "name": "Canopy",
      "one_line_profile": "RAG framework and context engine by Pinecone",
      "detailed_description": "An open-source RAG framework and context engine that handles chunking, embedding, and retrieval to simplify building RAG applications powered by Pinecone.",
      "domains": [
        "Sci Knowledge",
        "G1-04"
      ],
      "subtask_category": [
        "rag_framework",
        "context_management"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/pinecone-io/canopy",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "pinecone",
        "vector-database",
        "context-engine"
      ],
      "id": 619
    },
    {
      "name": "AutoFlow",
      "one_line_profile": "Graph RAG based conversational knowledge base tool",
      "detailed_description": "A conversational knowledge base tool that utilizes Graph RAG and TiDB Serverless Vector Storage to provide advanced retrieval and question answering capabilities.",
      "domains": [
        "Sci Knowledge",
        "G1-04"
      ],
      "subtask_category": [
        "graph_rag",
        "knowledge_base"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/pingcap/autoflow",
      "help_website": [
        "https://tidb.ai"
      ],
      "license": "Apache-2.0",
      "tags": [
        "graph-rag",
        "knowledge-base",
        "tidb",
        "conversational-ai"
      ],
      "id": 620
    },
    {
      "name": "Deep Research MCP",
      "one_line_profile": "MCP server for integrating Deep Research APIs",
      "detailed_description": "A Model Context Protocol (MCP) server that integrates OpenAI's Deep Research APIs and Hugging Face's Open Deep Research, enabling AI assistants to perform deep research tasks.",
      "domains": [
        "Sci Knowledge",
        "G1-04"
      ],
      "subtask_category": [
        "research_integration",
        "agent_tool"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/pminervini/deep-research-mcp",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mcp",
        "deep-research",
        "ai-agent",
        "integration"
      ],
      "id": 621
    },
    {
      "name": "QASMBench",
      "one_line_profile": "Low-level OpenQASM benchmark suite for NISQ evaluation",
      "detailed_description": "A benchmark suite for evaluating and simulating Noisy Intermediate-Scale Quantum (NISQ) devices using OpenQASM. It provides a collection of quantum circuits to assess the performance of quantum compilers, simulators, and hardware.",
      "domains": [
        "Quantum Computing"
      ],
      "subtask_category": [
        "benchmarking",
        "simulation",
        "quantum_circuit_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "OpenQASM",
      "repo_url": "https://github.com/pnnl/QASMBench",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "quantum-computing",
        "benchmark",
        "openqasm",
        "nisq"
      ],
      "id": 622
    },
    {
      "name": "SelfCheckGPT",
      "one_line_profile": "Hallucination detection tool for generative LLMs",
      "detailed_description": "A zero-resource, black-box tool for detecting hallucinations in generative Large Language Models (LLMs). It assesses the factual consistency of generated text without requiring external databases or ground truth.",
      "domains": [
        "G1-04",
        "NLP"
      ],
      "subtask_category": [
        "hallucination_detection",
        "fact_checking",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/potsawee/selfcheckgpt",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "hallucination",
        "llm",
        "fact-checking",
        "nlp"
      ],
      "id": 623
    },
    {
      "name": "Promptify",
      "one_line_profile": "Structured output generation tool for LLMs",
      "detailed_description": "A library for prompt engineering and versioning that helps generate structured outputs (like JSON) from LLMs. It is useful for extracting entities, relations, and other structured data from scientific text.",
      "domains": [
        "G1",
        "NLP"
      ],
      "subtask_category": [
        "prompt_engineering",
        "information_extraction",
        "ner"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/promptslab/Promptify",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "prompt-engineering",
        "nlp",
        "ner",
        "structured-output"
      ],
      "id": 624
    },
    {
      "name": "qKnow",
      "one_line_profile": "Open-source knowledge graph platform",
      "detailed_description": "A platform for knowledge extraction, fusion, graph construction, and visualization. It supports building structured knowledge systems, which is essential for scientific knowledge management and reasoning.",
      "domains": [
        "G1",
        "Knowledge Graph"
      ],
      "subtask_category": [
        "knowledge_extraction",
        "graph_construction",
        "knowledge_fusion"
      ],
      "application_level": "platform",
      "primary_language": "Java",
      "repo_url": "https://github.com/qiantongtech/qKnow",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "knowledge-graph",
        "extraction",
        "visualization",
        "kg"
      ],
      "id": 625
    },
    {
      "name": "RagaAI-Catalyst",
      "one_line_profile": "Agent AI observability and evaluation framework",
      "detailed_description": "A Python SDK for monitoring, tracing, and evaluating AI agents and LLMs. It provides tools for debugging multi-agent systems and analyzing execution graphs, supporting the development of reliable scientific AI agents.",
      "domains": [
        "G1-04",
        "AI Engineering"
      ],
      "subtask_category": [
        "agent_evaluation",
        "observability",
        "tracing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/raga-ai-hub/RagaAI-Catalyst",
      "help_website": [
        "https://docs.raga.ai"
      ],
      "license": "Apache-2.0",
      "tags": [
        "observability",
        "agent-evaluation",
        "llm-ops",
        "debugging"
      ],
      "id": 626
    },
    {
      "name": "RagApp",
      "one_line_profile": "Enterprise-grade Agentic RAG platform",
      "detailed_description": "A platform designed to simplify the deployment of Agentic RAG (Retrieval-Augmented Generation) systems. It facilitates the creation of AI assistants that can retrieve and process internal data, applicable to scientific knowledge bases.",
      "domains": [
        "G1-04"
      ],
      "subtask_category": [
        "rag",
        "agent_deployment",
        "knowledge_retrieval"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/ragapp/ragapp",
      "help_website": [
        "https://ragapp.org"
      ],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "agent",
        "enterprise-ai",
        "retrieval"
      ],
      "id": 627
    },
    {
      "name": "Knowledge Graph RAG",
      "one_line_profile": "Local LLM Graph RAG pipeline",
      "detailed_description": "A tool implementing Graph RAG using local LLMs, converting PDFs to Neo4J graphs for enhanced retrieval. It enables structured querying of document collections using knowledge graph techniques.",
      "domains": [
        "G1-04",
        "Knowledge Graph"
      ],
      "subtask_category": [
        "graph_rag",
        "pdf_parsing",
        "knowledge_graph_construction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/rathcoding/knowledge-graph-rag",
      "help_website": [],
      "license": null,
      "tags": [
        "graph-rag",
        "neo4j",
        "langchain",
        "local-llm"
      ],
      "id": 628
    },
    {
      "name": "AgentGPT",
      "one_line_profile": "Autonomous AI Agent deployment platform",
      "detailed_description": "A platform to assemble, configure, and deploy autonomous AI agents in the browser. While general-purpose, it is widely used to create agents for complex workflows, including research tasks and data gathering.",
      "domains": [
        "G1-04",
        "AI Agents"
      ],
      "subtask_category": [
        "agent_orchestration",
        "workflow_automation",
        "autonomous_agents"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/reworkd/AgentGPT",
      "help_website": [
        "https://agentgpt.reworkd.ai"
      ],
      "license": "GPL-3.0",
      "tags": [
        "autonomous-agents",
        "ai-agent",
        "workflow",
        "browser-based"
      ],
      "id": 629
    },
    {
      "name": "Chain of Density",
      "one_line_profile": "Iterative text summarization tool",
      "detailed_description": "An implementation of the 'Chain of Density' prompting technique for generating increasingly concise and entity-dense summaries. Useful for compressing scientific literature while retaining key information.",
      "domains": [
        "G1",
        "NLP"
      ],
      "subtask_category": [
        "summarization",
        "prompt_engineering",
        "text_compression"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/richawo/chain-of-density",
      "help_website": [],
      "license": null,
      "tags": [
        "summarization",
        "chain-of-density",
        "nlp",
        "llm"
      ],
      "id": 630
    },
    {
      "name": "Llama-Researcher",
      "one_line_profile": "Autonomous online research assistant",
      "detailed_description": "A research assistant tool built with LlamaIndex Workflows and Tavily API. It performs online research on given topics, synthesizing information similar to GPT-Researcher, aiding in literature review and data gathering.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "literature_review",
        "web_research",
        "information_synthesis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/rsrohan99/Llama-Researcher",
      "help_website": [],
      "license": null,
      "tags": [
        "research-assistant",
        "llamaindex",
        "literature-review",
        "agent"
      ],
      "id": 631
    },
    {
      "name": "LlamaIndexTS",
      "one_line_profile": "Data framework for LLM applications (TypeScript)",
      "detailed_description": "The TypeScript version of LlamaIndex, a data framework for building LLM applications. It enables ingesting, structuring, and accessing private or domain-specific data for RAG pipelines.",
      "domains": [
        "G1-04"
      ],
      "subtask_category": [
        "rag",
        "data_indexing",
        "retrieval"
      ],
      "application_level": "library",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/run-llama/LlamaIndexTS",
      "help_website": [
        "https://ts.llamaindex.ai/"
      ],
      "license": "MIT",
      "tags": [
        "rag",
        "typescript",
        "llm-framework",
        "data-indexing"
      ],
      "id": 632
    },
    {
      "name": "LlamaHub",
      "one_line_profile": "Library of data loaders for LLMs",
      "detailed_description": "A community-driven library of data loaders and plugins for LlamaIndex and LangChain. It provides connectors to various data sources (PDFs, APIs, databases), essential for building scientific RAG systems.",
      "domains": [
        "G1-04"
      ],
      "subtask_category": [
        "data_loading",
        "integration",
        "connector"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/run-llama/llama-hub",
      "help_website": [
        "https://llamahub.ai"
      ],
      "license": "MIT",
      "tags": [
        "data-loaders",
        "integration",
        "rag",
        "connectors"
      ],
      "id": 633
    },
    {
      "name": "LlamaIndex",
      "one_line_profile": "Data framework for building LLM-powered agents",
      "detailed_description": "A leading framework for connecting LLMs with external data. It provides tools for data ingestion, indexing (vector, graph, tree), and retrieval, forming the backbone of many scientific RAG and agent applications.",
      "domains": [
        "G1-04"
      ],
      "subtask_category": [
        "rag",
        "data_indexing",
        "agent_framework",
        "retrieval"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/run-llama/llama_index",
      "help_website": [
        "https://docs.llamaindex.ai"
      ],
      "license": "MIT",
      "tags": [
        "rag",
        "llm",
        "indexing",
        "agent"
      ],
      "id": 634
    },
    {
      "name": "Rags",
      "one_line_profile": "Natural language RAG builder",
      "detailed_description": "A tool to build RAG applications over data using natural language instructions. It simplifies the creation of chatbots and search tools for document collections.",
      "domains": [
        "G1-04"
      ],
      "subtask_category": [
        "rag",
        "chatbot_creation",
        "no_code_ai"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/run-llama/rags",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "chatbot",
        "automation",
        "nlp"
      ],
      "id": 635
    },
    {
      "name": "PaperGPT",
      "one_line_profile": "Tool to chat with research papers",
      "detailed_description": "A tool designed to facilitate interaction with research papers via a chat interface. It allows researchers to query and extract information from scientific documents using LLMs.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "literature_qa",
        "document_analysis",
        "rag"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ruogudu/PaperGPT",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "research-assistant",
        "pdf-chat",
        "rag",
        "academic"
      ],
      "id": 636
    },
    {
      "name": "Claude-Flow",
      "one_line_profile": "Agent orchestration platform for Claude",
      "detailed_description": "An agent orchestration platform specifically for Claude models. It supports deploying multi-agent swarms and coordinating autonomous workflows, which can be applied to complex scientific reasoning and data processing tasks.",
      "domains": [
        "G1-04",
        "AI Agents"
      ],
      "subtask_category": [
        "agent_orchestration",
        "multi_agent_system",
        "workflow_automation"
      ],
      "application_level": "platform",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/ruvnet/claude-flow",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "claude",
        "agent-orchestration",
        "swarm-intelligence",
        "workflow"
      ],
      "id": 637
    },
    {
      "name": "GuardRail",
      "one_line_profile": "Data analysis and content generation tool",
      "detailed_description": "A tool for data analysis and AI content generation using OpenAI models. It includes features for sentiment analysis, content classification, and trend analysis, useful for analyzing scientific text or social data.",
      "domains": [
        "G1",
        "NLP"
      ],
      "subtask_category": [
        "text_analysis",
        "classification",
        "sentiment_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ruvnet/guardrail",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "data-analysis",
        "nlp",
        "classification",
        "openai"
      ],
      "id": 638
    },
    {
      "name": "PsiloQA",
      "one_line_profile": "Hallucination detection dataset construction pipeline",
      "detailed_description": "A pipeline that automates the construction of multilingual, span-level hallucination detection datasets. It aids in creating benchmarks for evaluating the faithfulness of RAG and generation models.",
      "domains": [
        "G1-04",
        "NLP"
      ],
      "subtask_category": [
        "dataset_construction",
        "hallucination_research",
        "evaluation"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/s-nlp/PsiloQA",
      "help_website": [],
      "license": null,
      "tags": [
        "hallucination",
        "dataset",
        "nlp",
        "multilingual"
      ],
      "id": 639
    },
    {
      "name": "Summary of a Haystack",
      "one_line_profile": "Benchmark for RAG context retrieval evaluation",
      "detailed_description": "The codebase for the 'Summary of a Haystack' paper, providing a benchmark to evaluate the ability of LLMs to retrieve and summarize information from long contexts (needle-in-a-haystack tests).",
      "domains": [
        "G1-04",
        "NLP"
      ],
      "subtask_category": [
        "model_evaluation",
        "context_retrieval",
        "benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/salesforce/summary-of-a-haystack",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "benchmark",
        "rag",
        "long-context",
        "evaluation"
      ],
      "id": 640
    },
    {
      "name": "AIrXiv",
      "one_line_profile": "AI-powered research assistant specifically for arXiv papers",
      "detailed_description": "A prototype research assistant that leverages LLMs to search, retrieve, and answer questions based on scientific papers hosted on arXiv.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "literature_search",
        "paper_qa",
        "arxiv_mining"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/smsharma/AIrXiv",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "arxiv",
        "research-assistant",
        "rag"
      ],
      "id": 641
    },
    {
      "name": "Multi-Agent Medical Assistant",
      "one_line_profile": "GenAI powered multi-agent system for medical diagnostics and research",
      "detailed_description": "A multi-agent chatbot system designed for healthcare professionals and researchers to assist with medical diagnostics and healthcare research tasks.",
      "domains": [
        "G1",
        "G1-04",
        "Medicine"
      ],
      "subtask_category": [
        "medical_diagnosis",
        "clinical_research",
        "literature_review"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/souvikmajumder26/Multi-Agent-Medical-Assistant",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "healthcare",
        "medical-research",
        "multi-agent"
      ],
      "id": 642
    },
    {
      "name": "STORM",
      "one_line_profile": "LLM-powered knowledge curation system for generating cited research reports",
      "detailed_description": "A system that orchestrates LLMs to research a topic, ask clarifying questions, and generate full-length, Wikipedia-style reports with proper citations, automating the pre-writing stage of research.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "knowledge_curation",
        "report_generation",
        "citation_management"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/stanford-oval/storm",
      "help_website": [
        "https://storm.genie.stanford.edu/"
      ],
      "license": "MIT",
      "tags": [
        "knowledge-synthesis",
        "research-automation",
        "nlp"
      ],
      "id": 643
    },
    {
      "name": "DATAGEN",
      "one_line_profile": "AI-driven multi-agent research assistant for hypothesis generation and analysis",
      "detailed_description": "An automated research assistant that uses multi-agent systems to generate hypotheses, perform data analysis, and write reports, aiming to streamline the scientific research process.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "hypothesis_generation",
        "data_analysis",
        "report_writing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/starpig1129/DATAGEN",
      "help_website": [
        "https://datagen.digital/"
      ],
      "license": "MIT",
      "tags": [
        "research-automation",
        "multi-agent",
        "hypothesis-generation"
      ],
      "id": 644
    },
    {
      "name": "ChatGPT-Paper-Reader",
      "one_line_profile": "Interface for reading and summarizing research papers via OpenAI API",
      "detailed_description": "A tool designed to help researchers read and summarize PDF research papers, allowing for Q&A based on the paper's content using GPT models.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "paper_summarization",
        "literature_reading",
        "pdf_qa"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/talkingwallace/ChatGPT-Paper-Reader",
      "help_website": [],
      "license": null,
      "tags": [
        "pdf-reader",
        "research-papers",
        "summarization"
      ],
      "id": 645
    },
    {
      "name": "GPT Paper Assistant",
      "one_line_profile": "Personalized ArXiv paper assistant bot based on GPT-4",
      "detailed_description": "An automated bot that monitors ArXiv for new papers, summarizes them, and provides personalized recommendations and Q&A capabilities for researchers.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "arxiv_monitoring",
        "paper_recommendation",
        "summarization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/tatsu-lab/gpt_paper_assistant",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "arxiv",
        "gpt-4",
        "research-assistant"
      ],
      "id": 646
    },
    {
      "name": "STORM Research Assistant",
      "one_line_profile": "Implementation of the STORM methodology for AI-driven research",
      "detailed_description": "An implementation of the STORM (Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking) methodology to generate comprehensive research articles.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "knowledge_synthesis",
        "report_generation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/teddynote-lab/STORM-Research-Assistant",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "storm",
        "research-assistant",
        "article-generation"
      ],
      "id": 647
    },
    {
      "name": "Retrieval Framework",
      "one_line_profile": "Tool for converting scientific PDFs into plain text for RAG applications",
      "detailed_description": "A specialized tool developed to convert complex scientific PDF documents into clean plain text, facilitating the creation of RAG systems or agents for academic knowledge.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "pdf_parsing",
        "data_preprocessing",
        "scientific_literature"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/tensorsense/Retrieval-Framework",
      "help_website": [],
      "license": null,
      "tags": [
        "pdf-to-text",
        "scientific-papers",
        "rag-preprocessing"
      ],
      "id": 648
    },
    {
      "name": "CityGPT",
      "one_line_profile": "LLM-based framework for urban spatial cognition and analysis",
      "detailed_description": "A research framework that empowers Large Language Models with urban spatial cognition capabilities, enabling advanced analysis and reasoning for urban science and planning tasks.",
      "domains": [
        "G1",
        "Urban Science"
      ],
      "subtask_category": [
        "scientific_modeling",
        "spatial_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/tsinghua-fib-lab/CityGPT",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "urban-computing",
        "llm-agent",
        "spatial-cognition"
      ],
      "id": 649
    },
    {
      "name": "hlb-gpt",
      "one_line_profile": "High-performance minimalistic toolbench for GPT model research",
      "detailed_description": "A highly optimized, minimalistic library designed for AI researchers to train and experiment with GPT models efficiently. It focuses on speed and hackability for scientific experimentation in deep learning.",
      "domains": [
        "G1",
        "AI Research"
      ],
      "subtask_category": [
        "model_training",
        "scientific_modeling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/tysam-code/hlb-gpt",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "gpt",
        "deep-learning",
        "research-tool"
      ],
      "id": 650
    },
    {
      "name": "PodGPT",
      "one_line_profile": "Audio-augmented large language model for research applications",
      "detailed_description": "A multimodal large language model designed to integrate audio data, specifically tailored for research and educational applications, enabling analysis of audio-based scientific or educational content.",
      "domains": [
        "G1",
        "Multimodal Research"
      ],
      "subtask_category": [
        "scientific_modeling",
        "multimodal_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/vkola-lab/PodGPT",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "llm",
        "audio-processing",
        "multimodal"
      ],
      "id": 651
    },
    {
      "name": "MMGraphRAG",
      "one_line_profile": "Multi-modal knowledge graph-based RAG framework",
      "detailed_description": "A framework designed to enhance complex reasoning tasks in multi-modal document question-answering. It integrates text and image data into a structured knowledge graph, facilitating evidence extraction from scientific documents.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "knowledge_extraction",
        "document_analysis"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/wanxueyao/MMGraphRAG",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "knowledge-graph",
        "multi-modal",
        "document-qa"
      ],
      "id": 652
    },
    {
      "name": "ragflow_ocr",
      "one_line_profile": "OCR module for RAG document processing",
      "detailed_description": "A specialized OCR component derived from RAGFlow, designed to extract text from documents (PDFs, images) to support Retrieval-Augmented Generation pipelines in scientific literature processing.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "data_processing",
        "ocr"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/xuxianren/ragflow_ocr",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "ocr",
        "rag",
        "document-processing"
      ],
      "id": 653
    },
    {
      "name": "gpt_pdf_md",
      "one_line_profile": "PDF to Markdown converter with GPT-4V support",
      "detailed_description": "A tool for converting PDF documents into Markdown format, utilizing GPT-4V to extract and describe images. Essential for preprocessing scientific literature for downstream RAG or analysis tasks.",
      "domains": [
        "G1",
        "G1-04"
      ],
      "subtask_category": [
        "data_processing",
        "format_conversion"
      ],
      "application_level": "library",
      "primary_language": "Scala",
      "repo_url": "https://github.com/yachty66/gpt_pdf_md",
      "help_website": [],
      "license": null,
      "tags": [
        "pdf-parsing",
        "markdown",
        "gpt-4v"
      ],
      "id": 654
    },
    {
      "name": "SIFT",
      "one_line_profile": "Method for grounding LLM reasoning in contexts to reduce hallucinations",
      "detailed_description": "A framework/methodology designed to ground Large Language Model (LLM) reasoning within provided contexts using 'stickers', aiming to improve factual accuracy and reduce hallucinations in text generation tasks.",
      "domains": [
        "G1-04"
      ],
      "subtask_category": [
        "reasoning_grounding",
        "hallucination_reduction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/zhijie-group/SIFT",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "grounding",
        "reasoning",
        "hallucination"
      ],
      "id": 655
    },
    {
      "name": "FactCHD",
      "one_line_profile": "Benchmark and tool for fact-conflicting hallucination detection",
      "detailed_description": "A benchmark dataset and evaluation toolkit focused on detecting fact-conflicting hallucinations in LLMs, serving as a standard for assessing model faithfulness in knowledge-intensive tasks.",
      "domains": [
        "G1-04"
      ],
      "subtask_category": [
        "hallucination_detection",
        "benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/zjunlp/FactCHD",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "benchmark",
        "hallucination",
        "fact-checking"
      ],
      "id": 656
    },
    {
      "name": "Ragflow-Plus",
      "one_line_profile": "Enhanced RAG engine focusing on document understanding",
      "detailed_description": "A secondary development version of Ragflow, providing a streamlined Retrieval-Augmented Generation (RAG) engine optimized for deep document understanding, parsing, and knowledge retrieval from scientific or technical literature.",
      "domains": [
        "G1-04"
      ],
      "subtask_category": [
        "rag_framework",
        "document_parsing",
        "knowledge_retrieval"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/zstar1003/ragflow-plus",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "rag",
        "document-parsing",
        "knowledge-base",
        "nlp"
      ],
      "id": 657
    },
    {
      "name": "ARES OS",
      "one_line_profile": "Research software for creating closed-loop autonomous experimentation systems",
      "detailed_description": "ARES OS is a research software platform designed to streamline the creation and management of systems for closed-loop, autonomous experimentation in scientific laboratories (Self-Driving Labs).",
      "domains": [
        "Lab Automation",
        "Autonomous Experimentation"
      ],
      "subtask_category": [
        "autonomous_experimentation",
        "lab_automation"
      ],
      "application_level": "platform",
      "primary_language": "C#",
      "repo_url": "https://github.com/AFRL-ARES/ARES_OS",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "autonomous-experimentation",
        "robotics",
        "lab-automation"
      ],
      "id": 658
    },
    {
      "name": "robosuite",
      "one_line_profile": "Modular simulation framework and benchmark for robot learning",
      "detailed_description": "A modular simulation framework and benchmark suite for robot learning, powered by the MuJoCo physics engine. It provides a suite of benchmark environments for robotic manipulation tasks, supporting research in reproducibility and standardization.",
      "domains": [
        "Robotics",
        "Simulation"
      ],
      "subtask_category": [
        "simulation",
        "robot_learning",
        "benchmark"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/ARISE-Initiative/robosuite",
      "help_website": [
        "https://robosuite.ai"
      ],
      "license": "NOASSERTION",
      "tags": [
        "robotics",
        "simulation",
        "mujoco",
        "benchmark"
      ],
      "id": 659
    },
    {
      "name": "SenTrEv",
      "one_line_profile": "Evaluation tool for Sentence Transformers retrieval on PDFs",
      "detailed_description": "A simple and customizable evaluation tool designed to assess the text retrieval performance of Sentence Transformers embedders specifically on PDF documents, aiding in the selection of optimal embedding models for RAG systems.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "retrieval_evaluation",
        "embedding_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/AstraBert/SenTrEv",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "retrieval",
        "evaluation",
        "sentence-transformers",
        "pdf"
      ],
      "id": 660
    },
    {
      "name": "ko-lm-evaluation-harness",
      "one_line_profile": "Evaluation harness for Korean Large Language Models",
      "detailed_description": "A fork of the EleutherAI lm-evaluation-harness specialized for evaluating Korean Large Language Models (LLMs) on various Korean datasets and benchmarks.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmark"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Beomi/ko-lm-evaluation-harness",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "evaluation",
        "llm",
        "korean",
        "benchmark"
      ],
      "id": 661
    },
    {
      "name": "RAGEval",
      "one_line_profile": "Automated evaluation system for RAG pipelines",
      "detailed_description": "A one-stop automated evaluation solution for Retrieval-Augmented Generation (RAG) systems, designed to assess the performance and quality of RAG pipelines.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "rag_evaluation",
        "system_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/BytePioneer-AI/RAGEval",
      "help_website": [],
      "license": null,
      "tags": [
        "rag",
        "evaluation",
        "benchmark"
      ],
      "id": 662
    },
    {
      "name": "DeFactoNLP",
      "one_line_profile": "Automated fact-checking system using NER and attention models",
      "detailed_description": "An automated system designed for fact-checking claims by leveraging Named Entity Recognition (NER), TF-IDF vector comparison, and Decomposable Attention models to verify information against knowledge bases.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "fact_checking",
        "claim_verification"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/DeFacto/DeFactoNLP",
      "help_website": [],
      "license": null,
      "tags": [
        "fact-checking",
        "nlp",
        "automated-verification"
      ],
      "id": 663
    },
    {
      "name": "XRAG",
      "one_line_profile": "Benchmark framework for RAG component modules",
      "detailed_description": "A benchmarking framework designed to evaluate and examine the foundational component modules within Advanced Retrieval-Augmented Generation (RAG) systems, aiding in the optimization of retrieval and generation strategies.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "rag_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/DocAILab/XRAG",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "benchmark",
        "evaluation"
      ],
      "id": 664
    },
    {
      "name": "evox",
      "one_line_profile": "Distributed GPU-accelerated framework for evolutionary computation",
      "detailed_description": "A comprehensive library and framework for evolutionary algorithms and benchmark problems, leveraging JAX for distributed GPU acceleration to solve complex optimization tasks in scientific modeling.",
      "domains": [
        "Scientific Computing",
        "Optimization"
      ],
      "subtask_category": [
        "evolutionary_computation",
        "optimization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/EMI-Group/evox",
      "help_website": [
        "https://evox.readthedocs.io/"
      ],
      "license": "GPL-3.0",
      "tags": [
        "evolutionary-algorithms",
        "gpu-acceleration",
        "optimization",
        "jax"
      ],
      "id": 665
    },
    {
      "name": "lm-evaluation-harness",
      "one_line_profile": "Standard framework for few-shot evaluation of language models",
      "detailed_description": "A widely used framework for evaluating autoregressive language models across a large number of tasks, providing a standardized interface for few-shot performance measurement and comparison.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/EleutherAI/lm-evaluation-harness",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "evaluation",
        "few-shot",
        "nlp"
      ],
      "id": 666
    },
    {
      "name": "JamAIBase",
      "one_line_profile": "Collaborative platform for AI pipeline creation and evaluation",
      "detailed_description": "A 'spreadsheet for AI' platform that allows users to chain cells into pipelines, experiment with prompts and models, and evaluate LLM responses in real-time, facilitating collaborative development and testing of AI applications.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "pipeline_orchestration",
        "response_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/EmbeddedLLM/JamAIBase",
      "help_website": [
        "https://jamaibase.com/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm-ops",
        "evaluation",
        "collaboration",
        "no-code"
      ],
      "id": 667
    },
    {
      "name": "LLMZoo",
      "one_line_profile": "Data, models, and evaluation benchmark for LLMs",
      "detailed_description": "A project providing a collection of instruction-tuned data, efficient models, and evaluation benchmarks to facilitate research and development in large language models.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "model_evaluation",
        "dataset_management"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/FreedomIntelligence/LLMZoo",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "benchmark",
        "instruction-tuning"
      ],
      "id": 668
    },
    {
      "name": "DISC-MedLLM",
      "one_line_profile": "Medical LLM solution for conversational healthcare",
      "detailed_description": "A comprehensive solution leveraging Large Language Models to provide accurate and truthful medical responses, serving as a specialized tool for medical text generation and interaction.",
      "domains": [
        "G1",
        "Medical"
      ],
      "subtask_category": [
        "medical_inference",
        "domain_specific_llm"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/FudanDISC/DISC-MedLLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "medical-ai",
        "llm",
        "healthcare"
      ],
      "id": 669
    },
    {
      "name": "FacTool",
      "one_line_profile": "Factuality detection framework for generative AI",
      "detailed_description": "A tool designed to detect factuality errors in texts generated by large language models, assessing claims across various domains including knowledge-based QA, code generation, and math reasoning.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "factuality_detection",
        "hallucination_detection"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/GAIR-NLP/factool",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "fact-checking",
        "generative-ai",
        "hallucination"
      ],
      "id": 670
    },
    {
      "name": "FEAT",
      "one_line_profile": "Factcheck Explorer Analysis Tool",
      "detailed_description": "A tool designed to facilitate the exploration, analysis, and visualization of fact-checking data, helping researchers and fact-checkers gain insights from large datasets of verified claims.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "data_visualization",
        "fact_checking_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/GONZOsint/FEAT",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fact-checking",
        "visualization",
        "osint"
      ],
      "id": 671
    },
    {
      "name": "Giskard",
      "one_line_profile": "Evaluation and testing library for LLM agents",
      "detailed_description": "An open-source testing framework dedicated to ensuring the quality, security, and reliability of AI models and LLM agents through automated tests for hallucinations, bias, and performance issues.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "model_testing",
        "quality_assurance"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Giskard-AI/giskard-oss",
      "help_website": [
        "https://docs.giskard.ai/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "testing",
        "llm",
        "quality-control",
        "security"
      ],
      "id": 672
    },
    {
      "name": "GraphRAG-Benchmark",
      "one_line_profile": "Benchmark and dataset for evaluating GraphRAG models",
      "detailed_description": "A comprehensive benchmark suite designed to evaluate Retrieval-Augmented Generation (RAG) models that utilize knowledge graphs, providing metrics and datasets for performance assessment.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "rag_evaluation",
        "graph_rag"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/GraphRAG-Bench/GraphRAG-Benchmark",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "graph-rag",
        "benchmark",
        "evaluation"
      ],
      "id": 673
    },
    {
      "name": "Helicone",
      "one_line_profile": "Open source LLM observability and evaluation platform",
      "detailed_description": "A platform for monitoring, logging, and evaluating Large Language Model interactions, providing tools for tracking costs, latency, and quality metrics to improve AI applications.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "observability",
        "model_monitoring"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/Helicone/helicone",
      "help_website": [
        "https://docs.helicone.ai/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "observability",
        "llm-ops",
        "monitoring"
      ],
      "id": 674
    },
    {
      "name": "FactSumm",
      "one_line_profile": "Factual consistency scorer for abstractive summarization",
      "detailed_description": "A tool designed to evaluate the factual consistency of abstractive summaries generated by NLP models, helping to detect hallucinations and ensure summary accuracy.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "consistency_scoring",
        "summarization_eval"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/Huffon/factsumm",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "nlp",
        "summarization",
        "factuality"
      ],
      "id": 675
    },
    {
      "name": "ARES",
      "one_line_profile": "AI Robustness Evaluation System",
      "detailed_description": "A system developed by IBM for evaluating the robustness of AI models, likely focusing on adversarial attacks and defense mechanisms (based on name and context, though description is brief).",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "robustness_evaluation",
        "adversarial_testing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/IBM/ares",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "robustness",
        "ai-safety",
        "evaluation"
      ],
      "id": 676
    },
    {
      "name": "Prophecies",
      "one_line_profile": "Self-hosted data validation platform for fact checking",
      "detailed_description": "A platform developed by ICIJ to assist in labor-intensive fact-checking processes, providing tools for data validation and verification in investigative journalism contexts.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "fact_checking",
        "data_validation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/ICIJ/prophecies",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "fact-checking",
        "journalism",
        "verification"
      ],
      "id": 677
    },
    {
      "name": "Factorio Learning Environment",
      "one_line_profile": "Environment for evaluating LLMs in Factorio",
      "detailed_description": "A non-saturating, open-ended environment designed to evaluate the planning and problem-solving capabilities of Large Language Models within the game Factorio.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "agent_evaluation",
        "reinforcement_learning_env"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/JackHopkins/factorio-learning-environment",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "evaluation",
        "llm-agent",
        "environment"
      ],
      "id": 678
    },
    {
      "name": "OpenFactVerification (Loki)",
      "one_line_profile": "Automated fact verification solution",
      "detailed_description": "Loki is an open-source solution designed to automate the process of verifying factuality in text, supporting fact-checking workflows.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "fact_verification",
        "fact_checking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/Libr-AI/OpenFactVerification",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fact-checking",
        "nlp",
        "verification"
      ],
      "id": 679
    },
    {
      "name": "OmniSafe",
      "one_line_profile": "Framework for Safe Reinforcement Learning research",
      "detailed_description": "An infrastructural framework for accelerating research in Safe Reinforcement Learning (SafeRL), providing implementations of safety-constrained algorithms.",
      "domains": [
        "G1",
        "Scientific Modeling"
      ],
      "subtask_category": [
        "safe_rl",
        "reinforcement_learning",
        "safety_alignment"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/PKU-Alignment/omnisafe",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "safe-rl",
        "reinforcement-learning",
        "safety"
      ],
      "id": 680
    },
    {
      "name": "OmniEval",
      "one_line_profile": "Omnidirectional and automatic RAG evaluation benchmark",
      "detailed_description": "A benchmark suite for evaluating Retrieval-Augmented Generation (RAG) systems, specifically tailored for the financial domain but applicable to general RAG evaluation.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "rag_evaluation",
        "benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/RUC-NLPIR/OmniEval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "benchmark",
        "evaluation"
      ],
      "id": 681
    },
    {
      "name": "LLMBox",
      "one_line_profile": "Comprehensive library for LLM implementation and evaluation",
      "detailed_description": "A unified library for training and evaluating Large Language Models, providing pipelines for model assessment and implementation.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "llm_evaluation",
        "model_training",
        "pipeline"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/RUCAIBox/LLMBox",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "evaluation",
        "training"
      ],
      "id": 682
    },
    {
      "name": "RagView",
      "one_line_profile": "Unified evaluation platform for RAG benchmarking",
      "detailed_description": "A platform designed to benchmark different Retrieval-Augmented Generation (RAG) methods on custom datasets, facilitating comparative evaluation.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "rag_evaluation",
        "benchmarking"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/RagView/RagView",
      "help_website": [],
      "license": null,
      "tags": [
        "rag",
        "benchmark",
        "evaluation"
      ],
      "id": 683
    },
    {
      "name": "RAG-evaluation-harnesses",
      "one_line_profile": "Evaluation suite for Retrieval-Augmented Generation",
      "detailed_description": "A harness and evaluation suite designed to assess the performance of RAG systems.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "rag_evaluation",
        "benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/RulinShao/RAG-evaluation-harnesses",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "evaluation",
        "harness"
      ],
      "id": 684
    },
    {
      "name": "FactGraph",
      "one_line_profile": "Graph-based factuality evaluation for summarization",
      "detailed_description": "Implements the FactGraph metric which uses semantic graph representations to evaluate the factuality of abstractive summarization models, improving correlation with human judgment compared to n-gram metrics.",
      "domains": [
        "Sci Knowledge",
        "NLP"
      ],
      "subtask_category": [
        "factuality_evaluation",
        "summarization_metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/amazon-science/fact-graph",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "fact-checking",
        "summarization",
        "graph-representation",
        "nlp-evaluation"
      ],
      "id": 685
    },
    {
      "name": "fact-checking-rocks",
      "one_line_profile": "Baseline models for automated fact-checking",
      "detailed_description": "Provides a baseline implementation for fact-checking tasks, combining dense retrieval (using FAISS) and textual entailment models to verify claims against evidence.",
      "domains": [
        "Sci Knowledge",
        "NLP"
      ],
      "subtask_category": [
        "fact_checking",
        "claim_verification"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/anakin87/fact-checking-rocks",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "fact-checking",
        "retrieval",
        "textual-entailment",
        "baseline"
      ],
      "id": 686
    },
    {
      "name": "LTI_Neural_Navigator",
      "one_line_profile": "RAG-based framework for enhancing LLM factuality",
      "detailed_description": "A framework designed to counter hallucinations in Large Language Models by integrating Retrieval-Augmented Generation (RAG) with specific focus on domain-specific queries in private knowledge bases.",
      "domains": [
        "Sci Knowledge",
        "NLP"
      ],
      "subtask_category": [
        "hallucination_reduction",
        "rag_optimization"
      ],
      "application_level": "solver",
      "primary_language": "HTML",
      "repo_url": "https://github.com/anlp-team/LTI_Neural_Navigator",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "hallucination",
        "factuality",
        "llm"
      ],
      "id": 687
    },
    {
      "name": "factuality-eval",
      "one_line_profile": "Evaluation harness for LLM factuality",
      "detailed_description": "A library and set of notebooks for evaluating the factual accuracy of Large Language Models, providing tools to measure hallucination rates and response quality.",
      "domains": [
        "Sci Knowledge",
        "NLP"
      ],
      "subtask_category": [
        "factuality_evaluation",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/anyscale/factuality-eval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "evaluation",
        "factuality",
        "llm",
        "hallucination"
      ],
      "id": 688
    },
    {
      "name": "ChatGPT-RetrievalQA-CIKM2023",
      "one_line_profile": "Dataset for evaluating Retrieval-QA models on ChatGPT responses",
      "detailed_description": "A dataset resource designed for training and evaluating Question Answering Retrieval models, featuring ChatGPT responses and human verifications to assess retrieval performance and generation quality.",
      "domains": [
        "Sci Knowledge",
        "NLP"
      ],
      "subtask_category": [
        "qa_evaluation",
        "retrieval_benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/arian-askari/ChatGPT-RetrievalQA-CIKM2023",
      "help_website": [],
      "license": null,
      "tags": [
        "dataset",
        "qa",
        "chatgpt",
        "retrieval-evaluation"
      ],
      "id": 689
    },
    {
      "name": "FRANK",
      "one_line_profile": "Benchmark for evaluating factuality in document summarization",
      "detailed_description": "The FRANK benchmark offers a diverse set of model generated summaries with human annotations of factual errors, enabling robust evaluation of factuality metrics in summarization tasks.",
      "domains": [
        "Sci Knowledge",
        "NLP"
      ],
      "subtask_category": [
        "factuality_benchmarking",
        "summarization_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/artidoro/frank",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "benchmark",
        "factuality",
        "summarization",
        "nlp"
      ],
      "id": 690
    },
    {
      "name": "COVID-Fact",
      "one_line_profile": "Dataset and tools for COVID-19 claim verification",
      "detailed_description": "Contains the COVID-Fact dataset and associated code for automated fact extraction and verification of real-world claims related to the COVID-19 pandemic.",
      "domains": [
        "Sci Knowledge",
        "Public Health"
      ],
      "subtask_category": [
        "fact_verification",
        "claim_extraction"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/asaakyan/covidfact",
      "help_website": [],
      "license": null,
      "tags": [
        "covid-19",
        "fact-checking",
        "dataset",
        "misinformation"
      ],
      "id": 691
    },
    {
      "name": "ArtiFact",
      "one_line_profile": "Large-scale dataset for synthetic image detection",
      "detailed_description": "The ArtiFact dataset provides a comprehensive collection of real and AI-generated images to support the development and evaluation of robust synthetic image detection models.",
      "domains": [
        "Computer Vision",
        "Forensics"
      ],
      "subtask_category": [
        "synthetic_detection",
        "fake_detection"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/awsaf49/artifact",
      "help_website": [],
      "license": null,
      "tags": [
        "synthetic-images",
        "deepfake-detection",
        "dataset",
        "forensics"
      ],
      "id": 692
    },
    {
      "name": "BEIR",
      "one_line_profile": "Heterogeneous benchmark for zero-shot information retrieval",
      "detailed_description": "A comprehensive framework for evaluating information retrieval models across diverse datasets and domains, serving as a standard for assessing retrieval capabilities in RAG and search systems.",
      "domains": [
        "Sci Knowledge",
        "Information Retrieval"
      ],
      "subtask_category": [
        "retrieval_benchmarking",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/beir-cellar/beir",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "information-retrieval",
        "benchmark",
        "rag",
        "zero-shot"
      ],
      "id": 693
    },
    {
      "name": "TypeTruth",
      "one_line_profile": "Tool for detecting AI-generated text",
      "detailed_description": "A library designed to distinguish between human-written and AI-generated text, aiding in content validation and fact-checking workflows to prevent the spread of AI hallucinations or misinformation.",
      "domains": [
        "Sci Knowledge",
        "NLP"
      ],
      "subtask_category": [
        "ai_detection",
        "content_validation"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/bhaskatripathi/TypeTruth",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ai-detection",
        "fact-checking",
        "nlp",
        "validation"
      ],
      "id": 694
    },
    {
      "name": "BigCode Evaluation Harness",
      "one_line_profile": "Evaluation framework for code generation models",
      "detailed_description": "A specialized harness for evaluating autoregressive code generation models on various coding benchmarks, ensuring functional correctness and alignment of code LLMs.",
      "domains": [
        "Computer Science",
        "AI Evaluation"
      ],
      "subtask_category": [
        "code_evaluation",
        "model_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/bigcode-project/bigcode-evaluation-harness",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "code-generation",
        "evaluation",
        "llm",
        "benchmark"
      ],
      "id": 695
    },
    {
      "name": "Yet Another Applied LLM Benchmark",
      "one_line_profile": "Applied benchmark for practical LLM problem solving",
      "detailed_description": "A benchmark suite designed to evaluate Large Language Models on complex, applied questions and tasks, moving beyond simple factoid QA to test reasoning and problem-solving capabilities.",
      "domains": [
        "Sci Knowledge",
        "AI Evaluation"
      ],
      "subtask_category": [
        "model_benchmarking",
        "reasoning_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/carlini/yet-another-applied-llm-benchmark",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "benchmark",
        "llm",
        "reasoning",
        "evaluation"
      ],
      "id": 696
    },
    {
      "name": "ExpertQA",
      "one_line_profile": "Expert-curated QA dataset with attributed answers",
      "detailed_description": "A high-quality dataset featuring questions curated by experts across various fields, with answers that include citations and attributions, used for evaluating the expertise and factuality of QA systems.",
      "domains": [
        "Sci Knowledge",
        "NLP"
      ],
      "subtask_category": [
        "qa_evaluation",
        "attribution_analysis"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/chaitanyamalaviya/ExpertQA",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "dataset",
        "qa",
        "expert-verification",
        "attribution"
      ],
      "id": 697
    },
    {
      "name": "mir_ref",
      "one_line_profile": "Evaluation framework for Music Information Retrieval",
      "detailed_description": "A Python-based framework for evaluating representation learning models in Music Information Retrieval (MIR) tasks, facilitating comparative analysis of audio feature extraction methods.",
      "domains": [
        "Signal Processing",
        "Music Information Retrieval"
      ],
      "subtask_category": [
        "representation_evaluation",
        "audio_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/chrispla/mir_ref",
      "help_website": [],
      "license": null,
      "tags": [
        "mir",
        "evaluation",
        "audio",
        "representation-learning"
      ],
      "id": 698
    },
    {
      "name": "polygraphLLM",
      "one_line_profile": "Hallucination detection and factuality improvement tool",
      "detailed_description": "A library providing methods to detect hallucinations in LLM outputs and techniques to improve their factual consistency, essential for reliable deployment of generative AI in scientific contexts.",
      "domains": [
        "Sci Knowledge",
        "NLP"
      ],
      "subtask_category": [
        "hallucination_detection",
        "factuality_improvement"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/cisco-open/polygraphLLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "hallucination",
        "llm",
        "factuality",
        "detection"
      ],
      "id": 699
    },
    {
      "name": "CLEF2018 Fact Checking",
      "one_line_profile": "Evaluation resources for CLEF2018 Fact Checking Lab",
      "detailed_description": "Provides the official scorer, format checkers, and baseline systems for the CLEF2018 Fact Checking shared task, supporting research in automated claim verification.",
      "domains": [
        "Sci Knowledge",
        "NLP"
      ],
      "subtask_category": [
        "fact_checking_evaluation",
        "shared_task_scoring"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/clef2018-factchecking/clef2018-factchecking",
      "help_website": [],
      "license": null,
      "tags": [
        "clef",
        "fact-checking",
        "evaluation",
        "scorer"
      ],
      "id": 700
    },
    {
      "name": "BenchBase",
      "one_line_profile": "Multi-DBMS SQL benchmarking framework",
      "detailed_description": "A scalable framework for benchmarking various database management systems using standard workloads (TPC-C, TPC-E, etc.), supporting database research and performance analysis.",
      "domains": [
        "Computer Science",
        "Database Systems"
      ],
      "subtask_category": [
        "dbms_benchmarking",
        "performance_analysis"
      ],
      "application_level": "library",
      "primary_language": "Java",
      "repo_url": "https://github.com/cmu-db/benchbase",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "benchmark",
        "database",
        "sql",
        "performance"
      ],
      "id": 701
    },
    {
      "name": "Cofacts Open Data",
      "one_line_profile": "Open dataset from the Cofacts collaborative fact-checking system",
      "detailed_description": "Provides access to the crowdsourced database of fact-checked messages and rumors from the Cofacts platform, serving as a resource for training and evaluating misinformation detection models.",
      "domains": [
        "Sci Knowledge",
        "Social Science"
      ],
      "subtask_category": [
        "misinformation_analysis",
        "fact_checking_data"
      ],
      "application_level": "dataset",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/cofacts/opendata",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fact-checking",
        "dataset",
        "misinformation",
        "crowdsourcing"
      ],
      "id": 702
    },
    {
      "name": "LLM Multiagent Debate",
      "one_line_profile": "Method for improving LLM factuality and reasoning via multi-agent debate",
      "detailed_description": "Implementation of the ICML 2024 paper 'Improving Factuality and Reasoning in Language Models through Multiagent Debate'. It provides a framework where multiple LLM agents debate to reach a consensus, thereby reducing hallucinations and improving factual accuracy in responses.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "fact_checking",
        "reasoning_enhancement",
        "hallucination_reduction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/composable-models/llm_multiagent_debate",
      "help_website": [],
      "license": null,
      "tags": [
        "multi-agent",
        "factuality",
        "reasoning",
        "icml-2024"
      ],
      "id": 703
    },
    {
      "name": "DeepEval",
      "one_line_profile": "Comprehensive evaluation framework for LLMs",
      "detailed_description": "DeepEval is an open-source evaluation framework for Large Language Models. It offers a suite of metrics to measure hallucination, faithfulness, answer relevancy, and other performance indicators, facilitating the unit testing of LLM applications.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "evaluation",
        "unit_testing",
        "hallucination_detection"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/confident-ai/deepeval",
      "help_website": [
        "https://docs.confident-ai.com"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm-evaluation",
        "metrics",
        "testing",
        "rag"
      ],
      "id": 704
    },
    {
      "name": "pytrec_eval",
      "one_line_profile": "Python interface for the TREC information retrieval evaluation tool",
      "detailed_description": "pytrec_eval provides a Python interface to the standard trec_eval utility, allowing for the rigorous evaluation of information retrieval systems using standard metrics like MAP, NDCG, and Precision.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "ir_evaluation",
        "metrics_calculation"
      ],
      "application_level": "library",
      "primary_language": "C++",
      "repo_url": "https://github.com/cvangysel/pytrec_eval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "information-retrieval",
        "evaluation",
        "trec",
        "metrics"
      ],
      "id": 705
    },
    {
      "name": "Bisheng",
      "one_line_profile": "Open LLM DevOps platform for enterprise AI applications",
      "detailed_description": "Bisheng is a platform for developing next-generation enterprise AI applications. It includes features for GenAI workflows, RAG, agent management, and evaluation, supporting the lifecycle of knowledge-driven AI systems.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "rag",
        "workflow_management",
        "evaluation"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/dataelement/bisheng",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llmops",
        "rag",
        "agent",
        "evaluation"
      ],
      "id": 706
    },
    {
      "name": "HaloScope",
      "one_line_profile": "Hallucination detection using unlabeled LLM generations",
      "detailed_description": "Source code for the NeurIPS'24 paper 'HaloScope'. It implements a method for detecting hallucinations in LLMs by harnessing unlabeled generations, providing a tool for evaluating model faithfulness.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "hallucination_detection",
        "model_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/deeplearning-wisc/haloscope",
      "help_website": [],
      "license": null,
      "tags": [
        "hallucination",
        "llm",
        "neurips-2024",
        "detection"
      ],
      "id": 707
    },
    {
      "name": "RAGbits",
      "one_line_profile": "Modular building blocks for RAG application development",
      "detailed_description": "RAGbits provides a set of components for building Retrieval-Augmented Generation (RAG) applications, facilitating the development of systems that require retrieval, parsing, and processing of knowledge bases.",
      "domains": [
        "G1"
      ],
      "subtask_category": [
        "rag",
        "retrieval",
        "pipeline_construction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/deepsense-ai/ragbits",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "genai",
        "retrieval",
        "framework"
      ],
      "id": 708
    },
    {
      "name": "Ollama Grid Search",
      "one_line_profile": "Desktop application for evaluating and comparing LLM models",
      "detailed_description": "A multi-platform desktop tool designed to evaluate and compare different Large Language Models (LLMs) via grid search, helping researchers and developers assess model performance and suitability.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking",
        "comparison"
      ],
      "application_level": "application",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/dezoito/ollama-grid-search",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "evaluation",
        "grid-search",
        "ollama"
      ],
      "id": 709
    },
    {
      "name": "OLAPH",
      "one_line_profile": "Improving factuality in biomedical long-form QA",
      "detailed_description": "OLAPH is a framework designed to improve the factuality of biomedical long-form question answering systems. It addresses the specific needs of scientific knowledge retrieval and verification in the biomedical domain.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "factuality_improvement",
        "biomedical_qa",
        "hallucination_reduction"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/dmis-lab/OLAPH",
      "help_website": [],
      "license": null,
      "tags": [
        "biomedical",
        "qa",
        "factuality",
        "nlp"
      ],
      "id": 710
    },
    {
      "name": "MultiVerS",
      "one_line_profile": "Model for scientific claim verification",
      "detailed_description": "Code and model checkpoints for MultiVerS, a system designed for verifying scientific claims against evidence. It supports the task of fact-checking within scientific literature.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "claim_verification",
        "fact_checking",
        "scientific_nlp"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/dwadden/multivers",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fact-checking",
        "scientific-claims",
        "nlp",
        "verification"
      ],
      "id": 711
    },
    {
      "name": "Text-to-Image Eval",
      "one_line_profile": "Evaluation metrics for text-to-image models",
      "detailed_description": "A toolkit to evaluate custom and HuggingFace text-to-image and zero-shot image classification models. It includes metrics like Zero-shot accuracy, Linear Probe, and Image retrieval accuracy, useful for assessing generative models in scientific visualization or data generation contexts.",
      "domains": [
        "G1-05"
      ],
      "subtask_category": [
        "model_evaluation",
        "image_generation_metrics"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/encord-team/text-to-image-eval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "evaluation",
        "text-to-image",
        "metrics",
        "clip"
      ],
      "id": 712
    },
    {
      "name": "ChatProtect",
      "one_line_profile": "Evaluation, detection, and mitigation of LLM self-contradictory hallucinations",
      "detailed_description": "Code for the paper 'Self-contradictory Hallucinations of Large Language Models'. It provides methods for evaluating, detecting, and mitigating hallucinations in LLM outputs, directly addressing factuality in AI systems.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "hallucination_detection",
        "hallucination_mitigation",
        "evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/eth-sri/ChatProtect",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "hallucination",
        "llm",
        "safety",
        "factuality"
      ],
      "id": 713
    },
    {
      "name": "DSPy Micro Agent",
      "one_line_profile": "Minimal agent runtime with evaluation harness",
      "detailed_description": "A minimal agent runtime built with DSPy modules, including an evaluation harness with OpenAI/Ollama support. It facilitates the development and testing of agentic workflows, including those for scientific reasoning.",
      "domains": [
        "G1-05"
      ],
      "subtask_category": [
        "agent_evaluation",
        "workflow_automation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/evalops/dspy-micro-agent",
      "help_website": [],
      "license": null,
      "tags": [
        "dspy",
        "agent",
        "evaluation",
        "llm"
      ],
      "id": 714
    },
    {
      "name": "EvalPlus",
      "one_line_profile": "Rigorous evaluation framework for LLM-synthesized code",
      "detailed_description": "EvalPlus provides a rigorous evaluation framework for code generation models. While focused on code, it is a critical evaluation tool for AI4S applications where code generation (e.g., for simulation or data analysis) is a key component.",
      "domains": [
        "G1-05"
      ],
      "subtask_category": [
        "code_evaluation",
        "model_benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/evalplus/evalplus",
      "help_website": [
        "https://evalplus.github.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "code-generation",
        "evaluation",
        "llm",
        "benchmarking"
      ],
      "id": 715
    },
    {
      "name": "Evidently",
      "one_line_profile": "Open-source ML and LLM observability and evaluation framework",
      "detailed_description": "Evidently is a framework for evaluating, testing, and monitoring ML models and LLMs. It supports tabular data and text, providing metrics for data drift, model performance, and LLM quality (e.g., hallucinations, bias), essential for reliable scientific AI.",
      "domains": [
        "G1-05"
      ],
      "subtask_category": [
        "model_monitoring",
        "evaluation",
        "observability",
        "data_drift"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/evidentlyai/evidently",
      "help_website": [
        "https://docs.evidentlyai.com"
      ],
      "license": "Apache-2.0",
      "tags": [
        "mlops",
        "evaluation",
        "monitoring",
        "llm"
      ],
      "id": 716
    },
    {
      "name": "MLGym",
      "one_line_profile": "Framework and benchmark for advancing AI research agents",
      "detailed_description": "MLGym is a framework and benchmark designed to advance research in AI agents. It provides environments and metrics for evaluating agent performance, supporting the development of autonomous systems for scientific tasks.",
      "domains": [
        "G1-05"
      ],
      "subtask_category": [
        "agent_benchmarking",
        "reinforcement_learning_eval"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/MLGym",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "agents",
        "benchmark",
        "reinforcement-learning"
      ],
      "id": 717
    },
    {
      "name": "TruthRL",
      "one_line_profile": "Incentivizing truthful LLMs via Reinforcement Learning",
      "detailed_description": "TruthRL implements methods for incentivizing truthfulness in Large Language Models using Reinforcement Learning. It addresses the critical scientific need for factual and reliable AI generation.",
      "domains": [
        "G1-05"
      ],
      "subtask_category": [
        "truthfulness_alignment",
        "reinforcement_learning",
        "factuality"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/TruthRL",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "truthfulness",
        "rlhf",
        "llm",
        "alignment"
      ],
      "id": 718
    },
    {
      "name": "LSS Eval",
      "one_line_profile": "Metric for evaluating faithfulness of LLM generated text",
      "detailed_description": "LSS Eval provides a new metric to evaluate the faithfulness of text generated by Large Language Models, supporting the verification of content generated in scientific or factual contexts.",
      "domains": [
        "G1-05"
      ],
      "subtask_category": [
        "faithfulness_evaluation",
        "metrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/facebookresearch/lss_eval",
      "help_website": [],
      "license": "CC0-1.0",
      "tags": [
        "evaluation",
        "faithfulness",
        "llm",
        "nlp"
      ],
      "id": 719
    },
    {
      "name": "CNN Image Retrieval (MatConvNet)",
      "one_line_profile": "Training and evaluating CNNs for Image Retrieval in MatConvNet",
      "detailed_description": "A framework for training and evaluating Convolutional Neural Networks for image retrieval tasks using MatConvNet. Useful for scientific image database retrieval and analysis.",
      "domains": [
        "G1"
      ],
      "subtask_category": [
        "image_retrieval",
        "model_training",
        "evaluation"
      ],
      "application_level": "library",
      "primary_language": "MATLAB",
      "repo_url": "https://github.com/filipradenovic/cnnimageretrieval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "image-retrieval",
        "cnn",
        "matconvnet",
        "computer-vision"
      ],
      "id": 720
    },
    {
      "name": "CNN Image Retrieval (PyTorch)",
      "one_line_profile": "Training and evaluating CNNs for Image Retrieval in PyTorch",
      "detailed_description": "A PyTorch implementation for training and evaluating CNNs for image retrieval. It supports the development of systems for retrieving visual data, applicable in scientific imaging contexts.",
      "domains": [
        "G1"
      ],
      "subtask_category": [
        "image_retrieval",
        "model_training",
        "evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/filipradenovic/cnnimageretrieval-pytorch",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "image-retrieval",
        "pytorch",
        "cnn",
        "computer-vision"
      ],
      "id": 721
    },
    {
      "name": "RAG Arena",
      "one_line_profile": "Open-source RAG evaluation through user feedback",
      "detailed_description": "RAG Arena is a tool for evaluating Retrieval-Augmented Generation systems based on user feedback. It helps in benchmarking and improving RAG pipelines used for knowledge retrieval.",
      "domains": [
        "G1-05"
      ],
      "subtask_category": [
        "rag_evaluation",
        "benchmarking",
        "user_feedback"
      ],
      "application_level": "application",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/firecrawl/rag-arena",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "evaluation",
        "arena",
        "feedback"
      ],
      "id": 722
    },
    {
      "name": "Flow",
      "one_line_profile": "Computational framework for reinforcement learning in traffic control",
      "detailed_description": "Flow is a computational framework for deep reinforcement learning and control experiments for traffic microsimulation. It enables scientific modeling and optimization of complex traffic systems.",
      "domains": [
        "G1"
      ],
      "subtask_category": [
        "simulation",
        "reinforcement_learning",
        "traffic_control"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/flow-project/flow",
      "help_website": [
        "https://flow-project.github.io"
      ],
      "license": "MIT",
      "tags": [
        "reinforcement-learning",
        "traffic-simulation",
        "control-theory",
        "microsimulation"
      ],
      "id": 723
    },
    {
      "name": "Long-form Factuality",
      "one_line_profile": "Benchmarking tool for long-form factuality in Large Language Models",
      "detailed_description": "A benchmarking suite from Google DeepMind for evaluating the factuality of long-form text generated by LLMs, using the SAFE (Search-Augmented Factuality Evaluator) method.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "fact_checking",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/google-deepmind/long-form-factuality",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "factuality",
        "llm",
        "hallucination",
        "benchmarking"
      ],
      "id": 724
    },
    {
      "name": "TRUE",
      "one_line_profile": "Framework for re-evaluating factual consistency in text generation",
      "detailed_description": "A comprehensive evaluation framework that consolidates multiple existing factual consistency metrics and datasets to benchmark their performance on detecting hallucinations.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "fact_checking",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/google-research/true",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "factual-consistency",
        "hallucination",
        "nlp",
        "evaluation"
      ],
      "id": 725
    },
    {
      "name": "ManiSkill",
      "one_line_profile": "GPU-parallelized robotics simulator and benchmark framework",
      "detailed_description": "A high-performance robotics simulation platform and benchmark for learning manipulation skills, supporting physics-based modeling and data generation for embodied AI.",
      "domains": [
        "Sci Knowledge"
      ],
      "subtask_category": [
        "simulation",
        "robotics_modeling"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/haosulab/ManiSkill",
      "help_website": [
        "https://maniskill.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "robotics",
        "simulation",
        "physics-engine",
        "benchmark"
      ],
      "id": 726
    },
    {
      "name": "Big ANN Benchmarks",
      "one_line_profile": "Framework for evaluating approximate nearest neighbor search algorithms",
      "detailed_description": "A standard benchmarking framework for evaluating the performance and accuracy of billion-scale approximate nearest neighbor (ANN) search algorithms, critical for vector retrieval in science.",
      "domains": [
        "G1"
      ],
      "subtask_category": [
        "algorithm_evaluation",
        "vector_search"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/harsha-simhadri/big-ann-benchmarks",
      "help_website": [
        "http://big-ann-benchmarks.com/"
      ],
      "license": "MIT",
      "tags": [
        "ann",
        "vector-search",
        "benchmarking",
        "information-retrieval"
      ],
      "id": 727
    },
    {
      "name": "FELM",
      "one_line_profile": "Benchmarking factuality evaluation of Large Language Models",
      "detailed_description": "A benchmark and toolkit for evaluating the factuality of LLMs across various domains, providing annotated datasets and evaluation scripts to detect hallucinations.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "fact_checking",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/hkust-nlp/felm",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "factuality",
        "llm",
        "benchmark",
        "hallucination"
      ],
      "id": 728
    },
    {
      "name": "LRAGE",
      "one_line_profile": "Evaluation framework for RAG pipelines in the legal domain",
      "detailed_description": "A specialized framework for evaluating Retrieval-Augmented Generation systems, providing metrics and datasets tailored for high-precision domains like law, applicable to scientific regulation analysis.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "rag_benchmarking",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/hoorangyee/LRAGE",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "evaluation",
        "legal-tech",
        "llm"
      ],
      "id": 729
    },
    {
      "name": "LightEval",
      "one_line_profile": "Comprehensive toolkit for evaluating LLMs across multiple backends",
      "detailed_description": "A lightweight yet powerful evaluation library from Hugging Face for assessing Large Language Models on various benchmarks, including reasoning and factuality tasks.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/huggingface/lighteval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "evaluation",
        "llm",
        "huggingface",
        "nlp"
      ],
      "id": 730
    },
    {
      "name": "ChainForge",
      "one_line_profile": "Visual programming environment for prompt engineering and evaluation",
      "detailed_description": "An open-source visual tool for testing, evaluating, and comparing LLM prompts and responses, facilitating systematic analysis of model behavior and factuality.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "prompt_engineering",
        "model_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/ianarawjo/ChainForge",
      "help_website": [
        "https://chainforge.ai/"
      ],
      "license": "MIT",
      "tags": [
        "prompt-engineering",
        "visualization",
        "evaluation",
        "llm"
      ],
      "id": 731
    },
    {
      "name": "TruthX",
      "one_line_profile": "Tool for alleviating hallucinations by editing LLMs in truthful space",
      "detailed_description": "An implementation of a method to enhance LLM truthfulness by identifying and editing the internal representations (truthfulness hyperplane) to reduce hallucinations.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "model_editing",
        "hallucination_mitigation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/ictnlp/TruthX",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "hallucination",
        "model-editing",
        "llm",
        "truthfulness"
      ],
      "id": 732
    },
    {
      "name": "ViDoRe Benchmark",
      "one_line_profile": "Benchmark for Vision Document Retrieval systems",
      "detailed_description": "Evaluation code and benchmark for the Vision Document Retrieval (ViDoRe) task, assessing the ability of models to retrieve visually rich documents (PDFs, charts).",
      "domains": [
        "G1"
      ],
      "subtask_category": [
        "document_retrieval",
        "multimodal_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/illuin-tech/vidore-benchmark",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "retrieval",
        "vision-language",
        "benchmark",
        "document-analysis"
      ],
      "id": 733
    },
    {
      "name": "DebateLLM",
      "one_line_profile": "Benchmarking multi-agent debate for truthfulness in Q&A",
      "detailed_description": "A framework for evaluating whether multi-agent debate protocols improve the truthfulness and accuracy of Large Language Models in Question Answering tasks.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "model_evaluation",
        "multi_agent_simulation"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/instadeepai/DebateLLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "multi-agent",
        "truthfulness",
        "debate",
        "llm"
      ],
      "id": 734
    },
    {
      "name": "Metriks",
      "one_line_profile": "Metrics library for evaluating information retrieval models",
      "detailed_description": "A Python package implementing commonly used metrics (NDCG, Precision, Recall, etc.) for evaluating the performance of information retrieval and recommendation systems.",
      "domains": [
        "G1"
      ],
      "subtask_category": [
        "metrics_calculation",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/intuit/metriks",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "information-retrieval",
        "metrics",
        "evaluation"
      ],
      "id": 735
    },
    {
      "name": "SAC3",
      "one_line_profile": "Semantic-aware cross-check consistency for hallucination detection",
      "detailed_description": "A tool for detecting hallucinations in black-box language models by checking semantic consistency across multiple generated samples (Self-Consistency Check).",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "hallucination_detection",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/intuit/sac3",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "hallucination",
        "consistency-check",
        "llm",
        "reliability"
      ],
      "id": 736
    },
    {
      "name": "RagRank",
      "one_line_profile": "Toolkit for evaluating RAG application performance",
      "detailed_description": "A lightweight library to evaluate Retrieval-Augmented Generation (RAG) pipelines, assessing metrics like factual accuracy, context relevance, and response tone.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "rag_benchmarking",
        "model_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/izam-mohammed/ragrank",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "evaluation",
        "metrics",
        "llm"
      ],
      "id": 737
    },
    {
      "name": "ARES (Robotics)",
      "one_line_profile": "Automatic Robot Evaluation System",
      "detailed_description": "A scalable framework for evaluating robotics research, providing tools for automated testing and performance metrics collection for robot control systems.",
      "domains": [
        "Sci Knowledge"
      ],
      "subtask_category": [
        "robotics_evaluation",
        "simulation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/jacobphillips99/ares",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "robotics",
        "evaluation",
        "automation",
        "research-tool"
      ],
      "id": 738
    },
    {
      "name": "GraphRAG-Bench",
      "one_line_profile": "Benchmark for evaluating Graph Retrieval-Augmented Generation (RAG) systems",
      "detailed_description": "A dedicated benchmarking framework designed to evaluate the performance of Graph Retrieval-Augmented Generation systems, specifically focusing on challenging domain-specific reasoning tasks.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "evaluation",
        "benchmarking",
        "rag_assessment"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/jeremycp3/GraphRAG-Bench",
      "help_website": [],
      "license": null,
      "tags": [
        "benchmark",
        "graph-rag",
        "evaluation",
        "retrieval"
      ],
      "id": 739
    },
    {
      "name": "llm-eval",
      "one_line_profile": "Evaluation platform for Large Language Models and RAG systems",
      "detailed_description": "A comprehensive evaluation platform for Large Language Models (LLMs) that supports multiple benchmarks, custom datasets, and performance testing, with specific support for RAG (Retrieval-Augmented Generation) evaluation.",
      "domains": [
        "G1-05"
      ],
      "subtask_category": [
        "evaluation",
        "benchmarking",
        "rag_assessment"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/justplus/llm-eval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm-evaluation",
        "rag",
        "benchmark",
        "testing"
      ],
      "id": 740
    },
    {
      "name": "cde",
      "one_line_profile": "Library for training and evaluating Contextual Document Embeddings",
      "detailed_description": "A codebase and toolkit for training and evaluating Contextual Document Embedding (CDE) models, which are used to improve information retrieval and document understanding tasks.",
      "domains": [
        "G1"
      ],
      "subtask_category": [
        "embedding_training",
        "retrieval_evaluation",
        "document_embedding"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/jxmorris12/cde",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "embeddings",
        "information-retrieval",
        "contextual-embeddings"
      ],
      "id": 741
    },
    {
      "name": "InstructIR",
      "one_line_profile": "Benchmark for evaluating instruction-following capabilities in IR models",
      "detailed_description": "A novel benchmark specifically designed to evaluate the instruction-following ability of information retrieval models, focusing on user-aligned instructions tailored to query instances.",
      "domains": [
        "G1"
      ],
      "subtask_category": [
        "ir_evaluation",
        "benchmarking",
        "instruction_following"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/kaistAI/InstructIR",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "information-retrieval",
        "benchmark",
        "instruction-following"
      ],
      "id": 742
    },
    {
      "name": "SLM-Lab",
      "one_line_profile": "Modular Deep Reinforcement Learning framework in PyTorch",
      "detailed_description": "A modular framework for Deep Reinforcement Learning (DRL) implemented in PyTorch, designed for research in reinforcement learning algorithms and experiments.",
      "domains": [
        "AI_General"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "modeling",
        "algorithm_research"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/kengz/SLM-Lab",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "reinforcement-learning",
        "pytorch",
        "deep-learning",
        "framework"
      ],
      "id": 743
    },
    {
      "name": "rome",
      "one_line_profile": "Implementation of Rank-One Model Editing for LLM factuality",
      "detailed_description": "The official implementation of Rank-One Model Editing (ROME), a method for locating and editing factual associations in Large Language Models (specifically GPT), used for studying and improving model factuality.",
      "domains": [
        "G1-05"
      ],
      "subtask_category": [
        "model_editing",
        "factuality_correction",
        "interpretability"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/kmeng01/rome",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "model-editing",
        "factuality",
        "llm",
        "interpretability"
      ],
      "id": 744
    },
    {
      "name": "autoarena",
      "one_line_profile": "Automated head-to-head evaluation system for LLMs and RAG pipelines",
      "detailed_description": "A tool designed to rank Large Language Models (LLMs), RAG systems, and prompts using automated head-to-head evaluation methodologies, facilitating comparative analysis of model performance.",
      "domains": [
        "G1-05"
      ],
      "subtask_category": [
        "model_ranking",
        "evaluation",
        "benchmarking"
      ],
      "application_level": "platform",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/kolenaIO/autoarena",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "evaluation",
        "llm-ranking",
        "rag",
        "automation"
      ],
      "id": 745
    },
    {
      "name": "vec4ir",
      "one_line_profile": "Library for generating and using word embeddings in information retrieval",
      "detailed_description": "A Python library that integrates word embeddings into information retrieval tasks, providing methods to train, evaluate, and utilize embeddings for document ranking and retrieval.",
      "domains": [
        "G1"
      ],
      "subtask_category": [
        "information_retrieval",
        "embedding_generation",
        "ranking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/lgalke/vec4ir",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "word-embeddings",
        "information-retrieval",
        "nlp"
      ],
      "id": 746
    },
    {
      "name": "jingwei",
      "one_line_profile": "Framework for evaluating image tag assignment and tag-based image retrieval",
      "detailed_description": "A framework designed for the evaluation of image tag assignment, tag refinement, and tag-based image retrieval systems, facilitating research in multimedia information retrieval.",
      "domains": [
        "G1"
      ],
      "subtask_category": [
        "image_retrieval",
        "tagging_evaluation",
        "multimedia_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/li-xirong/jingwei",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "image-retrieval",
        "tagging",
        "evaluation",
        "multimedia"
      ],
      "id": 747
    },
    {
      "name": "honest_llama",
      "one_line_profile": "Inference-Time Intervention (ITI) method to elicit truthful answers from LLMs",
      "detailed_description": "Implementation of Inference-Time Intervention (ITI), a technique to improve the truthfulness of Large Language Models by intervening in their internal activations during inference, mitigating hallucinations.",
      "domains": [
        "G1-05"
      ],
      "subtask_category": [
        "truthfulness_intervention",
        "hallucination_mitigation",
        "model_steering"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/likenneth/honest_llama",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "truthfulness",
        "hallucination",
        "llm",
        "intervention"
      ],
      "id": 748
    },
    {
      "name": "LLaVA-RLHF",
      "one_line_profile": "Framework for aligning Large Multimodal Models (LMMs) using Factually Augmented RLHF",
      "detailed_description": "A codebase and framework for aligning Large Multimodal Models (specifically LLaVA) using Factually Augmented Reinforcement Learning from Human Feedback (RLHF) to reduce hallucinations and improve factual accuracy.",
      "domains": [
        "G1-05"
      ],
      "subtask_category": [
        "alignment",
        "factuality_enhancement",
        "rlhf",
        "multimodal_learning"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/llava-rlhf/LLaVA-RLHF",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "rlhf",
        "llava",
        "alignment",
        "factuality"
      ],
      "id": 749
    },
    {
      "name": "RouteLLM",
      "one_line_profile": "Framework for serving and evaluating LLM routers to optimize cost and quality",
      "detailed_description": "A framework designed to serve and evaluate routers for Large Language Models, enabling researchers to develop and test strategies for routing queries between different models to balance cost and response quality.",
      "domains": [
        "G1-05"
      ],
      "subtask_category": [
        "model_routing",
        "evaluation",
        "inference_optimization"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/lm-sys/RouteLLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-routing",
        "evaluation",
        "cost-optimization",
        "serving"
      ],
      "id": 750
    },
    {
      "name": "Video-ChatGPT",
      "one_line_profile": "Video conversation model with quantitative evaluation benchmarking",
      "detailed_description": "A video conversation model capable of generating meaningful conversation about videos, combining LLMs with a pretrained visual encoder. It includes a rigorous quantitative evaluation benchmarking suite for video-based conversational models.",
      "domains": [
        "G1",
        "Computer Vision"
      ],
      "subtask_category": [
        "evaluation",
        "multimodal_generation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/mbzuai-oryx/Video-ChatGPT",
      "help_website": [],
      "license": "CC-BY-4.0",
      "tags": [
        "video-llm",
        "benchmarking",
        "multimodal"
      ],
      "id": 751
    },
    {
      "name": "micronaire",
      "one_line_profile": "RAG evaluation pipeline for Semantic Kernel",
      "detailed_description": "An evaluation pipeline designed for Retrieval-Augmented Generation (RAG) systems, specifically integrated with Semantic Kernel to assess the quality and accuracy of retrieved information and generated responses.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "rag_evaluation",
        "quality_control"
      ],
      "application_level": "workflow",
      "primary_language": "C#",
      "repo_url": "https://github.com/microsoft/micronaire",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "evaluation",
        "semantic-kernel"
      ],
      "id": 752
    },
    {
      "name": "PromptBench",
      "one_line_profile": "Unified evaluation framework for large language models",
      "detailed_description": "A comprehensive framework for evaluating Large Language Models (LLMs) across various tasks, including adversarial robustness, prompt engineering, and general performance metrics.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/promptbench",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "evaluation",
        "robustness"
      ],
      "id": 753
    },
    {
      "name": "RAG Experiment Accelerator",
      "one_line_profile": "Tool for conducting RAG experiments and evaluations",
      "detailed_description": "A versatile tool designed to expedite the process of conducting experiments and evaluations for Retrieval-Augmented Generation (RAG) patterns, facilitating scientific assessment of RAG performance.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "rag_evaluation",
        "experimentation"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/microsoft/rag-experiment-accelerator",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "rag",
        "experiment",
        "azure"
      ],
      "id": 754
    },
    {
      "name": "mir_eval",
      "one_line_profile": "Evaluation functions for music/audio information retrieval",
      "detailed_description": "A library providing standard evaluation metrics and functions for music and audio information retrieval (MIR) tasks, widely used for scientific benchmarking in audio signal processing.",
      "domains": [
        "Audio",
        "Signal Processing"
      ],
      "subtask_category": [
        "evaluation",
        "signal_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/mir-evaluation/mir_eval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mir",
        "audio",
        "evaluation"
      ],
      "id": 755
    },
    {
      "name": "RAGTune",
      "one_line_profile": "Tuning and evaluation of RAG pipelines",
      "detailed_description": "A tool for tuning and evaluating Retrieval-Augmented Generation (RAG) pipelines, aiming to optimize retrieval accuracy and generation quality through automated metrics.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "rag_optimization",
        "evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/misbahsy/RAGTune",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "tuning",
        "optimization"
      ],
      "id": 756
    },
    {
      "name": "EvalScope",
      "one_line_profile": "Framework for efficient large model evaluation and benchmarking",
      "detailed_description": "A streamlined and customizable framework for evaluating and benchmarking large models (LLM, VLM, AIGC), supporting various datasets and metrics for scientific performance assessment.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/modelscope/evalscope",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "evaluation",
        "benchmark"
      ],
      "id": 757
    },
    {
      "name": "MRAG-Bench",
      "one_line_profile": "Vision-centric evaluation for retrieval-augmented multimodal models",
      "detailed_description": "A benchmark suite for evaluating Retrieval-Augmented Multimodal Models, specifically focusing on vision-centric tasks and metrics to assess model capabilities in handling multimodal data.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "multimodal_evaluation",
        "benchmarking"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/mragbench/MRAG-Bench",
      "help_website": [],
      "license": null,
      "tags": [
        "multimodal",
        "rag",
        "benchmark"
      ],
      "id": 758
    },
    {
      "name": "DEFAME",
      "one_line_profile": "Fact-checking system for textual and visual inputs",
      "detailed_description": "A multimodal fact-checking system designed to verify claims based on both textual and visual evidence, providing a framework for automated truthfulness assessment.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "fact_checking",
        "multimodal_analysis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/multimodal-ai-lab/DEFAME",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "fact-checking",
        "multimodal",
        "verification"
      ],
      "id": 759
    },
    {
      "name": "KnowledgeEditor",
      "one_line_profile": "Library for editing factual knowledge in Language Models",
      "detailed_description": "A Python library implementing hyper-network based knowledge editing methods (like KE and MEND) to modify factual knowledge stored in large language models without re-training.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "model_editing",
        "fact_correction"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/nicola-decao/KnowledgeEditor",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "knowledge-editing",
        "llm",
        "factuality"
      ],
      "id": 760
    },
    {
      "name": "hallucination_probes",
      "one_line_profile": "Probes for real-time detection of hallucinated entities in LLMs",
      "detailed_description": "A toolkit for training and evaluating probes to detect hallucinations in long-form text generation from Large Language Models, specifically focusing on entity correctness.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "hallucination_detection",
        "model_probing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/obalcells/hallucination_probes",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "hallucination-detection",
        "llm",
        "probing"
      ],
      "id": 761
    },
    {
      "name": "VLMEvalKit",
      "one_line_profile": "Evaluation toolkit for Large Multi-modality Models",
      "detailed_description": "An open-source evaluation toolkit designed for Large Multi-modality Models (LMMs), supporting evaluation of over 200 models across 80+ benchmarks, facilitating comprehensive performance analysis.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "model_evaluation",
        "multimodal_analysis"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/open-compass/VLMEvalKit",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "evaluation",
        "vlm",
        "benchmark"
      ],
      "id": 762
    },
    {
      "name": "OpenCompass",
      "one_line_profile": "Comprehensive evaluation platform for Large Language Models",
      "detailed_description": "A one-stop platform for evaluating Large Language Models (LLMs) covering various capabilities including reasoning, language understanding, and coding, with support for distributed evaluation.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmark_harness"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/open-compass/opencompass",
      "help_website": [
        "https://opencompass.readthedocs.io/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm-evaluation",
        "benchmark",
        "nlp"
      ],
      "id": 763
    },
    {
      "name": "OpenAI Evals",
      "one_line_profile": "Framework for evaluating LLMs and an open-source registry of benchmarks",
      "detailed_description": "A framework for evaluating OpenAI models and other LLMs, providing a registry of benchmarks to test model capabilities, factuality, and safety.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmark_registry"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/openai/evals",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "evaluation",
        "llm",
        "benchmark"
      ],
      "id": 764
    },
    {
      "name": "OHR-Bench",
      "one_line_profile": "Benchmark for evaluating OCR impact on RAG systems",
      "detailed_description": "A benchmarking tool and dataset designed to evaluate how Optical Character Recognition (OCR) errors cascade and affect the performance of Retrieval-Augmented Generation (RAG) systems.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "rag_evaluation",
        "ocr_analysis"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/opendatalab/OHR-Bench",
      "help_website": [],
      "license": null,
      "tags": [
        "rag",
        "ocr",
        "evaluation"
      ],
      "id": 765
    },
    {
      "name": "AutoML Benchmark",
      "one_line_profile": "Benchmarking framework for AutoML systems",
      "detailed_description": "An open-source tool for benchmarking various Automated Machine Learning (AutoML) systems on a wide variety of datasets, facilitating comparative analysis of ML algorithms.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "automl_evaluation",
        "algorithm_benchmarking"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/openml/automlbenchmark",
      "help_website": [
        "https://openml.github.io/automlbenchmark/"
      ],
      "license": "MIT",
      "tags": [
        "automl",
        "benchmark",
        "machine-learning"
      ],
      "id": 766
    },
    {
      "name": "Oumi",
      "one_line_profile": "Platform for fine-tuning, evaluating, and deploying open source LLMs",
      "detailed_description": "A comprehensive library for building, training, and evaluating Large Language Models (LLMs) and Vision Language Models (VLMs), including tools for data preparation and model assessment.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "model_training",
        "model_evaluation"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/oumi-ai/oumi",
      "help_website": [
        "https://oumi.ai/docs"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm-finetuning",
        "evaluation",
        "inference"
      ],
      "id": 767
    },
    {
      "name": "Prometheus-Eval",
      "one_line_profile": "LLM evaluation tool using Prometheus and GPT-4",
      "detailed_description": "A library for evaluating Large Language Model responses using the Prometheus model or GPT-4, providing a mechanism for automated grading and feedback generation.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "model_evaluation",
        "automated_grading"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/prometheus-eval/prometheus-eval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "evaluation",
        "llm",
        "grading"
      ],
      "id": 768
    },
    {
      "name": "promptfoo",
      "one_line_profile": "CLI tool for evaluating LLM prompts, agents, and RAG pipelines",
      "detailed_description": "A tool for testing and evaluating LLM prompts, agents, and RAG systems. It supports red teaming, pentesting, and vulnerability scanning, allowing researchers to compare performance across different models (GPT, Claude, Llama) using declarative configurations.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "model_evaluation",
        "hallucination_detection",
        "security_scanning"
      ],
      "application_level": "solver",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/promptfoo/promptfoo",
      "help_website": [
        "https://www.promptfoo.dev"
      ],
      "license": "MIT",
      "tags": [
        "llm-evaluation",
        "red-teaming",
        "rag-evaluation"
      ],
      "id": 769
    },
    {
      "name": "raga-llm-hub",
      "one_line_profile": "Framework for LLM evaluation and security guardrails",
      "detailed_description": "A framework designed for evaluating Large Language Models (LLMs), implementing guardrails, and ensuring security. It aids in assessing the reliability and safety of LLM outputs.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "model_evaluation",
        "security_guardrails"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/raga-ai-hub/raga-llm-hub",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "llm-evaluation",
        "guardrails",
        "security"
      ],
      "id": 770
    },
    {
      "name": "continuous-eval",
      "one_line_profile": "Data-driven evaluation framework for LLM applications",
      "detailed_description": "A framework for the continuous, data-driven evaluation of LLM-powered applications. It provides metrics and pipelines to assess the performance and reliability of language model outputs.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "model_evaluation",
        "continuous_testing"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/relari-ai/continuous-eval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-evaluation",
        "data-driven",
        "metrics"
      ],
      "id": 771
    },
    {
      "name": "RULE",
      "one_line_profile": "Reliable Multimodal RAG for factuality in medical VLMs",
      "detailed_description": "Implementation of the RULE framework for ensuring factuality in Medical Vision Language Models (VLMs) via Multimodal Retrieval-Augmented Generation (RAG). It addresses hallucination in medical AI contexts.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "factuality_evaluation",
        "multimodal_rag",
        "medical_ai"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/richard-peng-xia/RULE",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "medical-vlm",
        "factuality",
        "multimodal-rag"
      ],
      "id": 772
    },
    {
      "name": "multimodal_rag_for_industry",
      "one_line_profile": "Implementation and evaluation of multimodal RAG for industrial applications",
      "detailed_description": "A repository containing the implementation and evaluation of a multimodal Retrieval-Augmented Generation (RAG) system, processing both text and image inputs, specifically tailored for industrial use cases.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "rag_implementation",
        "multimodal_evaluation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/riedlerm/multimodal_rag_for_industry",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "multimodal-rag",
        "industrial-ai",
        "evaluation"
      ],
      "id": 773
    },
    {
      "name": "auto-evaluator",
      "one_line_profile": "Evaluation tool for LLM QA chains",
      "detailed_description": "A tool designed to evaluate Question Answering (QA) chains powered by LLMs. It automates the assessment of response quality, helping to identify issues in QA pipelines.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "qa_evaluation",
        "llm_chain_testing"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/rlancemartin/auto-evaluator",
      "help_website": [],
      "license": null,
      "tags": [
        "llm-qa",
        "evaluation",
        "langchain"
      ],
      "id": 774
    },
    {
      "name": "hallucination-index",
      "one_line_profile": "Ranking of LLMs based on hallucination propensity",
      "detailed_description": "An initiative and framework to evaluate and rank popular Large Language Models (LLMs) across various task types based on their tendency to hallucinate, providing a benchmark for model reliability.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "hallucination_benchmarking",
        "model_ranking"
      ],
      "application_level": "dataset",
      "primary_language": null,
      "repo_url": "https://github.com/rungalileo/hallucination-index",
      "help_website": [],
      "license": null,
      "tags": [
        "hallucination",
        "llm-benchmark",
        "reliability"
      ],
      "id": 775
    },
    {
      "name": "factCC",
      "one_line_profile": "Evaluating factual consistency of text summarization",
      "detailed_description": "Resources and code for evaluating the factual consistency of abstractive text summarization, based on the FactCC paper. It provides models to check if summaries remain faithful to the source text.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "factuality_evaluation",
        "summarization_consistency"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/salesforce/factCC",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "fact-checking",
        "summarization",
        "nlp"
      ],
      "id": 776
    },
    {
      "name": "factualNLG",
      "one_line_profile": "Evaluating LLMs as factual reasoners",
      "detailed_description": "Code for the paper 'LLMs as Factual Reasoners', providing benchmarks and evaluation scripts to assess the factual reasoning capabilities of Large Language Models.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "factual_reasoning",
        "model_benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/salesforce/factualNLG",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "factual-reasoning",
        "llm-benchmark",
        "nlg"
      ],
      "id": 777
    },
    {
      "name": "OpenCE",
      "one_line_profile": "Toolkit for evaluating LLM context strategies (RAG, ACE)",
      "detailed_description": "Open Context Engineering (OpenCE) is a community toolkit to implement, evaluate, and combine context strategies for LLMs, such as RAG, ACE, and compression, evolved from the ACE-open reproduction.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "context_engineering",
        "rag_evaluation",
        "context_compression"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/sci-m-wang/OpenCE",
      "help_website": [],
      "license": null,
      "tags": [
        "rag",
        "context-window",
        "llm-optimization"
      ],
      "id": 778
    },
    {
      "name": "neural-ranking-drmm",
      "one_line_profile": "Implementation of Deep Relevance Matching Model (DRMM)",
      "detailed_description": "Implementation and evaluation framework for the Deep Relevance Matching Model (DRMM) for ad-hoc retrieval, facilitating research into neural ranking models.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "neural_ranking",
        "relevance_matching"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/sebastian-hofstaetter/neural-ranking-drmm",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "drmm",
        "neural-ir",
        "ad-hoc-retrieval"
      ],
      "id": 779
    },
    {
      "name": "frai",
      "one_line_profile": "Toolkit for responsible AI and model evaluation",
      "detailed_description": "An open-source toolkit for responsible AI that includes a CLI and SDK to scan code, collect evidence, and generate model cards, risk files, and evaluations for RAG indexes.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "responsible_ai",
        "model_evaluation",
        "risk_assessment"
      ],
      "application_level": "workflow",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/sebuzdugan/frai",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "responsible-ai",
        "model-cards",
        "rag-evaluation"
      ],
      "id": 780
    },
    {
      "name": "gnn-benchmark",
      "one_line_profile": "Framework for evaluating Graph Neural Networks",
      "detailed_description": "A framework for evaluating Graph Neural Network (GNN) models on semi-supervised node classification tasks, providing standardized benchmarks for graph learning research.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "gnn_evaluation",
        "node_classification",
        "graph_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/shchur/gnn-benchmark",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "gnn",
        "benchmarking",
        "graph-neural-networks"
      ],
      "id": 781
    },
    {
      "name": "knowledgestream",
      "one_line_profile": "Fact checking using knowledge graph streams",
      "detailed_description": "Code to reproduce results for finding streams in Knowledge Graphs to support fact checking. It provides methods for utilizing KG structures to verify facts.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "fact_checking",
        "knowledge_graph_mining"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/shiralkarprashant/knowledgestream",
      "help_website": [],
      "license": null,
      "tags": [
        "knowledge-graph",
        "fact-checking",
        "stream-mining"
      ],
      "id": 782
    },
    {
      "name": "FActScore",
      "one_line_profile": "Fine-grained atomic evaluation of factual precision",
      "detailed_description": "A package to evaluate the factuality of long-form text generation by breaking it down into atomic facts. It implements the FActScore metric for measuring factual precision in LLM outputs.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "factuality_evaluation",
        "long_form_generation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/shmsw25/FActScore",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "factuality",
        "evaluation-metric",
        "nlp"
      ],
      "id": 783
    },
    {
      "name": "fever-transformers",
      "one_line_profile": "Evidence retrieval and claim verification for FEVER",
      "detailed_description": "A tool for Evidence Retrieval and Claim Verification using Transformer Networks, specifically designed for the FEVER (Fact Extraction and VERification) shared task.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "claim_verification",
        "evidence_retrieval",
        "fact_checking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/simonepri/fever-transformers",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fever",
        "fact-verification",
        "transformers"
      ],
      "id": 784
    },
    {
      "name": "SolarSystemSimulatorGame",
      "one_line_profile": "Solar system simulation with realistic orbital physics",
      "detailed_description": "A simulation of the Solar System that uses 4th-order Runge-Kutta and Leapfrog integrators to calculate realistic elliptical orbits for planets and spacecraft based on NASA data.",
      "domains": [
        "G1"
      ],
      "subtask_category": [
        "orbital_simulation",
        "physics_simulation"
      ],
      "application_level": "solver",
      "primary_language": "C#",
      "repo_url": "https://github.com/sotos82/SolarSystemSimulatorGame",
      "help_website": [],
      "license": "GPL-2.0",
      "tags": [
        "simulation",
        "astronomy",
        "orbital-mechanics"
      ],
      "id": 785
    },
    {
      "name": "HELM",
      "one_line_profile": "Holistic Evaluation of Language Models framework",
      "detailed_description": "A comprehensive framework for the holistic, reproducible, and transparent evaluation of foundation models (LLMs and multimodal models). It provides a standardized interface for benchmarking models across a wide range of scenarios and metrics.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmarking",
        "foundation_models"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/stanford-crfm/helm",
      "help_website": [
        "https://crfm.stanford.edu/helm/latest/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm-evaluation",
        "benchmarking",
        "foundation-models"
      ],
      "id": 786
    },
    {
      "name": "WikiChat",
      "one_line_profile": "RAG system with hallucination reduction via Wikipedia retrieval",
      "detailed_description": "A robust Retrieval Augmented Generation (RAG) system designed to minimize hallucinations in Large Language Models by grounding responses in a verified corpus (Wikipedia). It serves as a pipeline for fact-checking and trustworthy text generation.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "hallucination_reduction",
        "fact_checking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/stanford-oval/WikiChat",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "hallucination",
        "fact-checking",
        "llm"
      ],
      "id": 787
    },
    {
      "name": "RAG Genie",
      "one_line_profile": "Prototype for evaluating RAG embeddings and chunking strategies",
      "detailed_description": "A tool designed to test and evaluate the components of RAG pipelines, specifically focusing on embedding quality and chunk splitting strategies through Q&A generation and automated evaluation.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "rag_evaluation",
        "pipeline_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Java",
      "repo_url": "https://github.com/stephanj/rag-genie",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "evaluation",
        "embeddings",
        "chunking"
      ],
      "id": 788
    },
    {
      "name": "llm-rag-eval",
      "one_line_profile": "LLM-powered evaluator for RAG pipelines",
      "detailed_description": "A Python library that utilizes Large Language Models to evaluate the performance of Retrieval Augmented Generation pipelines, providing metrics for retrieval quality and generation fidelity.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "rag_evaluation",
        "metric_calculation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/sujitpal/llm-rag-eval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "evaluation",
        "llm"
      ],
      "id": 789
    },
    {
      "name": "Question Generation",
      "one_line_profile": "Automatic factual question generation from sentences",
      "detailed_description": "A tool that generates reading comprehension style factual questions from input sentences. This is useful for creating synthetic datasets for evaluating fact-checking systems and QA models.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "data_generation",
        "synthetic_data"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/sumehta/question-generation",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "nlp",
        "question-generation",
        "dataset-creation"
      ],
      "id": 790
    },
    {
      "name": "TARGET",
      "one_line_profile": "Benchmark for table retrieval in generative tasks",
      "detailed_description": "A benchmark suite designed to evaluate Table Retrieval methods for Generative Tasks, specifically focusing on Fact Verification and Text-to-SQL applications involving tabular data.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "fact_verification",
        "table_retrieval"
      ],
      "application_level": "dataset",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/target-benchmark/target",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "benchmark",
        "fact-verification",
        "table-retrieval"
      ],
      "id": 791
    },
    {
      "name": "Safety-Prompts",
      "one_line_profile": "Chinese safety prompts for evaluating LLM safety",
      "detailed_description": "A dataset and resource containing Chinese safety prompts designed to evaluate and improve the safety and robustness of Large Language Models against various types of attacks and harmful queries.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "safety_evaluation",
        "adversarial_testing"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/thu-coai/Safety-Prompts",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm-safety",
        "evaluation",
        "prompts"
      ],
      "id": 792
    },
    {
      "name": "MLA-Trust",
      "one_line_profile": "Benchmark toolbox for Multimodal LLM Agent trustworthiness",
      "detailed_description": "A comprehensive toolbox for benchmarking the trustworthiness of Multimodal LLM Agents. It evaluates dimensions such as truthfulness, controllability, safety, and privacy across interactive tasks.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "trustworthiness_evaluation",
        "multimodal_benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/thu-ml/MLA-Trust",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "multimodal",
        "agents",
        "trustworthiness",
        "benchmark"
      ],
      "id": 793
    },
    {
      "name": "MMTrustEval",
      "one_line_profile": "Toolbox for benchmarking trustworthiness of multimodal LLMs",
      "detailed_description": "A benchmarking toolbox designed to evaluate the trustworthiness of Multimodal Large Language Models (MLLMs), covering aspects like hallucination, safety, and fairness.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "trustworthiness_evaluation",
        "multimodal_benchmarking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/thu-ml/MMTrustEval",
      "help_website": [],
      "license": "CC-BY-SA-4.0",
      "tags": [
        "multimodal",
        "evaluation",
        "trustworthiness"
      ],
      "id": 794
    },
    {
      "name": "TransformerLab",
      "one_line_profile": "Platform for LLM engineering, training, and evaluation",
      "detailed_description": "An open-source application that provides a comprehensive environment for interacting with, training, fine-tuning, and evaluating Large Language Models locally. It includes tools for experiment tracking and model assessment.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "llm_evaluation",
        "model_training"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/transformerlab/transformerlab-app",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "llm",
        "fine-tuning",
        "evaluation",
        "platform"
      ],
      "id": 795
    },
    {
      "name": "WikiCheck",
      "one_line_profile": "Wikipedia-based fact-checking API implementation",
      "detailed_description": "An implementation of a fact-checking API that leverages Wikipedia to verify claims. Developed in cooperation with the Wikimedia Foundation, it serves as a tool for automated fact verification.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "fact_checking",
        "claim_verification"
      ],
      "application_level": "service",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/trokhymovych/WikiCheck",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fact-checking",
        "wikipedia",
        "api"
      ],
      "id": 796
    },
    {
      "name": "TruLens",
      "one_line_profile": "Evaluation and tracking library for LLM experiments",
      "detailed_description": "A library for evaluating and tracking Large Language Model applications. It implements the 'RAG Triad' (Context Relevance, Groundedness, Answer Relevance) to assess the quality and truthfulness of AI agents and RAG systems.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "rag_evaluation",
        "experiment_tracking"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/truera/trulens",
      "help_website": [
        "https://www.trulens.org"
      ],
      "license": "MIT",
      "tags": [
        "evaluation",
        "rag",
        "observability",
        "llm"
      ],
      "id": 797
    },
    {
      "name": "trec_eval",
      "one_line_profile": "Standard evaluation software for Text Retrieval Conference",
      "detailed_description": "The standard software for evaluating information retrieval systems using test collections. It computes a wide range of standard IR metrics (precision, recall, MAP, etc.) used in scientific literature retrieval evaluation.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "ir_evaluation",
        "metric_calculation"
      ],
      "application_level": "solver",
      "primary_language": "C",
      "repo_url": "https://github.com/usnistgov/trec_eval",
      "help_website": [
        "https://trec.nist.gov/trec_eval/"
      ],
      "license": null,
      "tags": [
        "information-retrieval",
        "evaluation",
        "metrics"
      ],
      "id": 798
    },
    {
      "name": "X-FACT",
      "one_line_profile": "Benchmark dataset for multilingual fact checking",
      "detailed_description": "A large-scale multilingual dataset and benchmark for fact-checking, enabling the evaluation of fact verification systems across multiple languages.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "fact_checking",
        "multilingual_evaluation"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/utahnlp/x-fact",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "fact-checking",
        "multilingual",
        "benchmark"
      ],
      "id": 799
    },
    {
      "name": "ClaimBuster Spotter",
      "one_line_profile": "Tool for identifying check-worthy factual claims",
      "detailed_description": "A tool that uses adversarial training on transformer networks to automatically identify sentences and claims in text that are worth fact-checking, serving as a preliminary step in automated fact verification pipelines.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "claim_detection",
        "fact_checking"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/utaresearch/claimbuster-spotter",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "fact-checking",
        "claim-detection",
        "nlp"
      ],
      "id": 800
    },
    {
      "name": "open-rag-eval",
      "one_line_profile": "RAG evaluation framework without golden answers",
      "detailed_description": "A framework for evaluating Retrieval Augmented Generation systems that does not require ground truth 'golden answers', making it easier to assess RAG performance in dynamic or open-ended domains.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "rag_evaluation",
        "reference_free_evaluation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/vectara/open-rag-eval",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "evaluation",
        "llm"
      ],
      "id": 801
    },
    {
      "name": "AI Facts",
      "one_line_profile": "Real-time fact checking tool for spoken statements",
      "detailed_description": "A tool that performs real-time fact-checking on spoken statements by integrating speech-to-text (DeepGram) with search and verification engines (Perplexity, AI SDK).",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "real_time_fact_checking",
        "speech_verification"
      ],
      "application_level": "solver",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/vercel-labs/ai-facts",
      "help_website": [],
      "license": null,
      "tags": [
        "fact-checking",
        "real-time",
        "speech-processing"
      ],
      "id": 802
    },
    {
      "name": "Ragas",
      "one_line_profile": "Framework for evaluating RAG pipelines",
      "detailed_description": "A comprehensive framework for evaluating Retrieval Augmented Generation (RAG) pipelines. It provides metrics like faithfulness, answer relevance, and context precision to quantify the performance of LLM applications.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "rag_evaluation",
        "metric_calculation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/vibrantlabsai/ragas",
      "help_website": [
        "https://docs.ragas.io"
      ],
      "license": "Apache-2.0",
      "tags": [
        "rag",
        "evaluation",
        "metrics",
        "llm"
      ],
      "id": 803
    },
    {
      "name": "TabFact",
      "one_line_profile": "Large-scale dataset and code for table-based fact verification",
      "detailed_description": "A benchmark dataset and associated code for verifying facts based on tabular data. It serves as a standard resource for evaluating models on table-based reasoning and fact-checking tasks.",
      "domains": [
        "G1",
        "G1-05"
      ],
      "subtask_category": [
        "fact_verification",
        "table_reasoning"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/wenhuchen/Table-Fact-Checking",
      "help_website": [
        "https://tabfact.github.io/"
      ],
      "license": "MIT",
      "tags": [
        "fact-verification",
        "table-reasoning",
        "benchmark"
      ],
      "id": 804
    },
    {
      "name": "gymfc",
      "one_line_profile": "Universal flight control tuning framework based on OpenAI Gym",
      "detailed_description": "A flight control tuning framework that acts as an OpenAI Gym environment, allowing for the development and benchmarking of intelligent flight control systems using reinforcement learning.",
      "domains": [
        "Robotics",
        "Control Systems"
      ],
      "subtask_category": [
        "flight_control",
        "reinforcement_learning",
        "simulation"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/wil3/gymfc",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "flight-control",
        "reinforcement-learning",
        "uav",
        "simulation"
      ],
      "id": 805
    },
    {
      "name": "Caveman Compression",
      "one_line_profile": "Semantic compression method for LLM contexts",
      "detailed_description": "A tool for compressing context for Large Language Models by removing predictable grammar while preserving factual content, optimizing token usage without losing semantic meaning.",
      "domains": [
        "NLP",
        "AI"
      ],
      "subtask_category": [
        "context_compression",
        "data_processing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/wilpel/caveman-compression",
      "help_website": [],
      "license": null,
      "tags": [
        "llm",
        "compression",
        "context-optimization"
      ],
      "id": 806
    },
    {
      "name": "GraphEval",
      "one_line_profile": "Framework for evaluating LLM factuality using Knowledge Graphs",
      "detailed_description": "A framework designed to evaluate the factuality of Large Language Models by leveraging large-scale knowledge graphs to detect hallucinations and verify claims.",
      "domains": [
        "NLP",
        "G1-05"
      ],
      "subtask_category": [
        "factuality_evaluation",
        "hallucination_detection"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/xz-liu/GraphEval",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "knowledge-graph",
        "hallucination",
        "evaluation",
        "llm"
      ],
      "id": 807
    },
    {
      "name": "MSRS",
      "one_line_profile": "Benchmark dataset and code for evaluating Multi-Source RAG",
      "detailed_description": "A resource containing data and evaluation code for assessing Multi-Source Retrieval-Augmented Generation systems, focusing on the integration and verification of information from diverse sources.",
      "domains": [
        "NLP",
        "G1-05"
      ],
      "subtask_category": [
        "rag_evaluation",
        "benchmark"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/yale-nlp/MSRS",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "evaluation",
        "dataset",
        "multi-source"
      ],
      "id": 808
    },
    {
      "name": "RaLLe",
      "one_line_profile": "Framework for developing and evaluating RAG systems",
      "detailed_description": "A comprehensive framework facilitating the development and evaluation of Retrieval-Augmented Large Language Models, providing tools for testing retrieval accuracy and generation quality.",
      "domains": [
        "NLP",
        "G1-05"
      ],
      "subtask_category": [
        "rag_development",
        "evaluation"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/yhoshi3/RaLLe",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rag",
        "llm",
        "framework",
        "evaluation"
      ],
      "id": 809
    },
    {
      "name": "MultiHop-RAG",
      "one_line_profile": "Dataset for evaluating RAG across multiple documents",
      "detailed_description": "A dataset designed to evaluate Retrieval-Augmented Generation systems specifically on multi-hop reasoning tasks across multiple documents, serving as a benchmark for RAG capabilities.",
      "domains": [
        "NLP",
        "G1-05"
      ],
      "subtask_category": [
        "rag_evaluation",
        "benchmark"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/yixuantt/MultiHop-RAG",
      "help_website": [],
      "license": null,
      "tags": [
        "rag",
        "multi-hop",
        "dataset",
        "evaluation"
      ],
      "id": 810
    },
    {
      "name": "VeriExCiting",
      "one_line_profile": "Tool for detecting AI-generated fake citations in academic papers",
      "detailed_description": "A verification tool designed to identify non-existent or hallucinated citations in academic texts generated by AI, ensuring the integrity of scientific references.",
      "domains": [
        "Sci Knowledge",
        "G1-05"
      ],
      "subtask_category": [
        "citation_verification",
        "fake_detection"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ykangw/VeriExCiting",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "citation-check",
        "hallucination",
        "academic-integrity"
      ],
      "id": 811
    },
    {
      "name": "torchdistill",
      "one_line_profile": "Reproducible knowledge distillation framework for PyTorch",
      "detailed_description": "A coding-free framework built on PyTorch for deep learning studies, specifically focusing on knowledge distillation methods, enabling reproducible experiments and model compression.",
      "domains": [
        "Deep Learning",
        "Computer Vision"
      ],
      "subtask_category": [
        "knowledge_distillation",
        "model_training"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/yoshitomo-matsubara/torchdistill",
      "help_website": [
        "https://torchdistill.readthedocs.io/"
      ],
      "license": "MIT",
      "tags": [
        "pytorch",
        "knowledge-distillation",
        "reproducibility"
      ],
      "id": 812
    },
    {
      "name": "EasyLM",
      "one_line_profile": "One-stop solution for LLM pre-training and serving in JAX/Flax",
      "detailed_description": "A scalable and easy-to-use framework for pre-training, fine-tuning, evaluating, and serving Large Language Models using JAX and Flax, designed for high-performance computing environments.",
      "domains": [
        "NLP",
        "AI"
      ],
      "subtask_category": [
        "model_training",
        "inference"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/young-geng/EasyLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "jax",
        "flax",
        "llm",
        "distributed-training"
      ],
      "id": 813
    },
    {
      "name": "IFCC",
      "one_line_profile": "Improves factual completeness and consistency of radiology reports",
      "detailed_description": "A model implementation for generating radiology reports from images with a focus on improving factual completeness and consistency, aiding in medical image analysis.",
      "domains": [
        "Medical AI",
        "G1-05"
      ],
      "subtask_category": [
        "report_generation",
        "factual_consistency"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ysmiura/ifcc",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "radiology",
        "image-captioning",
        "medical-nlp"
      ],
      "id": 814
    },
    {
      "name": "AlignScore",
      "one_line_profile": "Metric for factual consistency evaluation in text generation",
      "detailed_description": "A comprehensive metric tool designed to evaluate the factual consistency between source text and generated text, addressing the alignment problem in natural language generation.",
      "domains": [
        "NLP",
        "G1-05"
      ],
      "subtask_category": [
        "factuality_evaluation",
        "metric"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/yuh-zha/AlignScore",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "evaluation-metric",
        "factuality",
        "nlg"
      ],
      "id": 815
    },
    {
      "name": "Factcheck-GPT",
      "one_line_profile": "Tool for fact-checking LLM outputs via annotation and evaluation",
      "detailed_description": "A tool designed to facilitate the fact-checking of generative Large Language Model outputs, supporting both manual annotation processes and automated evaluation workflows.",
      "domains": [
        "NLP",
        "G1-05"
      ],
      "subtask_category": [
        "fact_checking",
        "annotation"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/yuxiaw/Factcheck-GPT",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "fact-checking",
        "llm",
        "annotation"
      ],
      "id": 816
    },
    {
      "name": "ESCNet",
      "one_line_profile": "Entity-enhanced and Stance Checking Network for multi-modal fact-checking",
      "detailed_description": "A deep learning model implementation for multi-modal fact-checking that utilizes entity enhancement and stance checking to verify claims against evidence.",
      "domains": [
        "NLP",
        "G1-05"
      ],
      "subtask_category": [
        "fact_checking",
        "stance_detection"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/zfr00/ESCNet",
      "help_website": [],
      "license": null,
      "tags": [
        "multi-modal",
        "fact-checking",
        "stance-detection"
      ],
      "id": 817
    },
    {
      "name": "LLM-RG4",
      "one_line_profile": "Flexible and factual radiology report generation model",
      "detailed_description": "A Large Language Model based approach for generating factual radiology reports from diverse input contexts, aiming to improve the accuracy and utility of automated medical reporting.",
      "domains": [
        "Medical AI",
        "G1-05"
      ],
      "subtask_category": [
        "report_generation",
        "medical_imaging"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/zh-Wang-Med/LLM-RG4",
      "help_website": [],
      "license": null,
      "tags": [
        "radiology",
        "llm",
        "medical-report"
      ],
      "id": 818
    },
    {
      "name": "FactualSceneGraph",
      "one_line_profile": "Textual scene graph parser and FACTUAL dataset",
      "detailed_description": "A tool and dataset for parsing textual scene graphs, enabling the structured representation of factual information from text for downstream tasks like image generation or verification.",
      "domains": [
        "NLP",
        "CV"
      ],
      "subtask_category": [
        "scene_graph_parsing",
        "dataset"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/zhuang-li/FactualSceneGraph",
      "help_website": [],
      "license": null,
      "tags": [
        "scene-graph",
        "parsing",
        "dataset"
      ],
      "id": 819
    },
    {
      "name": "KnowRL",
      "one_line_profile": "Knowledgeable Reinforcement Learning for Factuality",
      "detailed_description": "A framework exploring the use of Reinforcement Learning integrated with knowledge bases to improve the factuality and truthfulness of language model generations.",
      "domains": [
        "NLP",
        "G1-05"
      ],
      "subtask_category": [
        "reinforcement_learning",
        "factuality_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/zjunlp/KnowRL",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "rl",
        "knowledge-graph",
        "factuality"
      ],
      "id": 820
    },
    {
      "name": "openai_tools",
      "one_line_profile": "Scripts for summarizing scientific literature using OpenAI models",
      "detailed_description": "A collection of scripts developed by the Allen Institute to summarize and analyze scientific literature using large language models like ChatGPT.",
      "domains": [
        "G1"
      ],
      "subtask_category": [
        "summarization",
        "literature_mining"
      ],
      "application_level": "workflow",
      "primary_language": "HTML",
      "repo_url": "https://github.com/AllenInstitute/openai_tools",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "summarization",
        "allen-institute",
        "literature-analysis"
      ],
      "id": 821
    },
    {
      "name": "ArxivDigest",
      "one_line_profile": "Personalized arXiv paper recommendation system",
      "detailed_description": "A tool for generating personalized arXiv digests and recommendations using Large Language Models to filter and summarize daily papers.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "recommendation",
        "discovery"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/AutoLLM/ArxivDigest",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "arxiv",
        "recommendation",
        "llm"
      ],
      "id": 822
    },
    {
      "name": "climsight",
      "one_line_profile": "LLM-based climate information system",
      "detailed_description": "A climate information system that combines Large Language Models with high-resolution climate model data and scientific literature to provide localized climate assessments.",
      "domains": [
        "G1",
        "Earth Science"
      ],
      "subtask_category": [
        "climate_analysis",
        "literature_synthesis"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/CliDyn/climsight",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "climate-change",
        "llm",
        "scientific-assessment"
      ],
      "id": 823
    },
    {
      "name": "CAiRE-COVID",
      "one_line_profile": "QA and summarization system for mining COVID-19 scientific literature",
      "detailed_description": "A machine learning-based system combining Question Answering (QA) and summarization techniques to mine and extract information from scientific literature, specifically tailored for COVID-19 research.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "literature_mining",
        "question_answering",
        "summarization"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/HLTCHKUST/CAiRE-COVID",
      "help_website": [],
      "license": null,
      "tags": [
        "covid-19",
        "nlp",
        "literature-mining",
        "qa"
      ],
      "id": 824
    },
    {
      "name": "Arxiv-NLP-Reporter",
      "one_line_profile": "Daily automated crawler and reporter for NLP research papers from arXiv",
      "detailed_description": "A tool that automatically crawls the latest Natural Language Processing (NLP) papers from arXiv daily and generates reports, facilitating efficient literature tracking.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "paper_discovery",
        "data_retrieval"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/JackHCC/Arxiv-NLP-Reporter",
      "help_website": [],
      "license": null,
      "tags": [
        "arxiv",
        "nlp",
        "crawler",
        "daily-report"
      ],
      "id": 825
    },
    {
      "name": "ArxivCVDigest",
      "one_line_profile": "Daily digest generator for Computer Vision papers from arXiv",
      "detailed_description": "A tool that generates a daily feed of the latest Computer Vision research papers from arXiv, helping researchers stay updated with new publications.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "paper_discovery",
        "recommendation"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/Jasmine66Bloom/ArxivCVDigest",
      "help_website": [],
      "license": null,
      "tags": [
        "computer-vision",
        "arxiv",
        "digest",
        "research-feed"
      ],
      "id": 826
    },
    {
      "name": "PaperHelper",
      "one_line_profile": "LLM-based paper reading assistant with reliable reference support",
      "detailed_description": "A knowledge-based QA tool that assists in reading scientific papers by using Large Language Models to answer questions while providing reliable references from the text.",
      "domains": [
        "G1",
        "G1-03"
      ],
      "subtask_category": [
        "literature_analysis",
        "reading_assistant"
      ],
      "application_level": "application",
      "primary_language": "Python",
      "repo_url": "https://github.com/JerryYin777/PaperHelper",
      "help_website": [],
      "license": null,
      "tags": [
        "llm",
        "paper-reading",
        "qa",
        "research-assistant"
      ],
      "id": 827
    },
    {
      "name": "pubtrends",
      "one_line_profile": "Scientific literature explorer for visualizing search result structures",
      "detailed_description": "A tool that runs searches on Pubmed or Semantic Scholar and allows users to explore the high-level structure and trends of the resulting papers through visualization.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "visualization",
        "literature_discovery"
      ],
      "application_level": "application",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/JetBrains-Research/pubtrends",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "bibliometrics",
        "visualization",
        "literature-search",
        "trends"
      ],
      "id": 828
    },
    {
      "name": "customize-arxiv-daily",
      "one_line_profile": "Tool for generating customized daily arXiv paper recommendations",
      "detailed_description": "A Python-based tool that allows users to customize and automate the retrieval of daily arXiv paper recommendations based on specific interests.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "paper_recommendation",
        "personalization"
      ],
      "application_level": "application",
      "primary_language": "Python",
      "repo_url": "https://github.com/JoeLeelyf/customize-arxiv-daily",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "arxiv",
        "recommendation",
        "daily-updates",
        "customization"
      ],
      "id": 829
    },
    {
      "name": "EmbodiedAI-Robotics-arXiv-Daily-Reporter",
      "one_line_profile": "Daily arXiv paper reporter for Embodied AI and Robotics",
      "detailed_description": "A specialized tool for filtering and reporting daily arXiv papers in the fields of Embodied AI and Robotics, aiding researchers in efficient literature discovery.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "paper_discovery",
        "filtering"
      ],
      "application_level": "solver",
      "primary_language": null,
      "repo_url": "https://github.com/KJaebye/EmbodiedAI-Robotics-arXiv-Daily-Reporter",
      "help_website": [],
      "license": null,
      "tags": [
        "embodied-ai",
        "robotics",
        "arxiv",
        "daily-report"
      ],
      "id": 830
    },
    {
      "name": "semanticscholar-R",
      "one_line_profile": "R package for accessing Semantic Scholar API data",
      "detailed_description": "An R package developed by KTH Library to provide programmatic access to the Semantic Scholar API, facilitating the retrieval of scientific literature data for analysis.",
      "domains": [
        "G1",
        "G1-01"
      ],
      "subtask_category": [
        "data_retrieval",
        "api_client"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/KTH-Library/semanticscholar",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "r-package",
        "semantic-scholar",
        "bibliometrics",
        "api"
      ],
      "id": 831
    },
    {
      "name": "SciQAG",
      "one_line_profile": "Framework for generating science question-answer pairs from scientific literature",
      "detailed_description": "A framework designed to automatically generate high-quality question-answer pairs from large corpora of scientific literature using large language models, facilitating scientific knowledge extraction and QA system training.",
      "domains": [
        "G1",
        "Sci Knowledge"
      ],
      "subtask_category": [
        "text_mining",
        "data_generation",
        "question_answering"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/MasterAI-EAM/SciQAG",
      "help_website": [],
      "license": null,
      "tags": [
        "scientific-literature",
        "question-answering",
        "llm",
        "data-generation"
      ],
      "id": 832
    },
    {
      "name": "exsclaim",
      "one_line_profile": "Toolkit for constructing materials imaging datasets from scientific literature",
      "detailed_description": "A toolkit that automates the extraction and labeling of materials science images from scientific literature to construct large-scale, self-labeled imaging datasets for machine learning applications in materials science.",
      "domains": [
        "G1",
        "Materials Science"
      ],
      "subtask_category": [
        "image_mining",
        "dataset_construction",
        "literature_parsing"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/MaterialEyes/exsclaim",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "materials-science",
        "image-extraction",
        "literature-mining",
        "dataset-creation"
      ],
      "id": 833
    },
    {
      "name": "zotero-connected-papers",
      "one_line_profile": "Zotero plugin for visualizing citation networks via Connected Papers",
      "detailed_description": "A plugin for the Zotero reference manager that integrates with the Connected Papers service, allowing researchers to visualize citation graphs and discover relevant scientific literature directly from their reference library.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "literature_discovery",
        "visualization",
        "citation_analysis"
      ],
      "application_level": "solver",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/MuiseDestiny/zotero-connected-papers",
      "help_website": [],
      "license": null,
      "tags": [
        "zotero-plugin",
        "literature-discovery",
        "citation-network",
        "connected-papers"
      ],
      "id": 834
    },
    {
      "name": "litstudy",
      "one_line_profile": "Python library for automated scientific literature analysis and bibliometrics",
      "detailed_description": "A Python library developed by the Netherlands eScience Center that enables automated retrieval, analysis, and visualization of scientific literature and bibliometric data from various sources (Scopus, Semantic Scholar, etc.) within Jupyter notebooks.",
      "domains": [
        "G1",
        "Scientometrics"
      ],
      "subtask_category": [
        "bibliometrics",
        "literature_analysis",
        "visualization"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/NLeSC/litstudy",
      "help_website": [
        "https://nlesc.github.io/litstudy/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "bibliometrics",
        "literature-analysis",
        "visualization",
        "python"
      ],
      "id": 835
    },
    {
      "name": "SHELF",
      "one_line_profile": "Tools supporting the Sheffield Elicitation Framework for expert judgment",
      "detailed_description": "An R package providing tools to support the Sheffield Elicitation Framework (SHELF), used for eliciting probability distributions from experts in scientific risk analysis and uncertainty quantification.",
      "domains": [
        "Statistics",
        "Risk Analysis"
      ],
      "subtask_category": [
        "expert_elicitation",
        "probability_estimation",
        "uncertainty_quantification"
      ],
      "application_level": "library",
      "primary_language": "R",
      "repo_url": "https://github.com/OakleyJ/SHELF",
      "help_website": [
        "http://www.tonyohagan.co.uk/shelf/"
      ],
      "license": "NOASSERTION",
      "tags": [
        "expert-elicitation",
        "statistics",
        "risk-analysis",
        "r"
      ],
      "id": 836
    },
    {
      "name": "zotero-arxiv-daily",
      "one_line_profile": "Automated tool to recommend and import arXiv papers into Zotero based on user interests",
      "detailed_description": "A Python-based tool that recommends new arXiv papers daily according to the user's existing Zotero library content and automatically updates the library with relevant findings.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "literature_recommendation",
        "reference_management"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/TideDra/zotero-arxiv-daily",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "arxiv",
        "zotero",
        "paper-recommendation"
      ],
      "id": 837
    },
    {
      "name": "cv-arxiv-daily",
      "one_line_profile": "Automated workflow for daily Computer Vision paper updates from arXiv",
      "detailed_description": "A GitHub Actions-based tool that automatically fetches, filters, and updates a daily list of new arXiv papers in the field of Computer Vision, facilitating literature tracking.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "literature_discovery",
        "alerting_system"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/Vincentqyw/cv-arxiv-daily",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "computer-vision",
        "arxiv",
        "automation"
      ],
      "id": 838
    },
    {
      "name": "CCFrank4dblp",
      "one_line_profile": "Browser extension to display CCF rankings on academic search sites",
      "detailed_description": "A JavaScript-based browser extension that augments academic search results (dblp, Google Scholar, etc.) by displaying China Computer Federation (CCF) recommended rankings for conferences and journals.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "literature_filtering",
        "metadata_augmentation"
      ],
      "application_level": "solver",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/WenyanLiu/CCFrank4dblp",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "browser-extension",
        "ccf-ranking",
        "dblp"
      ],
      "id": 839
    },
    {
      "name": "cv-arxiv-daily (Xuchen-Li)",
      "one_line_profile": "Automated arXiv paper tracking for SOT, VLT, and Video Understanding",
      "detailed_description": "An automated tool using GitHub Actions to update and track daily arXiv papers specifically for Single Object Tracking, Visual Language Tracking, and Video Understanding domains.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "literature_discovery",
        "alerting_system"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/Xuchen-Li/cv-arxiv-daily",
      "help_website": [],
      "license": null,
      "tags": [
        "video-understanding",
        "arxiv",
        "tracking"
      ],
      "id": 840
    },
    {
      "name": "llm-arxiv-daily",
      "one_line_profile": "Automated arXiv paper tracking for LLM Reasoning and Evaluation",
      "detailed_description": "An automated workflow tool to track and update daily arXiv papers focused on Large Language Model reasoning, evaluation, and multimodal learning.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "literature_discovery",
        "alerting_system"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/Xuchen-Li/llm-arxiv-daily",
      "help_website": [],
      "license": null,
      "tags": [
        "llm",
        "arxiv",
        "automation"
      ],
      "id": 841
    },
    {
      "name": "ro-arxiv-daily",
      "one_line_profile": "Automated arXiv paper tracking for Robotics and Path Planning",
      "detailed_description": "An automated tool using GitHub Actions to update daily arXiv papers related to Path Planning, LLMs in robotics, and Autonomous Driving.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "literature_discovery",
        "alerting_system"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/XuzhaoLi/ro-arxiv-daily",
      "help_website": [],
      "license": null,
      "tags": [
        "robotics",
        "path-planning",
        "arxiv"
      ],
      "id": 842
    },
    {
      "name": "DelhiLM",
      "one_line_profile": "LLM-based pipeline for Delphi expert elicitation methodology",
      "detailed_description": "A pipeline tool that utilizes Large Language Models to automate and facilitate the Delphi expert elicitation process, a structured communication technique used in research for forecasting and decision making.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "survey_methodology",
        "expert_elicitation"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/YuzeHao2023/DelhiLM",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "delphi-method",
        "llm",
        "research-methodology"
      ],
      "id": 843
    },
    {
      "name": "Arxiv_daily",
      "one_line_profile": "Spider tool for fetching customized arXiv paper lists",
      "detailed_description": "A Python-based spider tool designed to help researchers automatically fetch and generate customized lists of papers from arXiv based on specific criteria.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "literature_discovery",
        "web_scraping"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ZihaoZhao/Arxiv_daily",
      "help_website": [],
      "license": null,
      "tags": [
        "arxiv",
        "spider",
        "literature-tracking"
      ],
      "id": 844
    },
    {
      "name": "origami",
      "one_line_profile": "Interactive graph visualization tool for scholar articles",
      "detailed_description": "A web application that visualizes scholar articles as an interactive graph, helping researchers explore connections and citation networks between papers.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "scientific_visualization",
        "citation_analysis"
      ],
      "application_level": "solver",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/aMarcireau/origami",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "visualization",
        "citation-graph",
        "literature-mapping"
      ],
      "id": 845
    },
    {
      "name": "academic-search-mcp-server",
      "one_line_profile": "MCP Server for searching academic papers via Semantic Scholar and Crossref",
      "detailed_description": "A Model Context Protocol (MCP) server implementation that enables AI assistants (like Claude) to search for and retrieve academic paper data from Semantic Scholar and Crossref databases.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "literature_discovery",
        "database_integration"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/afrise/academic-search-mcp-server",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "mcp",
        "semantic-scholar",
        "crossref"
      ],
      "id": 846
    },
    {
      "name": "PaperHunter",
      "one_line_profile": "Tool for fetching and filtering conference papers from DBLP",
      "detailed_description": "A tool designed to fetch research papers from top computer science conferences via DBLP, featuring advanced keyword logic filtering and multi-year query capabilities for precise literature discovery.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "literature_discovery",
        "metadata_filtering"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/ahlien/PaperHunter",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "dblp",
        "paper-search",
        "conference-papers"
      ],
      "id": 847
    },
    {
      "name": "AutoDiscovery",
      "one_line_profile": "Bayesian surprise-based framework for open-ended scientific discovery",
      "detailed_description": "A computational framework implementing the 'AutoDiscovery' algorithm, which uses Bayesian surprise to guide open-ended scientific discovery processes. It serves as a solver for generating and prioritizing scientific hypotheses or experimental paths.",
      "domains": [
        "Sci Knowledge",
        "Scientific Discovery"
      ],
      "subtask_category": [
        "hypothesis_generation",
        "discovery_planning"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/allenai/autodiscovery",
      "help_website": [],
      "license": null,
      "tags": [
        "bayesian-inference",
        "scientific-discovery",
        "hypothesis-generation"
      ],
      "id": 848
    },
    {
      "name": "S2Search",
      "one_line_profile": "Reranking library for scientific literature search",
      "detailed_description": "A library implementing the search reranker used by Semantic Scholar. It provides tools to improve the relevance of search results in scientific databases.",
      "domains": [
        "G1-06",
        "Sci Knowledge"
      ],
      "subtask_category": [
        "search_reranking",
        "information_retrieval"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/allenai/s2search",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "search",
        "reranking",
        "semantic-scholar"
      ],
      "id": 849
    },
    {
      "name": "Papermap",
      "one_line_profile": "Web-based visualization tool for scientific literature findings",
      "detailed_description": "A platform for mapping and visualizing scientific findings across literature, helping researchers gain an overview of answers to specific scientific questions.",
      "domains": [
        "G1-06",
        "Sci Knowledge"
      ],
      "subtask_category": [
        "literature_visualization",
        "knowledge_mapping"
      ],
      "application_level": "platform",
      "primary_language": "Svelte",
      "repo_url": "https://github.com/angeluriot/Papermap",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "visualization",
        "literature-review",
        "knowledge-graph"
      ],
      "id": 850
    },
    {
      "name": "Obsidian Reference Map",
      "one_line_profile": "Obsidian plugin for literature citation mapping",
      "detailed_description": "A plugin for the Obsidian knowledge base that generates interactive citation maps and reference visualizations to assist in literature review and discovery within a personal knowledge management workflow.",
      "domains": [
        "G1-06",
        "Sci Knowledge"
      ],
      "subtask_category": [
        "literature_management",
        "citation_visualization"
      ],
      "application_level": "workflow",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/anoopkcn/obsidian-reference-map",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "obsidian",
        "citation-map",
        "literature-review"
      ],
      "id": 851
    },
    {
      "name": "Nexus Now",
      "one_line_profile": "IPFS-backed browser tool for scientific literature access",
      "detailed_description": "A decentralized web tool leveraging IPFS to facilitate access to scientific literature using standard template constructs, aiming to improve the accessibility of research papers.",
      "domains": [
        "Sci Knowledge"
      ],
      "subtask_category": [
        "literature_access",
        "decentralized_science"
      ],
      "application_level": "service",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/aokellermann/nexus-now",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ipfs",
        "open-access",
        "literature-search"
      ],
      "id": 852
    },
    {
      "name": "Zotero Nexus",
      "one_line_profile": "Zotero extension for decentralized literature access",
      "detailed_description": "An extension for the Zotero reference manager that integrates IPFS-backed search and access capabilities, streamlining the retrieval of scientific literature directly within the reference management workflow.",
      "domains": [
        "Sci Knowledge"
      ],
      "subtask_category": [
        "literature_management",
        "reference_retrieval"
      ],
      "application_level": "workflow",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/aokellermann/zotero-nexus",
      "help_website": [],
      "license": "AGPL-3.0",
      "tags": [
        "zotero",
        "plugin",
        "literature-access"
      ],
      "id": 853
    },
    {
      "name": "PreliZ",
      "one_line_profile": "Tool for eliciting and exploring prior probability distributions",
      "detailed_description": "A library designed to assist researchers in selecting and eliciting prior distributions for Bayesian statistical modeling. It provides visualization and interaction tools to translate expert knowledge into statistical priors.",
      "domains": [
        "Statistics",
        "Data Analysis"
      ],
      "subtask_category": [
        "prior_elicitation",
        "statistical_modeling"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/arviz-devs/preliz",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "bayesian",
        "statistics",
        "visualization"
      ],
      "id": 854
    },
    {
      "name": "STONED-SELFIES",
      "one_line_profile": "Efficient algorithm for molecular generation and optimization",
      "detailed_description": "Implementation of the STONED (Superfast Traversal, Optimization, Novelty, Exploration and Discovery) algorithm. It uses SELFIES representation for efficient and robust molecular design and chemical space exploration.",
      "domains": [
        "Chemistry",
        "Drug Discovery"
      ],
      "subtask_category": [
        "molecular_generation",
        "chemical_optimization"
      ],
      "application_level": "solver",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/aspuru-guzik-group/stoned-selfies",
      "help_website": [],
      "license": null,
      "tags": [
        "molecular-design",
        "selfies",
        "generative-models"
      ],
      "id": 855
    },
    {
      "name": "WP-Scholar",
      "one_line_profile": "WordPress plugin for scientific and technical writing",
      "detailed_description": "A publishing tool that enables researchers to write technical articles on WordPress with support for Markdown, LaTeX, Jupyter notebooks, and interactive charts (Plotly/Bokeh), facilitating scientific communication.",
      "domains": [
        "Sci Communication"
      ],
      "subtask_category": [
        "scientific_writing",
        "publishing"
      ],
      "application_level": "workflow",
      "primary_language": "PHP",
      "repo_url": "https://github.com/aurelienpierre/wp-scholar",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "wordpress",
        "latex",
        "scientific-publishing"
      ],
      "id": 856
    },
    {
      "name": "MCP Semantic Scholar Server",
      "one_line_profile": "Model Context Protocol server for Semantic Scholar integration",
      "detailed_description": "A server implementation of the Model Context Protocol (MCP) that connects Large Language Models (LLMs) with the Semantic Scholar API, enabling AI agents to search and retrieve scientific literature.",
      "domains": [
        "G1-06",
        "Sci Knowledge"
      ],
      "subtask_category": [
        "literature_search",
        "llm_integration"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/benhaotang/mcp-semantic-scholar-server",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "mcp",
        "semantic-scholar",
        "llm-agent"
      ],
      "id": 857
    },
    {
      "name": "ChemGAN Challenge",
      "one_line_profile": "Generative adversarial network framework for drug discovery",
      "detailed_description": "Codebase for the ChemGAN challenge, providing implementations of Generative Adversarial Networks (GANs) specifically tuned for generating chemical structures and exploring chemical diversity in drug discovery.",
      "domains": [
        "Chemistry",
        "Drug Discovery"
      ],
      "subtask_category": [
        "molecular_generation",
        "drug_design"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/benstaf/ChemGAN-challenge",
      "help_website": [],
      "license": null,
      "tags": [
        "gan",
        "cheminformatics",
        "drug-discovery"
      ],
      "id": 858
    },
    {
      "name": "HypothesisHub",
      "one_line_profile": "Automated research question and hypothesis generation tool",
      "detailed_description": "An AI-driven tool designed to analyze scientific literature and automatically generate potential research questions and hypotheses, aiding researchers in the ideation phase.",
      "domains": [
        "Sci Knowledge"
      ],
      "subtask_category": [
        "hypothesis_generation",
        "literature_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/bhaskatripathi/HypothesisHub",
      "help_website": [],
      "license": null,
      "tags": [
        "hypothesis-generation",
        "nlp",
        "research-automation"
      ],
      "id": 859
    },
    {
      "name": "PlatCOVID",
      "one_line_profile": "Web platform for COVID-19 literature analysis",
      "detailed_description": "A web-based platform designed to cluster, classify, and analyze the vast volume of scientific literature related to COVID-19, supporting rapid information retrieval and synthesis during the pandemic.",
      "domains": [
        "Biology",
        "Medicine"
      ],
      "subtask_category": [
        "literature_analysis",
        "text_mining"
      ],
      "application_level": "platform",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/bio-hub/bio-hub.github.io",
      "help_website": [],
      "license": null,
      "tags": [
        "covid-19",
        "literature-mining",
        "web-platform"
      ],
      "id": 860
    },
    {
      "name": "BioAgents",
      "one_line_profile": "Multi-agent framework for autonomous biological research",
      "detailed_description": "An AI framework that orchestrates multiple agents (literature analysis, data science) to conduct autonomous deep research in biological sciences, integrating user feedback for iterative discovery.",
      "domains": [
        "Biology",
        "Sci Knowledge"
      ],
      "subtask_category": [
        "autonomous_research",
        "literature_analysis"
      ],
      "application_level": "workflow",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/bio-xyz/BioAgents",
      "help_website": [],
      "license": null,
      "tags": [
        "agent-framework",
        "biology",
        "autonomous-science"
      ],
      "id": 861
    },
    {
      "name": "Cool Papers",
      "one_line_profile": "Immersive daily paper discovery platform",
      "detailed_description": "A popular web-based tool for discovering and browsing daily arXiv papers (Kexue Kongjian), featuring automated summaries and filtering to help researchers stay updated with the latest literature.",
      "domains": [
        "G1-06",
        "Sci Knowledge"
      ],
      "subtask_category": [
        "paper_discovery",
        "literature_monitoring"
      ],
      "application_level": "platform",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/bojone/papers.cool",
      "help_website": [],
      "license": null,
      "tags": [
        "arxiv",
        "paper-discovery",
        "daily-updates"
      ],
      "id": 862
    },
    {
      "name": "NN for LBD",
      "one_line_profile": "Neural network models for Literature-based Discovery",
      "detailed_description": "A library implementing neural network approaches for Literature-based Discovery (LBD), enabling the identification of implicit connections and hypotheses within scientific text corpora.",
      "domains": [
        "Sci Knowledge"
      ],
      "subtask_category": [
        "literature_based_discovery",
        "hypothesis_generation"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/cambridgeltl/nn_for_LBD",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "lbd",
        "nlp",
        "discovery"
      ],
      "id": 863
    },
    {
      "name": "Paper Note Filler",
      "one_line_profile": "Obsidian plugin for automated paper note creation",
      "detailed_description": "An Obsidian plugin that automatically retrieves metadata from arXiv, ACL Anthology, and Semantic Scholar to create structured notes for scientific papers, streamlining the research note-taking process.",
      "domains": [
        "G1-06",
        "Sci Knowledge"
      ],
      "subtask_category": [
        "literature_management",
        "note_taking"
      ],
      "application_level": "workflow",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/chauff/paper-note-filler",
      "help_website": [],
      "license": null,
      "tags": [
        "obsidian",
        "metadata-extraction",
        "productivity"
      ],
      "id": 864
    },
    {
      "name": "ArXiv Slack Bot",
      "one_line_profile": "Slack bot for daily top ML paper notifications",
      "detailed_description": "A bot service that monitors arXiv for top machine learning papers and posts daily updates to Slack, facilitating team-based literature tracking and discovery.",
      "domains": [
        "G1-06",
        "Sci Knowledge"
      ],
      "subtask_category": [
        "paper_discovery",
        "alert_service"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/chintu619/citation-sorted-arxiv-slack-bot",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "slack-bot",
        "arxiv",
        "paper-alert"
      ],
      "id": 865
    },
    {
      "name": "zotero-markdb-connect",
      "one_line_profile": "Zotero plugin linking Markdown databases to Zotero items for integrated research note-taking",
      "detailed_description": "A Zotero plugin that establishes a link between a Zotero bibliography and a Markdown database (e.g., Obsidian). It allows researchers to jump directly from Zotero items to connected Markdown notes and automatically tags Zotero items to track which papers have associated notes.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "knowledge_management",
        "literature_annotation"
      ],
      "application_level": "workflow",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/daeh/zotero-markdb-connect",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "zotero",
        "markdown",
        "obsidian",
        "knowledge-base"
      ],
      "id": 866
    },
    {
      "name": "semanticscholar",
      "one_line_profile": "Unofficial Python client for the Semantic Scholar API",
      "detailed_description": "A Python client library that provides an interface to the Semantic Scholar API, enabling researchers to programmatically retrieve paper details, citations, references, and author information for bibliometric analysis and literature discovery.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "literature_retrieval",
        "bibliometrics"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/danielnsilva/semanticscholar",
      "help_website": [
        "https://pypi.org/project/semanticscholar/"
      ],
      "license": "MIT",
      "tags": [
        "semantic-scholar",
        "api-client",
        "literature-search"
      ],
      "id": 867
    },
    {
      "name": "zotero2SemanticScholar",
      "one_line_profile": "Utility to synchronize Zotero libraries with Semantic Scholar",
      "detailed_description": "A Python tool that connects a local Zotero library to Semantic Scholar. It facilitates the creation of citation alerts and helps manage literature discovery by bridging personal bibliography management with external citation databases.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "literature_discovery",
        "citation_alert"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/davidAlgis/zotero2SemanticScholar",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "zotero",
        "semantic-scholar",
        "automation"
      ],
      "id": 868
    },
    {
      "name": "paper-reviewer",
      "one_line_profile": "Automated pipeline for generating paper reviews and summaries using LLMs",
      "detailed_description": "A tool that leverages Large Language Models to generate comprehensive reviews and blog posts from arXiv papers. It powers the HuggingFace Daily Papers platform, automating the synthesis and dissemination of scientific literature.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "literature_summarization",
        "automated_review"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/deep-diver/paper-reviewer",
      "help_website": [
        "https://huggingface.co/papers"
      ],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "arxiv",
        "summarization",
        "science-communication"
      ],
      "id": 869
    },
    {
      "name": "LLM-SR",
      "one_line_profile": "LLM-based solver for scientific equation discovery and symbolic regression",
      "detailed_description": "The official implementation of LLM-SR, a method utilizing Large Language Models for Scientific Equation Discovery and Symbolic Regression. It serves as a solver to infer mathematical laws and equations from scientific data.",
      "domains": [
        "Sci Knowledge",
        "Physics"
      ],
      "subtask_category": [
        "symbolic_regression",
        "equation_discovery"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/deep-symbolic-mathematics/LLM-SR",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "symbolic-regression",
        "llm",
        "scientific-discovery"
      ],
      "id": 870
    },
    {
      "name": "SciAssess",
      "one_line_profile": "Benchmark for evaluating LLMs on scientific literature analysis",
      "detailed_description": "A comprehensive benchmark suite designed to evaluate the proficiency of Large Language Models in analyzing scientific literature. It covers tasks such as memorization, comprehension, and analysis across various scientific fields.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "model_evaluation",
        "literature_analysis"
      ],
      "application_level": "dataset",
      "primary_language": "Python",
      "repo_url": "https://github.com/deepmodeling/SciAssess",
      "help_website": [],
      "license": "LGPL-3.0",
      "tags": [
        "benchmark",
        "llm",
        "scientific-literature"
      ],
      "id": 871
    },
    {
      "name": "daily-arXiv-ai-enhanced",
      "one_line_profile": "Automated workflow to crawl, summarize, and visualize daily arXiv papers",
      "detailed_description": "An automated tool that crawls arXiv for new papers daily, uses AI to generate summaries, and publishes the results to a website. It aids researchers in keeping up with the latest literature through AI-enhanced curation.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "literature_monitoring",
        "automated_summarization"
      ],
      "application_level": "workflow",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/dw-dengwei/daily-arXiv-ai-enhanced",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "arxiv",
        "automation",
        "ai-summary"
      ],
      "id": 872
    },
    {
      "name": "etudier",
      "one_line_profile": "Command-line tool to extract citation networks from Google Scholar",
      "detailed_description": "A command-line utility that scrapes Google Scholar to extract citation networks for a given set of papers or authors. It outputs data in formats suitable for network analysis and visualization.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "citation_network_extraction",
        "web_scraping"
      ],
      "application_level": "solver",
      "primary_language": "HTML",
      "repo_url": "https://github.com/edsu/etudier",
      "help_website": [],
      "license": null,
      "tags": [
        "google-scholar",
        "citation-network",
        "scraping"
      ],
      "id": 873
    },
    {
      "name": "openalexnet",
      "one_line_profile": "Python library for retrieving and analyzing OpenAlex citation networks",
      "detailed_description": "A helper library designed to process and obtain data from the OpenAlex dataset via its API. It provides functionality to generate and analyze citation and co-authorship networks from search queries.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "citation_network_analysis",
        "bibliometrics"
      ],
      "application_level": "library",
      "primary_language": "Jupyter Notebook",
      "repo_url": "https://github.com/filipinascimento/openalexnet",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "openalex",
        "bibliometrics",
        "network-analysis"
      ],
      "id": 874
    },
    {
      "name": "Valmont-F",
      "one_line_profile": "Literature Based Discovery (LBD) system implementing the Arrowsmith algorithm",
      "detailed_description": "A system for Literature Based Discovery (LBD) that re-implements the Arrowsmith algorithm to identify hidden connections between scientific concepts in literature.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "literature_discovery",
        "hypothesis_generation"
      ],
      "application_level": "solver",
      "primary_language": "CSS",
      "repo_url": "https://github.com/fogbeam/Valmont-F",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "literature-based-discovery",
        "arrowsmith",
        "text-mining"
      ],
      "id": 875
    },
    {
      "name": "Daily-Arxiv-Tracking-Automation",
      "one_line_profile": "Automated pipeline for tracking and downloading daily ArXiv papers",
      "detailed_description": "A Python-based automation tool that monitors daily ArXiv RSS feeds and automatically downloads PDFs, facilitating literature tracking for researchers.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "literature_tracking",
        "data_retrieval"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/gisbi-kim/Daily-Arxiv-Tracking-Automation",
      "help_website": [],
      "license": null,
      "tags": [
        "arxiv",
        "automation",
        "literature-tracking"
      ],
      "id": 876
    },
    {
      "name": "molminer",
      "one_line_profile": "Tool for extracting chemical compounds from scientific literature",
      "detailed_description": "A Python library and command-line tool designed to extract chemical compound names and structures from scientific text, aiding in chemical literature mining.",
      "domains": [
        "G1",
        "Sci Knowledge"
      ],
      "subtask_category": [
        "entity_extraction",
        "text_mining"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/gorgitko/molminer",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "chemistry",
        "nlp",
        "entity-extraction"
      ],
      "id": 877
    },
    {
      "name": "citation-graph-pubmed",
      "one_line_profile": "R script to generate citation graphs from PubMed data",
      "detailed_description": "A lightweight tool using R to construct and visualize citation networks specifically from PubMed bibliographic data.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "citation_analysis",
        "network_construction"
      ],
      "application_level": "solver",
      "primary_language": "R",
      "repo_url": "https://github.com/guillaumelobet/citation-graph-pubmed",
      "help_website": [],
      "license": null,
      "tags": [
        "pubmed",
        "citation-graph",
        "bibliometrics"
      ],
      "id": 878
    },
    {
      "name": "AIRA-SemanticScholar",
      "one_line_profile": "Model Context Protocol (MCP) server for Semantic Scholar",
      "detailed_description": "An integration tool implementing the Model Context Protocol to allow AI agents to query and retrieve data from the Semantic Scholar academic graph.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "literature_retrieval",
        "agent_integration"
      ],
      "application_level": "service",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/hamid-vakilzadeh/AIRA-SemanticScholar",
      "help_website": [],
      "license": null,
      "tags": [
        "mcp",
        "semantic-scholar",
        "ai-agent"
      ],
      "id": 879
    },
    {
      "name": "text_mining_tools",
      "one_line_profile": "NLP utilities for scientific literature processing",
      "detailed_description": "A collection of Python tools designed to facilitate natural language processing tasks specifically for scientific literature.",
      "domains": [
        "G1",
        "Sci Knowledge"
      ],
      "subtask_category": [
        "text_mining",
        "nlp"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/hjkgrp/text_mining_tools",
      "help_website": [],
      "license": "GPL-3.0",
      "tags": [
        "nlp",
        "text-mining",
        "scientific-literature"
      ],
      "id": 880
    },
    {
      "name": "AI-Co-Scientist",
      "one_line_profile": "AI agent framework for automated research discovery",
      "detailed_description": "An advanced research assistant platform that utilizes AI agents (CrewAI) to generate research directions and analyze scientific literature.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "hypothesis_generation",
        "literature_analysis"
      ],
      "application_level": "platform",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/iabheejit/AI-Co-Scientist",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "ai-scientist",
        "agent",
        "research-assistant"
      ],
      "id": 881
    },
    {
      "name": "ScholarlyRecommender",
      "one_line_profile": "End-to-end academic paper recommendation feed generator",
      "detailed_description": "A tool that scrapes recent academic publications and prepares a personalized feed of recommended readings using content-based filtering.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "paper_recommendation",
        "literature_monitoring"
      ],
      "application_level": "application",
      "primary_language": "Python",
      "repo_url": "https://github.com/iansnyder333/ScholarlyRecommender",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "recommender-system",
        "arxiv",
        "feed"
      ],
      "id": 882
    },
    {
      "name": "inciteful-zotero-plugin",
      "one_line_profile": "Zotero plugin for Inciteful.xyz literature discovery",
      "detailed_description": "A plugin for the Zotero reference manager that integrates Inciteful.xyz's citation graph based literature discovery features directly into the user's library.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "literature_discovery",
        "reference_management"
      ],
      "application_level": "plugin",
      "primary_language": "TypeScript",
      "repo_url": "https://github.com/inciteful-xyz/inciteful-zotero-plugin",
      "help_website": [
        "https://inciteful.xyz"
      ],
      "license": "AGPL-3.0",
      "tags": [
        "zotero",
        "citation-graph",
        "plugin"
      ],
      "id": 883
    },
    {
      "name": "infoLink",
      "one_line_profile": "Services for linking scientific literature and research datasets",
      "detailed_description": "A set of services designed to establish and manage links between scientific publications and their underlying research datasets.",
      "domains": [
        "G1",
        "Sci Knowledge"
      ],
      "subtask_category": [
        "data_linking",
        "metadata_management"
      ],
      "application_level": "service",
      "primary_language": "Java",
      "repo_url": "https://github.com/infolis/infoLink",
      "help_website": [],
      "license": "GPL-2.0",
      "tags": [
        "data-linking",
        "research-data",
        "infrastructure"
      ],
      "id": 884
    },
    {
      "name": "ChemRxnExtractor",
      "one_line_profile": "Toolkit for chemical reaction extraction from literature",
      "detailed_description": "A toolkit designed to automatically extract chemical reaction information from scientific literature, supporting data mining in chemistry.",
      "domains": [
        "G1",
        "Sci Knowledge"
      ],
      "subtask_category": [
        "reaction_extraction",
        "text_mining"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/jiangfeng1124/ChemRxnExtractor",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "chemistry",
        "reaction-extraction",
        "nlp"
      ],
      "id": 885
    },
    {
      "name": "Kosmos",
      "one_line_profile": "AI Scientist agent for autonomous discovery",
      "detailed_description": "An implementation of an AI Scientist agent capable of autonomous discovery, designed to be driven by LLM APIs for literature analysis and hypothesis generation.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "autonomous_discovery",
        "agent"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/jimmc414/Kosmos",
      "help_website": [],
      "license": null,
      "tags": [
        "ai-scientist",
        "autonomous-discovery",
        "llm"
      ],
      "id": 886
    },
    {
      "name": "ges",
      "one_line_profile": "Implementation of Greedy Equivalence Search (GES) for causal discovery",
      "detailed_description": "A Python implementation of the GES algorithm, a standard method for causal discovery (structure learning) from data, widely used in scientific modeling.",
      "domains": [
        "G1",
        "Sci Knowledge"
      ],
      "subtask_category": [
        "causal_discovery",
        "structure_learning"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/juangamella/ges",
      "help_website": [],
      "license": "BSD-3-Clause",
      "tags": [
        "causal-inference",
        "causal-discovery",
        "ges"
      ],
      "id": 887
    },
    {
      "name": "ChatDailyPapers",
      "one_line_profile": "Automated daily academic paper subscription and summarization pipeline",
      "detailed_description": "A pipeline that automatically fetches daily ArXiv papers based on keywords and generates summaries using ChatGPT, deployed via GitHub Actions.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "literature_tracking",
        "summarization"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/justchenhao/ChatDailyPapers",
      "help_website": [],
      "license": null,
      "tags": [
        "arxiv",
        "chatgpt",
        "summarization"
      ],
      "id": 888
    },
    {
      "name": "Cogito",
      "one_line_profile": "Computational framework for literature analysis via LLMs",
      "detailed_description": "A framework employing synthesis and critique systems to autonomously accelerate expert research on scientific literature using LLMs and ArXiv data.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "literature_analysis",
        "synthesis"
      ],
      "application_level": "framework",
      "primary_language": "Python",
      "repo_url": "https://github.com/justinlietz93/Cogito",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "llm",
        "research-assistant",
        "arxiv"
      ],
      "id": 889
    },
    {
      "name": "arxiv-sanity-lite",
      "one_line_profile": "Lightweight ArXiv paper recommendation and tagging system",
      "detailed_description": "A web-based tool for tagging ArXiv papers and receiving recommendations based on TF-IDF feature vectors of abstracts, designed to help researchers manage literature.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "paper_recommendation",
        "literature_management"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/karpathy/arxiv-sanity-lite",
      "help_website": [
        "https://arxiv-sanity-lite.com"
      ],
      "license": "MIT",
      "tags": [
        "arxiv",
        "recommender-system",
        "web-app"
      ],
      "id": 890
    },
    {
      "name": "arxiv-sanity-preserver",
      "one_line_profile": "Web interface for browsing and filtering ArXiv submissions",
      "detailed_description": "The original ArXiv Sanity web interface for browsing, searching, and filtering recent ArXiv submissions to facilitate research discovery.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "literature_discovery",
        "browsing"
      ],
      "application_level": "platform",
      "primary_language": "Python",
      "repo_url": "https://github.com/karpathy/arxiv-sanity-preserver",
      "help_website": [
        "http://www.arxiv-sanity.com"
      ],
      "license": "MIT",
      "tags": [
        "arxiv",
        "literature-discovery",
        "web-app"
      ],
      "id": 891
    },
    {
      "name": "researchpooler",
      "one_line_profile": "Automated research publication discovery and analysis tool",
      "detailed_description": "A tool designed to automate the discovery of similar papers and analysis of research publications, aiming to reimagine the literature review process.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "literature_review",
        "similarity_search"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/karpathy/researchpooler",
      "help_website": [],
      "license": null,
      "tags": [
        "literature-review",
        "automation",
        "discovery"
      ],
      "id": 892
    },
    {
      "name": "cli-arxiv",
      "one_line_profile": "Command-line interface for exploring and downloading arXiv papers",
      "detailed_description": "A CLI tool inspired by ArXiv Sanity Preserver that allows users to search, explore, and download research papers from arXiv directly from the terminal.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "literature_retrieval",
        "paper_discovery"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/knguyenanhoa/cli-arxiv",
      "help_website": [],
      "license": "GPL-2.0",
      "tags": [
        "arxiv",
        "cli",
        "literature-search"
      ],
      "id": 893
    },
    {
      "name": "daily-arxiv-noti",
      "one_line_profile": "Automated daily arXiv notification system based on keywords",
      "detailed_description": "A workflow tool that sends daily notifications about new arXiv papers matching user-defined keywords, helping researchers stay updated with specific domains.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "literature_monitoring",
        "alerting_system"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/kobiso/daily-arxiv-noti",
      "help_website": [],
      "license": null,
      "tags": [
        "arxiv",
        "notification",
        "automation"
      ],
      "id": 894
    },
    {
      "name": "get-daily-arxiv-noti",
      "one_line_profile": "Script to fetch daily arXiv notifications for specific keywords",
      "detailed_description": "A Python-based utility to retrieve and filter daily arXiv submissions based on pre-defined keywords, facilitating literature tracking.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "literature_monitoring",
        "paper_discovery"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/kobiso/get-daily-arxiv-noti",
      "help_website": [],
      "license": null,
      "tags": [
        "arxiv",
        "literature-tracking"
      ],
      "id": 895
    },
    {
      "name": "LISC",
      "one_line_profile": "Literature Scanner for automated collection and analysis of scientific literature",
      "detailed_description": "A Python library designed to collect and analyze scientific literature from databases like PubMed and OpenCitations, enabling automated meta-analysis and term co-occurrence studies.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "literature_mining",
        "meta_analysis"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/lisc-tools/lisc",
      "help_website": [
        "https://lisc-tools.github.io/lisc/"
      ],
      "license": "Apache-2.0",
      "tags": [
        "literature-mining",
        "pubmed",
        "bibliometrics"
      ],
      "id": 896
    },
    {
      "name": "TTS-arxiv-daily",
      "one_line_profile": "Automated daily updates for Text-to-Speech research papers",
      "detailed_description": "A GitHub Action-based workflow tool that automatically tracks, filters, and updates a repository with the latest Text-to-Speech papers from arXiv.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "literature_monitoring",
        "paper_discovery"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/liutaocode/TTS-arxiv-daily",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "arxiv",
        "tts",
        "automation"
      ],
      "id": 897
    },
    {
      "name": "nlp-arxiv-daily",
      "one_line_profile": "Automated daily updates for NLP research papers",
      "detailed_description": "A GitHub Action-based workflow that automatically updates a repository with the latest Natural Language Processing papers from arXiv, serving as a literature tracking tool.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "literature_monitoring",
        "paper_discovery"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/monologg/nlp-arxiv-daily",
      "help_website": [],
      "license": null,
      "tags": [
        "nlp",
        "arxiv",
        "automation"
      ],
      "id": 898
    },
    {
      "name": "arxiv_daily_aigc",
      "one_line_profile": "AI-driven daily arXiv paper crawler, analyzer, and organizer focusing on AIGC",
      "detailed_description": "An automated tool that crawls arXiv for new papers in the AIGC domain, utilizes AI to analyze and summarize them, and organizes the results into daily reports for researchers.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "literature_tracking",
        "summarization"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/onion-liu/arxiv_daily_aigc",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "arxiv",
        "crawler",
        "llm",
        "literature-discovery"
      ],
      "id": 899
    },
    {
      "name": "DRIFT",
      "one_line_profile": "Tool for diachronic analysis of scientific literature to track evolution of ideas",
      "detailed_description": "A tool designed for the diachronic analysis of scientific literature, enabling researchers to track how scientific concepts and terminology evolve over time.",
      "domains": [
        "G1"
      ],
      "subtask_category": [
        "trend_analysis",
        "bibliometrics"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/rajaswa/DRIFT",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "bibliometrics",
        "text-analysis",
        "science-of-science"
      ],
      "id": 900
    },
    {
      "name": "sciencefair",
      "one_line_profile": "Desktop application for discovering, collecting, and reading scientific literature (P2P)",
      "detailed_description": "A desktop application that provides a peer-to-peer platform for discovering, collecting, and reading scientific papers, aiming to make access to scientific knowledge more open and user-friendly.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "literature_management",
        "discovery"
      ],
      "application_level": "platform",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/sciencefair-land/sciencefair",
      "help_website": [
        "http://sciencefair-land.github.io/"
      ],
      "license": "MIT",
      "tags": [
        "p2p",
        "literature-manager",
        "open-science"
      ],
      "id": 901
    },
    {
      "name": "scirate",
      "one_line_profile": "Open source platform for arXiv preprint peer review and discovery",
      "detailed_description": "The source code for Scirate, an open platform that allows researchers to follow arXiv categories, read preprints, and participate in open peer review through voting and commenting.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "peer_review",
        "literature_discovery"
      ],
      "application_level": "platform",
      "primary_language": "Ruby",
      "repo_url": "https://github.com/scirate/scirate",
      "help_website": [
        "https://scirate.com"
      ],
      "license": "MIT",
      "tags": [
        "arxiv",
        "peer-review",
        "open-access"
      ],
      "id": 902
    },
    {
      "name": "LitLLM",
      "one_line_profile": "Toolkit for scientific literature review using Large Language Models",
      "detailed_description": "A toolkit designed to assist in scientific literature reviews by leveraging Large Language Models (LLMs) to summarize papers, extract key information, and synthesize findings.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "literature_review",
        "summarization"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/shubhamagarwal92/LitLLM",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "llm",
        "literature-review",
        "automation"
      ],
      "id": 903
    },
    {
      "name": "gpt_paper_assistant",
      "one_line_profile": "GPT-4 based personalized ArXiv paper assistant bot",
      "detailed_description": "A tool that utilizes GPT-4 to assist researchers in discovering, filtering, and summarizing ArXiv papers based on personalized research interests.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "literature_discovery",
        "summarization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/tatsu-lab/gpt_paper_assistant",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "arxiv",
        "gpt-4",
        "literature-review",
        "automation"
      ],
      "id": 904
    },
    {
      "name": "Summarxiv",
      "one_line_profile": "Daily latest arXiv paper summary digest generator",
      "detailed_description": "A Python tool that generates daily summaries of the latest ArXiv papers using AI models to help researchers efficiently monitor new literature.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "summarization",
        "literature_monitoring"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/theeluwin/Summarxiv",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "arxiv",
        "summarization",
        "nlp",
        "digest"
      ],
      "id": 905
    },
    {
      "name": "PaperMemory",
      "one_line_profile": "Browser-based research paper management and discovery tool",
      "detailed_description": "A browser extension and tool that acts as a reference manager, providing automatic paper detection, venue matching, and code repository discovery for researchers.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "reference_management",
        "literature_discovery"
      ],
      "application_level": "solver",
      "primary_language": "JavaScript",
      "repo_url": "https://github.com/vict0rsch/PaperMemory",
      "help_website": [
        "https://papermemory.org"
      ],
      "license": "MIT",
      "tags": [
        "reference-manager",
        "arxiv",
        "productivity",
        "browser-extension"
      ],
      "id": 906
    },
    {
      "name": "arxiv-sh",
      "one_line_profile": "CLI tool for querying new arXiv articles via RSS",
      "detailed_description": "A shell script that allows users to query and filter new arXiv articles in selected topics directly from the command line interface.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "literature_monitoring"
      ],
      "application_level": "solver",
      "primary_language": "Shell",
      "repo_url": "https://github.com/victoriastuart/arxiv-sh",
      "help_website": [],
      "license": null,
      "tags": [
        "arxiv",
        "cli",
        "rss",
        "automation"
      ],
      "id": 907
    },
    {
      "name": "arxiv_paper_downloader",
      "one_line_profile": "Automated ArXiv paper downloader and manager",
      "detailed_description": "A Python utility to automatically download daily papers from ArXiv and manage them with markdown previews for efficient reading.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "data_acquisition",
        "literature_management"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/wbbeyourself/arxiv_paper_downloader",
      "help_website": [],
      "license": null,
      "tags": [
        "arxiv",
        "downloader",
        "automation",
        "markdown"
      ],
      "id": 908
    },
    {
      "name": "ArxivDailyOverview",
      "one_line_profile": "Tool to extract and crop key information from ArXiv papers",
      "detailed_description": "A utility that automatically downloads ArXiv papers and crops key figures or information to generate a daily overview for rapid literature scanning.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "data_processing",
        "visualization"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/wmpscc/ArxivDailyOverview",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "arxiv",
        "image-processing",
        "automation",
        "literature-review"
      ],
      "id": 909
    },
    {
      "name": "SciArena",
      "one_line_profile": "Open evaluation platform for foundation models in scientific literature tasks",
      "detailed_description": "A benchmarking and evaluation platform designed to assess the performance of foundation models on various scientific literature understanding tasks, facilitating standardized comparisons.",
      "domains": [
        "G1",
        "Sci Knowledge"
      ],
      "subtask_category": [
        "model_evaluation",
        "benchmark"
      ],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "https://github.com/yale-nlp/SciArena",
      "help_website": [],
      "license": null,
      "tags": [
        "evaluation",
        "benchmark",
        "scientific-literature",
        "llm"
      ],
      "id": 910
    },
    {
      "name": "naimai",
      "one_line_profile": "Python package to assist with scientific literature research",
      "detailed_description": "A utility package designed to streamline scientific literature research processes, likely providing functions for retrieval or management of bibliographic data.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "literature_search",
        "retrieval"
      ],
      "application_level": "library",
      "primary_language": "Python",
      "repo_url": "https://github.com/yassinekdi/naimai",
      "help_website": [],
      "license": "NOASSERTION",
      "tags": [
        "literature-research",
        "utility",
        "python"
      ],
      "id": 911
    },
    {
      "name": "arxiv-daily",
      "one_line_profile": "Automated workflow for daily arXiv paper updates",
      "detailed_description": "A GitHub Actions workflow tool that automatically fetches and updates daily arXiv papers for specific fields, generating reports to assist researchers in tracking new literature.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "literature_monitoring",
        "discovery"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/yyyanbj/arxiv-daily",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "arxiv",
        "automation",
        "literature-tracking",
        "github-actions"
      ],
      "id": 912
    },
    {
      "name": "Paper-Daily-Notice",
      "one_line_profile": "Personalized daily arXiv paper notification tool",
      "detailed_description": "A tool designed to fetch and notify users of new arXiv papers based on user-defined interests, facilitating personalized scientific literature discovery.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "literature_monitoring",
        "discovery"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/zhuhu00/Paper-Daily-Notice",
      "help_website": [],
      "license": null,
      "tags": [
        "arxiv",
        "notification",
        "personalization"
      ],
      "id": 913
    },
    {
      "name": "daily_arxiv",
      "one_line_profile": "Automated collection of daily arXiv papers with code",
      "detailed_description": "A GitHub Action-based tool that collects daily arXiv paper lists, specifically highlighting those with publicly available source code, to streamline literature and code discovery.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "literature_monitoring",
        "discovery"
      ],
      "application_level": "workflow",
      "primary_language": "Python",
      "repo_url": "https://github.com/zhuwenxing/daily_arxiv",
      "help_website": [],
      "license": "Apache-2.0",
      "tags": [
        "arxiv",
        "papers-with-code",
        "automation",
        "github-actions"
      ],
      "id": 914
    },
    {
      "name": "semantic-scholar-fastmcp-mcp-server",
      "one_line_profile": "FastMCP server for Semantic Scholar API access",
      "detailed_description": "A server implementation using the Model Context Protocol (MCP) to provide LLM agents and tools with structured access to the Semantic Scholar API for retrieving academic paper data and citation networks.",
      "domains": [
        "G1",
        "G1-06"
      ],
      "subtask_category": [
        "data_access",
        "api_connector"
      ],
      "application_level": "service",
      "primary_language": "Python",
      "repo_url": "https://github.com/zongmin-yu/semantic-scholar-fastmcp-mcp-server",
      "help_website": [],
      "license": "MIT",
      "tags": [
        "semantic-scholar",
        "mcp",
        "api",
        "llm-tool"
      ],
      "id": 915
    }
  ]
}